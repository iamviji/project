{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arch1:Arch2:Arch3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamviji/project/blob/master/PostMidTerm/Experiment/Tensorflow_OneHot_1_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tYUch9S4_RA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a873dbd2-c5f1-4c74-e140-9293fcd05d59"
      },
      "source": [
        "import numpy \n",
        "import time\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior ()\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bmlgeG86RFf"
      },
      "source": [
        "SNR_BEGIN = 0\n",
        "SNR_END = 10\n",
        "SNR_STEP_SIZE = 0.5\n",
        "NUM_OF_INPUT_MESSAGE = 1000\n",
        "\n",
        "bler_per_iter_uncoded_commpy_psk_2= [0.521, 0.473, 0.436, 0.37,  0.304, 0.259, 0.187, 0.138, 0.098, 0.098, 0.052, 0.028, 0.012, 0.011, 0.009, 0.002, 0.0,  0.001, 0.,    0.0]\n",
        "bler_per_iter_uncoded_itpp_psk_2= [0.518, 0.478, 0.415, 0.355, 0.305, 0.227, 0.177, 0.149, 0.11,  0.075, 0.055, 0.023, 0.014, 0.014, 0.015, 0.001, 0.003, 0.001, 0.,    0. ]\n",
        "bler_per_iter_uncoded_commpy_psk_4 = [0.815, 0.793, 0.75,  0.714, 0.64,  0.639, 0.526, 0.49,  0.433, 0.371, 0.335, 0.236, 0.204, 0.154, 0.129, 0.08,  0.063, 0.046, 0.023, 0.018]\n",
        "bler_per_iter_uncoded_itpp_psk_4 = [0.814, 0.767, 0.729, 0.702, 0.66,  0.616, 0.563, 0.511, 0.442, 0.4,   0.294, 0.277, 0.228, 0.17,  0.114, 0.087, 0.05,  0.037, 0.022, 0.017]\n",
        "bler_per_iter_uncoded_itpp_psk_8 = [0.921, 0.917, 0.912, 0.867, 0.86,  0.857, 0.826, 0.808, 0.77,  0.737, 0.704, 0.657, 0.608, 0.6,   0.547, 0.487, 0.426, 0.361, 0.323, 0.293]\n",
        "bler_per_iter_ldpc_itpp_psk_4 = [0.584, 0.488, 0.404, 0.332, 0.218, 0.151, 0.097, 0.058, 0.041, 0.024, 0.007, 0.004, 0.002, 0.001, 0.001, 0.,    0.,    0.,    0.,    0.,   ]\n",
        "bler_per_iter_ham_itpp_psk_4= [0.51, 0.479, 0.419, 0.333, 0.313, 0.247, 0.212, 0.132, 0.114, 0.093, 0.042, 0.027, 0.024, 0.016, 0.006, 0.005, 0.003, 0.002, 0.,    0.  ]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiHoGEs063E4"
      },
      "source": [
        "# Helper Function\n",
        "def bits2int(a, axis=-1):\n",
        "    return numpy.right_shift(numpy.packbits(a, axis=axis), 8 - a.shape[axis]).squeeze()\n",
        "\n",
        "\n",
        "def row_bits2int(arr):\n",
        "    n = arr.shape[1]  # number of columns\n",
        "    # shift the bits of the first column to the left by n - 1\n",
        "    a = arr[:, 0] << n - 1  \n",
        "\n",
        "    for j in range(1, n):\n",
        "        # \"overlay\" with the shifted bits of the next column\n",
        "        a |= arr[:, j] << n - 1 - j  \n",
        "    return a\n",
        "\n",
        "def Snr2Sigma(snr):\n",
        "  sigma = 10 ** (- snr / 20)\n",
        "  return sigma\n",
        "\n",
        "def timer_update(i,current,time_tot,tic_incr=500):\n",
        "    last = current\n",
        "    current = time.time()\n",
        "    t_diff = current-last\n",
        "    print('SNR: {:04.3f} - Iter: {} - Last {} iterations took {:03.2f}s'.format(snr,i+1,tic_incr,t_diff))\n",
        "    return time_tot + t_diff"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr5UAobM3g1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d465cc85-9359-4c19-e221-e627cd03fa6f"
      },
      "source": [
        "\n",
        "class GetOutOfLoop( Exception ):\n",
        "    pass\n",
        "\n",
        "# ARCH-2 : 1-1-3 : But One Hot Input\n",
        "\n",
        "channel_size = 1\n",
        "input_message_length = 3\n",
        "num_hidden_1 = 2 * channel_size\n",
        "print(num_hidden_1) \n",
        "print (\"input_message_length=\", input_message_length, \"channel_size\", channel_size)\n",
        "\n",
        "lr_x = tf.placeholder(dtype=tf.float32,shape=[])\n",
        "input_message_x_label = tf.placeholder(\"int32\", [None], name=\"input_message_x_label\")\n",
        "input_message_x = tf.placeholder(\"float32\", [None, 2**input_message_length], name=\"input_message_x\")\n",
        "awgn_noise_std_dev_x = tf.placeholder(\"float32\", name =\"awgn_noise_std_dev\")\n",
        "input_channel_x = tf.placeholder(\"float32\", [None, 2 * channel_size], name=\"input_channel_x\")\n",
        "\n",
        "weights = {\n",
        "  \"encoder_l1\" : tf.Variable (tf.random_uniform([2**input_message_length, 2*input_message_length], -1, 1), name=\"encoder_l1_weights\"),\n",
        "  \"encoder_l2\" : tf.Variable (tf.random_uniform([2*input_message_length, num_hidden_1], -1, 1), name=\"encoder_l2_weights\"),\n",
        "  \"decoder_l1\" : tf.Variable (tf.random_uniform([num_hidden_1, 2*input_message_length], -1, 1), name=\"decoder_l1_weights\"),\n",
        "  \"decoder_l2\" : tf.Variable (tf.random_uniform([2*input_message_length, 2**input_message_length], -1, 1), name=\"decoder_l2_weights\"),\n",
        "}\n",
        "\n",
        "biases = {\n",
        "  \"encoder_l1\" : tf.Variable (tf.random_uniform([2*input_message_length], -1,1), name=\"encoder_l1_bias\"),\n",
        "  \"encoder_l2\" : tf.Variable (tf.random_uniform([num_hidden_1], -1,1), name=\"encoder_l2_bias\"),\n",
        "  \"decoder_l1\" : tf.Variable (tf.random_uniform([2*input_message_length], -1,1), name=\"decoder_l1_bias\"),\n",
        "  \"decoder_l2\" : tf.Variable (tf.random_uniform([2**input_message_length], -1,1), name=\"decoder_l2_bias\"),\n",
        "}\n",
        "\n",
        "def dl_encoder (x):\n",
        "  layer_1 = tf.nn.tanh (tf.matmul(x, weights['encoder_l1']) + biases['encoder_l1'])\n",
        "  layer_2 = tf.nn.tanh (tf.matmul(layer_1, weights['encoder_l2']) + biases['encoder_l2'])\n",
        "  #layer_2 = tf.round(layer_1)\n",
        "  layer_3 =   tf.sqrt(tf.reduce_sum(tf.square(layer_2), axis=1))\n",
        "  layer_3 = tf.reshape (layer_3, (-1,1))\n",
        "  layer_3 = layer_2 /layer_3\n",
        "  #layer_2 =  tf.nn.relu(layer_1)\n",
        "  return layer_3\n",
        "\n",
        "def dl_decoder (x):\n",
        "  layer_1 = tf.nn.tanh (tf.matmul(x, weights['decoder_l1']) + biases['decoder_l1'])\n",
        "  #layer_2 = tf.nn.sigmoid (tf.matmul(layer_1, weights['decoder_l2']) + biases['decoder_l2'])\n",
        "  layer_2 = (tf.matmul(layer_1, weights['decoder_l2']) + biases['decoder_l2'])\n",
        "  return layer_2\n",
        "\n",
        "def awgn_layer(x):\n",
        "  awgn_noise = tf.random.normal(tf.shape(x), stddev=awgn_noise_std_dev_x,  name=\"awgn_noise\")\n",
        "  awgn_channel_output = tf.add(x, awgn_noise, name =\"x_and_noise\")\n",
        "  return awgn_channel_output\n",
        "\n",
        "\n",
        "dl_encoder_output = dl_encoder(input_message_x)\n",
        "dl_decoder_input = awgn_layer(dl_encoder_output)\n",
        "#awgn_noise = tf.random.normal(tf.shape(dl_encoder_output), stddev=awgn_noise_std_dev,  name=\"awgn_noise\")\n",
        "#dl_decoder_input = tf.add(dl_encoder_output, awgn_noise, name =\"x_and_noise\")\n",
        "dl_decoder_output = dl_decoder (dl_decoder_input)\n",
        "dl_decoder_only_output = dl_decoder(input_channel_x)\n",
        "\n",
        "\n",
        "#loss1 = tf.reduce_mean (-1 * (input_message_x*tf.log(dl_decoder_output) + (1 - input_message_x)*tf.log(1 - dl_decoder_output) ))\n",
        "loss = tf.losses.sparse_softmax_cross_entropy(labels=input_message_x_label,logits=dl_decoder_output)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr_x).minimize (loss)\n",
        "\n",
        "\n",
        "\n",
        "awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [2*channel_size])\n",
        "awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "input_message_length= 3 channel_size 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFblWNtR5Kgx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b9293d-cf1f-4c6a-84c9-275680f7ce0c"
      },
      "source": [
        "training_input_message = numpy.random.randint(2**input_message_length, size=(1,NUM_OF_INPUT_MESSAGE*10))\n",
        "training_input_message_one_hot = numpy.zeros((training_input_message.size, 2**input_message_length))\n",
        "training_input_message_one_hot[numpy.arange(training_input_message.size),training_input_message] = 1\n",
        "print(training_input_message_one_hot)\n",
        "print (training_input_message_one_hot.shape)\n",
        "print (training_input_message.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(10000, 8)\n",
            "(1, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozia6LZP5M30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "7354f633-1b54-44b0-baca-7a09980a3316"
      },
      "source": [
        "#input_message_length=8\n",
        "#training_input_message = numpy.random.randint(2, size=(NUM_OF_INPUT_MESSAGE*10,input_message_length))\n",
        "#training_input_message_decimal = row_bits2int(training_input_message)\n",
        "#uniqueValues, indicesList = numpy.unique(training_input_message_decimal, return_index=True)\n",
        "x_axis = numpy.arange(0,2**input_message_length)\n",
        "#y_axis = row_bits2int(training_input_message)\n",
        "#x=numpy.histogram(training_input_message_decimal,bins = x_axis) \n",
        "plt.hist(training_input_message[0][:-1], bins = x_axis[:-1]) \n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO04bvMo5P5W"
      },
      "source": [
        "x_axis = numpy.arange(0,NUM_OF_INPUT_MESSAGE*10)\n",
        "plt.figure(figsize=(80, 8))\n",
        "plt.plot(x_axis[:-1],training_input_message[0][:-1], '-ok')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCtCWKYP5S8h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "632f0639-e557-42fa-ba82-8bfdf324e51d"
      },
      "source": [
        "batch_size = 500\n",
        "\n",
        "# Training\n",
        "train_init = tf.global_variables_initializer ()\n",
        "train_sess = tf.Session ()\n",
        "\n",
        "epochs = 10\n",
        "outer_ephocs = 1\n",
        "display_step = 2\n",
        "num_of_batches = training_input_message.shape[1] / batch_size\n",
        "print (\"batch_size:\", batch_size, \"num_of_batcches:\", num_of_batches)\n",
        "train_sess.run(train_init)\n",
        "l = 0\n",
        "lrate = 0.1\n",
        "i = 0\n",
        "snr_min = 9.5\n",
        "snr_max = 10.5\n",
        "snr_step_size = 0.5\n",
        "max_iteration = epochs * num_of_batches * (snr_max - snr_min) / snr_step_size\n",
        "print (\"max iteration :\",max_iteration,\"num_of_batches:\", num_of_batches)\n",
        "try:\n",
        "  for oe in range(outer_ephocs):\n",
        "    for snr in (numpy.arange (0, 10, SNR_STEP_SIZE)):\n",
        "    #for snr in (numpy.arange (snr_min, snr_max, SNR_STEP_SIZE)):\n",
        "      sigma = 1.0*Snr2Sigma (snr)\n",
        "      print (\"Training for SNR=\", snr, \" sigma=\", sigma, \"iteratin:\", oe) \n",
        "      for e in range(epochs):\n",
        "        for j in range (int(num_of_batches)):\n",
        "          i = i + 1\n",
        "          x_train_batch_one_hot = training_input_message_one_hot [j*batch_size:(j+1)*batch_size]\n",
        "          x_train_batch_one_hot = x_train_batch_one_hot.astype(\"float32\")\n",
        "          x_train_batch_label = training_input_message.reshape(training_input_message.shape[1]) [j*batch_size:(j+1)*batch_size]        \n",
        "          if (i < 100): \n",
        "            lr = 0.1\n",
        "          elif(i < 200):\n",
        "            lr = 0.01\n",
        "          else:\n",
        "            lr = 0.001 \n",
        "          _, l = train_sess.run ([optimizer, loss], feed_dict={input_message_x:x_train_batch_one_hot, awgn_noise_std_dev_x:sigma, lr_x:lr, input_message_x_label:x_train_batch_label.astype(\"int32\")})\n",
        "          if i % display_step == 0:          \n",
        "            print('Step %i: Minibatch Loss: %f' % (i, l ))\n",
        "          if (l < 0.05 and snr >= 9): \n",
        "            print (\"Loss=\", l)\n",
        "            raise GetOutOfLoop\n",
        "except GetOutOfLoop:\n",
        "  print(\"Early Stop\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch_size: 500 num_of_batcches: 20.0\n",
            "max iteration : 400.0 num_of_batches: 20.0\n",
            "Training for SNR= 0.0  sigma= 1.0 iteratin: 0\n",
            "Step 2: Minibatch Loss: 2.130752\n",
            "Step 4: Minibatch Loss: 1.959637\n",
            "Step 6: Minibatch Loss: 1.909321\n",
            "Step 8: Minibatch Loss: 1.882898\n",
            "Step 10: Minibatch Loss: 1.846185\n",
            "Step 12: Minibatch Loss: 1.789485\n",
            "Step 14: Minibatch Loss: 1.846573\n",
            "Step 16: Minibatch Loss: 1.748958\n",
            "Step 18: Minibatch Loss: 1.814124\n",
            "Step 20: Minibatch Loss: 1.725021\n",
            "Step 22: Minibatch Loss: 1.782996\n",
            "Step 24: Minibatch Loss: 1.734907\n",
            "Step 26: Minibatch Loss: 1.766773\n",
            "Step 28: Minibatch Loss: 1.718674\n",
            "Step 30: Minibatch Loss: 1.694403\n",
            "Step 32: Minibatch Loss: 1.723493\n",
            "Step 34: Minibatch Loss: 1.674984\n",
            "Step 36: Minibatch Loss: 1.663247\n",
            "Step 38: Minibatch Loss: 1.700203\n",
            "Step 40: Minibatch Loss: 1.697320\n",
            "Step 42: Minibatch Loss: 1.668983\n",
            "Step 44: Minibatch Loss: 1.711033\n",
            "Step 46: Minibatch Loss: 1.707706\n",
            "Step 48: Minibatch Loss: 1.730309\n",
            "Step 50: Minibatch Loss: 1.627536\n",
            "Step 52: Minibatch Loss: 1.681011\n",
            "Step 54: Minibatch Loss: 1.723637\n",
            "Step 56: Minibatch Loss: 1.676486\n",
            "Step 58: Minibatch Loss: 1.656266\n",
            "Step 60: Minibatch Loss: 1.670299\n",
            "Step 62: Minibatch Loss: 1.728173\n",
            "Step 64: Minibatch Loss: 1.671491\n",
            "Step 66: Minibatch Loss: 1.670015\n",
            "Step 68: Minibatch Loss: 1.650566\n",
            "Step 70: Minibatch Loss: 1.682245\n",
            "Step 72: Minibatch Loss: 1.749180\n",
            "Step 74: Minibatch Loss: 1.625163\n",
            "Step 76: Minibatch Loss: 1.658600\n",
            "Step 78: Minibatch Loss: 1.706122\n",
            "Step 80: Minibatch Loss: 1.708483\n",
            "Step 82: Minibatch Loss: 1.722942\n",
            "Step 84: Minibatch Loss: 1.657617\n",
            "Step 86: Minibatch Loss: 1.680965\n",
            "Step 88: Minibatch Loss: 1.636474\n",
            "Step 90: Minibatch Loss: 1.619815\n",
            "Step 92: Minibatch Loss: 1.726580\n",
            "Step 94: Minibatch Loss: 1.680973\n",
            "Step 96: Minibatch Loss: 1.670331\n",
            "Step 98: Minibatch Loss: 1.666825\n",
            "Step 100: Minibatch Loss: 1.687180\n",
            "Step 102: Minibatch Loss: 1.719745\n",
            "Step 104: Minibatch Loss: 1.683740\n",
            "Step 106: Minibatch Loss: 1.693924\n",
            "Step 108: Minibatch Loss: 1.653771\n",
            "Step 110: Minibatch Loss: 1.670177\n",
            "Step 112: Minibatch Loss: 1.637961\n",
            "Step 114: Minibatch Loss: 1.708267\n",
            "Step 116: Minibatch Loss: 1.689808\n",
            "Step 118: Minibatch Loss: 1.681437\n",
            "Step 120: Minibatch Loss: 1.701450\n",
            "Step 122: Minibatch Loss: 1.696721\n",
            "Step 124: Minibatch Loss: 1.656765\n",
            "Step 126: Minibatch Loss: 1.675133\n",
            "Step 128: Minibatch Loss: 1.672351\n",
            "Step 130: Minibatch Loss: 1.667929\n",
            "Step 132: Minibatch Loss: 1.658156\n",
            "Step 134: Minibatch Loss: 1.700354\n",
            "Step 136: Minibatch Loss: 1.668147\n",
            "Step 138: Minibatch Loss: 1.641732\n",
            "Step 140: Minibatch Loss: 1.663610\n",
            "Step 142: Minibatch Loss: 1.639568\n",
            "Step 144: Minibatch Loss: 1.684549\n",
            "Step 146: Minibatch Loss: 1.716832\n",
            "Step 148: Minibatch Loss: 1.648038\n",
            "Step 150: Minibatch Loss: 1.722365\n",
            "Step 152: Minibatch Loss: 1.690026\n",
            "Step 154: Minibatch Loss: 1.697986\n",
            "Step 156: Minibatch Loss: 1.664562\n",
            "Step 158: Minibatch Loss: 1.711606\n",
            "Step 160: Minibatch Loss: 1.661472\n",
            "Step 162: Minibatch Loss: 1.668770\n",
            "Step 164: Minibatch Loss: 1.665808\n",
            "Step 166: Minibatch Loss: 1.724521\n",
            "Step 168: Minibatch Loss: 1.773206\n",
            "Step 170: Minibatch Loss: 1.691248\n",
            "Step 172: Minibatch Loss: 1.665731\n",
            "Step 174: Minibatch Loss: 1.639965\n",
            "Step 176: Minibatch Loss: 1.697489\n",
            "Step 178: Minibatch Loss: 1.680126\n",
            "Step 180: Minibatch Loss: 1.667027\n",
            "Step 182: Minibatch Loss: 1.675579\n",
            "Step 184: Minibatch Loss: 1.677239\n",
            "Step 186: Minibatch Loss: 1.692230\n",
            "Step 188: Minibatch Loss: 1.686519\n",
            "Step 190: Minibatch Loss: 1.686297\n",
            "Step 192: Minibatch Loss: 1.681567\n",
            "Step 194: Minibatch Loss: 1.725595\n",
            "Step 196: Minibatch Loss: 1.690705\n",
            "Step 198: Minibatch Loss: 1.629685\n",
            "Step 200: Minibatch Loss: 1.768452\n",
            "Training for SNR= 0.5  sigma= 0.9440608762859234 iteratin: 0\n",
            "Step 202: Minibatch Loss: 1.635209\n",
            "Step 204: Minibatch Loss: 1.665872\n",
            "Step 206: Minibatch Loss: 1.660192\n",
            "Step 208: Minibatch Loss: 1.687297\n",
            "Step 210: Minibatch Loss: 1.653130\n",
            "Step 212: Minibatch Loss: 1.649950\n",
            "Step 214: Minibatch Loss: 1.671569\n",
            "Step 216: Minibatch Loss: 1.625486\n",
            "Step 218: Minibatch Loss: 1.610379\n",
            "Step 220: Minibatch Loss: 1.648862\n",
            "Step 222: Minibatch Loss: 1.619026\n",
            "Step 224: Minibatch Loss: 1.638631\n",
            "Step 226: Minibatch Loss: 1.623992\n",
            "Step 228: Minibatch Loss: 1.646760\n",
            "Step 230: Minibatch Loss: 1.667373\n",
            "Step 232: Minibatch Loss: 1.628730\n",
            "Step 234: Minibatch Loss: 1.635047\n",
            "Step 236: Minibatch Loss: 1.595515\n",
            "Step 238: Minibatch Loss: 1.644790\n",
            "Step 240: Minibatch Loss: 1.686674\n",
            "Step 242: Minibatch Loss: 1.660954\n",
            "Step 244: Minibatch Loss: 1.653382\n",
            "Step 246: Minibatch Loss: 1.617379\n",
            "Step 248: Minibatch Loss: 1.564043\n",
            "Step 250: Minibatch Loss: 1.674585\n",
            "Step 252: Minibatch Loss: 1.615063\n",
            "Step 254: Minibatch Loss: 1.623239\n",
            "Step 256: Minibatch Loss: 1.664457\n",
            "Step 258: Minibatch Loss: 1.671063\n",
            "Step 260: Minibatch Loss: 1.623152\n",
            "Step 262: Minibatch Loss: 1.608150\n",
            "Step 264: Minibatch Loss: 1.663284\n",
            "Step 266: Minibatch Loss: 1.646858\n",
            "Step 268: Minibatch Loss: 1.609765\n",
            "Step 270: Minibatch Loss: 1.666457\n",
            "Step 272: Minibatch Loss: 1.696298\n",
            "Step 274: Minibatch Loss: 1.555051\n",
            "Step 276: Minibatch Loss: 1.674666\n",
            "Step 278: Minibatch Loss: 1.669762\n",
            "Step 280: Minibatch Loss: 1.595731\n",
            "Step 282: Minibatch Loss: 1.591591\n",
            "Step 284: Minibatch Loss: 1.650543\n",
            "Step 286: Minibatch Loss: 1.634206\n",
            "Step 288: Minibatch Loss: 1.629375\n",
            "Step 290: Minibatch Loss: 1.709929\n",
            "Step 292: Minibatch Loss: 1.646231\n",
            "Step 294: Minibatch Loss: 1.610163\n",
            "Step 296: Minibatch Loss: 1.702414\n",
            "Step 298: Minibatch Loss: 1.640174\n",
            "Step 300: Minibatch Loss: 1.685364\n",
            "Step 302: Minibatch Loss: 1.592425\n",
            "Step 304: Minibatch Loss: 1.702570\n",
            "Step 306: Minibatch Loss: 1.652610\n",
            "Step 308: Minibatch Loss: 1.639515\n",
            "Step 310: Minibatch Loss: 1.643073\n",
            "Step 312: Minibatch Loss: 1.616872\n",
            "Step 314: Minibatch Loss: 1.656127\n",
            "Step 316: Minibatch Loss: 1.641956\n",
            "Step 318: Minibatch Loss: 1.639168\n",
            "Step 320: Minibatch Loss: 1.639133\n",
            "Step 322: Minibatch Loss: 1.662294\n",
            "Step 324: Minibatch Loss: 1.647907\n",
            "Step 326: Minibatch Loss: 1.680454\n",
            "Step 328: Minibatch Loss: 1.621449\n",
            "Step 330: Minibatch Loss: 1.635462\n",
            "Step 332: Minibatch Loss: 1.669507\n",
            "Step 334: Minibatch Loss: 1.633306\n",
            "Step 336: Minibatch Loss: 1.640475\n",
            "Step 338: Minibatch Loss: 1.621165\n",
            "Step 340: Minibatch Loss: 1.649545\n",
            "Step 342: Minibatch Loss: 1.641951\n",
            "Step 344: Minibatch Loss: 1.612873\n",
            "Step 346: Minibatch Loss: 1.611875\n",
            "Step 348: Minibatch Loss: 1.641600\n",
            "Step 350: Minibatch Loss: 1.632365\n",
            "Step 352: Minibatch Loss: 1.604223\n",
            "Step 354: Minibatch Loss: 1.644956\n",
            "Step 356: Minibatch Loss: 1.688116\n",
            "Step 358: Minibatch Loss: 1.566924\n",
            "Step 360: Minibatch Loss: 1.664973\n",
            "Step 362: Minibatch Loss: 1.656750\n",
            "Step 364: Minibatch Loss: 1.654625\n",
            "Step 366: Minibatch Loss: 1.643730\n",
            "Step 368: Minibatch Loss: 1.644482\n",
            "Step 370: Minibatch Loss: 1.586337\n",
            "Step 372: Minibatch Loss: 1.599904\n",
            "Step 374: Minibatch Loss: 1.718611\n",
            "Step 376: Minibatch Loss: 1.629780\n",
            "Step 378: Minibatch Loss: 1.621964\n",
            "Step 380: Minibatch Loss: 1.591621\n",
            "Step 382: Minibatch Loss: 1.623213\n",
            "Step 384: Minibatch Loss: 1.708116\n",
            "Step 386: Minibatch Loss: 1.639839\n",
            "Step 388: Minibatch Loss: 1.631706\n",
            "Step 390: Minibatch Loss: 1.651887\n",
            "Step 392: Minibatch Loss: 1.666933\n",
            "Step 394: Minibatch Loss: 1.679977\n",
            "Step 396: Minibatch Loss: 1.645902\n",
            "Step 398: Minibatch Loss: 1.695935\n",
            "Step 400: Minibatch Loss: 1.694181\n",
            "Training for SNR= 1.0  sigma= 0.8912509381337456 iteratin: 0\n",
            "Step 402: Minibatch Loss: 1.604020\n",
            "Step 404: Minibatch Loss: 1.610098\n",
            "Step 406: Minibatch Loss: 1.579427\n",
            "Step 408: Minibatch Loss: 1.597634\n",
            "Step 410: Minibatch Loss: 1.587251\n",
            "Step 412: Minibatch Loss: 1.613402\n",
            "Step 414: Minibatch Loss: 1.659874\n",
            "Step 416: Minibatch Loss: 1.592654\n",
            "Step 418: Minibatch Loss: 1.636481\n",
            "Step 420: Minibatch Loss: 1.602786\n",
            "Step 422: Minibatch Loss: 1.665568\n",
            "Step 424: Minibatch Loss: 1.626951\n",
            "Step 426: Minibatch Loss: 1.546615\n",
            "Step 428: Minibatch Loss: 1.628826\n",
            "Step 430: Minibatch Loss: 1.589883\n",
            "Step 432: Minibatch Loss: 1.557491\n",
            "Step 434: Minibatch Loss: 1.626831\n",
            "Step 436: Minibatch Loss: 1.580494\n",
            "Step 438: Minibatch Loss: 1.612143\n",
            "Step 440: Minibatch Loss: 1.574762\n",
            "Step 442: Minibatch Loss: 1.614715\n",
            "Step 444: Minibatch Loss: 1.594952\n",
            "Step 446: Minibatch Loss: 1.652881\n",
            "Step 448: Minibatch Loss: 1.569006\n",
            "Step 450: Minibatch Loss: 1.620186\n",
            "Step 452: Minibatch Loss: 1.617346\n",
            "Step 454: Minibatch Loss: 1.614434\n",
            "Step 456: Minibatch Loss: 1.609213\n",
            "Step 458: Minibatch Loss: 1.592136\n",
            "Step 460: Minibatch Loss: 1.666401\n",
            "Step 462: Minibatch Loss: 1.650220\n",
            "Step 464: Minibatch Loss: 1.601789\n",
            "Step 466: Minibatch Loss: 1.611058\n",
            "Step 468: Minibatch Loss: 1.627341\n",
            "Step 470: Minibatch Loss: 1.609478\n",
            "Step 472: Minibatch Loss: 1.564456\n",
            "Step 474: Minibatch Loss: 1.579024\n",
            "Step 476: Minibatch Loss: 1.599442\n",
            "Step 478: Minibatch Loss: 1.604163\n",
            "Step 480: Minibatch Loss: 1.615892\n",
            "Step 482: Minibatch Loss: 1.609829\n",
            "Step 484: Minibatch Loss: 1.588704\n",
            "Step 486: Minibatch Loss: 1.574251\n",
            "Step 488: Minibatch Loss: 1.610528\n",
            "Step 490: Minibatch Loss: 1.579187\n",
            "Step 492: Minibatch Loss: 1.563776\n",
            "Step 494: Minibatch Loss: 1.609172\n",
            "Step 496: Minibatch Loss: 1.581591\n",
            "Step 498: Minibatch Loss: 1.656491\n",
            "Step 500: Minibatch Loss: 1.604325\n",
            "Step 502: Minibatch Loss: 1.640158\n",
            "Step 504: Minibatch Loss: 1.595362\n",
            "Step 506: Minibatch Loss: 1.548120\n",
            "Step 508: Minibatch Loss: 1.612628\n",
            "Step 510: Minibatch Loss: 1.600155\n",
            "Step 512: Minibatch Loss: 1.612965\n",
            "Step 514: Minibatch Loss: 1.614565\n",
            "Step 516: Minibatch Loss: 1.650026\n",
            "Step 518: Minibatch Loss: 1.584105\n",
            "Step 520: Minibatch Loss: 1.583915\n",
            "Step 522: Minibatch Loss: 1.539869\n",
            "Step 524: Minibatch Loss: 1.625090\n",
            "Step 526: Minibatch Loss: 1.597531\n",
            "Step 528: Minibatch Loss: 1.597857\n",
            "Step 530: Minibatch Loss: 1.679349\n",
            "Step 532: Minibatch Loss: 1.661710\n",
            "Step 534: Minibatch Loss: 1.623669\n",
            "Step 536: Minibatch Loss: 1.623248\n",
            "Step 538: Minibatch Loss: 1.516998\n",
            "Step 540: Minibatch Loss: 1.571468\n",
            "Step 542: Minibatch Loss: 1.634082\n",
            "Step 544: Minibatch Loss: 1.617723\n",
            "Step 546: Minibatch Loss: 1.676032\n",
            "Step 548: Minibatch Loss: 1.584774\n",
            "Step 550: Minibatch Loss: 1.596916\n",
            "Step 552: Minibatch Loss: 1.602766\n",
            "Step 554: Minibatch Loss: 1.631463\n",
            "Step 556: Minibatch Loss: 1.598722\n",
            "Step 558: Minibatch Loss: 1.585281\n",
            "Step 560: Minibatch Loss: 1.558895\n",
            "Step 562: Minibatch Loss: 1.560143\n",
            "Step 564: Minibatch Loss: 1.607894\n",
            "Step 566: Minibatch Loss: 1.611663\n",
            "Step 568: Minibatch Loss: 1.572620\n",
            "Step 570: Minibatch Loss: 1.582542\n",
            "Step 572: Minibatch Loss: 1.607163\n",
            "Step 574: Minibatch Loss: 1.576691\n",
            "Step 576: Minibatch Loss: 1.617849\n",
            "Step 578: Minibatch Loss: 1.562127\n",
            "Step 580: Minibatch Loss: 1.593324\n",
            "Step 582: Minibatch Loss: 1.594043\n",
            "Step 584: Minibatch Loss: 1.573377\n",
            "Step 586: Minibatch Loss: 1.561728\n",
            "Step 588: Minibatch Loss: 1.579235\n",
            "Step 590: Minibatch Loss: 1.582316\n",
            "Step 592: Minibatch Loss: 1.603691\n",
            "Step 594: Minibatch Loss: 1.583885\n",
            "Step 596: Minibatch Loss: 1.618066\n",
            "Step 598: Minibatch Loss: 1.564792\n",
            "Step 600: Minibatch Loss: 1.616229\n",
            "Training for SNR= 1.5  sigma= 0.8413951416451951 iteratin: 0\n",
            "Step 602: Minibatch Loss: 1.575950\n",
            "Step 604: Minibatch Loss: 1.555765\n",
            "Step 606: Minibatch Loss: 1.508308\n",
            "Step 608: Minibatch Loss: 1.516886\n",
            "Step 610: Minibatch Loss: 1.531967\n",
            "Step 612: Minibatch Loss: 1.607211\n",
            "Step 614: Minibatch Loss: 1.562313\n",
            "Step 616: Minibatch Loss: 1.574409\n",
            "Step 618: Minibatch Loss: 1.568123\n",
            "Step 620: Minibatch Loss: 1.585245\n",
            "Step 622: Minibatch Loss: 1.571259\n",
            "Step 624: Minibatch Loss: 1.545773\n",
            "Step 626: Minibatch Loss: 1.539737\n",
            "Step 628: Minibatch Loss: 1.589585\n",
            "Step 630: Minibatch Loss: 1.560290\n",
            "Step 632: Minibatch Loss: 1.564550\n",
            "Step 634: Minibatch Loss: 1.514657\n",
            "Step 636: Minibatch Loss: 1.543701\n",
            "Step 638: Minibatch Loss: 1.538838\n",
            "Step 640: Minibatch Loss: 1.551494\n",
            "Step 642: Minibatch Loss: 1.549469\n",
            "Step 644: Minibatch Loss: 1.524728\n",
            "Step 646: Minibatch Loss: 1.605352\n",
            "Step 648: Minibatch Loss: 1.552610\n",
            "Step 650: Minibatch Loss: 1.537479\n",
            "Step 652: Minibatch Loss: 1.523825\n",
            "Step 654: Minibatch Loss: 1.645204\n",
            "Step 656: Minibatch Loss: 1.575712\n",
            "Step 658: Minibatch Loss: 1.552212\n",
            "Step 660: Minibatch Loss: 1.576482\n",
            "Step 662: Minibatch Loss: 1.560434\n",
            "Step 664: Minibatch Loss: 1.521574\n",
            "Step 666: Minibatch Loss: 1.533587\n",
            "Step 668: Minibatch Loss: 1.546088\n",
            "Step 670: Minibatch Loss: 1.532983\n",
            "Step 672: Minibatch Loss: 1.617443\n",
            "Step 674: Minibatch Loss: 1.539006\n",
            "Step 676: Minibatch Loss: 1.535685\n",
            "Step 678: Minibatch Loss: 1.574349\n",
            "Step 680: Minibatch Loss: 1.526558\n",
            "Step 682: Minibatch Loss: 1.563303\n",
            "Step 684: Minibatch Loss: 1.533874\n",
            "Step 686: Minibatch Loss: 1.515387\n",
            "Step 688: Minibatch Loss: 1.551371\n",
            "Step 690: Minibatch Loss: 1.565615\n",
            "Step 692: Minibatch Loss: 1.529789\n",
            "Step 694: Minibatch Loss: 1.596041\n",
            "Step 696: Minibatch Loss: 1.542077\n",
            "Step 698: Minibatch Loss: 1.549000\n",
            "Step 700: Minibatch Loss: 1.592400\n",
            "Step 702: Minibatch Loss: 1.688688\n",
            "Step 704: Minibatch Loss: 1.557685\n",
            "Step 706: Minibatch Loss: 1.518961\n",
            "Step 708: Minibatch Loss: 1.563226\n",
            "Step 710: Minibatch Loss: 1.566113\n",
            "Step 712: Minibatch Loss: 1.591949\n",
            "Step 714: Minibatch Loss: 1.572618\n",
            "Step 716: Minibatch Loss: 1.598141\n",
            "Step 718: Minibatch Loss: 1.622765\n",
            "Step 720: Minibatch Loss: 1.519138\n",
            "Step 722: Minibatch Loss: 1.592906\n",
            "Step 724: Minibatch Loss: 1.632680\n",
            "Step 726: Minibatch Loss: 1.533722\n",
            "Step 728: Minibatch Loss: 1.580256\n",
            "Step 730: Minibatch Loss: 1.508310\n",
            "Step 732: Minibatch Loss: 1.498293\n",
            "Step 734: Minibatch Loss: 1.546904\n",
            "Step 736: Minibatch Loss: 1.559529\n",
            "Step 738: Minibatch Loss: 1.545900\n",
            "Step 740: Minibatch Loss: 1.564339\n",
            "Step 742: Minibatch Loss: 1.490048\n",
            "Step 744: Minibatch Loss: 1.539006\n",
            "Step 746: Minibatch Loss: 1.539852\n",
            "Step 748: Minibatch Loss: 1.552834\n",
            "Step 750: Minibatch Loss: 1.598825\n",
            "Step 752: Minibatch Loss: 1.552909\n",
            "Step 754: Minibatch Loss: 1.587808\n",
            "Step 756: Minibatch Loss: 1.602241\n",
            "Step 758: Minibatch Loss: 1.485127\n",
            "Step 760: Minibatch Loss: 1.593670\n",
            "Step 762: Minibatch Loss: 1.511870\n",
            "Step 764: Minibatch Loss: 1.585741\n",
            "Step 766: Minibatch Loss: 1.567124\n",
            "Step 768: Minibatch Loss: 1.511937\n",
            "Step 770: Minibatch Loss: 1.525420\n",
            "Step 772: Minibatch Loss: 1.530488\n",
            "Step 774: Minibatch Loss: 1.569871\n",
            "Step 776: Minibatch Loss: 1.522518\n",
            "Step 778: Minibatch Loss: 1.533680\n",
            "Step 780: Minibatch Loss: 1.508854\n",
            "Step 782: Minibatch Loss: 1.567118\n",
            "Step 784: Minibatch Loss: 1.532666\n",
            "Step 786: Minibatch Loss: 1.544105\n",
            "Step 788: Minibatch Loss: 1.531796\n",
            "Step 790: Minibatch Loss: 1.536380\n",
            "Step 792: Minibatch Loss: 1.561712\n",
            "Step 794: Minibatch Loss: 1.622779\n",
            "Step 796: Minibatch Loss: 1.523163\n",
            "Step 798: Minibatch Loss: 1.510084\n",
            "Step 800: Minibatch Loss: 1.595466\n",
            "Training for SNR= 2.0  sigma= 0.7943282347242815 iteratin: 0\n",
            "Step 802: Minibatch Loss: 1.505311\n",
            "Step 804: Minibatch Loss: 1.486751\n",
            "Step 806: Minibatch Loss: 1.470153\n",
            "Step 808: Minibatch Loss: 1.485947\n",
            "Step 810: Minibatch Loss: 1.481116\n",
            "Step 812: Minibatch Loss: 1.467803\n",
            "Step 814: Minibatch Loss: 1.476894\n",
            "Step 816: Minibatch Loss: 1.513728\n",
            "Step 818: Minibatch Loss: 1.550072\n",
            "Step 820: Minibatch Loss: 1.497478\n",
            "Step 822: Minibatch Loss: 1.540369\n",
            "Step 824: Minibatch Loss: 1.483330\n",
            "Step 826: Minibatch Loss: 1.501471\n",
            "Step 828: Minibatch Loss: 1.492514\n",
            "Step 830: Minibatch Loss: 1.438486\n",
            "Step 832: Minibatch Loss: 1.605358\n",
            "Step 834: Minibatch Loss: 1.527515\n",
            "Step 836: Minibatch Loss: 1.479080\n",
            "Step 838: Minibatch Loss: 1.508543\n",
            "Step 840: Minibatch Loss: 1.530902\n",
            "Step 842: Minibatch Loss: 1.503562\n",
            "Step 844: Minibatch Loss: 1.508940\n",
            "Step 846: Minibatch Loss: 1.518917\n",
            "Step 848: Minibatch Loss: 1.436544\n",
            "Step 850: Minibatch Loss: 1.513518\n",
            "Step 852: Minibatch Loss: 1.618480\n",
            "Step 854: Minibatch Loss: 1.516249\n",
            "Step 856: Minibatch Loss: 1.519524\n",
            "Step 858: Minibatch Loss: 1.492834\n",
            "Step 860: Minibatch Loss: 1.478265\n",
            "Step 862: Minibatch Loss: 1.440876\n",
            "Step 864: Minibatch Loss: 1.544795\n",
            "Step 866: Minibatch Loss: 1.549584\n",
            "Step 868: Minibatch Loss: 1.533938\n",
            "Step 870: Minibatch Loss: 1.514221\n",
            "Step 872: Minibatch Loss: 1.483290\n",
            "Step 874: Minibatch Loss: 1.533054\n",
            "Step 876: Minibatch Loss: 1.554255\n",
            "Step 878: Minibatch Loss: 1.468002\n",
            "Step 880: Minibatch Loss: 1.550174\n",
            "Step 882: Minibatch Loss: 1.480307\n",
            "Step 884: Minibatch Loss: 1.479142\n",
            "Step 886: Minibatch Loss: 1.492684\n",
            "Step 888: Minibatch Loss: 1.564104\n",
            "Step 890: Minibatch Loss: 1.512675\n",
            "Step 892: Minibatch Loss: 1.486714\n",
            "Step 894: Minibatch Loss: 1.553870\n",
            "Step 896: Minibatch Loss: 1.496167\n",
            "Step 898: Minibatch Loss: 1.522900\n",
            "Step 900: Minibatch Loss: 1.500828\n",
            "Step 902: Minibatch Loss: 1.456651\n",
            "Step 904: Minibatch Loss: 1.511488\n",
            "Step 906: Minibatch Loss: 1.533702\n",
            "Step 908: Minibatch Loss: 1.502220\n",
            "Step 910: Minibatch Loss: 1.543437\n",
            "Step 912: Minibatch Loss: 1.525212\n",
            "Step 914: Minibatch Loss: 1.514672\n",
            "Step 916: Minibatch Loss: 1.486443\n",
            "Step 918: Minibatch Loss: 1.521805\n",
            "Step 920: Minibatch Loss: 1.531372\n",
            "Step 922: Minibatch Loss: 1.465696\n",
            "Step 924: Minibatch Loss: 1.482640\n",
            "Step 926: Minibatch Loss: 1.546078\n",
            "Step 928: Minibatch Loss: 1.464745\n",
            "Step 930: Minibatch Loss: 1.484348\n",
            "Step 932: Minibatch Loss: 1.529088\n",
            "Step 934: Minibatch Loss: 1.530102\n",
            "Step 936: Minibatch Loss: 1.579350\n",
            "Step 938: Minibatch Loss: 1.532152\n",
            "Step 940: Minibatch Loss: 1.548921\n",
            "Step 942: Minibatch Loss: 1.498306\n",
            "Step 944: Minibatch Loss: 1.453484\n",
            "Step 946: Minibatch Loss: 1.557736\n",
            "Step 948: Minibatch Loss: 1.464362\n",
            "Step 950: Minibatch Loss: 1.535240\n",
            "Step 952: Minibatch Loss: 1.511272\n",
            "Step 954: Minibatch Loss: 1.532683\n",
            "Step 956: Minibatch Loss: 1.511250\n",
            "Step 958: Minibatch Loss: 1.524652\n",
            "Step 960: Minibatch Loss: 1.490185\n",
            "Step 962: Minibatch Loss: 1.484637\n",
            "Step 964: Minibatch Loss: 1.500486\n",
            "Step 966: Minibatch Loss: 1.552085\n",
            "Step 968: Minibatch Loss: 1.507408\n",
            "Step 970: Minibatch Loss: 1.507255\n",
            "Step 972: Minibatch Loss: 1.539792\n",
            "Step 974: Minibatch Loss: 1.515345\n",
            "Step 976: Minibatch Loss: 1.527357\n",
            "Step 978: Minibatch Loss: 1.603614\n",
            "Step 980: Minibatch Loss: 1.505787\n",
            "Step 982: Minibatch Loss: 1.465408\n",
            "Step 984: Minibatch Loss: 1.509279\n",
            "Step 986: Minibatch Loss: 1.488325\n",
            "Step 988: Minibatch Loss: 1.621245\n",
            "Step 990: Minibatch Loss: 1.522245\n",
            "Step 992: Minibatch Loss: 1.535917\n",
            "Step 994: Minibatch Loss: 1.574369\n",
            "Step 996: Minibatch Loss: 1.489356\n",
            "Step 998: Minibatch Loss: 1.499724\n",
            "Step 1000: Minibatch Loss: 1.522199\n",
            "Training for SNR= 2.5  sigma= 0.7498942093324559 iteratin: 0\n",
            "Step 1002: Minibatch Loss: 1.456788\n",
            "Step 1004: Minibatch Loss: 1.508657\n",
            "Step 1006: Minibatch Loss: 1.489076\n",
            "Step 1008: Minibatch Loss: 1.473949\n",
            "Step 1010: Minibatch Loss: 1.522933\n",
            "Step 1012: Minibatch Loss: 1.451085\n",
            "Step 1014: Minibatch Loss: 1.418508\n",
            "Step 1016: Minibatch Loss: 1.450937\n",
            "Step 1018: Minibatch Loss: 1.457778\n",
            "Step 1020: Minibatch Loss: 1.451949\n",
            "Step 1022: Minibatch Loss: 1.432336\n",
            "Step 1024: Minibatch Loss: 1.535845\n",
            "Step 1026: Minibatch Loss: 1.472080\n",
            "Step 1028: Minibatch Loss: 1.481894\n",
            "Step 1030: Minibatch Loss: 1.453768\n",
            "Step 1032: Minibatch Loss: 1.464488\n",
            "Step 1034: Minibatch Loss: 1.451365\n",
            "Step 1036: Minibatch Loss: 1.478651\n",
            "Step 1038: Minibatch Loss: 1.466933\n",
            "Step 1040: Minibatch Loss: 1.485101\n",
            "Step 1042: Minibatch Loss: 1.549450\n",
            "Step 1044: Minibatch Loss: 1.391948\n",
            "Step 1046: Minibatch Loss: 1.424210\n",
            "Step 1048: Minibatch Loss: 1.542071\n",
            "Step 1050: Minibatch Loss: 1.485731\n",
            "Step 1052: Minibatch Loss: 1.463876\n",
            "Step 1054: Minibatch Loss: 1.491765\n",
            "Step 1056: Minibatch Loss: 1.446684\n",
            "Step 1058: Minibatch Loss: 1.447742\n",
            "Step 1060: Minibatch Loss: 1.440209\n",
            "Step 1062: Minibatch Loss: 1.468009\n",
            "Step 1064: Minibatch Loss: 1.463599\n",
            "Step 1066: Minibatch Loss: 1.469087\n",
            "Step 1068: Minibatch Loss: 1.452973\n",
            "Step 1070: Minibatch Loss: 1.465661\n",
            "Step 1072: Minibatch Loss: 1.482752\n",
            "Step 1074: Minibatch Loss: 1.486886\n",
            "Step 1076: Minibatch Loss: 1.429287\n",
            "Step 1078: Minibatch Loss: 1.402947\n",
            "Step 1080: Minibatch Loss: 1.471933\n",
            "Step 1082: Minibatch Loss: 1.479821\n",
            "Step 1084: Minibatch Loss: 1.450891\n",
            "Step 1086: Minibatch Loss: 1.424075\n",
            "Step 1088: Minibatch Loss: 1.474979\n",
            "Step 1090: Minibatch Loss: 1.488060\n",
            "Step 1092: Minibatch Loss: 1.464776\n",
            "Step 1094: Minibatch Loss: 1.489914\n",
            "Step 1096: Minibatch Loss: 1.480258\n",
            "Step 1098: Minibatch Loss: 1.429047\n",
            "Step 1100: Minibatch Loss: 1.481712\n",
            "Step 1102: Minibatch Loss: 1.428622\n",
            "Step 1104: Minibatch Loss: 1.435131\n",
            "Step 1106: Minibatch Loss: 1.448981\n",
            "Step 1108: Minibatch Loss: 1.529548\n",
            "Step 1110: Minibatch Loss: 1.431116\n",
            "Step 1112: Minibatch Loss: 1.525680\n",
            "Step 1114: Minibatch Loss: 1.464357\n",
            "Step 1116: Minibatch Loss: 1.472207\n",
            "Step 1118: Minibatch Loss: 1.430712\n",
            "Step 1120: Minibatch Loss: 1.511070\n",
            "Step 1122: Minibatch Loss: 1.421462\n",
            "Step 1124: Minibatch Loss: 1.434678\n",
            "Step 1126: Minibatch Loss: 1.467546\n",
            "Step 1128: Minibatch Loss: 1.440954\n",
            "Step 1130: Minibatch Loss: 1.428640\n",
            "Step 1132: Minibatch Loss: 1.517642\n",
            "Step 1134: Minibatch Loss: 1.497051\n",
            "Step 1136: Minibatch Loss: 1.464089\n",
            "Step 1138: Minibatch Loss: 1.519658\n",
            "Step 1140: Minibatch Loss: 1.473994\n",
            "Step 1142: Minibatch Loss: 1.447107\n",
            "Step 1144: Minibatch Loss: 1.452337\n",
            "Step 1146: Minibatch Loss: 1.484702\n",
            "Step 1148: Minibatch Loss: 1.505579\n",
            "Step 1150: Minibatch Loss: 1.507178\n",
            "Step 1152: Minibatch Loss: 1.429744\n",
            "Step 1154: Minibatch Loss: 1.469165\n",
            "Step 1156: Minibatch Loss: 1.495542\n",
            "Step 1158: Minibatch Loss: 1.479184\n",
            "Step 1160: Minibatch Loss: 1.520123\n",
            "Step 1162: Minibatch Loss: 1.461538\n",
            "Step 1164: Minibatch Loss: 1.482248\n",
            "Step 1166: Minibatch Loss: 1.449802\n",
            "Step 1168: Minibatch Loss: 1.454641\n",
            "Step 1170: Minibatch Loss: 1.466195\n",
            "Step 1172: Minibatch Loss: 1.532580\n",
            "Step 1174: Minibatch Loss: 1.442076\n",
            "Step 1176: Minibatch Loss: 1.433285\n",
            "Step 1178: Minibatch Loss: 1.423977\n",
            "Step 1180: Minibatch Loss: 1.476534\n",
            "Step 1182: Minibatch Loss: 1.473336\n",
            "Step 1184: Minibatch Loss: 1.454905\n",
            "Step 1186: Minibatch Loss: 1.420017\n",
            "Step 1188: Minibatch Loss: 1.452988\n",
            "Step 1190: Minibatch Loss: 1.443283\n",
            "Step 1192: Minibatch Loss: 1.471112\n",
            "Step 1194: Minibatch Loss: 1.502585\n",
            "Step 1196: Minibatch Loss: 1.505015\n",
            "Step 1198: Minibatch Loss: 1.414039\n",
            "Step 1200: Minibatch Loss: 1.444841\n",
            "Training for SNR= 3.0  sigma= 0.7079457843841379 iteratin: 0\n",
            "Step 1202: Minibatch Loss: 1.442459\n",
            "Step 1204: Minibatch Loss: 1.348157\n",
            "Step 1206: Minibatch Loss: 1.434706\n",
            "Step 1208: Minibatch Loss: 1.415710\n",
            "Step 1210: Minibatch Loss: 1.434901\n",
            "Step 1212: Minibatch Loss: 1.398485\n",
            "Step 1214: Minibatch Loss: 1.374995\n",
            "Step 1216: Minibatch Loss: 1.403443\n",
            "Step 1218: Minibatch Loss: 1.415269\n",
            "Step 1220: Minibatch Loss: 1.366886\n",
            "Step 1222: Minibatch Loss: 1.385054\n",
            "Step 1224: Minibatch Loss: 1.440146\n",
            "Step 1226: Minibatch Loss: 1.452745\n",
            "Step 1228: Minibatch Loss: 1.393982\n",
            "Step 1230: Minibatch Loss: 1.378983\n",
            "Step 1232: Minibatch Loss: 1.450606\n",
            "Step 1234: Minibatch Loss: 1.410595\n",
            "Step 1236: Minibatch Loss: 1.326014\n",
            "Step 1238: Minibatch Loss: 1.488768\n",
            "Step 1240: Minibatch Loss: 1.492913\n",
            "Step 1242: Minibatch Loss: 1.366078\n",
            "Step 1244: Minibatch Loss: 1.419129\n",
            "Step 1246: Minibatch Loss: 1.348665\n",
            "Step 1248: Minibatch Loss: 1.471004\n",
            "Step 1250: Minibatch Loss: 1.415355\n",
            "Step 1252: Minibatch Loss: 1.377977\n",
            "Step 1254: Minibatch Loss: 1.386405\n",
            "Step 1256: Minibatch Loss: 1.415630\n",
            "Step 1258: Minibatch Loss: 1.432300\n",
            "Step 1260: Minibatch Loss: 1.383515\n",
            "Step 1262: Minibatch Loss: 1.412730\n",
            "Step 1264: Minibatch Loss: 1.387071\n",
            "Step 1266: Minibatch Loss: 1.374195\n",
            "Step 1268: Minibatch Loss: 1.419776\n",
            "Step 1270: Minibatch Loss: 1.426478\n",
            "Step 1272: Minibatch Loss: 1.368040\n",
            "Step 1274: Minibatch Loss: 1.367498\n",
            "Step 1276: Minibatch Loss: 1.391789\n",
            "Step 1278: Minibatch Loss: 1.398417\n",
            "Step 1280: Minibatch Loss: 1.353724\n",
            "Step 1282: Minibatch Loss: 1.380304\n",
            "Step 1284: Minibatch Loss: 1.405445\n",
            "Step 1286: Minibatch Loss: 1.419652\n",
            "Step 1288: Minibatch Loss: 1.425590\n",
            "Step 1290: Minibatch Loss: 1.391740\n",
            "Step 1292: Minibatch Loss: 1.408738\n",
            "Step 1294: Minibatch Loss: 1.411654\n",
            "Step 1296: Minibatch Loss: 1.502187\n",
            "Step 1298: Minibatch Loss: 1.440256\n",
            "Step 1300: Minibatch Loss: 1.415793\n",
            "Step 1302: Minibatch Loss: 1.360799\n",
            "Step 1304: Minibatch Loss: 1.396846\n",
            "Step 1306: Minibatch Loss: 1.408049\n",
            "Step 1308: Minibatch Loss: 1.398400\n",
            "Step 1310: Minibatch Loss: 1.428750\n",
            "Step 1312: Minibatch Loss: 1.331923\n",
            "Step 1314: Minibatch Loss: 1.413287\n",
            "Step 1316: Minibatch Loss: 1.402284\n",
            "Step 1318: Minibatch Loss: 1.459297\n",
            "Step 1320: Minibatch Loss: 1.351683\n",
            "Step 1322: Minibatch Loss: 1.410328\n",
            "Step 1324: Minibatch Loss: 1.423192\n",
            "Step 1326: Minibatch Loss: 1.376222\n",
            "Step 1328: Minibatch Loss: 1.366931\n",
            "Step 1330: Minibatch Loss: 1.381840\n",
            "Step 1332: Minibatch Loss: 1.393981\n",
            "Step 1334: Minibatch Loss: 1.463733\n",
            "Step 1336: Minibatch Loss: 1.449398\n",
            "Step 1338: Minibatch Loss: 1.427546\n",
            "Step 1340: Minibatch Loss: 1.351030\n",
            "Step 1342: Minibatch Loss: 1.430222\n",
            "Step 1344: Minibatch Loss: 1.446033\n",
            "Step 1346: Minibatch Loss: 1.454268\n",
            "Step 1348: Minibatch Loss: 1.472245\n",
            "Step 1350: Minibatch Loss: 1.457874\n",
            "Step 1352: Minibatch Loss: 1.395409\n",
            "Step 1354: Minibatch Loss: 1.432393\n",
            "Step 1356: Minibatch Loss: 1.351008\n",
            "Step 1358: Minibatch Loss: 1.433323\n",
            "Step 1360: Minibatch Loss: 1.415911\n",
            "Step 1362: Minibatch Loss: 1.390543\n",
            "Step 1364: Minibatch Loss: 1.376275\n",
            "Step 1366: Minibatch Loss: 1.394861\n",
            "Step 1368: Minibatch Loss: 1.373503\n",
            "Step 1370: Minibatch Loss: 1.379743\n",
            "Step 1372: Minibatch Loss: 1.463012\n",
            "Step 1374: Minibatch Loss: 1.347978\n",
            "Step 1376: Minibatch Loss: 1.405949\n",
            "Step 1378: Minibatch Loss: 1.410536\n",
            "Step 1380: Minibatch Loss: 1.418949\n",
            "Step 1382: Minibatch Loss: 1.353312\n",
            "Step 1384: Minibatch Loss: 1.428272\n",
            "Step 1386: Minibatch Loss: 1.385871\n",
            "Step 1388: Minibatch Loss: 1.434942\n",
            "Step 1390: Minibatch Loss: 1.378228\n",
            "Step 1392: Minibatch Loss: 1.395567\n",
            "Step 1394: Minibatch Loss: 1.367161\n",
            "Step 1396: Minibatch Loss: 1.393602\n",
            "Step 1398: Minibatch Loss: 1.418805\n",
            "Step 1400: Minibatch Loss: 1.397866\n",
            "Training for SNR= 3.5  sigma= 0.6683439175686147 iteratin: 0\n",
            "Step 1402: Minibatch Loss: 1.355770\n",
            "Step 1404: Minibatch Loss: 1.342648\n",
            "Step 1406: Minibatch Loss: 1.359064\n",
            "Step 1408: Minibatch Loss: 1.385823\n",
            "Step 1410: Minibatch Loss: 1.355254\n",
            "Step 1412: Minibatch Loss: 1.343044\n",
            "Step 1414: Minibatch Loss: 1.379588\n",
            "Step 1416: Minibatch Loss: 1.380757\n",
            "Step 1418: Minibatch Loss: 1.339051\n",
            "Step 1420: Minibatch Loss: 1.343820\n",
            "Step 1422: Minibatch Loss: 1.380220\n",
            "Step 1424: Minibatch Loss: 1.389184\n",
            "Step 1426: Minibatch Loss: 1.311481\n",
            "Step 1428: Minibatch Loss: 1.376723\n",
            "Step 1430: Minibatch Loss: 1.395778\n",
            "Step 1432: Minibatch Loss: 1.387231\n",
            "Step 1434: Minibatch Loss: 1.360640\n",
            "Step 1436: Minibatch Loss: 1.311726\n",
            "Step 1438: Minibatch Loss: 1.370747\n",
            "Step 1440: Minibatch Loss: 1.357530\n",
            "Step 1442: Minibatch Loss: 1.332958\n",
            "Step 1444: Minibatch Loss: 1.369440\n",
            "Step 1446: Minibatch Loss: 1.322604\n",
            "Step 1448: Minibatch Loss: 1.376377\n",
            "Step 1450: Minibatch Loss: 1.366744\n",
            "Step 1452: Minibatch Loss: 1.376501\n",
            "Step 1454: Minibatch Loss: 1.353044\n",
            "Step 1456: Minibatch Loss: 1.356714\n",
            "Step 1458: Minibatch Loss: 1.370054\n",
            "Step 1460: Minibatch Loss: 1.364668\n",
            "Step 1462: Minibatch Loss: 1.386798\n",
            "Step 1464: Minibatch Loss: 1.366289\n",
            "Step 1466: Minibatch Loss: 1.363039\n",
            "Step 1468: Minibatch Loss: 1.358563\n",
            "Step 1470: Minibatch Loss: 1.362638\n",
            "Step 1472: Minibatch Loss: 1.351147\n",
            "Step 1474: Minibatch Loss: 1.374663\n",
            "Step 1476: Minibatch Loss: 1.336684\n",
            "Step 1478: Minibatch Loss: 1.384453\n",
            "Step 1480: Minibatch Loss: 1.290996\n",
            "Step 1482: Minibatch Loss: 1.396491\n",
            "Step 1484: Minibatch Loss: 1.356087\n",
            "Step 1486: Minibatch Loss: 1.366250\n",
            "Step 1488: Minibatch Loss: 1.379645\n",
            "Step 1490: Minibatch Loss: 1.339023\n",
            "Step 1492: Minibatch Loss: 1.320494\n",
            "Step 1494: Minibatch Loss: 1.369365\n",
            "Step 1496: Minibatch Loss: 1.370627\n",
            "Step 1498: Minibatch Loss: 1.323110\n",
            "Step 1500: Minibatch Loss: 1.302453\n",
            "Step 1502: Minibatch Loss: 1.366900\n",
            "Step 1504: Minibatch Loss: 1.376367\n",
            "Step 1506: Minibatch Loss: 1.336752\n",
            "Step 1508: Minibatch Loss: 1.351597\n",
            "Step 1510: Minibatch Loss: 1.355688\n",
            "Step 1512: Minibatch Loss: 1.291364\n",
            "Step 1514: Minibatch Loss: 1.361767\n",
            "Step 1516: Minibatch Loss: 1.331638\n",
            "Step 1518: Minibatch Loss: 1.351983\n",
            "Step 1520: Minibatch Loss: 1.346783\n",
            "Step 1522: Minibatch Loss: 1.340363\n",
            "Step 1524: Minibatch Loss: 1.320041\n",
            "Step 1526: Minibatch Loss: 1.338102\n",
            "Step 1528: Minibatch Loss: 1.403008\n",
            "Step 1530: Minibatch Loss: 1.393658\n",
            "Step 1532: Minibatch Loss: 1.367606\n",
            "Step 1534: Minibatch Loss: 1.345448\n",
            "Step 1536: Minibatch Loss: 1.432857\n",
            "Step 1538: Minibatch Loss: 1.377453\n",
            "Step 1540: Minibatch Loss: 1.348973\n",
            "Step 1542: Minibatch Loss: 1.349859\n",
            "Step 1544: Minibatch Loss: 1.386174\n",
            "Step 1546: Minibatch Loss: 1.316187\n",
            "Step 1548: Minibatch Loss: 1.341196\n",
            "Step 1550: Minibatch Loss: 1.287231\n",
            "Step 1552: Minibatch Loss: 1.336518\n",
            "Step 1554: Minibatch Loss: 1.351341\n",
            "Step 1556: Minibatch Loss: 1.323114\n",
            "Step 1558: Minibatch Loss: 1.332657\n",
            "Step 1560: Minibatch Loss: 1.388229\n",
            "Step 1562: Minibatch Loss: 1.330995\n",
            "Step 1564: Minibatch Loss: 1.369073\n",
            "Step 1566: Minibatch Loss: 1.332306\n",
            "Step 1568: Minibatch Loss: 1.378283\n",
            "Step 1570: Minibatch Loss: 1.370885\n",
            "Step 1572: Minibatch Loss: 1.321720\n",
            "Step 1574: Minibatch Loss: 1.332969\n",
            "Step 1576: Minibatch Loss: 1.312617\n",
            "Step 1578: Minibatch Loss: 1.383827\n",
            "Step 1580: Minibatch Loss: 1.325617\n",
            "Step 1582: Minibatch Loss: 1.312626\n",
            "Step 1584: Minibatch Loss: 1.366447\n",
            "Step 1586: Minibatch Loss: 1.441721\n",
            "Step 1588: Minibatch Loss: 1.316273\n",
            "Step 1590: Minibatch Loss: 1.421145\n",
            "Step 1592: Minibatch Loss: 1.360010\n",
            "Step 1594: Minibatch Loss: 1.341466\n",
            "Step 1596: Minibatch Loss: 1.363887\n",
            "Step 1598: Minibatch Loss: 1.316854\n",
            "Step 1600: Minibatch Loss: 1.387875\n",
            "Training for SNR= 4.0  sigma= 0.6309573444801932 iteratin: 0\n",
            "Step 1602: Minibatch Loss: 1.276680\n",
            "Step 1604: Minibatch Loss: 1.276378\n",
            "Step 1606: Minibatch Loss: 1.252638\n",
            "Step 1608: Minibatch Loss: 1.305202\n",
            "Step 1610: Minibatch Loss: 1.323730\n",
            "Step 1612: Minibatch Loss: 1.313449\n",
            "Step 1614: Minibatch Loss: 1.286217\n",
            "Step 1616: Minibatch Loss: 1.258981\n",
            "Step 1618: Minibatch Loss: 1.338532\n",
            "Step 1620: Minibatch Loss: 1.299689\n",
            "Step 1622: Minibatch Loss: 1.295767\n",
            "Step 1624: Minibatch Loss: 1.285138\n",
            "Step 1626: Minibatch Loss: 1.334877\n",
            "Step 1628: Minibatch Loss: 1.285361\n",
            "Step 1630: Minibatch Loss: 1.268950\n",
            "Step 1632: Minibatch Loss: 1.288969\n",
            "Step 1634: Minibatch Loss: 1.284767\n",
            "Step 1636: Minibatch Loss: 1.375806\n",
            "Step 1638: Minibatch Loss: 1.268958\n",
            "Step 1640: Minibatch Loss: 1.296748\n",
            "Step 1642: Minibatch Loss: 1.314382\n",
            "Step 1644: Minibatch Loss: 1.263782\n",
            "Step 1646: Minibatch Loss: 1.316053\n",
            "Step 1648: Minibatch Loss: 1.281450\n",
            "Step 1650: Minibatch Loss: 1.290624\n",
            "Step 1652: Minibatch Loss: 1.272284\n",
            "Step 1654: Minibatch Loss: 1.253448\n",
            "Step 1656: Minibatch Loss: 1.299685\n",
            "Step 1658: Minibatch Loss: 1.314179\n",
            "Step 1660: Minibatch Loss: 1.306282\n",
            "Step 1662: Minibatch Loss: 1.293914\n",
            "Step 1664: Minibatch Loss: 1.297745\n",
            "Step 1666: Minibatch Loss: 1.268432\n",
            "Step 1668: Minibatch Loss: 1.241291\n",
            "Step 1670: Minibatch Loss: 1.277072\n",
            "Step 1672: Minibatch Loss: 1.277242\n",
            "Step 1674: Minibatch Loss: 1.292979\n",
            "Step 1676: Minibatch Loss: 1.252594\n",
            "Step 1678: Minibatch Loss: 1.319227\n",
            "Step 1680: Minibatch Loss: 1.345076\n",
            "Step 1682: Minibatch Loss: 1.298090\n",
            "Step 1684: Minibatch Loss: 1.294797\n",
            "Step 1686: Minibatch Loss: 1.301170\n",
            "Step 1688: Minibatch Loss: 1.272900\n",
            "Step 1690: Minibatch Loss: 1.292890\n",
            "Step 1692: Minibatch Loss: 1.297533\n",
            "Step 1694: Minibatch Loss: 1.276881\n",
            "Step 1696: Minibatch Loss: 1.298097\n",
            "Step 1698: Minibatch Loss: 1.292503\n",
            "Step 1700: Minibatch Loss: 1.337514\n",
            "Step 1702: Minibatch Loss: 1.267974\n",
            "Step 1704: Minibatch Loss: 1.327312\n",
            "Step 1706: Minibatch Loss: 1.287074\n",
            "Step 1708: Minibatch Loss: 1.221716\n",
            "Step 1710: Minibatch Loss: 1.279339\n",
            "Step 1712: Minibatch Loss: 1.317415\n",
            "Step 1714: Minibatch Loss: 1.291789\n",
            "Step 1716: Minibatch Loss: 1.315315\n",
            "Step 1718: Minibatch Loss: 1.314884\n",
            "Step 1720: Minibatch Loss: 1.307619\n",
            "Step 1722: Minibatch Loss: 1.354707\n",
            "Step 1724: Minibatch Loss: 1.257222\n",
            "Step 1726: Minibatch Loss: 1.273093\n",
            "Step 1728: Minibatch Loss: 1.423533\n",
            "Step 1730: Minibatch Loss: 1.304942\n",
            "Step 1732: Minibatch Loss: 1.280843\n",
            "Step 1734: Minibatch Loss: 1.251524\n",
            "Step 1736: Minibatch Loss: 1.247631\n",
            "Step 1738: Minibatch Loss: 1.288102\n",
            "Step 1740: Minibatch Loss: 1.281208\n",
            "Step 1742: Minibatch Loss: 1.279944\n",
            "Step 1744: Minibatch Loss: 1.374232\n",
            "Step 1746: Minibatch Loss: 1.353202\n",
            "Step 1748: Minibatch Loss: 1.273631\n",
            "Step 1750: Minibatch Loss: 1.321948\n",
            "Step 1752: Minibatch Loss: 1.263253\n",
            "Step 1754: Minibatch Loss: 1.300270\n",
            "Step 1756: Minibatch Loss: 1.319907\n",
            "Step 1758: Minibatch Loss: 1.324736\n",
            "Step 1760: Minibatch Loss: 1.350490\n",
            "Step 1762: Minibatch Loss: 1.325639\n",
            "Step 1764: Minibatch Loss: 1.341558\n",
            "Step 1766: Minibatch Loss: 1.334613\n",
            "Step 1768: Minibatch Loss: 1.300174\n",
            "Step 1770: Minibatch Loss: 1.364978\n",
            "Step 1772: Minibatch Loss: 1.274022\n",
            "Step 1774: Minibatch Loss: 1.309786\n",
            "Step 1776: Minibatch Loss: 1.383753\n",
            "Step 1778: Minibatch Loss: 1.386669\n",
            "Step 1780: Minibatch Loss: 1.265252\n",
            "Step 1782: Minibatch Loss: 1.316794\n",
            "Step 1784: Minibatch Loss: 1.350704\n",
            "Step 1786: Minibatch Loss: 1.285789\n",
            "Step 1788: Minibatch Loss: 1.250415\n",
            "Step 1790: Minibatch Loss: 1.300274\n",
            "Step 1792: Minibatch Loss: 1.344774\n",
            "Step 1794: Minibatch Loss: 1.289737\n",
            "Step 1796: Minibatch Loss: 1.309761\n",
            "Step 1798: Minibatch Loss: 1.228844\n",
            "Step 1800: Minibatch Loss: 1.286717\n",
            "Training for SNR= 4.5  sigma= 0.5956621435290105 iteratin: 0\n",
            "Step 1802: Minibatch Loss: 1.275279\n",
            "Step 1804: Minibatch Loss: 1.216711\n",
            "Step 1806: Minibatch Loss: 1.227329\n",
            "Step 1808: Minibatch Loss: 1.202047\n",
            "Step 1810: Minibatch Loss: 1.260441\n",
            "Step 1812: Minibatch Loss: 1.287335\n",
            "Step 1814: Minibatch Loss: 1.262576\n",
            "Step 1816: Minibatch Loss: 1.341210\n",
            "Step 1818: Minibatch Loss: 1.245316\n",
            "Step 1820: Minibatch Loss: 1.228776\n",
            "Step 1822: Minibatch Loss: 1.249461\n",
            "Step 1824: Minibatch Loss: 1.241716\n",
            "Step 1826: Minibatch Loss: 1.330589\n",
            "Step 1828: Minibatch Loss: 1.263296\n",
            "Step 1830: Minibatch Loss: 1.226266\n",
            "Step 1832: Minibatch Loss: 1.282113\n",
            "Step 1834: Minibatch Loss: 1.303997\n",
            "Step 1836: Minibatch Loss: 1.303850\n",
            "Step 1838: Minibatch Loss: 1.237375\n",
            "Step 1840: Minibatch Loss: 1.281614\n",
            "Step 1842: Minibatch Loss: 1.270092\n",
            "Step 1844: Minibatch Loss: 1.235399\n",
            "Step 1846: Minibatch Loss: 1.227522\n",
            "Step 1848: Minibatch Loss: 1.243571\n",
            "Step 1850: Minibatch Loss: 1.272033\n",
            "Step 1852: Minibatch Loss: 1.174890\n",
            "Step 1854: Minibatch Loss: 1.258347\n",
            "Step 1856: Minibatch Loss: 1.216365\n",
            "Step 1858: Minibatch Loss: 1.282877\n",
            "Step 1860: Minibatch Loss: 1.252750\n",
            "Step 1862: Minibatch Loss: 1.291785\n",
            "Step 1864: Minibatch Loss: 1.284311\n",
            "Step 1866: Minibatch Loss: 1.216797\n",
            "Step 1868: Minibatch Loss: 1.249454\n",
            "Step 1870: Minibatch Loss: 1.295051\n",
            "Step 1872: Minibatch Loss: 1.264787\n",
            "Step 1874: Minibatch Loss: 1.246849\n",
            "Step 1876: Minibatch Loss: 1.220866\n",
            "Step 1878: Minibatch Loss: 1.225202\n",
            "Step 1880: Minibatch Loss: 1.307640\n",
            "Step 1882: Minibatch Loss: 1.216417\n",
            "Step 1884: Minibatch Loss: 1.253780\n",
            "Step 1886: Minibatch Loss: 1.260801\n",
            "Step 1888: Minibatch Loss: 1.229395\n",
            "Step 1890: Minibatch Loss: 1.209181\n",
            "Step 1892: Minibatch Loss: 1.270513\n",
            "Step 1894: Minibatch Loss: 1.213536\n",
            "Step 1896: Minibatch Loss: 1.263338\n",
            "Step 1898: Minibatch Loss: 1.299784\n",
            "Step 1900: Minibatch Loss: 1.248510\n",
            "Step 1902: Minibatch Loss: 1.304729\n",
            "Step 1904: Minibatch Loss: 1.296294\n",
            "Step 1906: Minibatch Loss: 1.224091\n",
            "Step 1908: Minibatch Loss: 1.237377\n",
            "Step 1910: Minibatch Loss: 1.267561\n",
            "Step 1912: Minibatch Loss: 1.212903\n",
            "Step 1914: Minibatch Loss: 1.220490\n",
            "Step 1916: Minibatch Loss: 1.220564\n",
            "Step 1918: Minibatch Loss: 1.227539\n",
            "Step 1920: Minibatch Loss: 1.296926\n",
            "Step 1922: Minibatch Loss: 1.221556\n",
            "Step 1924: Minibatch Loss: 1.296680\n",
            "Step 1926: Minibatch Loss: 1.238801\n",
            "Step 1928: Minibatch Loss: 1.267292\n",
            "Step 1930: Minibatch Loss: 1.209024\n",
            "Step 1932: Minibatch Loss: 1.219758\n",
            "Step 1934: Minibatch Loss: 1.207893\n",
            "Step 1936: Minibatch Loss: 1.262156\n",
            "Step 1938: Minibatch Loss: 1.270367\n",
            "Step 1940: Minibatch Loss: 1.213001\n",
            "Step 1942: Minibatch Loss: 1.215251\n",
            "Step 1944: Minibatch Loss: 1.277360\n",
            "Step 1946: Minibatch Loss: 1.271107\n",
            "Step 1948: Minibatch Loss: 1.185292\n",
            "Step 1950: Minibatch Loss: 1.276018\n",
            "Step 1952: Minibatch Loss: 1.248333\n",
            "Step 1954: Minibatch Loss: 1.278915\n",
            "Step 1956: Minibatch Loss: 1.298470\n",
            "Step 1958: Minibatch Loss: 1.263247\n",
            "Step 1960: Minibatch Loss: 1.184994\n",
            "Step 1962: Minibatch Loss: 1.222821\n",
            "Step 1964: Minibatch Loss: 1.180210\n",
            "Step 1966: Minibatch Loss: 1.206639\n",
            "Step 1968: Minibatch Loss: 1.217892\n",
            "Step 1970: Minibatch Loss: 1.267016\n",
            "Step 1972: Minibatch Loss: 1.233920\n",
            "Step 1974: Minibatch Loss: 1.207551\n",
            "Step 1976: Minibatch Loss: 1.224037\n",
            "Step 1978: Minibatch Loss: 1.235894\n",
            "Step 1980: Minibatch Loss: 1.273331\n",
            "Step 1982: Minibatch Loss: 1.274617\n",
            "Step 1984: Minibatch Loss: 1.246103\n",
            "Step 1986: Minibatch Loss: 1.221374\n",
            "Step 1988: Minibatch Loss: 1.228198\n",
            "Step 1990: Minibatch Loss: 1.246362\n",
            "Step 1992: Minibatch Loss: 1.252745\n",
            "Step 1994: Minibatch Loss: 1.224896\n",
            "Step 1996: Minibatch Loss: 1.186343\n",
            "Step 1998: Minibatch Loss: 1.245162\n",
            "Step 2000: Minibatch Loss: 1.246371\n",
            "Training for SNR= 5.0  sigma= 0.5623413251903491 iteratin: 0\n",
            "Step 2002: Minibatch Loss: 1.198025\n",
            "Step 2004: Minibatch Loss: 1.161438\n",
            "Step 2006: Minibatch Loss: 1.200454\n",
            "Step 2008: Minibatch Loss: 1.175612\n",
            "Step 2010: Minibatch Loss: 1.199229\n",
            "Step 2012: Minibatch Loss: 1.200348\n",
            "Step 2014: Minibatch Loss: 1.195850\n",
            "Step 2016: Minibatch Loss: 1.175819\n",
            "Step 2018: Minibatch Loss: 1.204545\n",
            "Step 2020: Minibatch Loss: 1.199106\n",
            "Step 2022: Minibatch Loss: 1.169051\n",
            "Step 2024: Minibatch Loss: 1.212679\n",
            "Step 2026: Minibatch Loss: 1.176380\n",
            "Step 2028: Minibatch Loss: 1.196702\n",
            "Step 2030: Minibatch Loss: 1.130968\n",
            "Step 2032: Minibatch Loss: 1.236005\n",
            "Step 2034: Minibatch Loss: 1.168649\n",
            "Step 2036: Minibatch Loss: 1.223701\n",
            "Step 2038: Minibatch Loss: 1.195273\n",
            "Step 2040: Minibatch Loss: 1.149750\n",
            "Step 2042: Minibatch Loss: 1.222413\n",
            "Step 2044: Minibatch Loss: 1.211719\n",
            "Step 2046: Minibatch Loss: 1.212202\n",
            "Step 2048: Minibatch Loss: 1.194303\n",
            "Step 2050: Minibatch Loss: 1.226665\n",
            "Step 2052: Minibatch Loss: 1.111651\n",
            "Step 2054: Minibatch Loss: 1.231623\n",
            "Step 2056: Minibatch Loss: 1.228741\n",
            "Step 2058: Minibatch Loss: 1.156824\n",
            "Step 2060: Minibatch Loss: 1.210507\n",
            "Step 2062: Minibatch Loss: 1.212773\n",
            "Step 2064: Minibatch Loss: 1.191290\n",
            "Step 2066: Minibatch Loss: 1.186927\n",
            "Step 2068: Minibatch Loss: 1.142537\n",
            "Step 2070: Minibatch Loss: 1.246886\n",
            "Step 2072: Minibatch Loss: 1.187374\n",
            "Step 2074: Minibatch Loss: 1.165946\n",
            "Step 2076: Minibatch Loss: 1.180742\n",
            "Step 2078: Minibatch Loss: 1.208728\n",
            "Step 2080: Minibatch Loss: 1.112109\n",
            "Step 2082: Minibatch Loss: 1.149521\n",
            "Step 2084: Minibatch Loss: 1.218623\n",
            "Step 2086: Minibatch Loss: 1.166803\n",
            "Step 2088: Minibatch Loss: 1.218654\n",
            "Step 2090: Minibatch Loss: 1.147796\n",
            "Step 2092: Minibatch Loss: 1.121584\n",
            "Step 2094: Minibatch Loss: 1.193753\n",
            "Step 2096: Minibatch Loss: 1.128399\n",
            "Step 2098: Minibatch Loss: 1.189670\n",
            "Step 2100: Minibatch Loss: 1.217286\n",
            "Step 2102: Minibatch Loss: 1.239049\n",
            "Step 2104: Minibatch Loss: 1.196755\n",
            "Step 2106: Minibatch Loss: 1.264567\n",
            "Step 2108: Minibatch Loss: 1.200948\n",
            "Step 2110: Minibatch Loss: 1.129799\n",
            "Step 2112: Minibatch Loss: 1.167291\n",
            "Step 2114: Minibatch Loss: 1.166913\n",
            "Step 2116: Minibatch Loss: 1.175609\n",
            "Step 2118: Minibatch Loss: 1.185951\n",
            "Step 2120: Minibatch Loss: 1.227399\n",
            "Step 2122: Minibatch Loss: 1.170290\n",
            "Step 2124: Minibatch Loss: 1.206035\n",
            "Step 2126: Minibatch Loss: 1.205211\n",
            "Step 2128: Minibatch Loss: 1.132865\n",
            "Step 2130: Minibatch Loss: 1.204660\n",
            "Step 2132: Minibatch Loss: 1.178695\n",
            "Step 2134: Minibatch Loss: 1.233754\n",
            "Step 2136: Minibatch Loss: 1.189689\n",
            "Step 2138: Minibatch Loss: 1.226150\n",
            "Step 2140: Minibatch Loss: 1.162657\n",
            "Step 2142: Minibatch Loss: 1.175841\n",
            "Step 2144: Minibatch Loss: 1.217934\n",
            "Step 2146: Minibatch Loss: 1.169713\n",
            "Step 2148: Minibatch Loss: 1.181608\n",
            "Step 2150: Minibatch Loss: 1.250295\n",
            "Step 2152: Minibatch Loss: 1.188013\n",
            "Step 2154: Minibatch Loss: 1.175112\n",
            "Step 2156: Minibatch Loss: 1.176394\n",
            "Step 2158: Minibatch Loss: 1.165354\n",
            "Step 2160: Minibatch Loss: 1.183435\n",
            "Step 2162: Minibatch Loss: 1.174448\n",
            "Step 2164: Minibatch Loss: 1.190689\n",
            "Step 2166: Minibatch Loss: 1.184390\n",
            "Step 2168: Minibatch Loss: 1.157791\n",
            "Step 2170: Minibatch Loss: 1.201992\n",
            "Step 2172: Minibatch Loss: 1.162108\n",
            "Step 2174: Minibatch Loss: 1.183439\n",
            "Step 2176: Minibatch Loss: 1.156357\n",
            "Step 2178: Minibatch Loss: 1.152883\n",
            "Step 2180: Minibatch Loss: 1.148789\n",
            "Step 2182: Minibatch Loss: 1.181979\n",
            "Step 2184: Minibatch Loss: 1.172288\n",
            "Step 2186: Minibatch Loss: 1.185154\n",
            "Step 2188: Minibatch Loss: 1.189013\n",
            "Step 2190: Minibatch Loss: 1.205639\n",
            "Step 2192: Minibatch Loss: 1.140619\n",
            "Step 2194: Minibatch Loss: 1.198154\n",
            "Step 2196: Minibatch Loss: 1.162409\n",
            "Step 2198: Minibatch Loss: 1.179575\n",
            "Step 2200: Minibatch Loss: 1.208107\n",
            "Training for SNR= 5.5  sigma= 0.5308844442309884 iteratin: 0\n",
            "Step 2202: Minibatch Loss: 1.122631\n",
            "Step 2204: Minibatch Loss: 1.122696\n",
            "Step 2206: Minibatch Loss: 1.105275\n",
            "Step 2208: Minibatch Loss: 1.143463\n",
            "Step 2210: Minibatch Loss: 1.090654\n",
            "Step 2212: Minibatch Loss: 1.218397\n",
            "Step 2214: Minibatch Loss: 1.191564\n",
            "Step 2216: Minibatch Loss: 1.122779\n",
            "Step 2218: Minibatch Loss: 1.119051\n",
            "Step 2220: Minibatch Loss: 1.148902\n",
            "Step 2222: Minibatch Loss: 1.138292\n",
            "Step 2224: Minibatch Loss: 1.090667\n",
            "Step 2226: Minibatch Loss: 1.100269\n",
            "Step 2228: Minibatch Loss: 1.126477\n",
            "Step 2230: Minibatch Loss: 1.130484\n",
            "Step 2232: Minibatch Loss: 1.179581\n",
            "Step 2234: Minibatch Loss: 1.110704\n",
            "Step 2236: Minibatch Loss: 1.036696\n",
            "Step 2238: Minibatch Loss: 1.123160\n",
            "Step 2240: Minibatch Loss: 1.147993\n",
            "Step 2242: Minibatch Loss: 1.047845\n",
            "Step 2244: Minibatch Loss: 1.123433\n",
            "Step 2246: Minibatch Loss: 1.132879\n",
            "Step 2248: Minibatch Loss: 1.115737\n",
            "Step 2250: Minibatch Loss: 1.126941\n",
            "Step 2252: Minibatch Loss: 1.169259\n",
            "Step 2254: Minibatch Loss: 1.122830\n",
            "Step 2256: Minibatch Loss: 1.082285\n",
            "Step 2258: Minibatch Loss: 1.141244\n",
            "Step 2260: Minibatch Loss: 1.154266\n",
            "Step 2262: Minibatch Loss: 1.139532\n",
            "Step 2264: Minibatch Loss: 1.112564\n",
            "Step 2266: Minibatch Loss: 1.119387\n",
            "Step 2268: Minibatch Loss: 1.123692\n",
            "Step 2270: Minibatch Loss: 1.099582\n",
            "Step 2272: Minibatch Loss: 1.090901\n",
            "Step 2274: Minibatch Loss: 1.139741\n",
            "Step 2276: Minibatch Loss: 1.163161\n",
            "Step 2278: Minibatch Loss: 1.081366\n",
            "Step 2280: Minibatch Loss: 1.172274\n",
            "Step 2282: Minibatch Loss: 1.117114\n",
            "Step 2284: Minibatch Loss: 1.083173\n",
            "Step 2286: Minibatch Loss: 1.107685\n",
            "Step 2288: Minibatch Loss: 1.126863\n",
            "Step 2290: Minibatch Loss: 1.064605\n",
            "Step 2292: Minibatch Loss: 1.046716\n",
            "Step 2294: Minibatch Loss: 1.121284\n",
            "Step 2296: Minibatch Loss: 1.091902\n",
            "Step 2298: Minibatch Loss: 1.167675\n",
            "Step 2300: Minibatch Loss: 1.097561\n",
            "Step 2302: Minibatch Loss: 1.170780\n",
            "Step 2304: Minibatch Loss: 1.130870\n",
            "Step 2306: Minibatch Loss: 1.220582\n",
            "Step 2308: Minibatch Loss: 1.197974\n",
            "Step 2310: Minibatch Loss: 1.104655\n",
            "Step 2312: Minibatch Loss: 1.116870\n",
            "Step 2314: Minibatch Loss: 1.162364\n",
            "Step 2316: Minibatch Loss: 1.144950\n",
            "Step 2318: Minibatch Loss: 1.171855\n",
            "Step 2320: Minibatch Loss: 1.136143\n",
            "Step 2322: Minibatch Loss: 1.097582\n",
            "Step 2324: Minibatch Loss: 1.118345\n",
            "Step 2326: Minibatch Loss: 1.083622\n",
            "Step 2328: Minibatch Loss: 1.118423\n",
            "Step 2330: Minibatch Loss: 1.141688\n",
            "Step 2332: Minibatch Loss: 1.167646\n",
            "Step 2334: Minibatch Loss: 1.065684\n",
            "Step 2336: Minibatch Loss: 1.106315\n",
            "Step 2338: Minibatch Loss: 1.120748\n",
            "Step 2340: Minibatch Loss: 1.133998\n",
            "Step 2342: Minibatch Loss: 1.139200\n",
            "Step 2344: Minibatch Loss: 1.139802\n",
            "Step 2346: Minibatch Loss: 1.127827\n",
            "Step 2348: Minibatch Loss: 1.146812\n",
            "Step 2350: Minibatch Loss: 1.099130\n",
            "Step 2352: Minibatch Loss: 1.126320\n",
            "Step 2354: Minibatch Loss: 1.133175\n",
            "Step 2356: Minibatch Loss: 1.170949\n",
            "Step 2358: Minibatch Loss: 1.074547\n",
            "Step 2360: Minibatch Loss: 1.195205\n",
            "Step 2362: Minibatch Loss: 1.067724\n",
            "Step 2364: Minibatch Loss: 1.079188\n",
            "Step 2366: Minibatch Loss: 1.121368\n",
            "Step 2368: Minibatch Loss: 1.186949\n",
            "Step 2370: Minibatch Loss: 1.135995\n",
            "Step 2372: Minibatch Loss: 1.111325\n",
            "Step 2374: Minibatch Loss: 1.193309\n",
            "Step 2376: Minibatch Loss: 1.130622\n",
            "Step 2378: Minibatch Loss: 1.093490\n",
            "Step 2380: Minibatch Loss: 1.118890\n",
            "Step 2382: Minibatch Loss: 1.109856\n",
            "Step 2384: Minibatch Loss: 1.073669\n",
            "Step 2386: Minibatch Loss: 1.162872\n",
            "Step 2388: Minibatch Loss: 1.141755\n",
            "Step 2390: Minibatch Loss: 1.156098\n",
            "Step 2392: Minibatch Loss: 1.133512\n",
            "Step 2394: Minibatch Loss: 1.161692\n",
            "Step 2396: Minibatch Loss: 1.067454\n",
            "Step 2398: Minibatch Loss: 1.166483\n",
            "Step 2400: Minibatch Loss: 1.086408\n",
            "Training for SNR= 6.0  sigma= 0.5011872336272722 iteratin: 0\n",
            "Step 2402: Minibatch Loss: 1.042065\n",
            "Step 2404: Minibatch Loss: 1.079667\n",
            "Step 2406: Minibatch Loss: 1.066843\n",
            "Step 2408: Minibatch Loss: 1.070405\n",
            "Step 2410: Minibatch Loss: 1.095874\n",
            "Step 2412: Minibatch Loss: 1.082176\n",
            "Step 2414: Minibatch Loss: 1.072415\n",
            "Step 2416: Minibatch Loss: 1.067090\n",
            "Step 2418: Minibatch Loss: 1.103454\n",
            "Step 2420: Minibatch Loss: 1.047742\n",
            "Step 2422: Minibatch Loss: 1.059099\n",
            "Step 2424: Minibatch Loss: 1.115780\n",
            "Step 2426: Minibatch Loss: 1.079503\n",
            "Step 2428: Minibatch Loss: 1.049972\n",
            "Step 2430: Minibatch Loss: 1.075059\n",
            "Step 2432: Minibatch Loss: 1.098948\n",
            "Step 2434: Minibatch Loss: 1.056066\n",
            "Step 2436: Minibatch Loss: 1.049555\n",
            "Step 2438: Minibatch Loss: 1.079982\n",
            "Step 2440: Minibatch Loss: 1.064505\n",
            "Step 2442: Minibatch Loss: 1.080572\n",
            "Step 2444: Minibatch Loss: 1.128298\n",
            "Step 2446: Minibatch Loss: 1.028286\n",
            "Step 2448: Minibatch Loss: 0.993940\n",
            "Step 2450: Minibatch Loss: 1.039283\n",
            "Step 2452: Minibatch Loss: 1.058528\n",
            "Step 2454: Minibatch Loss: 1.088874\n",
            "Step 2456: Minibatch Loss: 1.153832\n",
            "Step 2458: Minibatch Loss: 1.078419\n",
            "Step 2460: Minibatch Loss: 1.045737\n",
            "Step 2462: Minibatch Loss: 1.076299\n",
            "Step 2464: Minibatch Loss: 1.071720\n",
            "Step 2466: Minibatch Loss: 1.090397\n",
            "Step 2468: Minibatch Loss: 1.056556\n",
            "Step 2470: Minibatch Loss: 1.024806\n",
            "Step 2472: Minibatch Loss: 1.051826\n",
            "Step 2474: Minibatch Loss: 0.975652\n",
            "Step 2476: Minibatch Loss: 1.055087\n",
            "Step 2478: Minibatch Loss: 1.022892\n",
            "Step 2480: Minibatch Loss: 1.073309\n",
            "Step 2482: Minibatch Loss: 1.053533\n",
            "Step 2484: Minibatch Loss: 1.072539\n",
            "Step 2486: Minibatch Loss: 1.041487\n",
            "Step 2488: Minibatch Loss: 1.069878\n",
            "Step 2490: Minibatch Loss: 1.058421\n",
            "Step 2492: Minibatch Loss: 1.090790\n",
            "Step 2494: Minibatch Loss: 1.022687\n",
            "Step 2496: Minibatch Loss: 1.121917\n",
            "Step 2498: Minibatch Loss: 1.052408\n",
            "Step 2500: Minibatch Loss: 1.033528\n",
            "Step 2502: Minibatch Loss: 1.025119\n",
            "Step 2504: Minibatch Loss: 1.025529\n",
            "Step 2506: Minibatch Loss: 1.029895\n",
            "Step 2508: Minibatch Loss: 1.025179\n",
            "Step 2510: Minibatch Loss: 1.092729\n",
            "Step 2512: Minibatch Loss: 1.093939\n",
            "Step 2514: Minibatch Loss: 1.110977\n",
            "Step 2516: Minibatch Loss: 1.078569\n",
            "Step 2518: Minibatch Loss: 1.023834\n",
            "Step 2520: Minibatch Loss: 1.028036\n",
            "Step 2522: Minibatch Loss: 1.013524\n",
            "Step 2524: Minibatch Loss: 1.037112\n",
            "Step 2526: Minibatch Loss: 1.029959\n",
            "Step 2528: Minibatch Loss: 1.065168\n",
            "Step 2530: Minibatch Loss: 1.075545\n",
            "Step 2532: Minibatch Loss: 1.060550\n",
            "Step 2534: Minibatch Loss: 1.024213\n",
            "Step 2536: Minibatch Loss: 1.080452\n",
            "Step 2538: Minibatch Loss: 1.091460\n",
            "Step 2540: Minibatch Loss: 1.120138\n",
            "Step 2542: Minibatch Loss: 1.038523\n",
            "Step 2544: Minibatch Loss: 1.071608\n",
            "Step 2546: Minibatch Loss: 1.068972\n",
            "Step 2548: Minibatch Loss: 1.084548\n",
            "Step 2550: Minibatch Loss: 1.046138\n",
            "Step 2552: Minibatch Loss: 1.036955\n",
            "Step 2554: Minibatch Loss: 1.022084\n",
            "Step 2556: Minibatch Loss: 1.018243\n",
            "Step 2558: Minibatch Loss: 1.027509\n",
            "Step 2560: Minibatch Loss: 1.079970\n",
            "Step 2562: Minibatch Loss: 1.029110\n",
            "Step 2564: Minibatch Loss: 1.129021\n",
            "Step 2566: Minibatch Loss: 1.067633\n",
            "Step 2568: Minibatch Loss: 1.064629\n",
            "Step 2570: Minibatch Loss: 1.067065\n",
            "Step 2572: Minibatch Loss: 1.098385\n",
            "Step 2574: Minibatch Loss: 1.048615\n",
            "Step 2576: Minibatch Loss: 1.074373\n",
            "Step 2578: Minibatch Loss: 1.073220\n",
            "Step 2580: Minibatch Loss: 1.066734\n",
            "Step 2582: Minibatch Loss: 1.066110\n",
            "Step 2584: Minibatch Loss: 1.015904\n",
            "Step 2586: Minibatch Loss: 1.020155\n",
            "Step 2588: Minibatch Loss: 1.026916\n",
            "Step 2590: Minibatch Loss: 1.011963\n",
            "Step 2592: Minibatch Loss: 1.017441\n",
            "Step 2594: Minibatch Loss: 1.029178\n",
            "Step 2596: Minibatch Loss: 1.032002\n",
            "Step 2598: Minibatch Loss: 1.031803\n",
            "Step 2600: Minibatch Loss: 1.060442\n",
            "Training for SNR= 6.5  sigma= 0.47315125896148047 iteratin: 0\n",
            "Step 2602: Minibatch Loss: 0.990337\n",
            "Step 2604: Minibatch Loss: 1.027300\n",
            "Step 2606: Minibatch Loss: 0.988552\n",
            "Step 2608: Minibatch Loss: 0.962140\n",
            "Step 2610: Minibatch Loss: 0.978330\n",
            "Step 2612: Minibatch Loss: 1.094393\n",
            "Step 2614: Minibatch Loss: 0.984092\n",
            "Step 2616: Minibatch Loss: 1.000290\n",
            "Step 2618: Minibatch Loss: 0.981674\n",
            "Step 2620: Minibatch Loss: 1.026879\n",
            "Step 2622: Minibatch Loss: 0.950808\n",
            "Step 2624: Minibatch Loss: 0.986457\n",
            "Step 2626: Minibatch Loss: 1.021903\n",
            "Step 2628: Minibatch Loss: 0.985105\n",
            "Step 2630: Minibatch Loss: 0.965255\n",
            "Step 2632: Minibatch Loss: 0.980368\n",
            "Step 2634: Minibatch Loss: 0.971560\n",
            "Step 2636: Minibatch Loss: 1.002449\n",
            "Step 2638: Minibatch Loss: 0.980172\n",
            "Step 2640: Minibatch Loss: 1.004761\n",
            "Step 2642: Minibatch Loss: 1.070381\n",
            "Step 2644: Minibatch Loss: 0.993074\n",
            "Step 2646: Minibatch Loss: 1.059375\n",
            "Step 2648: Minibatch Loss: 0.982538\n",
            "Step 2650: Minibatch Loss: 0.942800\n",
            "Step 2652: Minibatch Loss: 1.014694\n",
            "Step 2654: Minibatch Loss: 1.056293\n",
            "Step 2656: Minibatch Loss: 1.007476\n",
            "Step 2658: Minibatch Loss: 0.992817\n",
            "Step 2660: Minibatch Loss: 1.020528\n",
            "Step 2662: Minibatch Loss: 1.034510\n",
            "Step 2664: Minibatch Loss: 0.980748\n",
            "Step 2666: Minibatch Loss: 0.955957\n",
            "Step 2668: Minibatch Loss: 1.013744\n",
            "Step 2670: Minibatch Loss: 1.009761\n",
            "Step 2672: Minibatch Loss: 0.971800\n",
            "Step 2674: Minibatch Loss: 0.988431\n",
            "Step 2676: Minibatch Loss: 1.010276\n",
            "Step 2678: Minibatch Loss: 0.983660\n",
            "Step 2680: Minibatch Loss: 1.031204\n",
            "Step 2682: Minibatch Loss: 0.939651\n",
            "Step 2684: Minibatch Loss: 1.032223\n",
            "Step 2686: Minibatch Loss: 0.990303\n",
            "Step 2688: Minibatch Loss: 0.957811\n",
            "Step 2690: Minibatch Loss: 0.967890\n",
            "Step 2692: Minibatch Loss: 1.005306\n",
            "Step 2694: Minibatch Loss: 0.962892\n",
            "Step 2696: Minibatch Loss: 1.002462\n",
            "Step 2698: Minibatch Loss: 0.936781\n",
            "Step 2700: Minibatch Loss: 1.044237\n",
            "Step 2702: Minibatch Loss: 1.020219\n",
            "Step 2704: Minibatch Loss: 1.023053\n",
            "Step 2706: Minibatch Loss: 1.025414\n",
            "Step 2708: Minibatch Loss: 0.958401\n",
            "Step 2710: Minibatch Loss: 0.989016\n",
            "Step 2712: Minibatch Loss: 1.052228\n",
            "Step 2714: Minibatch Loss: 1.019446\n",
            "Step 2716: Minibatch Loss: 0.987107\n",
            "Step 2718: Minibatch Loss: 1.059296\n",
            "Step 2720: Minibatch Loss: 0.980487\n",
            "Step 2722: Minibatch Loss: 1.020714\n",
            "Step 2724: Minibatch Loss: 1.030853\n",
            "Step 2726: Minibatch Loss: 0.995582\n",
            "Step 2728: Minibatch Loss: 0.999023\n",
            "Step 2730: Minibatch Loss: 1.001545\n",
            "Step 2732: Minibatch Loss: 0.985336\n",
            "Step 2734: Minibatch Loss: 0.977706\n",
            "Step 2736: Minibatch Loss: 1.009717\n",
            "Step 2738: Minibatch Loss: 1.064836\n",
            "Step 2740: Minibatch Loss: 0.992646\n",
            "Step 2742: Minibatch Loss: 0.974114\n",
            "Step 2744: Minibatch Loss: 0.961149\n",
            "Step 2746: Minibatch Loss: 1.021593\n",
            "Step 2748: Minibatch Loss: 0.989924\n",
            "Step 2750: Minibatch Loss: 0.992022\n",
            "Step 2752: Minibatch Loss: 0.986414\n",
            "Step 2754: Minibatch Loss: 1.007589\n",
            "Step 2756: Minibatch Loss: 0.987077\n",
            "Step 2758: Minibatch Loss: 0.959957\n",
            "Step 2760: Minibatch Loss: 0.947584\n",
            "Step 2762: Minibatch Loss: 1.025805\n",
            "Step 2764: Minibatch Loss: 1.018005\n",
            "Step 2766: Minibatch Loss: 1.013921\n",
            "Step 2768: Minibatch Loss: 0.973957\n",
            "Step 2770: Minibatch Loss: 0.979289\n",
            "Step 2772: Minibatch Loss: 1.033122\n",
            "Step 2774: Minibatch Loss: 0.946114\n",
            "Step 2776: Minibatch Loss: 0.923669\n",
            "Step 2778: Minibatch Loss: 0.973713\n",
            "Step 2780: Minibatch Loss: 1.008174\n",
            "Step 2782: Minibatch Loss: 0.997409\n",
            "Step 2784: Minibatch Loss: 1.020842\n",
            "Step 2786: Minibatch Loss: 0.969354\n",
            "Step 2788: Minibatch Loss: 0.975512\n",
            "Step 2790: Minibatch Loss: 0.988389\n",
            "Step 2792: Minibatch Loss: 0.992479\n",
            "Step 2794: Minibatch Loss: 1.049864\n",
            "Step 2796: Minibatch Loss: 1.010128\n",
            "Step 2798: Minibatch Loss: 1.005375\n",
            "Step 2800: Minibatch Loss: 1.037619\n",
            "Training for SNR= 7.0  sigma= 0.44668359215096315 iteratin: 0\n",
            "Step 2802: Minibatch Loss: 0.966086\n",
            "Step 2804: Minibatch Loss: 0.998331\n",
            "Step 2806: Minibatch Loss: 0.971443\n",
            "Step 2808: Minibatch Loss: 0.883650\n",
            "Step 2810: Minibatch Loss: 0.965239\n",
            "Step 2812: Minibatch Loss: 0.916798\n",
            "Step 2814: Minibatch Loss: 0.970500\n",
            "Step 2816: Minibatch Loss: 0.897561\n",
            "Step 2818: Minibatch Loss: 0.967009\n",
            "Step 2820: Minibatch Loss: 0.951531\n",
            "Step 2822: Minibatch Loss: 0.901972\n",
            "Step 2824: Minibatch Loss: 0.968238\n",
            "Step 2826: Minibatch Loss: 0.965071\n",
            "Step 2828: Minibatch Loss: 0.962292\n",
            "Step 2830: Minibatch Loss: 0.938607\n",
            "Step 2832: Minibatch Loss: 0.963024\n",
            "Step 2834: Minibatch Loss: 0.954640\n",
            "Step 2836: Minibatch Loss: 0.928340\n",
            "Step 2838: Minibatch Loss: 0.937774\n",
            "Step 2840: Minibatch Loss: 0.974535\n",
            "Step 2842: Minibatch Loss: 0.953045\n",
            "Step 2844: Minibatch Loss: 0.903632\n",
            "Step 2846: Minibatch Loss: 0.931960\n",
            "Step 2848: Minibatch Loss: 0.953489\n",
            "Step 2850: Minibatch Loss: 0.953102\n",
            "Step 2852: Minibatch Loss: 0.992465\n",
            "Step 2854: Minibatch Loss: 0.845584\n",
            "Step 2856: Minibatch Loss: 0.868447\n",
            "Step 2858: Minibatch Loss: 0.909197\n",
            "Step 2860: Minibatch Loss: 0.909969\n",
            "Step 2862: Minibatch Loss: 0.955469\n",
            "Step 2864: Minibatch Loss: 0.951816\n",
            "Step 2866: Minibatch Loss: 0.973323\n",
            "Step 2868: Minibatch Loss: 0.887991\n",
            "Step 2870: Minibatch Loss: 0.959186\n",
            "Step 2872: Minibatch Loss: 0.964441\n",
            "Step 2874: Minibatch Loss: 0.949503\n",
            "Step 2876: Minibatch Loss: 0.911866\n",
            "Step 2878: Minibatch Loss: 0.923270\n",
            "Step 2880: Minibatch Loss: 0.913173\n",
            "Step 2882: Minibatch Loss: 0.918948\n",
            "Step 2884: Minibatch Loss: 0.965896\n",
            "Step 2886: Minibatch Loss: 0.916281\n",
            "Step 2888: Minibatch Loss: 0.952593\n",
            "Step 2890: Minibatch Loss: 0.925611\n",
            "Step 2892: Minibatch Loss: 0.898125\n",
            "Step 2894: Minibatch Loss: 0.923871\n",
            "Step 2896: Minibatch Loss: 0.928805\n",
            "Step 2898: Minibatch Loss: 0.941417\n",
            "Step 2900: Minibatch Loss: 0.900887\n",
            "Step 2902: Minibatch Loss: 0.930166\n",
            "Step 2904: Minibatch Loss: 0.939391\n",
            "Step 2906: Minibatch Loss: 0.908076\n",
            "Step 2908: Minibatch Loss: 0.957301\n",
            "Step 2910: Minibatch Loss: 0.936194\n",
            "Step 2912: Minibatch Loss: 0.964443\n",
            "Step 2914: Minibatch Loss: 0.919122\n",
            "Step 2916: Minibatch Loss: 0.887184\n",
            "Step 2918: Minibatch Loss: 0.966306\n",
            "Step 2920: Minibatch Loss: 0.930535\n",
            "Step 2922: Minibatch Loss: 0.978357\n",
            "Step 2924: Minibatch Loss: 0.910153\n",
            "Step 2926: Minibatch Loss: 0.918062\n",
            "Step 2928: Minibatch Loss: 0.915833\n",
            "Step 2930: Minibatch Loss: 0.956653\n",
            "Step 2932: Minibatch Loss: 0.933823\n",
            "Step 2934: Minibatch Loss: 0.944180\n",
            "Step 2936: Minibatch Loss: 0.920654\n",
            "Step 2938: Minibatch Loss: 0.932373\n",
            "Step 2940: Minibatch Loss: 0.999005\n",
            "Step 2942: Minibatch Loss: 0.892650\n",
            "Step 2944: Minibatch Loss: 0.936507\n",
            "Step 2946: Minibatch Loss: 0.941074\n",
            "Step 2948: Minibatch Loss: 0.931086\n",
            "Step 2950: Minibatch Loss: 0.947438\n",
            "Step 2952: Minibatch Loss: 0.939467\n",
            "Step 2954: Minibatch Loss: 0.942231\n",
            "Step 2956: Minibatch Loss: 0.955620\n",
            "Step 2958: Minibatch Loss: 0.942116\n",
            "Step 2960: Minibatch Loss: 0.933564\n",
            "Step 2962: Minibatch Loss: 0.939193\n",
            "Step 2964: Minibatch Loss: 0.946472\n",
            "Step 2966: Minibatch Loss: 0.940145\n",
            "Step 2968: Minibatch Loss: 0.921261\n",
            "Step 2970: Minibatch Loss: 0.895439\n",
            "Step 2972: Minibatch Loss: 0.932532\n",
            "Step 2974: Minibatch Loss: 0.957845\n",
            "Step 2976: Minibatch Loss: 0.929657\n",
            "Step 2978: Minibatch Loss: 0.925736\n",
            "Step 2980: Minibatch Loss: 0.951345\n",
            "Step 2982: Minibatch Loss: 0.955720\n",
            "Step 2984: Minibatch Loss: 0.945508\n",
            "Step 2986: Minibatch Loss: 0.878217\n",
            "Step 2988: Minibatch Loss: 0.971828\n",
            "Step 2990: Minibatch Loss: 1.015955\n",
            "Step 2992: Minibatch Loss: 0.937741\n",
            "Step 2994: Minibatch Loss: 0.946931\n",
            "Step 2996: Minibatch Loss: 0.913348\n",
            "Step 2998: Minibatch Loss: 0.961001\n",
            "Step 3000: Minibatch Loss: 0.927845\n",
            "Training for SNR= 7.5  sigma= 0.4216965034285822 iteratin: 0\n",
            "Step 3002: Minibatch Loss: 0.861304\n",
            "Step 3004: Minibatch Loss: 0.847225\n",
            "Step 3006: Minibatch Loss: 0.845896\n",
            "Step 3008: Minibatch Loss: 0.871372\n",
            "Step 3010: Minibatch Loss: 0.914741\n",
            "Step 3012: Minibatch Loss: 0.888470\n",
            "Step 3014: Minibatch Loss: 0.875874\n",
            "Step 3016: Minibatch Loss: 0.897066\n",
            "Step 3018: Minibatch Loss: 0.878171\n",
            "Step 3020: Minibatch Loss: 0.891547\n",
            "Step 3022: Minibatch Loss: 0.890462\n",
            "Step 3024: Minibatch Loss: 0.880771\n",
            "Step 3026: Minibatch Loss: 0.853502\n",
            "Step 3028: Minibatch Loss: 0.936778\n",
            "Step 3030: Minibatch Loss: 0.857195\n",
            "Step 3032: Minibatch Loss: 0.861256\n",
            "Step 3034: Minibatch Loss: 0.858765\n",
            "Step 3036: Minibatch Loss: 0.864789\n",
            "Step 3038: Minibatch Loss: 0.815788\n",
            "Step 3040: Minibatch Loss: 0.843010\n",
            "Step 3042: Minibatch Loss: 0.872800\n",
            "Step 3044: Minibatch Loss: 0.861374\n",
            "Step 3046: Minibatch Loss: 0.847744\n",
            "Step 3048: Minibatch Loss: 0.875043\n",
            "Step 3050: Minibatch Loss: 0.855449\n",
            "Step 3052: Minibatch Loss: 0.827404\n",
            "Step 3054: Minibatch Loss: 0.846894\n",
            "Step 3056: Minibatch Loss: 0.942347\n",
            "Step 3058: Minibatch Loss: 0.887938\n",
            "Step 3060: Minibatch Loss: 0.842493\n",
            "Step 3062: Minibatch Loss: 0.885520\n",
            "Step 3064: Minibatch Loss: 0.848964\n",
            "Step 3066: Minibatch Loss: 0.865537\n",
            "Step 3068: Minibatch Loss: 0.873180\n",
            "Step 3070: Minibatch Loss: 0.869619\n",
            "Step 3072: Minibatch Loss: 0.843298\n",
            "Step 3074: Minibatch Loss: 0.888203\n",
            "Step 3076: Minibatch Loss: 0.833502\n",
            "Step 3078: Minibatch Loss: 0.890854\n",
            "Step 3080: Minibatch Loss: 0.911203\n",
            "Step 3082: Minibatch Loss: 0.894792\n",
            "Step 3084: Minibatch Loss: 0.871611\n",
            "Step 3086: Minibatch Loss: 0.849998\n",
            "Step 3088: Minibatch Loss: 0.881318\n",
            "Step 3090: Minibatch Loss: 0.909546\n",
            "Step 3092: Minibatch Loss: 0.845051\n",
            "Step 3094: Minibatch Loss: 0.919917\n",
            "Step 3096: Minibatch Loss: 0.798480\n",
            "Step 3098: Minibatch Loss: 0.848864\n",
            "Step 3100: Minibatch Loss: 0.869521\n",
            "Step 3102: Minibatch Loss: 0.904530\n",
            "Step 3104: Minibatch Loss: 0.804870\n",
            "Step 3106: Minibatch Loss: 0.884522\n",
            "Step 3108: Minibatch Loss: 0.911391\n",
            "Step 3110: Minibatch Loss: 0.894622\n",
            "Step 3112: Minibatch Loss: 0.912456\n",
            "Step 3114: Minibatch Loss: 0.864780\n",
            "Step 3116: Minibatch Loss: 0.826499\n",
            "Step 3118: Minibatch Loss: 0.942315\n",
            "Step 3120: Minibatch Loss: 0.831955\n",
            "Step 3122: Minibatch Loss: 0.886709\n",
            "Step 3124: Minibatch Loss: 0.885818\n",
            "Step 3126: Minibatch Loss: 0.830452\n",
            "Step 3128: Minibatch Loss: 0.853492\n",
            "Step 3130: Minibatch Loss: 0.892733\n",
            "Step 3132: Minibatch Loss: 0.887237\n",
            "Step 3134: Minibatch Loss: 0.816184\n",
            "Step 3136: Minibatch Loss: 0.861882\n",
            "Step 3138: Minibatch Loss: 0.857766\n",
            "Step 3140: Minibatch Loss: 0.884222\n",
            "Step 3142: Minibatch Loss: 0.877202\n",
            "Step 3144: Minibatch Loss: 0.913154\n",
            "Step 3146: Minibatch Loss: 0.903930\n",
            "Step 3148: Minibatch Loss: 0.784054\n",
            "Step 3150: Minibatch Loss: 0.890613\n",
            "Step 3152: Minibatch Loss: 0.838248\n",
            "Step 3154: Minibatch Loss: 0.840722\n",
            "Step 3156: Minibatch Loss: 0.886274\n",
            "Step 3158: Minibatch Loss: 0.873746\n",
            "Step 3160: Minibatch Loss: 0.886970\n",
            "Step 3162: Minibatch Loss: 0.852675\n",
            "Step 3164: Minibatch Loss: 0.832794\n",
            "Step 3166: Minibatch Loss: 0.858829\n",
            "Step 3168: Minibatch Loss: 0.876900\n",
            "Step 3170: Minibatch Loss: 0.916957\n",
            "Step 3172: Minibatch Loss: 0.902938\n",
            "Step 3174: Minibatch Loss: 0.864386\n",
            "Step 3176: Minibatch Loss: 0.874026\n",
            "Step 3178: Minibatch Loss: 0.872760\n",
            "Step 3180: Minibatch Loss: 0.888822\n",
            "Step 3182: Minibatch Loss: 0.877800\n",
            "Step 3184: Minibatch Loss: 0.947975\n",
            "Step 3186: Minibatch Loss: 0.876219\n",
            "Step 3188: Minibatch Loss: 0.869681\n",
            "Step 3190: Minibatch Loss: 0.844220\n",
            "Step 3192: Minibatch Loss: 0.881636\n",
            "Step 3194: Minibatch Loss: 0.916628\n",
            "Step 3196: Minibatch Loss: 0.901088\n",
            "Step 3198: Minibatch Loss: 0.886004\n",
            "Step 3200: Minibatch Loss: 0.917383\n",
            "Training for SNR= 8.0  sigma= 0.3981071705534972 iteratin: 0\n",
            "Step 3202: Minibatch Loss: 0.787867\n",
            "Step 3204: Minibatch Loss: 0.802246\n",
            "Step 3206: Minibatch Loss: 0.789416\n",
            "Step 3208: Minibatch Loss: 0.828166\n",
            "Step 3210: Minibatch Loss: 0.837789\n",
            "Step 3212: Minibatch Loss: 0.752250\n",
            "Step 3214: Minibatch Loss: 0.840915\n",
            "Step 3216: Minibatch Loss: 0.822095\n",
            "Step 3218: Minibatch Loss: 0.839834\n",
            "Step 3220: Minibatch Loss: 0.805677\n",
            "Step 3222: Minibatch Loss: 0.804885\n",
            "Step 3224: Minibatch Loss: 0.846904\n",
            "Step 3226: Minibatch Loss: 0.838118\n",
            "Step 3228: Minibatch Loss: 0.801540\n",
            "Step 3230: Minibatch Loss: 0.795332\n",
            "Step 3232: Minibatch Loss: 0.797953\n",
            "Step 3234: Minibatch Loss: 0.777775\n",
            "Step 3236: Minibatch Loss: 0.797620\n",
            "Step 3238: Minibatch Loss: 0.819732\n",
            "Step 3240: Minibatch Loss: 0.861083\n",
            "Step 3242: Minibatch Loss: 0.786605\n",
            "Step 3244: Minibatch Loss: 0.862480\n",
            "Step 3246: Minibatch Loss: 0.876792\n",
            "Step 3248: Minibatch Loss: 0.783866\n",
            "Step 3250: Minibatch Loss: 0.858507\n",
            "Step 3252: Minibatch Loss: 0.782541\n",
            "Step 3254: Minibatch Loss: 0.812850\n",
            "Step 3256: Minibatch Loss: 0.852744\n",
            "Step 3258: Minibatch Loss: 0.798744\n",
            "Step 3260: Minibatch Loss: 0.782907\n",
            "Step 3262: Minibatch Loss: 0.823315\n",
            "Step 3264: Minibatch Loss: 0.766638\n",
            "Step 3266: Minibatch Loss: 0.840777\n",
            "Step 3268: Minibatch Loss: 0.796915\n",
            "Step 3270: Minibatch Loss: 0.848194\n",
            "Step 3272: Minibatch Loss: 0.778193\n",
            "Step 3274: Minibatch Loss: 0.836926\n",
            "Step 3276: Minibatch Loss: 0.817628\n",
            "Step 3278: Minibatch Loss: 0.787285\n",
            "Step 3280: Minibatch Loss: 0.789436\n",
            "Step 3282: Minibatch Loss: 0.811014\n",
            "Step 3284: Minibatch Loss: 0.820200\n",
            "Step 3286: Minibatch Loss: 0.758570\n",
            "Step 3288: Minibatch Loss: 0.803876\n",
            "Step 3290: Minibatch Loss: 0.812016\n",
            "Step 3292: Minibatch Loss: 0.790071\n",
            "Step 3294: Minibatch Loss: 0.855599\n",
            "Step 3296: Minibatch Loss: 0.836176\n",
            "Step 3298: Minibatch Loss: 0.781168\n",
            "Step 3300: Minibatch Loss: 0.833443\n",
            "Step 3302: Minibatch Loss: 0.789682\n",
            "Step 3304: Minibatch Loss: 0.745593\n",
            "Step 3306: Minibatch Loss: 0.841103\n",
            "Step 3308: Minibatch Loss: 0.774823\n",
            "Step 3310: Minibatch Loss: 0.858415\n",
            "Step 3312: Minibatch Loss: 0.768969\n",
            "Step 3314: Minibatch Loss: 0.759611\n",
            "Step 3316: Minibatch Loss: 0.840067\n",
            "Step 3318: Minibatch Loss: 0.821495\n",
            "Step 3320: Minibatch Loss: 0.848539\n",
            "Step 3322: Minibatch Loss: 0.747412\n",
            "Step 3324: Minibatch Loss: 0.762825\n",
            "Step 3326: Minibatch Loss: 0.796977\n",
            "Step 3328: Minibatch Loss: 0.868213\n",
            "Step 3330: Minibatch Loss: 0.825166\n",
            "Step 3332: Minibatch Loss: 0.822889\n",
            "Step 3334: Minibatch Loss: 0.797280\n",
            "Step 3336: Minibatch Loss: 0.845277\n",
            "Step 3338: Minibatch Loss: 0.785294\n",
            "Step 3340: Minibatch Loss: 0.833710\n",
            "Step 3342: Minibatch Loss: 0.804789\n",
            "Step 3344: Minibatch Loss: 0.790644\n",
            "Step 3346: Minibatch Loss: 0.793195\n",
            "Step 3348: Minibatch Loss: 0.745262\n",
            "Step 3350: Minibatch Loss: 0.852306\n",
            "Step 3352: Minibatch Loss: 0.752010\n",
            "Step 3354: Minibatch Loss: 0.816944\n",
            "Step 3356: Minibatch Loss: 0.814254\n",
            "Step 3358: Minibatch Loss: 0.837558\n",
            "Step 3360: Minibatch Loss: 0.819404\n",
            "Step 3362: Minibatch Loss: 0.788971\n",
            "Step 3364: Minibatch Loss: 0.796851\n",
            "Step 3366: Minibatch Loss: 0.843012\n",
            "Step 3368: Minibatch Loss: 0.800191\n",
            "Step 3370: Minibatch Loss: 0.835065\n",
            "Step 3372: Minibatch Loss: 0.796110\n",
            "Step 3374: Minibatch Loss: 0.825361\n",
            "Step 3376: Minibatch Loss: 0.807145\n",
            "Step 3378: Minibatch Loss: 0.823511\n",
            "Step 3380: Minibatch Loss: 0.828008\n",
            "Step 3382: Minibatch Loss: 0.798648\n",
            "Step 3384: Minibatch Loss: 0.757578\n",
            "Step 3386: Minibatch Loss: 0.809405\n",
            "Step 3388: Minibatch Loss: 0.817057\n",
            "Step 3390: Minibatch Loss: 0.820394\n",
            "Step 3392: Minibatch Loss: 0.780465\n",
            "Step 3394: Minibatch Loss: 0.763437\n",
            "Step 3396: Minibatch Loss: 0.748112\n",
            "Step 3398: Minibatch Loss: 0.829114\n",
            "Step 3400: Minibatch Loss: 0.826820\n",
            "Training for SNR= 8.5  sigma= 0.3758374042884442 iteratin: 0\n",
            "Step 3402: Minibatch Loss: 0.746490\n",
            "Step 3404: Minibatch Loss: 0.795665\n",
            "Step 3406: Minibatch Loss: 0.738425\n",
            "Step 3408: Minibatch Loss: 0.754826\n",
            "Step 3410: Minibatch Loss: 0.738328\n",
            "Step 3412: Minibatch Loss: 0.746128\n",
            "Step 3414: Minibatch Loss: 0.707036\n",
            "Step 3416: Minibatch Loss: 0.730948\n",
            "Step 3418: Minibatch Loss: 0.753343\n",
            "Step 3420: Minibatch Loss: 0.729190\n",
            "Step 3422: Minibatch Loss: 0.786586\n",
            "Step 3424: Minibatch Loss: 0.764521\n",
            "Step 3426: Minibatch Loss: 0.702882\n",
            "Step 3428: Minibatch Loss: 0.745034\n",
            "Step 3430: Minibatch Loss: 0.773671\n",
            "Step 3432: Minibatch Loss: 0.766101\n",
            "Step 3434: Minibatch Loss: 0.786109\n",
            "Step 3436: Minibatch Loss: 0.719256\n",
            "Step 3438: Minibatch Loss: 0.742332\n",
            "Step 3440: Minibatch Loss: 0.802501\n",
            "Step 3442: Minibatch Loss: 0.782593\n",
            "Step 3444: Minibatch Loss: 0.780378\n",
            "Step 3446: Minibatch Loss: 0.752529\n",
            "Step 3448: Minibatch Loss: 0.788586\n",
            "Step 3450: Minibatch Loss: 0.730333\n",
            "Step 3452: Minibatch Loss: 0.772854\n",
            "Step 3454: Minibatch Loss: 0.725414\n",
            "Step 3456: Minibatch Loss: 0.789047\n",
            "Step 3458: Minibatch Loss: 0.778947\n",
            "Step 3460: Minibatch Loss: 0.798061\n",
            "Step 3462: Minibatch Loss: 0.697710\n",
            "Step 3464: Minibatch Loss: 0.734345\n",
            "Step 3466: Minibatch Loss: 0.754208\n",
            "Step 3468: Minibatch Loss: 0.722796\n",
            "Step 3470: Minibatch Loss: 0.741440\n",
            "Step 3472: Minibatch Loss: 0.749003\n",
            "Step 3474: Minibatch Loss: 0.720736\n",
            "Step 3476: Minibatch Loss: 0.742621\n",
            "Step 3478: Minibatch Loss: 0.759255\n",
            "Step 3480: Minibatch Loss: 0.729917\n",
            "Step 3482: Minibatch Loss: 0.755392\n",
            "Step 3484: Minibatch Loss: 0.762980\n",
            "Step 3486: Minibatch Loss: 0.755801\n",
            "Step 3488: Minibatch Loss: 0.715151\n",
            "Step 3490: Minibatch Loss: 0.727649\n",
            "Step 3492: Minibatch Loss: 0.715443\n",
            "Step 3494: Minibatch Loss: 0.725389\n",
            "Step 3496: Minibatch Loss: 0.772552\n",
            "Step 3498: Minibatch Loss: 0.768180\n",
            "Step 3500: Minibatch Loss: 0.768434\n",
            "Step 3502: Minibatch Loss: 0.749749\n",
            "Step 3504: Minibatch Loss: 0.757437\n",
            "Step 3506: Minibatch Loss: 0.734886\n",
            "Step 3508: Minibatch Loss: 0.750450\n",
            "Step 3510: Minibatch Loss: 0.747341\n",
            "Step 3512: Minibatch Loss: 0.799049\n",
            "Step 3514: Minibatch Loss: 0.723358\n",
            "Step 3516: Minibatch Loss: 0.699729\n",
            "Step 3518: Minibatch Loss: 0.732730\n",
            "Step 3520: Minibatch Loss: 0.766292\n",
            "Step 3522: Minibatch Loss: 0.731997\n",
            "Step 3524: Minibatch Loss: 0.725253\n",
            "Step 3526: Minibatch Loss: 0.773989\n",
            "Step 3528: Minibatch Loss: 0.721556\n",
            "Step 3530: Minibatch Loss: 0.768225\n",
            "Step 3532: Minibatch Loss: 0.764559\n",
            "Step 3534: Minibatch Loss: 0.730721\n",
            "Step 3536: Minibatch Loss: 0.706655\n",
            "Step 3538: Minibatch Loss: 0.732828\n",
            "Step 3540: Minibatch Loss: 0.758945\n",
            "Step 3542: Minibatch Loss: 0.781633\n",
            "Step 3544: Minibatch Loss: 0.787050\n",
            "Step 3546: Minibatch Loss: 0.753317\n",
            "Step 3548: Minibatch Loss: 0.732099\n",
            "Step 3550: Minibatch Loss: 0.802120\n",
            "Step 3552: Minibatch Loss: 0.753600\n",
            "Step 3554: Minibatch Loss: 0.750538\n",
            "Step 3556: Minibatch Loss: 0.778735\n",
            "Step 3558: Minibatch Loss: 0.687333\n",
            "Step 3560: Minibatch Loss: 0.775514\n",
            "Step 3562: Minibatch Loss: 0.731850\n",
            "Step 3564: Minibatch Loss: 0.764902\n",
            "Step 3566: Minibatch Loss: 0.728285\n",
            "Step 3568: Minibatch Loss: 0.737105\n",
            "Step 3570: Minibatch Loss: 0.690547\n",
            "Step 3572: Minibatch Loss: 0.754993\n",
            "Step 3574: Minibatch Loss: 0.721276\n",
            "Step 3576: Minibatch Loss: 0.798660\n",
            "Step 3578: Minibatch Loss: 0.740469\n",
            "Step 3580: Minibatch Loss: 0.732121\n",
            "Step 3582: Minibatch Loss: 0.813054\n",
            "Step 3584: Minibatch Loss: 0.779372\n",
            "Step 3586: Minibatch Loss: 0.764143\n",
            "Step 3588: Minibatch Loss: 0.716822\n",
            "Step 3590: Minibatch Loss: 0.723103\n",
            "Step 3592: Minibatch Loss: 0.754912\n",
            "Step 3594: Minibatch Loss: 0.702990\n",
            "Step 3596: Minibatch Loss: 0.748054\n",
            "Step 3598: Minibatch Loss: 0.758436\n",
            "Step 3600: Minibatch Loss: 0.776458\n",
            "Training for SNR= 9.0  sigma= 0.35481338923357547 iteratin: 0\n",
            "Step 3602: Minibatch Loss: 0.720114\n",
            "Step 3604: Minibatch Loss: 0.752597\n",
            "Step 3606: Minibatch Loss: 0.698512\n",
            "Step 3608: Minibatch Loss: 0.642080\n",
            "Step 3610: Minibatch Loss: 0.694848\n",
            "Step 3612: Minibatch Loss: 0.689644\n",
            "Step 3614: Minibatch Loss: 0.696187\n",
            "Step 3616: Minibatch Loss: 0.688289\n",
            "Step 3618: Minibatch Loss: 0.703871\n",
            "Step 3620: Minibatch Loss: 0.710446\n",
            "Step 3622: Minibatch Loss: 0.718284\n",
            "Step 3624: Minibatch Loss: 0.693365\n",
            "Step 3626: Minibatch Loss: 0.722132\n",
            "Step 3628: Minibatch Loss: 0.699252\n",
            "Step 3630: Minibatch Loss: 0.703359\n",
            "Step 3632: Minibatch Loss: 0.701030\n",
            "Step 3634: Minibatch Loss: 0.714744\n",
            "Step 3636: Minibatch Loss: 0.689717\n",
            "Step 3638: Minibatch Loss: 0.652640\n",
            "Step 3640: Minibatch Loss: 0.664356\n",
            "Step 3642: Minibatch Loss: 0.720849\n",
            "Step 3644: Minibatch Loss: 0.689789\n",
            "Step 3646: Minibatch Loss: 0.704376\n",
            "Step 3648: Minibatch Loss: 0.665138\n",
            "Step 3650: Minibatch Loss: 0.708313\n",
            "Step 3652: Minibatch Loss: 0.701504\n",
            "Step 3654: Minibatch Loss: 0.678887\n",
            "Step 3656: Minibatch Loss: 0.687794\n",
            "Step 3658: Minibatch Loss: 0.684727\n",
            "Step 3660: Minibatch Loss: 0.717314\n",
            "Step 3662: Minibatch Loss: 0.723705\n",
            "Step 3664: Minibatch Loss: 0.685397\n",
            "Step 3666: Minibatch Loss: 0.721726\n",
            "Step 3668: Minibatch Loss: 0.675861\n",
            "Step 3670: Minibatch Loss: 0.702072\n",
            "Step 3672: Minibatch Loss: 0.676526\n",
            "Step 3674: Minibatch Loss: 0.707714\n",
            "Step 3676: Minibatch Loss: 0.740034\n",
            "Step 3678: Minibatch Loss: 0.688657\n",
            "Step 3680: Minibatch Loss: 0.703355\n",
            "Step 3682: Minibatch Loss: 0.669252\n",
            "Step 3684: Minibatch Loss: 0.714549\n",
            "Step 3686: Minibatch Loss: 0.738495\n",
            "Step 3688: Minibatch Loss: 0.664190\n",
            "Step 3690: Minibatch Loss: 0.728851\n",
            "Step 3692: Minibatch Loss: 0.690407\n",
            "Step 3694: Minibatch Loss: 0.619508\n",
            "Step 3696: Minibatch Loss: 0.658915\n",
            "Step 3698: Minibatch Loss: 0.661411\n",
            "Step 3700: Minibatch Loss: 0.617011\n",
            "Step 3702: Minibatch Loss: 0.669423\n",
            "Step 3704: Minibatch Loss: 0.702085\n",
            "Step 3706: Minibatch Loss: 0.719829\n",
            "Step 3708: Minibatch Loss: 0.705535\n",
            "Step 3710: Minibatch Loss: 0.702282\n",
            "Step 3712: Minibatch Loss: 0.697702\n",
            "Step 3714: Minibatch Loss: 0.695458\n",
            "Step 3716: Minibatch Loss: 0.715890\n",
            "Step 3718: Minibatch Loss: 0.726391\n",
            "Step 3720: Minibatch Loss: 0.706150\n",
            "Step 3722: Minibatch Loss: 0.755172\n",
            "Step 3724: Minibatch Loss: 0.675399\n",
            "Step 3726: Minibatch Loss: 0.689492\n",
            "Step 3728: Minibatch Loss: 0.712685\n",
            "Step 3730: Minibatch Loss: 0.682204\n",
            "Step 3732: Minibatch Loss: 0.686440\n",
            "Step 3734: Minibatch Loss: 0.652269\n",
            "Step 3736: Minibatch Loss: 0.660547\n",
            "Step 3738: Minibatch Loss: 0.657654\n",
            "Step 3740: Minibatch Loss: 0.669535\n",
            "Step 3742: Minibatch Loss: 0.703505\n",
            "Step 3744: Minibatch Loss: 0.655454\n",
            "Step 3746: Minibatch Loss: 0.703502\n",
            "Step 3748: Minibatch Loss: 0.681240\n",
            "Step 3750: Minibatch Loss: 0.720206\n",
            "Step 3752: Minibatch Loss: 0.663763\n",
            "Step 3754: Minibatch Loss: 0.666450\n",
            "Step 3756: Minibatch Loss: 0.676999\n",
            "Step 3758: Minibatch Loss: 0.690219\n",
            "Step 3760: Minibatch Loss: 0.700014\n",
            "Step 3762: Minibatch Loss: 0.669745\n",
            "Step 3764: Minibatch Loss: 0.724920\n",
            "Step 3766: Minibatch Loss: 0.710462\n",
            "Step 3768: Minibatch Loss: 0.662246\n",
            "Step 3770: Minibatch Loss: 0.682094\n",
            "Step 3772: Minibatch Loss: 0.689988\n",
            "Step 3774: Minibatch Loss: 0.680318\n",
            "Step 3776: Minibatch Loss: 0.650153\n",
            "Step 3778: Minibatch Loss: 0.721128\n",
            "Step 3780: Minibatch Loss: 0.659706\n",
            "Step 3782: Minibatch Loss: 0.696425\n",
            "Step 3784: Minibatch Loss: 0.665096\n",
            "Step 3786: Minibatch Loss: 0.709388\n",
            "Step 3788: Minibatch Loss: 0.670987\n",
            "Step 3790: Minibatch Loss: 0.633029\n",
            "Step 3792: Minibatch Loss: 0.683330\n",
            "Step 3794: Minibatch Loss: 0.673710\n",
            "Step 3796: Minibatch Loss: 0.664053\n",
            "Step 3798: Minibatch Loss: 0.632661\n",
            "Step 3800: Minibatch Loss: 0.670373\n",
            "Training for SNR= 9.5  sigma= 0.33496543915782767 iteratin: 0\n",
            "Step 3802: Minibatch Loss: 0.655589\n",
            "Step 3804: Minibatch Loss: 0.628352\n",
            "Step 3806: Minibatch Loss: 0.625691\n",
            "Step 3808: Minibatch Loss: 0.606648\n",
            "Step 3810: Minibatch Loss: 0.657420\n",
            "Step 3812: Minibatch Loss: 0.674782\n",
            "Step 3814: Minibatch Loss: 0.655551\n",
            "Step 3816: Minibatch Loss: 0.634758\n",
            "Step 3818: Minibatch Loss: 0.630612\n",
            "Step 3820: Minibatch Loss: 0.638353\n",
            "Step 3822: Minibatch Loss: 0.640023\n",
            "Step 3824: Minibatch Loss: 0.626311\n",
            "Step 3826: Minibatch Loss: 0.619005\n",
            "Step 3828: Minibatch Loss: 0.622559\n",
            "Step 3830: Minibatch Loss: 0.648473\n",
            "Step 3832: Minibatch Loss: 0.622954\n",
            "Step 3834: Minibatch Loss: 0.676639\n",
            "Step 3836: Minibatch Loss: 0.664146\n",
            "Step 3838: Minibatch Loss: 0.633496\n",
            "Step 3840: Minibatch Loss: 0.630920\n",
            "Step 3842: Minibatch Loss: 0.639687\n",
            "Step 3844: Minibatch Loss: 0.638342\n",
            "Step 3846: Minibatch Loss: 0.669897\n",
            "Step 3848: Minibatch Loss: 0.642708\n",
            "Step 3850: Minibatch Loss: 0.618991\n",
            "Step 3852: Minibatch Loss: 0.643782\n",
            "Step 3854: Minibatch Loss: 0.645079\n",
            "Step 3856: Minibatch Loss: 0.644916\n",
            "Step 3858: Minibatch Loss: 0.606850\n",
            "Step 3860: Minibatch Loss: 0.593151\n",
            "Step 3862: Minibatch Loss: 0.610602\n",
            "Step 3864: Minibatch Loss: 0.635887\n",
            "Step 3866: Minibatch Loss: 0.603794\n",
            "Step 3868: Minibatch Loss: 0.657826\n",
            "Step 3870: Minibatch Loss: 0.689509\n",
            "Step 3872: Minibatch Loss: 0.633773\n",
            "Step 3874: Minibatch Loss: 0.620209\n",
            "Step 3876: Minibatch Loss: 0.550795\n",
            "Step 3878: Minibatch Loss: 0.636063\n",
            "Step 3880: Minibatch Loss: 0.661213\n",
            "Step 3882: Minibatch Loss: 0.627377\n",
            "Step 3884: Minibatch Loss: 0.624957\n",
            "Step 3886: Minibatch Loss: 0.632099\n",
            "Step 3888: Minibatch Loss: 0.607392\n",
            "Step 3890: Minibatch Loss: 0.639556\n",
            "Step 3892: Minibatch Loss: 0.634102\n",
            "Step 3894: Minibatch Loss: 0.623754\n",
            "Step 3896: Minibatch Loss: 0.680494\n",
            "Step 3898: Minibatch Loss: 0.670066\n",
            "Step 3900: Minibatch Loss: 0.614898\n",
            "Step 3902: Minibatch Loss: 0.643268\n",
            "Step 3904: Minibatch Loss: 0.627771\n",
            "Step 3906: Minibatch Loss: 0.646978\n",
            "Step 3908: Minibatch Loss: 0.567674\n",
            "Step 3910: Minibatch Loss: 0.662836\n",
            "Step 3912: Minibatch Loss: 0.674165\n",
            "Step 3914: Minibatch Loss: 0.635134\n",
            "Step 3916: Minibatch Loss: 0.672674\n",
            "Step 3918: Minibatch Loss: 0.630217\n",
            "Step 3920: Minibatch Loss: 0.615473\n",
            "Step 3922: Minibatch Loss: 0.653234\n",
            "Step 3924: Minibatch Loss: 0.599393\n",
            "Step 3926: Minibatch Loss: 0.654566\n",
            "Step 3928: Minibatch Loss: 0.620796\n",
            "Step 3930: Minibatch Loss: 0.636737\n",
            "Step 3932: Minibatch Loss: 0.651615\n",
            "Step 3934: Minibatch Loss: 0.605072\n",
            "Step 3936: Minibatch Loss: 0.616122\n",
            "Step 3938: Minibatch Loss: 0.646087\n",
            "Step 3940: Minibatch Loss: 0.630749\n",
            "Step 3942: Minibatch Loss: 0.575544\n",
            "Step 3944: Minibatch Loss: 0.639996\n",
            "Step 3946: Minibatch Loss: 0.645748\n",
            "Step 3948: Minibatch Loss: 0.644551\n",
            "Step 3950: Minibatch Loss: 0.640821\n",
            "Step 3952: Minibatch Loss: 0.599399\n",
            "Step 3954: Minibatch Loss: 0.674591\n",
            "Step 3956: Minibatch Loss: 0.655254\n",
            "Step 3958: Minibatch Loss: 0.624335\n",
            "Step 3960: Minibatch Loss: 0.641243\n",
            "Step 3962: Minibatch Loss: 0.645487\n",
            "Step 3964: Minibatch Loss: 0.641829\n",
            "Step 3966: Minibatch Loss: 0.653104\n",
            "Step 3968: Minibatch Loss: 0.615784\n",
            "Step 3970: Minibatch Loss: 0.604140\n",
            "Step 3972: Minibatch Loss: 0.632331\n",
            "Step 3974: Minibatch Loss: 0.716913\n",
            "Step 3976: Minibatch Loss: 0.637755\n",
            "Step 3978: Minibatch Loss: 0.663294\n",
            "Step 3980: Minibatch Loss: 0.644484\n",
            "Step 3982: Minibatch Loss: 0.643918\n",
            "Step 3984: Minibatch Loss: 0.617464\n",
            "Step 3986: Minibatch Loss: 0.604969\n",
            "Step 3988: Minibatch Loss: 0.614691\n",
            "Step 3990: Minibatch Loss: 0.665967\n",
            "Step 3992: Minibatch Loss: 0.632129\n",
            "Step 3994: Minibatch Loss: 0.565459\n",
            "Step 3996: Minibatch Loss: 0.631118\n",
            "Step 3998: Minibatch Loss: 0.631083\n",
            "Step 4000: Minibatch Loss: 0.599929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmxyAbXa6l1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ef1a66c-0da7-4b57-b15a-90ebf2c5f87b"
      },
      "source": [
        "# Here I am using trained model\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_dl_tensor  = numpy.array(())\n",
        "times_per_iter_dl_tensor = numpy.array(())\n",
        "\n",
        "channel_in = []\n",
        "channel_out = []\n",
        "\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  lrate = 0.001\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    input_message_xx = training_input_message_one_hot [i:i+1]\n",
        "    input_message_xx = input_message_xx.astype(\"float32\")\n",
        "    #,input_message_x_label:training_input_message [i]\n",
        "    encoded_message = train_sess.run ([dl_encoder_output], feed_dict={input_message_x:input_message_xx })\n",
        "    channel_in.append(encoded_message[0][0])\n",
        "    encoded_message = encoded_message[0][0]\n",
        "    #encoded_message = numpy.around(encoded_message[0][0]> 0).astype(int)\n",
        "    #print (encoded_message[0][0])\n",
        "    awgn_channel_output_message = train_sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message})\n",
        "    channel_out.append(awgn_channel_output_message[0]) \n",
        "    #print (awgn_channel_output_message)\n",
        "    decoded_message = train_sess.run ([dl_decoder_only_output], feed_dict={input_channel_x:awgn_channel_output_message})\n",
        "    #print (\"input\", input_message[i])\n",
        "    #decoded_message = numpy.around(decoded_message[0][0]> 0).astype(int)\n",
        "    #rint (\"output\", decoded_message)\n",
        "    #print (\"output\", numpy.argmax(training_input_message_one_hot[i]), numpy.argmax(decoded_message[0][0]))\n",
        "    if (numpy.argmax(training_input_message_one_hot[i]) != numpy.argmax(decoded_message[0][0])):\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_dl_tensor=numpy.append(ber_per_iter_dl_tensor ,ber)\n",
        "  times_per_iter_dl_tensor=numpy.append(times_per_iter_dl_tensor, total_time)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 0.77s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 1.03s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.66\n",
            " -> Total Time: 2.60s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.66\n",
            " -> Total Time: 2.52s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.65\n",
            " -> Total Time: 2.46s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 0.78s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 1.05s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.63\n",
            " -> Total Time: 2.59s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 0.54s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 0.79s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 1.04s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.61\n",
            " -> Total Time: 2.65s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 0.76s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 1.01s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.58\n",
            " -> Total Time: 2.53s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 0.77s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.02s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.57\n",
            " -> Total Time: 2.55s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.56\n",
            " -> Total Time: 2.50s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.76s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.01s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.54\n",
            " -> Total Time: 2.53s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.76s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.01s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.51\n",
            " -> Total Time: 2.54s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.52\n",
            " -> Total Time: 2.52s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.43\n",
            " -> Total Time: 2.46s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.76s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 1.02s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.45\n",
            " -> Total Time: 2.51s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.76s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.40\n",
            " -> Total Time: 2.52s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.76s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.41\n",
            " -> Total Time: 2.54s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.36\n",
            " -> Total Time: 2.48s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.34\n",
            " -> Total Time: 2.48s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.31\n",
            " -> Total Time: 2.49s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.76s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 1.01s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.28\n",
            " -> Total Time: 2.56s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 1.01s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.23\n",
            " -> Total Time: 2.50s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuTBnLWp60br",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "22f1f81d-3ade-4b14-f1dd-5e0b3a24a97f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1) = plt.subplots(1,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,bler_per_iter_ldpc_itpp_psk_4,'', label=\"itpp-ldpc(18,9)-qpsk(channel=9)\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor,'', label=\"ai-dl-onehot(input=9,channel=9)\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_uncoded_commpy_psk_2,'', label=\"commpy-psk2-uncoded\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_uncoded_itpp_psk_2,'', label=\"itpp-psk2-uncoded\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_uncoded_itpp_psk_8,'', label=\"itpp-psk8-uncoded\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_ham_itpp_psk_4,'', label=\"itpp-ham(7,4)(input=8,channel=7)\") # plot BER vs SNR\n",
        "ax1.set_ylabel('BLER')\n",
        "ax1.set_title('Arch:2 ({},{},{})'.format(input_message_length,2*input_message_length,channel_size))\n",
        "#ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "#ax2.set_xlabel('$E_b/$N_0$')\n",
        "#ax2.set_ylabel('Decoding Time [s]')\n",
        "#ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "#            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "#            xy=(1, 0.35), xycoords='axes fraction',\n",
        "#            xytext=(-20, 20), textcoords='offset pixels',\n",
        "#            horizontalalignment='right',\n",
        "#            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(2*channel_size,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqlsF7KuBqyB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "997b2342-2fd2-4a95-9f81-777b3b8383f2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "channel_in_array = numpy.transpose(channel_in)\n",
        "\n",
        "for i in range (int(channel_size)):\n",
        "  plt.scatter(channel_in_array[i*2], channel_in_array[i*2+1])\n",
        "  plt.show()\n",
        "  plt.hist2d(channel_in_array[i*2], channel_in_array[i*2+1], cmap=plt.cm.jet)\n",
        "  plt.colorbar()\n",
        "  plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPmLLdERBxWD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "739213ee-46d1-4d12-a304-25a6fee87651"
      },
      "source": [
        "channel_out_array = numpy.transpose(channel_out)\n",
        "\n",
        "for i in range (int(channel_size)):\n",
        "  plt.hist2d(channel_out_array[2*i], channel_out_array[2*i+1], (50, 50), cmap=plt.cm.jet)\n",
        "  plt.colorbar()\n",
        "  plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}