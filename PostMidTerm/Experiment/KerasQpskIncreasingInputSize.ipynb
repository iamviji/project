{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KerasMultiL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamviji/project/blob/master/PostMidTerm/Experiment/KerasQpskIncreasingInputSize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5dLvr1Rr1K8"
      },
      "source": [
        "import numpy\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, GaussianNoise\n",
        "from tensorflow.keras import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior ()\n",
        "\n",
        "input_message_length = 2\n",
        "encoder_output_length = 2\n",
        "channel_size = 1\n",
        "NUM_OF_INPUT_MESSAGE = 1000\n",
        "SNR_STEP_SIZE = .5\n",
        "\n",
        "SNR_BEGIN = 0\n",
        "SNR_END = 10\n",
        "\n",
        "bler_per_iter_uncoded_commpy_psk_2= [0.521, 0.473, 0.436, 0.37,  0.304, 0.259, 0.187, 0.138, 0.098, 0.098, 0.052, 0.028, 0.012, 0.011, 0.009, 0.002, 0.0,  0.001, 0.,    0.0]\n",
        "bler_per_iter_uncoded_itpp_psk_2= [0.518, 0.478, 0.415, 0.355, 0.305, 0.227, 0.177, 0.149, 0.11,  0.075, 0.055, 0.023, 0.014, 0.014, 0.015, 0.001, 0.003, 0.001, 0.,    0. ]\n",
        "bler_per_iter_uncoded_commpy_psk_4 = [0.815, 0.793, 0.75,  0.714, 0.64,  0.639, 0.526, 0.49,  0.433, 0.371, 0.335, 0.236, 0.204, 0.154, 0.129, 0.08,  0.063, 0.046, 0.023, 0.018]\n",
        "bler_per_iter_uncoded_itpp_psk_4 = [0.814, 0.767, 0.729, 0.702, 0.66,  0.616, 0.563, 0.511, 0.442, 0.4,   0.294, 0.277, 0.228, 0.17,  0.114, 0.087, 0.05,  0.037, 0.022, 0.017]\n",
        "bler_per_iter_ldpc_itpp_psk_4 = [0.584, 0.488, 0.404, 0.332, 0.218, 0.151, 0.097, 0.058, 0.041, 0.024, 0.007, 0.004, 0.002, 0.001, 0.001, 0.,    0.,    0.,    0.,    0.,   ]\n",
        "bler_per_iter_ham_itpp_psk_4= [0.51, 0.479, 0.419, 0.333, 0.313, 0.247, 0.212, 0.132, 0.114, 0.093, 0.042, 0.027, 0.024, 0.016, 0.006, 0.005, 0.003, 0.002, 0.,    0.  ]\n",
        "\n",
        "\n",
        "def Snr2Sigma(snr):\n",
        "  sigma = 10 ** (- snr / 20)\n",
        "  return sigma\n",
        "\n",
        "\n",
        "def timer_update(i,current,time_tot,tic_incr=500):\n",
        "    last = current\n",
        "    current = time.time()\n",
        "    t_diff = current-last\n",
        "    print('SNR: {:04.3f} - Iter: {} - Last {} iterations took {:03.2f}s'.format(snr,i+1,tic_incr,t_diff))\n",
        "    return time_tot + t_diff\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIqZA3RCrJgd",
        "outputId": "fc5a1022-010b-4814-f904-0d135058295c"
      },
      "source": [
        "\n",
        "awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [2*channel_size])\n",
        "awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)\n",
        "\n",
        "\n",
        "decoder_input_x = tf.placeholder(\"float32\", [None, input_message_length], name=\"decoder_input_x\")\n",
        "\n",
        "snr_std = 7.0\n",
        "\n",
        "input_message_x = Input(shape=(input_message_length,))\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "enc_layer1 = Dense(encoder_output_length, activation='tanh')(input_message_x)\n",
        "enc_layer2 = Dense(2*channel_size, activation='tanh')(enc_layer1)\n",
        "enc_layer3 =  enc_layer2 / tf.sqrt(tf.reduce_mean(tf.square(enc_layer2)))\n",
        "encoder = Model(input_message_x, enc_layer3)\n",
        "\n",
        "awgn_channel = GaussianNoise(Snr2Sigma(snr_std),input_shape=(2*channel_size,))\n",
        "\n",
        "# create a placeholder for an encoded (32-dimensional) input\n",
        "encoded_input = Input(shape=(2*channel_size,))\n",
        "dec_layer1 = Dense(encoder_output_length, activation='tanh')(encoded_input)\n",
        "dec_layer2 = Dense(input_message_length, activation='sigmoid')(dec_layer1)\n",
        "# this model maps an encoded input to its decoder representation\n",
        "decoder = Model(encoded_input, dec_layer2)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))\n",
        "\n",
        "decoder_output = (tf.nn.sigmoid(decoder_input_x))\n",
        "\n",
        "print(encoder.summary())\n",
        "print(decoder.summary())\n",
        "print(autoencoder.summary())"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_185\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            [(None, 2)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 2)            6           input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 2)            6           dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Square_4 (TensorFlo multiple             0           dense_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mean_4 (TensorFlowO multiple             0           tf_op_layer_Square_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sqrt_4 (TensorFlowO multiple             0           tf_op_layer_Mean_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_truediv_4 (TensorFl multiple             0           dense_17[0][0]                   \n",
            "                                                                 tf_op_layer_Sqrt_4[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 12\n",
            "Trainable params: 12\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"functional_187\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        [(None, 2)]               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 2)                 6         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 2)                 6         \n",
            "=================================================================\n",
            "Total params: 12\n",
            "Trainable params: 12\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"functional_189\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 2)]               0         \n",
            "_________________________________________________________________\n",
            "functional_185 (Functional)  (None, 2)                 12        \n",
            "_________________________________________________________________\n",
            "gaussian_noise_84 (GaussianN (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "functional_187 (Functional)  (None, 2)                 12        \n",
            "=================================================================\n",
            "Total params: 24\n",
            "Trainable params: 24\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IidQMKlts65l",
        "outputId": "66d5fb9b-1f41-4611-e4d3-28f5bfc4e3ec"
      },
      "source": [
        "training_input_message = numpy.random.randint(2, size=(NUM_OF_INPUT_MESSAGE*10,input_message_length))\n",
        "print (training_input_message)\n",
        "print (len(training_input_message))\n",
        "print(input_message_length)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0]\n",
            " [1 1]\n",
            " [0 1]\n",
            " ...\n",
            " [1 1]\n",
            " [0 0]\n",
            " [1 0]]\n",
            "10000\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORw0oaAjsrXG",
        "outputId": "79510d44-579b-444d-e9c9-7c3fccbc9569"
      },
      "source": [
        "test_input_message = numpy.random.randint(2, size=(NUM_OF_INPUT_MESSAGE*10,input_message_length))\n",
        "print (test_input_message)\n",
        "print (len(test_input_message))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " ...\n",
            " [0 1]\n",
            " [1 0]\n",
            " [0 0]]\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbiBvRFNtUly",
        "outputId": "4ae8477b-ec9f-4626-a065-d248304bb253"
      },
      "source": [
        "import keras\n",
        "\n",
        "#def custom_losff_fucntion (act, pred):\n",
        "#  return (tf.reduce_mean(-1*(act * tf.log(pred) + (1-act)*tf.log(1-pred))))\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "autoencoder.compile(optimizer=opt, loss='binary_crossentropy')\n",
        "#autoencoder.compile(optimizer=opt, loss=custom_losff_fucntion)\n",
        "#loss='mean_squared_error'\n",
        "#for snr in (numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)):\n",
        "for snr in (numpy.arange (0, 10, SNR_STEP_SIZE)):\n",
        "  sigma = 1.0*Snr2Sigma (snr)\n",
        "  snr_std = sigma\n",
        "  print (\"Training for SNR=\", snr, \" sigma=\", sigma) \n",
        "  awgn_channel = GaussianNoise(sigma,input_shape=(channel_size,))\n",
        "  autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))\n",
        "  opt = keras.optimizers.Adam(learning_rate=0.003)\n",
        "  autoencoder.compile(optimizer=opt, loss='binary_crossentropy')\n",
        "  autoencoder.fit(training_input_message, training_input_message,\n",
        "                #epochs=50, original\n",
        "                epochs=25,\n",
        "                batch_size=500,\n",
        "                shuffle=False,\n",
        "                validation_data=(test_input_message, test_input_message))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for SNR= 0.0  sigma= 1.0\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 1s 137us/sample - loss: 0.6880 - val_loss: 0.6525\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.6678 - val_loss: 0.6167\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.6422 - val_loss: 0.5736\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.6092 - val_loss: 0.5320\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.5705 - val_loss: 0.4992\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.5460 - val_loss: 0.4760\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.5244 - val_loss: 0.4587\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.5128 - val_loss: 0.4449\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.5010 - val_loss: 0.4325\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4958 - val_loss: 0.4207\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.4890 - val_loss: 0.4089\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.4764 - val_loss: 0.3963\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.4773 - val_loss: 0.3834\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.4669 - val_loss: 0.3692\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.4594 - val_loss: 0.3547\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.4543 - val_loss: 0.3383\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4466 - val_loss: 0.3220\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4419 - val_loss: 0.3067\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4374 - val_loss: 0.2901\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4272 - val_loss: 0.2759\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.4267 - val_loss: 0.2629\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.4169 - val_loss: 0.2500\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4176 - val_loss: 0.2385\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.4157 - val_loss: 0.2291\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.4105 - val_loss: 0.2189\n",
            "Training for SNR= 0.5  sigma= 0.9440608762859234\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 1s 138us/sample - loss: 0.3820 - val_loss: 0.2040\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.3772 - val_loss: 0.1909\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3750 - val_loss: 0.1804\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3729 - val_loss: 0.1711\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.3696 - val_loss: 0.1622\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.3680 - val_loss: 0.1547\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3650 - val_loss: 0.1486\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.3656 - val_loss: 0.1428\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3601 - val_loss: 0.1382\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3539 - val_loss: 0.1335\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3520 - val_loss: 0.1287\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3557 - val_loss: 0.1249\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3642 - val_loss: 0.1223\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3558 - val_loss: 0.1201\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3552 - val_loss: 0.1179\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3530 - val_loss: 0.1158\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3615 - val_loss: 0.1143\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.3594 - val_loss: 0.1131\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3521 - val_loss: 0.1121\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.3512 - val_loss: 0.1107\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3546 - val_loss: 0.1099\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3528 - val_loss: 0.1095\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3461 - val_loss: 0.1083\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3523 - val_loss: 0.1075\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3432 - val_loss: 0.1064\n",
            "Training for SNR= 1.0  sigma= 0.8912509381337456\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 1s 143us/sample - loss: 0.3273 - val_loss: 0.1019\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3175 - val_loss: 0.0971\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.3214 - val_loss: 0.0934\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3252 - val_loss: 0.0911\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3180 - val_loss: 0.0887\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3110 - val_loss: 0.0859\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3164 - val_loss: 0.0843\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.3194 - val_loss: 0.0835\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.3147 - val_loss: 0.0826\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.3081 - val_loss: 0.0809\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.3125 - val_loss: 0.0800\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3080 - val_loss: 0.0786\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3096 - val_loss: 0.0784\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.3137 - val_loss: 0.0780\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.3166 - val_loss: 0.0784\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.3113 - val_loss: 0.0782\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.3163 - val_loss: 0.0780\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3114 - val_loss: 0.0780\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3153 - val_loss: 0.0788\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3139 - val_loss: 0.0790\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.3070 - val_loss: 0.0782\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.3059 - val_loss: 0.0772\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.3106 - val_loss: 0.0770\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3142 - val_loss: 0.0772\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.3099 - val_loss: 0.0777\n",
            "Training for SNR= 1.5  sigma= 0.8413951416451951\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 1s 143us/sample - loss: 0.2789 - val_loss: 0.0713\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.2840 - val_loss: 0.0679\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2876 - val_loss: 0.0655\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2852 - val_loss: 0.0643\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2820 - val_loss: 0.0631\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.2760 - val_loss: 0.0615\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.2775 - val_loss: 0.0599\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2908 - val_loss: 0.0605\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2835 - val_loss: 0.0608\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2767 - val_loss: 0.0601\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2829 - val_loss: 0.0600\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2856 - val_loss: 0.0602\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2822 - val_loss: 0.0607\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2812 - val_loss: 0.0608\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2846 - val_loss: 0.0602\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2759 - val_loss: 0.0591\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2816 - val_loss: 0.0587\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2852 - val_loss: 0.0595\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2813 - val_loss: 0.0599\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2838 - val_loss: 0.0601\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2840 - val_loss: 0.0603\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2781 - val_loss: 0.0600\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2743 - val_loss: 0.0587\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2777 - val_loss: 0.0572\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2761 - val_loss: 0.0575\n",
            "Training for SNR= 2.0  sigma= 0.7943282347242815\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 1s 145us/sample - loss: 0.2561 - val_loss: 0.0540\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2588 - val_loss: 0.0509\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2456 - val_loss: 0.0484\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2465 - val_loss: 0.0463\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2500 - val_loss: 0.0457\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2539 - val_loss: 0.0451\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2543 - val_loss: 0.0457\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2542 - val_loss: 0.0456\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2536 - val_loss: 0.0456\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2541 - val_loss: 0.0454\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2577 - val_loss: 0.0454\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2530 - val_loss: 0.0453\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2555 - val_loss: 0.0450\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2518 - val_loss: 0.0449\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2536 - val_loss: 0.0454\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2553 - val_loss: 0.0455\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2521 - val_loss: 0.0451\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2490 - val_loss: 0.0450\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2567 - val_loss: 0.0444\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2508 - val_loss: 0.0443\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2462 - val_loss: 0.0440\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2482 - val_loss: 0.0437\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2521 - val_loss: 0.0436\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2434 - val_loss: 0.0431\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2512 - val_loss: 0.0431\n",
            "Training for SNR= 2.5  sigma= 0.7498942093324559\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 1s 144us/sample - loss: 0.2246 - val_loss: 0.0395\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2219 - val_loss: 0.0368\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2218 - val_loss: 0.0353\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2248 - val_loss: 0.0343\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2224 - val_loss: 0.0333\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2215 - val_loss: 0.0329\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2207 - val_loss: 0.0325\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2251 - val_loss: 0.0323\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2152 - val_loss: 0.0319\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2264 - val_loss: 0.0318\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2199 - val_loss: 0.0314\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2225 - val_loss: 0.0313\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2210 - val_loss: 0.0313\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2265 - val_loss: 0.0320\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2214 - val_loss: 0.0322\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2237 - val_loss: 0.0325\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2246 - val_loss: 0.0324\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2235 - val_loss: 0.0325\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2162 - val_loss: 0.0321\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2258 - val_loss: 0.0320\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2189 - val_loss: 0.0318\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2250 - val_loss: 0.0321\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2252 - val_loss: 0.0326\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2244 - val_loss: 0.0328\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2264 - val_loss: 0.0327\n",
            "Training for SNR= 3.0  sigma= 0.7079457843841379\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 1s 147us/sample - loss: 0.1941 - val_loss: 0.0293\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1961 - val_loss: 0.0266\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1977 - val_loss: 0.0255\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.1946 - val_loss: 0.0248\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1951 - val_loss: 0.0240\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1920 - val_loss: 0.0235\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1961 - val_loss: 0.0234\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1952 - val_loss: 0.0233\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1917 - val_loss: 0.0228\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2033 - val_loss: 0.0231\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1988 - val_loss: 0.0236\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1943 - val_loss: 0.0236\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2006 - val_loss: 0.0237\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1936 - val_loss: 0.0237\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.1983 - val_loss: 0.0232\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.1958 - val_loss: 0.0230\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.1981 - val_loss: 0.0228\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.2018 - val_loss: 0.0233\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1905 - val_loss: 0.0232\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1960 - val_loss: 0.0230\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1985 - val_loss: 0.0234\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1958 - val_loss: 0.0235\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2013 - val_loss: 0.0237\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.1959 - val_loss: 0.0234\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1939 - val_loss: 0.0230\n",
            "Training for SNR= 3.5  sigma= 0.6683439175686147\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 154us/sample - loss: 0.1737 - val_loss: 0.0212\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1687 - val_loss: 0.0195\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1731 - val_loss: 0.0182\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1654 - val_loss: 0.0173\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.1635 - val_loss: 0.0166\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.1688 - val_loss: 0.0160\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.1693 - val_loss: 0.0159\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1692 - val_loss: 0.0157\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1707 - val_loss: 0.0158\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1765 - val_loss: 0.0160\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1711 - val_loss: 0.0161\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.1674 - val_loss: 0.0160\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.1738 - val_loss: 0.0160\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1711 - val_loss: 0.0161\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1708 - val_loss: 0.0163\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.1695 - val_loss: 0.0157\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1719 - val_loss: 0.0159\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1609 - val_loss: 0.0156\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1678 - val_loss: 0.0152\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1677 - val_loss: 0.0153\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1637 - val_loss: 0.0150\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.1650 - val_loss: 0.0148\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1634 - val_loss: 0.0145\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1668 - val_loss: 0.0145\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1647 - val_loss: 0.0144\n",
            "Training for SNR= 4.0  sigma= 0.6309573444801932\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 151us/sample - loss: 0.1493 - val_loss: 0.0134\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1466 - val_loss: 0.0127\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1435 - val_loss: 0.0121\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.1466 - val_loss: 0.0118\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1454 - val_loss: 0.0114\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1414 - val_loss: 0.0110\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1436 - val_loss: 0.0108\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1430 - val_loss: 0.0107\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1384 - val_loss: 0.0104\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1523 - val_loss: 0.0106\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1400 - val_loss: 0.0105\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1471 - val_loss: 0.0104\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1435 - val_loss: 0.0104\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1428 - val_loss: 0.0104\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1473 - val_loss: 0.0103\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1474 - val_loss: 0.0105\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1432 - val_loss: 0.0107\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1415 - val_loss: 0.0105\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1447 - val_loss: 0.0104\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1462 - val_loss: 0.0103\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1420 - val_loss: 0.0102\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1363 - val_loss: 0.0100\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1403 - val_loss: 0.0100\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1441 - val_loss: 0.0102\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.1455 - val_loss: 0.0101\n",
            "Training for SNR= 4.5  sigma= 0.5956621435290105\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 152us/sample - loss: 0.1165 - val_loss: 0.0092\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1246 - val_loss: 0.0086\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1207 - val_loss: 0.0082\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1241 - val_loss: 0.0079\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1218 - val_loss: 0.0078\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1196 - val_loss: 0.0075\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1268 - val_loss: 0.0074\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.1215 - val_loss: 0.0073\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1182 - val_loss: 0.0072\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1222 - val_loss: 0.0071\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1212 - val_loss: 0.0070\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1212 - val_loss: 0.0070\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1222 - val_loss: 0.0070\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1216 - val_loss: 0.0070\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1224 - val_loss: 0.0070\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1234 - val_loss: 0.0070\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1216 - val_loss: 0.0069\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1210 - val_loss: 0.0069\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1205 - val_loss: 0.0068\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.1220 - val_loss: 0.0068\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1182 - val_loss: 0.0068\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1182 - val_loss: 0.0066\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1213 - val_loss: 0.0067\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.1186 - val_loss: 0.0066\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1213 - val_loss: 0.0067\n",
            "Training for SNR= 5.0  sigma= 0.5623413251903491\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 155us/sample - loss: 0.0990 - val_loss: 0.0061\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1018 - val_loss: 0.0057\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1012 - val_loss: 0.0054\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0985 - val_loss: 0.0052\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0990 - val_loss: 0.0049\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0987 - val_loss: 0.0048\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0973 - val_loss: 0.0047\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0974 - val_loss: 0.0045\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0930 - val_loss: 0.0045\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0991 - val_loss: 0.0043\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0986 - val_loss: 0.0043\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0984 - val_loss: 0.0042\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0967 - val_loss: 0.0042\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0986 - val_loss: 0.0042\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1029 - val_loss: 0.0043\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1021 - val_loss: 0.0044\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0931 - val_loss: 0.0043\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0937 - val_loss: 0.0041\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1008 - val_loss: 0.0042\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0971 - val_loss: 0.0041\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.1019 - val_loss: 0.0042\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0977 - val_loss: 0.0042\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0956 - val_loss: 0.0041\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0937 - val_loss: 0.0041\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1005 - val_loss: 0.0040\n",
            "Training for SNR= 5.5  sigma= 0.5308844442309884\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 155us/sample - loss: 0.0825 - val_loss: 0.0037\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0829 - val_loss: 0.0036\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0795 - val_loss: 0.0034\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0787 - val_loss: 0.0032\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0815 - val_loss: 0.0031\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0771 - val_loss: 0.0030\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0807 - val_loss: 0.0029\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0821 - val_loss: 0.0029\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0786 - val_loss: 0.0029\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0794 - val_loss: 0.0028\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0820 - val_loss: 0.0028\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0779 - val_loss: 0.0028\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0778 - val_loss: 0.0027\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0797 - val_loss: 0.0027\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0746 - val_loss: 0.0026\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0818 - val_loss: 0.0026\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0800 - val_loss: 0.0026\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0797 - val_loss: 0.0026\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0782 - val_loss: 0.0026\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0818 - val_loss: 0.0026\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0765 - val_loss: 0.0026\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0778 - val_loss: 0.0025\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0781 - val_loss: 0.0025\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0773 - val_loss: 0.0025\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0795 - val_loss: 0.0025\n",
            "Training for SNR= 6.0  sigma= 0.5011872336272722\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 157us/sample - loss: 0.0598 - val_loss: 0.0022\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0631 - val_loss: 0.0021\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0581 - val_loss: 0.0020\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0638 - val_loss: 0.0019\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0640 - val_loss: 0.0019\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0619 - val_loss: 0.0018\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0661 - val_loss: 0.0018\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0644 - val_loss: 0.0018\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0620 - val_loss: 0.0018\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0607 - val_loss: 0.0017\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0619 - val_loss: 0.0017\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0633 - val_loss: 0.0017\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0631 - val_loss: 0.0016\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0613 - val_loss: 0.0016\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0596 - val_loss: 0.0016\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0605 - val_loss: 0.0016\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0637 - val_loss: 0.0016\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0654 - val_loss: 0.0016\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0600 - val_loss: 0.0016\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0613 - val_loss: 0.0015\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0636 - val_loss: 0.0015\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0592 - val_loss: 0.0015\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0615 - val_loss: 0.0015\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0596 - val_loss: 0.0015\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0597 - val_loss: 0.0015\n",
            "Training for SNR= 6.5  sigma= 0.47315125896148047\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 158us/sample - loss: 0.0444 - val_loss: 0.0014\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0474 - val_loss: 0.0013\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0451 - val_loss: 0.0012\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0445 - val_loss: 0.0011\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0487 - val_loss: 0.0011\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0495 - val_loss: 0.0011\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0475 - val_loss: 0.0011\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0468 - val_loss: 0.0010\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0461 - val_loss: 0.0010\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0474 - val_loss: 9.8617e-04\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0495 - val_loss: 9.6390e-04\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0493 - val_loss: 9.7096e-04\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0486 - val_loss: 9.7683e-04\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0448 - val_loss: 9.6567e-04\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0474 - val_loss: 9.3330e-04\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0429 - val_loss: 9.1958e-04\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0456 - val_loss: 8.9745e-04\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0477 - val_loss: 8.9986e-04\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0474 - val_loss: 8.9674e-04\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0458 - val_loss: 8.8979e-04\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0441 - val_loss: 8.8555e-04\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0477 - val_loss: 8.7244e-04\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0443 - val_loss: 8.7357e-04\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0489 - val_loss: 8.7346e-04\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0448 - val_loss: 8.7838e-04\n",
            "Training for SNR= 7.0  sigma= 0.44668359215096315\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 160us/sample - loss: 0.0351 - val_loss: 8.2030e-04\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0348 - val_loss: 7.7502e-04\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0334 - val_loss: 7.2858e-04\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0340 - val_loss: 6.9525e-04\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0324 - val_loss: 6.6613e-04\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0349 - val_loss: 6.3635e-04\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0344 - val_loss: 6.1565e-04\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0363 - val_loss: 6.0195e-04\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0395 - val_loss: 6.1080e-04\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0343 - val_loss: 6.0260e-04\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0338 - val_loss: 5.8911e-04\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0359 - val_loss: 5.7609e-04\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0355 - val_loss: 5.6171e-04\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0369 - val_loss: 5.6209e-04\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0370 - val_loss: 5.6123e-04\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0361 - val_loss: 5.6073e-04\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0319 - val_loss: 5.5536e-04\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0352 - val_loss: 5.4378e-04\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0347 - val_loss: 5.4270e-04\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0344 - val_loss: 5.3355e-04\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0339 - val_loss: 5.2595e-04\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0354 - val_loss: 5.2616e-04\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0339 - val_loss: 5.2178e-04\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0352 - val_loss: 5.2066e-04\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0416 - val_loss: 5.3503e-04\n",
            "Training for SNR= 7.5  sigma= 0.4216965034285822\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 164us/sample - loss: 0.0255 - val_loss: 5.0007e-04\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0287 - val_loss: 4.8369e-04\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0262 - val_loss: 4.6923e-04\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0273 - val_loss: 4.5944e-04\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0242 - val_loss: 4.3933e-04\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0261 - val_loss: 4.2526e-04\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0227 - val_loss: 4.0944e-04\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0236 - val_loss: 3.9279e-04\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0247 - val_loss: 3.8129e-04\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0242 - val_loss: 3.6844e-04\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0259 - val_loss: 3.6016e-04\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0248 - val_loss: 3.5004e-04\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0237 - val_loss: 3.4125e-04\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0253 - val_loss: 3.3546e-04\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0223 - val_loss: 3.2772e-04\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0269 - val_loss: 3.2166e-04\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0266 - val_loss: 3.1624e-04\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0247 - val_loss: 3.1537e-04\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0229 - val_loss: 3.0839e-04\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0251 - val_loss: 3.0808e-04\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0237 - val_loss: 2.9505e-04\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0215 - val_loss: 2.8789e-04\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0236 - val_loss: 2.8159e-04\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0254 - val_loss: 2.7698e-04\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0264 - val_loss: 2.7921e-04\n",
            "Training for SNR= 8.0  sigma= 0.3981071705534972\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 163us/sample - loss: 0.0175 - val_loss: 2.6741e-04\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0174 - val_loss: 2.5861e-04\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0163 - val_loss: 2.4771e-04\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0195 - val_loss: 2.3854e-04\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0188 - val_loss: 2.3548e-04\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0197 - val_loss: 2.3010e-04\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0161 - val_loss: 2.2528e-04\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0178 - val_loss: 2.1884e-04\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0182 - val_loss: 2.1442e-04\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0167 - val_loss: 2.0848e-04\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0173 - val_loss: 2.0391e-04\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0165 - val_loss: 1.9837e-04\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0142 - val_loss: 1.9017e-04\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0179 - val_loss: 1.8684e-04\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0166 - val_loss: 1.8370e-04\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0175 - val_loss: 1.8055e-04\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0160 - val_loss: 1.7787e-04\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0184 - val_loss: 1.7553e-04\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0143 - val_loss: 1.7120e-04\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0174 - val_loss: 1.6875e-04\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0168 - val_loss: 1.6725e-04\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0162 - val_loss: 1.6571e-04\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0167 - val_loss: 1.6112e-04\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0177 - val_loss: 1.5761e-04\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0180 - val_loss: 1.5806e-04\n",
            "Training for SNR= 8.5  sigma= 0.3758374042884442\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 167us/sample - loss: 0.0118 - val_loss: 1.5304e-04\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0108 - val_loss: 1.4461e-04\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0107 - val_loss: 1.3783e-04\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0107 - val_loss: 1.3338e-04\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0122 - val_loss: 1.2788e-04\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0092 - val_loss: 1.2391e-04\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0116 - val_loss: 1.2047e-04\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0109 - val_loss: 1.1466e-04\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0125 - val_loss: 1.1294e-04\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0114 - val_loss: 1.1096e-04\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0125 - val_loss: 1.0984e-04\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0108 - val_loss: 1.0751e-04\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0100 - val_loss: 1.0556e-04\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0097 - val_loss: 1.0390e-04\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0133 - val_loss: 1.0331e-04\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0112 - val_loss: 1.0104e-04\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0125 - val_loss: 9.9142e-05\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0128 - val_loss: 9.8389e-05\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0123 - val_loss: 9.8681e-05\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0135 - val_loss: 9.7881e-05\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0111 - val_loss: 9.7478e-05\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0107 - val_loss: 9.5773e-05\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0105 - val_loss: 9.3230e-05\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0102 - val_loss: 9.0651e-05\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0117 - val_loss: 8.9061e-05\n",
            "Training for SNR= 9.0  sigma= 0.35481338923357547\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 179us/sample - loss: 0.0075 - val_loss: 8.6249e-05\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0071 - val_loss: 8.4407e-05\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0061 - val_loss: 7.9934e-05\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0063 - val_loss: 7.6322e-05\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 17us/sample - loss: 0.0064 - val_loss: 7.4335e-05\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0091 - val_loss: 7.3047e-05\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0081 - val_loss: 7.1516e-05\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 16us/sample - loss: 0.0068 - val_loss: 7.0017e-05\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0071 - val_loss: 6.8434e-05\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0071 - val_loss: 6.6836e-05\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0063 - val_loss: 6.4864e-05\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0066 - val_loss: 6.3463e-05\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0077 - val_loss: 6.1690e-05\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0070 - val_loss: 6.0743e-05\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0072 - val_loss: 5.9234e-05\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0074 - val_loss: 5.8108e-05\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0074 - val_loss: 5.7302e-05\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0071 - val_loss: 5.6710e-05\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0075 - val_loss: 5.6169e-05\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0057 - val_loss: 5.4277e-05\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0074 - val_loss: 5.3316e-05\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0077 - val_loss: 5.2934e-05\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0070 - val_loss: 5.2473e-05\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0070 - val_loss: 5.1678e-05\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0057 - val_loss: 5.0507e-05\n",
            "Training for SNR= 9.5  sigma= 0.33496543915782767\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 169us/sample - loss: 0.0039 - val_loss: 4.7650e-05\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0037 - val_loss: 4.5711e-05\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0049 - val_loss: 4.4566e-05\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0041 - val_loss: 4.3093e-05\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0046 - val_loss: 4.1487e-05\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0051 - val_loss: 4.0894e-05\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0038 - val_loss: 3.9214e-05\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0037 - val_loss: 3.8080e-05\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0048 - val_loss: 3.7720e-05\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0054 - val_loss: 3.7143e-05\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0039 - val_loss: 3.6146e-05\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0038 - val_loss: 3.4853e-05\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0046 - val_loss: 3.4073e-05\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0045 - val_loss: 3.3283e-05\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0045 - val_loss: 3.2637e-05\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0042 - val_loss: 3.2142e-05\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0032 - val_loss: 3.1273e-05\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0043 - val_loss: 3.0903e-05\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0040 - val_loss: 2.9958e-05\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0032 - val_loss: 2.9086e-05\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0046 - val_loss: 2.8692e-05\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0036 - val_loss: 2.8092e-05\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0043 - val_loss: 2.7330e-05\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0050 - val_loss: 2.6892e-05\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0045 - val_loss: 2.6931e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjVpOnoOuF0o",
        "outputId": "1a62e01f-d637-4124-e796-94f69725fd78"
      },
      "source": [
        "\n",
        "# Here I am using trained model\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_dl_tensor_2  = numpy.array(())\n",
        "times_per_iter_dl_tensor_2 = numpy.array(())\n",
        "\n",
        "#awgn_channel_tx = GaussianNoise(0.5,input_shape=(channel_size,))\n",
        "\n",
        "#awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [channel_size])\n",
        "#awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "#awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "#awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)\n",
        "\n",
        "train_init = tf.global_variables_initializer ()\n",
        "train_sess = tf.Session ()\n",
        "\n",
        "channel_in = []\n",
        "channel_out = []\n",
        "\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    input_message_xx = training_input_message [i:i+1]\n",
        "    #print (\"input\", input_message_xx)\n",
        "    encoded_message = encoder.predict(input_message_xx)\n",
        "    channel_in.append(encoded_message[0])\n",
        "    #encoded_message = numpy.around(encoded_message > 0.5).astype(int)\n",
        "    #print(\"encoded:\",encoded_message)\n",
        "    #print (\"encoded\", encoded_message)\n",
        "    #noised_message = awgn_channel.predict (encoded_message)\n",
        "    #noised_message = commpy.channels.awgn(encoded_message, snr)\n",
        "    noised_message = train_sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message[0]})[0].reshape([1,2*channel_size])\n",
        "    channel_out.append(noised_message[0]) \n",
        "    #noised_message = encoded_message[0] + numpy.random.normal(0, sigma, [1,channel_size])\n",
        "    #print (noised_message)\n",
        "    #awgn_channel = GaussianNoise(sigma,input_shape=(channel_size,))\n",
        "    #noised_message = awgn_channel.predict(encoded_message)\n",
        "    #noised_message = awgn_layer (encoded_message)    \n",
        "    #print(noised_message)\n",
        "    decoded_message = decoder.predict(noised_message)\n",
        "    #print (\"decoded1:\", decoded_message)\n",
        "    #decoded_message = train_sess.run ([decoder_output], feed_dict={decoder_input_x:decoded_message})\n",
        "    #print (\"decoded2:\", decoded_message)\n",
        "    #decoded_message = autoencoder.predict(input_message_xx)\n",
        "    decoded_message = numpy.around(decoded_message[0]).astype(int)\n",
        "    #print (\"decoded3:\", decoded_message)\n",
        "    #decoded_message = numpy.around(decoded_message > 0.5).astype(int)\n",
        "    #print (\"decoded:\", decoded_message)\n",
        "    #print (\".\")\n",
        "    #autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))\n",
        "    #decoded_message = autoencoder.predict(input_message_xx)\n",
        "    #print (\"output\", decoded_message)\n",
        "    if abs(decoded_message-training_input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "      #print (\"Error\")\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_dl_tensor_2 = numpy.append(ber_per_iter_dl_tensor_2 ,ber)\n",
        "  times_per_iter_dl_tensor_2 = numpy.append(times_per_iter_dl_tensor_2, total_time)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 7.08s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 8.27s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 9.53s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 10.71s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.31\n",
            " -> Total Time: 35.59s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.18s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.38s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 3.60s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 4.79s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.27\n",
            " -> Total Time: 11.95s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 1.19s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 2.46s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 3.69s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 4.89s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.25\n",
            " -> Total Time: 12.23s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 1.26s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 2.50s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 3.75s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 4.90s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.23\n",
            " -> Total Time: 12.42s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 1.16s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 2.39s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 3.61s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 4.82s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.18\n",
            " -> Total Time: 11.98s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 1.21s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 2.47s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 3.70s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 4.99s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.18\n",
            " -> Total Time: 12.37s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 1.19s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 2.42s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 3.71s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 4.92s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.15\n",
            " -> Total Time: 12.24s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 1.28s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 2.47s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 3.74s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 4.91s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.13\n",
            " -> Total Time: 12.40s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 1.28s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 2.52s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 3.72s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 4.90s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.13\n",
            " -> Total Time: 12.42s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 1.19s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 2.48s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 3.90s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 5.14s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.08\n",
            " -> Total Time: 12.71s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 1.18s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 2.39s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 3.75s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 5.16s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.07\n",
            " -> Total Time: 12.47s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 1.61s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 3.15s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 4.42s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 5.60s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.05\n",
            " -> Total Time: 14.78s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 1.18s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 2.45s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 3.70s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 4.93s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.05\n",
            " -> Total Time: 12.26s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 1.28s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 2.48s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 3.66s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 4.89s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 12.31s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 1.18s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 2.43s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 3.72s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 4.92s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 12.25s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 1.24s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 2.48s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 3.72s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 4.91s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 12.35s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 1.23s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 2.45s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 3.69s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 4.88s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 12.24s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 1.23s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 2.49s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 3.75s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 4.92s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 12.40s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 1.18s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 2.45s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 3.73s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 4.97s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 12.33s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 1.23s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 2.49s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 3.77s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 4.99s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 12.49s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "cALSMP2YvKvC",
        "outputId": "0f887386-f092-4ce1-8aba-0338df354628"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1) = plt.subplots(1,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,bler_per_iter_ldpc_itpp_psk_4,'', label=\"itpp-ldpc(18,9)-qpsk(channel=9)\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor_2,'', label=\"ai-dl(2-2-1)\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_uncoded_commpy_psk_2,'', label=\"commpy-psk2-uncoded\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_uncoded_commpy_psk_4,'', label=\"commpy-psk4-uncoded\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_uncoded_itpp_psk_2,'', label=\"itpp-psk2-uncoded\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_ham_itpp_psk_4,'', label=\"itpp-ham(7,4)(input=8,channel=7)\") # plot BER vs SNR\n",
        "ax1.set_ylabel('BLER')\n",
        "ax1.set_title('Arch-2 ({},{},{})'.format(input_message_length,2*input_message_length, channel_size))\n",
        "#ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "#ax2.set_xlabel('$E_b/$N_0$')\n",
        "#ax2.set_ylabel('Decoding Time [s]')\n",
        "#ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "#            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "#            xy=(1, 0.35), xycoords='axes fraction',\n",
        "#            xytext=(-20, 20), textcoords='offset pixels',\n",
        "#            horizontalalignment='right',\n",
        "#            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(2*channel_size,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnVP37k_BSJp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "9f1488a1-f4b6-4ef7-d099-cd711c4e9350"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "channel_in_array = numpy.transpose(channel_in)\n",
        "\n",
        "for i in range (int(channel_size)):\n",
        "  plt.scatter(channel_in_array[i*2], channel_in_array[i*2+1])\n",
        "  plt.show()\n",
        "  plt.hist2d(channel_in_array[i*2], channel_in_array[i*2+1], cmap=plt.cm.jet)\n",
        "  plt.colorbar()\n",
        "  plt.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD4CAYAAAATpHZ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWBklEQVR4nO3db6xd1Znf8e8PQ2CmnQIJKTF/FIhiacqoHZJawDR9kYaEGFphZiaJQqvBiYicUUFtpUod0kQDJYma9EXTRGJoLbBiRm2Apo3wTK2hDn+EqooEp0n4W8oNMyn2EAiY0FYEGNtPX5x14cTcc++xz77nnLv5fqStu/fa6+61zvXVc7efvfZaqSokSf1wzKw7IEnqjkFdknrEoC5JPWJQl6QeMahLUo8cO+sOjJL8csFJs+6GxnAaT0+trT9n/dTa0lry9HNV9fZJrvDupF4atzW4s6o2TdLeapnboD4I6J+edSc0hn/IdVNr63P+TmhJ1/140iu8xPgR5zo4ZdL2VsscB3VJmp7Qj4DYh88gSRM7BvilWXeiAwZ1SWJwp37crDvRAYO6JNGf9ItDGiWJ1+/Ux9nGul7yZ0keSvKDJHta2VuT7E7yRPt6citPkq8lWUjyYJL3Dl1nS6v/RJItK7VrUJckXr9TH2c7An+nqs6tqo3t+BrgrqraANzVjgEuBja0bStwIwz+CADXAucD5wHXLv4hGMWgLkl0f6c+wmZgR9vfAVw2VH5LDdwPnJRkPfBhYHdV7a+qF4DdwLLj4/uQQpKkia3C6JcC/muSAv5dVW0DTq2qxbf1fgKc2vZPB54a+t69rWxU+UgGdUniiEe/nLKYJ2+2taA97G9X1b4kfxXYneR/Dp+sqmoBv1MGdUlqjiAgPjeUJ19SVe1rX59N8i0GOfFnkqyvqqdbeuXZVn0fcObQt5/RyvYB7z+s/N7l2jWnLkl0m1NP8peS/MriPnAR8DCwE1gcwbIFuKPt7wSuaKNgLgBebGmaO4GLkpzcHpBe1MpG8k5dkuh8nPqpwLeS0C77H6rqT5I8ANye5Ergx8DHWv1dwCXAAoNpaD4JUFX7k3weeKDVu76q9i/XsEFdkuj2QWlVPQn8+hLlzwMXLlFewFUjrrUd2D5u2wZ1ScJpAiSpV/oyTUAfPoMkTcw7dUnqEe/UJalHvFOXpB4JLpIhSb0R4LhxI+KB1ezJZAzqkgQkcKxBXYLPcd2suyBNLIHj1s26F5MzqEsSR3inPsd68BEkaXIJHHf8rHsxOYO6JEFvBqr34CNIUgcM6pLUMz2IiJ0skpFke5Jnkzw84nySfC3JQpIHk7y3i3YlqTMB1o25zbGuVj76OsuvcH0xsKFtW4EbO2pXkrqxmH4ZZ5tjnXSvqu5LctYyVTYDt7SJ4O9PctLiOn1dtC9JEwvg6JexnQ48NXS8t5X9QlBPspXBnTxw4pS6Jkn4oHQ1VNU2YBtAclrNuDuS3kwM6kdkH3Dm0PEZrUyS5secPwQdR1cPSleyE7iijYK5AHjRfLqkueKD0tcl+QbwfuCUJHuBa2nzzVfVvwV2AZcAC8BLwCe7aFeSOmP65XVVdfkK5wu4qou2JGlVOPpFknrEO3VJ6hGDuiT1yOI0AWucQV2SwDt1SeoVH5RKUo94py5JPWJQl6Se6UFE7MFHkKQOOPpFknqkJ+mXaU3oJUnzbXH0yzjbOJdL1iX5fpI/bsdnJ/lOW9bztiRvaeXHt+OFdv6soWt8ppU/nuTD47RrUJckWI1ZGv8x8NjQ8ZeBr1TVu4EXgCtb+ZXAC638K60eSc4BPg78GoPlQv8gyYoJIoO6JEGnQT3JGcDfBW5qxwE+AHyzVdkBXNb2N7dj2vkLW/3NwK1V9UpV/SmDWW7PW6ntHmSQJKkDR5ZTPyXJnqHjbW3ltkX/BvhnwK+047cBP6uqA+14cUlPGFrus6oOJHmx1T8duH/omsPfM5JBXZIWjT/65bmq2rjUiSR/D3i2qr6X5P0d9WxsBnVJgi5Hv7wPuDTJJcAJwF8BvgqclOTYdrc+vKTn4nKfe5McC5wIPM9RLgNqTl2SoLPRL1X1mao6o6rOYvCg8+6q+gfAPcBHWrUtwB1tf2c7pp2/uy0stBP4eBsdczawAfjuSh/DO3VJgmmMU/894NYkXwC+D9zcym8G/jDJArCfwR8CquqRJLcDjwIHgKuq6uBKjRjUJQlWJahX1b3AvW3/SZYYvVJVLwMfHfH9XwS+eCRtGtQlCZwmQJJ6pSfTBPTgI0hSB8JgrMoaZ1CXJDD9Ikm9YvpFknqmBxGxBx9Bkjpg+kWSesT0iyT1yOI0AWucQV2SoDd36p1M6JVkU1tuaSHJNUuc/0SSnyb5Qds+1UW7ktSZ7lc+momJu9eWV7oB+BCDSdwfSLKzqh49rOptVXX1pO1J0qrwTv015wELVfVkVb0K3MpgGSZJWlvWjbnNsS6C+mtLMTWjllz67SQPJvlmkjOXOC9Js9OT9Mu0Fsn4I+CsqvobwG5eX2T1FyTZmmTPYO2/l6bUNUmis0UyZq2LoL7ikktV9XxVvdIObwL+5lIXqqptVbVxsPbfL3fQNUkak3fqr3kA2JDk7CRvYbBqx87hCknWDx1eCjzWQbuS1J2eBPWJu1dVB5JcDdzJ4BHC9rYM0/XAnqraCfyjJJcyWJJpP/CJSduVpE71ZPRLJx+hqnYBuw4r+/2h/c8An+miLUlaLTXnI1vG0YO/S5I0uToGXnWRDEnqhwocWDfuY8ZDq9qXSRjUJQmohIPHjhsSX13VvkzCoC5JzcF1az+pblCXJKAIB+d9DoAxGNQliUFQP2BQl6R+KMKr8z4HwBgM6pKE6RdJ6h2DuiT1hDl1SeqRQfpl7YfEtf8JJKkDgwelb5l1NyZmUJe0Kq7juim2NbkC0y+S1B/9SL9Mazk7SZpri0Max9lWkuSEJN9N8sMkjyT5F6387CTfSbKQ5La2sBBJjm/HC+38WUPX+kwrfzzJh1dq26AuSU1XQR14BfhAVf06cC6wKckFwJeBr1TVu4EXgCtb/SuBF1r5V1o9kpzDYDW5XwM2AX+QZNkOGNQliW7v1Gvg/7XD49pWwAeAb7byHcBlbX9zO6advzBJWvmtVfVKVf0psACct1zbaz+BJEkdKMIr408TcEqSPUPH26pq23CFdkf9PeDdwA3Aj4CfVdWBVmUvcHrbPx14Cl5bIvRF4G2t/P6hyw5/z5IM6pLEEU8T8FxVbVz2elUHgXOTnAR8C/jVCbs4FoO6JLF6c79U1c+S3AP8BnBSkmPb3foZwL5WbR9wJrA3ybHAicDzQ+WLhr9nSebUJak5wLqxtpUkeXu7QyfJLwEfAh4D7gE+0qptAe5o+zvbMe383VVVrfzjbXTM2cAG4LvLte2duiTR+TQB64EdLa9+DHB7Vf1xkkeBW5N8Afg+cHOrfzPwh0kWgP0MRrxQVY8kuR14FDgAXNXSOiMZ1CWJbtMvVfUg8J4lyp9kidErVfUy8NER1/oi8MVx2zaoSxKLo1+c+0WSesFZGiWpZ1wkQ5J6wuXsJKlHDOqS1CNHOE3A3DKoSxL9uVPv5I3SJJvaXL8LSa5Z4vzIuYIlaV50OPXuzEwc1NsbUzcAFwPnAJe3OYCHLTlXsCTNiyKdTRMwS13cqZ8HLFTVk1X1KnArgzmAh42aK1iS5sLiOPVxtnnWRe9emwe42QucP6rOYXMFPzdcKclWYOvg6MQOuiZJ45v31Mo45upPTptkfhvAxmNTe066birt5vnptCO9mVzHdVNtbVJFeNVpAoDx5vsdNVewJM2FxZz6WtdFTv0BYENbJfstDKaM3HlYnVFzBUvSXDCn3rQc+dXAncA6YHubA/h6YE9V7WTEXMGSNE/MqTdVtQvYdVjZ7w/tj5wrWJLmQV9ePprv/0dI0pT0JaduUJckFke/OPeLJPWC6RdJ6hmDuiT1hDl1SeoR1yiVpB5xmgBJ6hHTL5LUM6ZfJKknHNIoST1iUJeknjGnLkk9cYhjnCZAkvrE9Isk9YQ5dUnqkcKc+qr63sH15PlPz7obkt40upsmIMmZwC3AqQz+Xmyrqq8meStwG3AW8GfAx6rqhSQBvgpcArwEfKKq/ke71hbgc+3SX6iqHcu13cUapZK05i2mX8bZxnAA+KdVdQ5wAXBVknOAa4C7qmoDcFc7BrgY2NC2rcCNAO2PwLXA+cB5wLVJTl6u4bm9U5ekaSrCKx3N/VJVTwNPt/3/m+Qx4HRgM/D+Vm0HcC/we638lqoq4P4kJyVZ3+rurqr9AEl2A5uAb4xq26AuSRzxLI2nJNkzdLytqrYtVTHJWcB7gO8Ap7aAD/ATBukZGAT8p4a+bW8rG1U+kkFdkpojGP3yXFVtXKlSkr8M/Cfgn1TV/xmkzgeqqpLUUXV0GebUJYnOc+okOY5BQP/3VfWfW/EzLa1C+/psK98HnDn07We0slHlIxnUJYkW1A+tG2tbSRvNcjPwWFX966FTO4EtbX8LcMdQ+RUZuAB4saVp7gQuSnJye0B6USsbyfSLJAF1KLzycmfTBLwP+B3goSQ/aGX/HPgScHuSK4EfAx9r53YxGM64wGBI4ycBqmp/ks8DD7R61y8+NB3FoC5JQFU4eKCbl4+q6r8BGXH6wiXqF3DViGttB7aP27ZBXZIAis6C+iwZ1CWJwZ36gb8wqEtST4RDB9d+SFz7n0CSulDAmz39MmpymiXqHQQeaof/u6ounaRdSercocDLa/8+d9Jx6qMmpzncz6vq3LYZ0CXNpwNjbnNs0qC+mcGkNLSvl014PUmajcGE6m/6oD5qcprDnZBkT5L7k4wM/Em2tnp7BuPvJWlKehLUV0wgJfk28I4lTn12+GCFyWneWVX7krwLuDvJQ1X1o8MrtVnOtg3aPa3ziW4kaaQC/mLWnZjcikG9qj446lySZ5Ksr6qnD5uc5vBr7Gtfn0xyL4NpKN8Q1CVpZgp4ZdadmNyk6ZdRk9O8pk1Ec3zbP4XBnAiPTtiuJHWrJ+mXSYP6l4APJXkC+GA7JsnGJDe1On8N2JPkh8A9wJeqyqAuab70JKhPNCizqp5n6clp9gCfavv/Hfjrk7QjSatuMaivcWt/pL0kdcGgLkk9Y1CXpJ44BLw8605MzqAuSWD6RZJ6xaAuST1iUJeknjGoS1JPeKcuST1yCPj5rDsxOYO6JMHgTv3grDsxOYO6JC0y/SJJPWFOXZJ6xKAuST3iNAGS1DPeqUtST5h+kaQeebMsPC1JbwqOU5ekHjH9Ikk9UvRimoBjZt0BSZoLi+mXcbYVJNme5NkkDw+VvTXJ7iRPtK8nt/Ik+VqShSQPJnnv0PdsafWfSLJlnI9hUJckeD39Ms62sq8Dmw4ruwa4q6o2AHe1Y4CLgQ1t2wrcCIM/AsC1wPnAecC1i38IlmNQlyToNKhX1X3A/sOKNwM72v4O4LKh8ltq4H7gpCTrgQ8Du6tqf1W9AOzmjX8o3sCcuiTBkQ5pPCXJnqHjbVW1bYXvObWqnm77PwFObfunA08N1dvbykaVL8ugLkmLxh/S+FxVbTzaZqqqktTRfv9yTL9IErw+98s429F5pqVVaF+fbeX7gDOH6p3RykaVL8ugLknwevplnO3o7AQWR7BsAe4YKr+ijYK5AHixpWnuBC5KcnJ7QHpRK1uW6RdJgk7fKE3yDeD9DHLvexmMYvkScHuSK4EfAx9r1XcBlwALwEvAJwGqan+SzwMPtHrXV9XhD1/fwKAuSYs6eqO0qi4fcerCJeoWcNWI62wHth9J2wZ1SYLeTBMwUU49yUeTPJLkUJKRT4KTbEryeHtj6ppR9SRpZlb/QelUTPqg9GHgt4D7RlVIsg64gcFbU+cAlyc5Z8J2Jalb3b5ROjMTpV+q6jGAJMtVOw9YqKonW91bGbxB9egkbUtS5+Y8YI9jGjn1pd6KOn+pikm2Mpj7ADhxtfslSa97syySkeTbwDuWOPXZqrpjifKj1l6z3TZo97RVedtKkpb0Zlkko6o+OGEbR/VWlCRNVU9Gv0wj/fIAsCHJ2QyC+ceBvz+FdiVpfIdwkYwkv9nelvoN4L8kubOVn5ZkF0BVHQCuZvB662PA7VX1yGTdlqRV0NEiGbM06eiXbwHfWqL8zxm89rp4vIvBq7CSNL968CTPCb0kqUcM6pLUIwZ1SeoRJ/SSJKAvw18M6pIE9OWVUoO61pRdXDe1ti6ZYluaB/14+8igLkmAd+qS1CsGdUnqkcIHpZLUG+bUJalHTL9IUo94py5JPeKduiT1iHfqktQjThMgST1i+kWSesb0iyT1hHfqktQjBnVJ6hFHv0hSjzj6RZJ6xPSLJPVIP9IvLjwtScDrd+rjbCtLsinJ40kWklyzKl1egnfqkgR0eaeeZB1wA/AhYC/wQJKdVfVoJw0sw6AuSUDHD0rPAxaq6kmAJLcCm4FVD+qpqtVu46gk+Snw41n3YxmnAM/NuhNHyb7Pzlru/zz3/Z1V9fZJLpDkTxh8xnGcALw8dLytqrYNXesjwKaq+lQ7/h3g/Kq6epI+jmNu79Qn/QdabUn2VNXGWffjaNj32VnL/V/LfR9HVW2adR+64INSSerePuDMoeMzWtmqM6hLUvceADYkOTvJW4CPAzun0fDcpl/WgG0rV5lb9n121nL/13Lfp6qqDiS5GrgTWAdsr6pHptH23D4olSQdOdMvktQjBnVJ6hGD+piSfDTJI0kOJRk5rGtWrwYvJ8lbk+xO8kT7evKIegeT/KBtU3moM8pKP8ckxye5rZ3/TpKzpt/LpY3R908k+enQz/pTs+jnUpJsT/JskodHnE+Sr7XP9mCS9067j1qeQX18DwO/Bdw3qsLQq8EXA+cAlyc5ZzrdW9Y1wF1VtQG4qx0v5edVdW7bLp1e937RmD/HK4EXqurdwFeAL0+3l0s7gt+B24Z+1jdNtZPL+zqw3Hjti4ENbdsK3DiFPukIGNTHVFWPVdXjK1R77dXgqnoVWHw1eNY2Azva/g7gshn2ZRzj/ByHP9M3gQuTZIp9HGVefwfGUlX3AfuXqbIZuKUG7gdOSrJ+Or3TOAzq3TodeGroeG8rm7VTq+rptv8T4NQR9U5IsifJ/UlmGfjH+Tm+VqeqDgAvAm+bSu+WN+7vwG+39MU3k5y5xPl5Na+/42ocpz4kybeBdyxx6rNVdce0+3Mkluv78EFVVZJR41jfWVX7krwLuDvJQ1X1o677Kv4I+EZVvZLk0wz+x/GBGfdJPWFQH1JVH5zwEjN7NXi5vid5Jsn6qnq6/Vf52RHX2Ne+PpnkXuA9wCyC+jg/x8U6e5McC5wIPD+d7i1rxb5X1XA/bwL+1RT61ZWZ/Y5rPKZfujWzV4NXsBPY0va3AG/4X0eSk5Mc3/ZPAd7HFKYJHWGcn+PwZ/oIcHfNx5t0K/b9sBz0pcBjU+zfpHYCV7RRMBcALw6l9jQPqsptjA34TQb5w1eAZ4A7W/lpwK6hepcA/4vBHe5nZ93v1qe3MRj18gTwbeCtrXwjcFPb/1vAQ8AP29crZ9znN/wcgeuBS9v+CcB/BBaA7wLvmvXP+Qj6/i+BR9rP+h7gV2fd56G+fwN4msHyPnsZjDL6XeB32/kwGN3zo/Z7snHWfXb7xc1pAiSpR0y/SFKPGNQlqUcM6pLUIwZ1SeoRg7ok9YhBXZJ6xKAuST3y/wEhVoBaDEhEFAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beBR7QoLBUr-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "d6b97fbf-8ae5-4fbd-8af7-d60799cf8968"
      },
      "source": [
        "channel_out_array = numpy.transpose(channel_out)\n",
        "\n",
        "for i in range (int(channel_size)):\n",
        "  plt.hist2d(channel_out_array[2*i], channel_out_array[2*i+1], (50, 50), cmap=plt.cm.jet)\n",
        "  plt.colorbar()\n",
        "  plt.show()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxvEzpqnTPVB",
        "outputId": "20604d4c-25db-4eac-d151-bae6d8b0b5fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input_message_length = 4\n",
        "encoder_output_length = 4\n",
        "channel_size = 2\n",
        "\n",
        "\n",
        "awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [2*channel_size])\n",
        "awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)\n",
        "\n",
        "\n",
        "decoder_input_x = tf.placeholder(\"float32\", [None, input_message_length], name=\"decoder_input_x\")\n",
        "\n",
        "snr_std = 7.0\n",
        "\n",
        "input_message_x = Input(shape=(input_message_length,))\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "enc_layer1 = Dense(encoder_output_length, activation='tanh')(input_message_x)\n",
        "enc_layer2 = Dense(2*channel_size, activation='tanh')(enc_layer1)\n",
        "enc_layer3 =  enc_layer2 / tf.sqrt(tf.reduce_mean(tf.square(enc_layer2)))\n",
        "encoder = Model(input_message_x, enc_layer3)\n",
        "\n",
        "awgn_channel = GaussianNoise(Snr2Sigma(snr_std),input_shape=(2*channel_size,))\n",
        "\n",
        "# create a placeholder for an encoded (32-dimensional) input\n",
        "encoded_input = Input(shape=(2*channel_size,))\n",
        "dec_layer1 = Dense(encoder_output_length, activation='tanh')(encoded_input)\n",
        "dec_layer2 = Dense(input_message_length, activation='sigmoid')(dec_layer1)\n",
        "# this model maps an encoded input to its decoder representation\n",
        "decoder = Model(encoded_input, dec_layer2)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))\n",
        "\n",
        "decoder_output = (tf.nn.sigmoid(decoder_input_x))\n",
        "\n",
        "print(encoder.summary())\n",
        "print(decoder.summary())\n",
        "print(autoencoder.summary())"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_231\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_11 (InputLayer)           [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_20 (Dense)                (None, 4)            20          input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_21 (Dense)                (None, 4)            20          dense_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Square_5 (TensorFlo multiple             0           dense_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mean_5 (TensorFlowO multiple             0           tf_op_layer_Square_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sqrt_5 (TensorFlowO multiple             0           tf_op_layer_Mean_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_truediv_5 (TensorFl multiple             0           dense_21[0][0]                   \n",
            "                                                                 tf_op_layer_Sqrt_5[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 40\n",
            "Trainable params: 40\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"functional_233\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_12 (InputLayer)        [(None, 4)]               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 4)                 20        \n",
            "=================================================================\n",
            "Total params: 40\n",
            "Trainable params: 40\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"functional_235\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_11 (InputLayer)        [(None, 4)]               0         \n",
            "_________________________________________________________________\n",
            "functional_231 (Functional)  (None, 4)                 40        \n",
            "_________________________________________________________________\n",
            "gaussian_noise_105 (Gaussian (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "functional_233 (Functional)  (None, 4)                 40        \n",
            "=================================================================\n",
            "Total params: 80\n",
            "Trainable params: 80\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFrTQ7WATei5",
        "outputId": "7040d282-6a84-4b54-fc06-918251e67149",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "training_input_message = numpy.random.randint(2, size=(NUM_OF_INPUT_MESSAGE*10,input_message_length))\n",
        "print (training_input_message)\n",
        "print (len(training_input_message))\n",
        "print(input_message_length)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 1 0]\n",
            " [0 0 1 0]\n",
            " [1 1 0 1]\n",
            " ...\n",
            " [0 1 1 0]\n",
            " [0 0 1 1]\n",
            " [1 1 1 1]]\n",
            "10000\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf-4ohBLT0m_",
        "outputId": "abb5ee8a-e1e4-4b18-901b-02aa484989ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_input_message = numpy.random.randint(2, size=(NUM_OF_INPUT_MESSAGE*10,input_message_length))\n",
        "print (test_input_message)\n",
        "print (len(test_input_message))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 1 1]\n",
            " [1 1 1 0]\n",
            " [1 1 1 0]\n",
            " ...\n",
            " [0 1 1 0]\n",
            " [1 0 0 0]\n",
            " [0 1 1 0]]\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqhq1SZKTuk-",
        "outputId": "bbcddea1-decb-4ba0-b5eb-574b907aa615",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "\n",
        "#def custom_losff_fucntion (act, pred):\n",
        "#  return (tf.reduce_mean(-1*(act * tf.log(pred) + (1-act)*tf.log(1-pred))))\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "autoencoder.compile(optimizer=opt, loss='binary_crossentropy')\n",
        "#autoencoder.compile(optimizer=opt, loss=custom_losff_fucntion)\n",
        "#loss='mean_squared_error'\n",
        "#for snr in (numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)):\n",
        "for snr in (numpy.arange (0, 10, SNR_STEP_SIZE)):\n",
        "  sigma = 1.0*Snr2Sigma (snr)\n",
        "  snr_std = sigma\n",
        "  print (\"Training for SNR=\", snr, \" sigma=\", sigma) \n",
        "  awgn_channel = GaussianNoise(sigma,input_shape=(channel_size,))\n",
        "  autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))\n",
        "  opt = keras.optimizers.Adam(learning_rate=0.003)\n",
        "  autoencoder.compile(optimizer=opt, loss='binary_crossentropy')\n",
        "  autoencoder.fit(training_input_message, training_input_message,\n",
        "                #epochs=50, original\n",
        "                epochs=25,\n",
        "                batch_size=500,\n",
        "                shuffle=False,\n",
        "                validation_data=(test_input_message, test_input_message))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for SNR= 0.0  sigma= 1.0\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 170us/sample - loss: 0.7267 - val_loss: 0.6465\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.6681 - val_loss: 0.5898\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.6185 - val_loss: 0.5525\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.5858 - val_loss: 0.5249\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.5625 - val_loss: 0.4933\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.5387 - val_loss: 0.4580\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.5182 - val_loss: 0.4290\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.5028 - val_loss: 0.4081\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4909 - val_loss: 0.3896\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4812 - val_loss: 0.3732\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4720 - val_loss: 0.3587\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4636 - val_loss: 0.3456\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4614 - val_loss: 0.3356\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4541 - val_loss: 0.3262\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4511 - val_loss: 0.3177\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4470 - val_loss: 0.3102\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4416 - val_loss: 0.3040\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4397 - val_loss: 0.2979\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4352 - val_loss: 0.2925\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4279 - val_loss: 0.2873\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4277 - val_loss: 0.2818\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4241 - val_loss: 0.2773\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4237 - val_loss: 0.2728\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4181 - val_loss: 0.2681\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4174 - val_loss: 0.2644\n",
            "Training for SNR= 0.5  sigma= 0.9440608762859234\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 174us/sample - loss: 0.3965 - val_loss: 0.2571\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3966 - val_loss: 0.2510\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3922 - val_loss: 0.2461\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3920 - val_loss: 0.2421\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3887 - val_loss: 0.2385\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3845 - val_loss: 0.2358\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3855 - val_loss: 0.2325\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3850 - val_loss: 0.2304\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3812 - val_loss: 0.2284\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3838 - val_loss: 0.2267\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3865 - val_loss: 0.2257\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3794 - val_loss: 0.2245\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3811 - val_loss: 0.2233\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3816 - val_loss: 0.2220\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3823 - val_loss: 0.2214\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3784 - val_loss: 0.2207\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3785 - val_loss: 0.2199\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3791 - val_loss: 0.2194\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3820 - val_loss: 0.2188\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3816 - val_loss: 0.2186\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3795 - val_loss: 0.2183\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3764 - val_loss: 0.2176\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3784 - val_loss: 0.2172\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3818 - val_loss: 0.2174\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3777 - val_loss: 0.2172\n",
            "Training for SNR= 1.0  sigma= 0.8912509381337456\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 173us/sample - loss: 0.3551 - val_loss: 0.2143\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3589 - val_loss: 0.2122\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3548 - val_loss: 0.2104\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3598 - val_loss: 0.2093\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3534 - val_loss: 0.2080\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3580 - val_loss: 0.2073\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3514 - val_loss: 0.2064\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3597 - val_loss: 0.2062\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3580 - val_loss: 0.2060\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3546 - val_loss: 0.2055\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3544 - val_loss: 0.2050\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3560 - val_loss: 0.2051\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3529 - val_loss: 0.2046\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3534 - val_loss: 0.2041\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3533 - val_loss: 0.2040\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3579 - val_loss: 0.2042\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3523 - val_loss: 0.2038\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3554 - val_loss: 0.2037\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3555 - val_loss: 0.2036\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3534 - val_loss: 0.2036\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3545 - val_loss: 0.2038\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3522 - val_loss: 0.2037\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3517 - val_loss: 0.2034\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3508 - val_loss: 0.2028\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3522 - val_loss: 0.2026\n",
            "Training for SNR= 1.5  sigma= 0.8413951416451951\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 175us/sample - loss: 0.3332 - val_loss: 0.2006\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3332 - val_loss: 0.1990\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3303 - val_loss: 0.1977\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3298 - val_loss: 0.1969\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3303 - val_loss: 0.1960\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3311 - val_loss: 0.1958\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3311 - val_loss: 0.1955\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3334 - val_loss: 0.1953\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3324 - val_loss: 0.1953\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.3310 - val_loss: 0.1950\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3322 - val_loss: 0.1949\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3263 - val_loss: 0.1946\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3336 - val_loss: 0.1945\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3327 - val_loss: 0.1947\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3320 - val_loss: 0.1945\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3332 - val_loss: 0.1948\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3261 - val_loss: 0.1943\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3317 - val_loss: 0.1943\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3290 - val_loss: 0.1941\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3321 - val_loss: 0.1943\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3307 - val_loss: 0.1943\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3308 - val_loss: 0.1941\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3314 - val_loss: 0.1941\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3311 - val_loss: 0.1940\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3327 - val_loss: 0.1942\n",
            "Training for SNR= 2.0  sigma= 0.7943282347242815\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 173us/sample - loss: 0.3104 - val_loss: 0.1924\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3085 - val_loss: 0.1908\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3129 - val_loss: 0.1900\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3098 - val_loss: 0.1896\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3105 - val_loss: 0.1891\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3139 - val_loss: 0.1891\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3135 - val_loss: 0.1888\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3111 - val_loss: 0.1886\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3088 - val_loss: 0.1885\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3105 - val_loss: 0.1883\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3077 - val_loss: 0.1882\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3084 - val_loss: 0.1879\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3079 - val_loss: 0.1876\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3102 - val_loss: 0.1877\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3115 - val_loss: 0.1878\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3109 - val_loss: 0.1877\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3123 - val_loss: 0.1877\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3116 - val_loss: 0.1879\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3078 - val_loss: 0.1877\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3069 - val_loss: 0.1875\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3115 - val_loss: 0.1875\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3083 - val_loss: 0.1875\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3113 - val_loss: 0.1875\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3128 - val_loss: 0.1878\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3124 - val_loss: 0.1879\n",
            "Training for SNR= 2.5  sigma= 0.7498942093324559\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 175us/sample - loss: 0.2916 - val_loss: 0.1863\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2927 - val_loss: 0.1855\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2934 - val_loss: 0.1849\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2920 - val_loss: 0.1846\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2906 - val_loss: 0.1842\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2936 - val_loss: 0.1840\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2920 - val_loss: 0.1839\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2902 - val_loss: 0.1837\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2939 - val_loss: 0.1837\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2915 - val_loss: 0.1837\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2903 - val_loss: 0.1834\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2892 - val_loss: 0.1833\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2931 - val_loss: 0.1834\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2921 - val_loss: 0.1834\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2934 - val_loss: 0.1834\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2931 - val_loss: 0.1835\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2900 - val_loss: 0.1832\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2898 - val_loss: 0.1832\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2913 - val_loss: 0.1832\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2924 - val_loss: 0.1833\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2951 - val_loss: 0.1833\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2932 - val_loss: 0.1835\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2904 - val_loss: 0.1833\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2933 - val_loss: 0.1832\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2898 - val_loss: 0.1832\n",
            "Training for SNR= 3.0  sigma= 0.7079457843841379\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 180us/sample - loss: 0.2739 - val_loss: 0.1822\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2751 - val_loss: 0.1815\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2735 - val_loss: 0.1811\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2714 - val_loss: 0.1807\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2775 - val_loss: 0.1805\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2689 - val_loss: 0.1803\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2749 - val_loss: 0.1802\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2731 - val_loss: 0.1801\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2756 - val_loss: 0.1803\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2750 - val_loss: 0.1801\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2732 - val_loss: 0.1801\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2731 - val_loss: 0.1800\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2707 - val_loss: 0.1799\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2727 - val_loss: 0.1798\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2724 - val_loss: 0.1798\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2721 - val_loss: 0.1798\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2735 - val_loss: 0.1797\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2723 - val_loss: 0.1798\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2714 - val_loss: 0.1797\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2747 - val_loss: 0.1798\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2762 - val_loss: 0.1796\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2709 - val_loss: 0.1797\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2698 - val_loss: 0.1795\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2739 - val_loss: 0.1795\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2728 - val_loss: 0.1797\n",
            "Training for SNR= 3.5  sigma= 0.6683439175686147\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 183us/sample - loss: 0.2574 - val_loss: 0.1791\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2555 - val_loss: 0.1787\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2518 - val_loss: 0.1783\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2533 - val_loss: 0.1780\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2574 - val_loss: 0.1778\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2567 - val_loss: 0.1778\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2532 - val_loss: 0.1777\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2549 - val_loss: 0.1775\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2542 - val_loss: 0.1775\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2535 - val_loss: 0.1774\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2576 - val_loss: 0.1775\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2537 - val_loss: 0.1774\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2545 - val_loss: 0.1774\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2573 - val_loss: 0.1773\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2579 - val_loss: 0.1774\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2553 - val_loss: 0.1775\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2546 - val_loss: 0.1774\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2580 - val_loss: 0.1775\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2575 - val_loss: 0.1774\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2557 - val_loss: 0.1775\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2561 - val_loss: 0.1775\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2505 - val_loss: 0.1773\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2591 - val_loss: 0.1774\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2534 - val_loss: 0.1774\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2534 - val_loss: 0.1773\n",
            "Training for SNR= 4.0  sigma= 0.6309573444801932\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 182us/sample - loss: 0.2415 - val_loss: 0.1770\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2422 - val_loss: 0.1768\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2416 - val_loss: 0.1766\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2401 - val_loss: 0.1764\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2401 - val_loss: 0.1763\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2419 - val_loss: 0.1762\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2390 - val_loss: 0.1761\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2401 - val_loss: 0.1761\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2427 - val_loss: 0.1760\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2407 - val_loss: 0.1760\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2388 - val_loss: 0.1759\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2416 - val_loss: 0.1760\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2408 - val_loss: 0.1758\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2388 - val_loss: 0.1759\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2411 - val_loss: 0.1758\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2398 - val_loss: 0.1758\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2402 - val_loss: 0.1758\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2391 - val_loss: 0.1757\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2400 - val_loss: 0.1758\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2422 - val_loss: 0.1758\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2398 - val_loss: 0.1758\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2423 - val_loss: 0.1758\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2432 - val_loss: 0.1759\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2385 - val_loss: 0.1759\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2398 - val_loss: 0.1758\n",
            "Training for SNR= 4.5  sigma= 0.5956621435290105\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 185us/sample - loss: 0.2268 - val_loss: 0.1756\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2282 - val_loss: 0.1755\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2278 - val_loss: 0.1754\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2232 - val_loss: 0.1753\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2264 - val_loss: 0.1752\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2262 - val_loss: 0.1751\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2281 - val_loss: 0.1750\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2258 - val_loss: 0.1750\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2284 - val_loss: 0.1750\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2274 - val_loss: 0.1750\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2276 - val_loss: 0.1749\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2263 - val_loss: 0.1750\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2317 - val_loss: 0.1750\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2285 - val_loss: 0.1749\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2243 - val_loss: 0.1751\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2262 - val_loss: 0.1749\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2270 - val_loss: 0.1749\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2246 - val_loss: 0.1749\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2255 - val_loss: 0.1749\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2260 - val_loss: 0.1748\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2264 - val_loss: 0.1748\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2249 - val_loss: 0.1748\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2244 - val_loss: 0.1747\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2286 - val_loss: 0.1747\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2265 - val_loss: 0.1747\n",
            "Training for SNR= 5.0  sigma= 0.5623413251903491\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 188us/sample - loss: 0.2152 - val_loss: 0.1747\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2140 - val_loss: 0.1746\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2142 - val_loss: 0.1746\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2127 - val_loss: 0.1745\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2169 - val_loss: 0.1744\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2150 - val_loss: 0.1743\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.2137 - val_loss: 0.1742\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2125 - val_loss: 0.1742\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2150 - val_loss: 0.1742\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2143 - val_loss: 0.1743\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2194 - val_loss: 0.1741\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2129 - val_loss: 0.1741\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2141 - val_loss: 0.1741\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2126 - val_loss: 0.1739\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2137 - val_loss: 0.1740\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.2144 - val_loss: 0.1738\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2160 - val_loss: 0.1738\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2149 - val_loss: 0.1737\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2134 - val_loss: 0.1736\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2154 - val_loss: 0.1735\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2140 - val_loss: 0.1733\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2132 - val_loss: 0.1731\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2124 - val_loss: 0.1726\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2127 - val_loss: 0.1718\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2142 - val_loss: 0.1707\n",
            "Training for SNR= 5.5  sigma= 0.5308844442309884\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 191us/sample - loss: 0.2014 - val_loss: 0.1680\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1983 - val_loss: 0.1636\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1949 - val_loss: 0.1548\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1903 - val_loss: 0.1393\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1780 - val_loss: 0.1177\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1674 - val_loss: 0.1011\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1585 - val_loss: 0.0901\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1538 - val_loss: 0.0828\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1496 - val_loss: 0.0768\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1440 - val_loss: 0.0706\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1395 - val_loss: 0.0663\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1385 - val_loss: 0.0643\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1332 - val_loss: 0.0599\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1321 - val_loss: 0.0576\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1299 - val_loss: 0.0542\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1265 - val_loss: 0.0517\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1254 - val_loss: 0.0516\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1241 - val_loss: 0.0476\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1208 - val_loss: 0.0461\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1193 - val_loss: 0.0443\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1159 - val_loss: 0.0428\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.1149 - val_loss: 0.0415\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1122 - val_loss: 0.0395\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1117 - val_loss: 0.0379\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1117 - val_loss: 0.0374\n",
            "Training for SNR= 6.0  sigma= 0.5011872336272722\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 190us/sample - loss: 0.0967 - val_loss: 0.0348\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0922 - val_loss: 0.0326\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0902 - val_loss: 0.0313\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0924 - val_loss: 0.0303\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0883 - val_loss: 0.0283\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0874 - val_loss: 0.0267\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0868 - val_loss: 0.0257\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0855 - val_loss: 0.0243\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0796 - val_loss: 0.0228\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0804 - val_loss: 0.0218\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0775 - val_loss: 0.0208\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0779 - val_loss: 0.0195\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0787 - val_loss: 0.0189\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0796 - val_loss: 0.0180\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0767 - val_loss: 0.0170\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0760 - val_loss: 0.0164\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0748 - val_loss: 0.0156\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0735 - val_loss: 0.0151\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0739 - val_loss: 0.0145\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0736 - val_loss: 0.0139\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0760 - val_loss: 0.0133\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0716 - val_loss: 0.0129\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0705 - val_loss: 0.0125\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0718 - val_loss: 0.0121\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0735 - val_loss: 0.0116\n",
            "Training for SNR= 6.5  sigma= 0.47315125896148047\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 192us/sample - loss: 0.0568 - val_loss: 0.0109\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0563 - val_loss: 0.0103\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0589 - val_loss: 0.0097\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0534 - val_loss: 0.0092\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0562 - val_loss: 0.0087\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0532 - val_loss: 0.0083\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0543 - val_loss: 0.0079\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0506 - val_loss: 0.0075\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0529 - val_loss: 0.0072\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0502 - val_loss: 0.0069\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0532 - val_loss: 0.0066\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0539 - val_loss: 0.0063\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0542 - val_loss: 0.0061\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0510 - val_loss: 0.0059\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0529 - val_loss: 0.0056\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0518 - val_loss: 0.0055\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0513 - val_loss: 0.0053\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0491 - val_loss: 0.0051\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0503 - val_loss: 0.0049\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0513 - val_loss: 0.0048\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0517 - val_loss: 0.0046\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0496 - val_loss: 0.0045\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0519 - val_loss: 0.0044\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0529 - val_loss: 0.0043\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0541 - val_loss: 0.0042\n",
            "Training for SNR= 7.0  sigma= 0.44668359215096315\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "10000/10000 [==============================] - 2s 192us/sample - loss: 0.0405 - val_loss: 0.0040\n",
            "Epoch 2/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0384 - val_loss: 0.0037\n",
            "Epoch 3/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0381 - val_loss: 0.0035\n",
            "Epoch 4/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0354 - val_loss: 0.0034\n",
            "Epoch 5/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0360 - val_loss: 0.0032\n",
            "Epoch 6/25\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0368 - val_loss: 0.0030\n",
            "Epoch 7/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0372 - val_loss: 0.0029\n",
            "Epoch 8/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0360 - val_loss: 0.0028\n",
            "Epoch 9/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0380 - val_loss: 0.0026\n",
            "Epoch 10/25\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0368 - val_loss: 0.0025\n",
            "Epoch 11/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0355 - val_loss: 0.0025\n",
            "Epoch 12/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0379 - val_loss: 0.0024\n",
            "Epoch 13/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0365 - val_loss: 0.0023\n",
            "Epoch 14/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0382 - val_loss: 0.0022\n",
            "Epoch 15/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0353 - val_loss: 0.0022\n",
            "Epoch 16/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0380 - val_loss: 0.0021\n",
            "Epoch 17/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0367 - val_loss: 0.0020\n",
            "Epoch 18/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0360 - val_loss: 0.0020\n",
            "Epoch 19/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0397 - val_loss: 0.0019\n",
            "Epoch 20/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0359 - val_loss: 0.0019\n",
            "Epoch 21/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0375 - val_loss: 0.0018\n",
            "Epoch 22/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0398 - val_loss: 0.0018\n",
            "Epoch 23/25\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0384 - val_loss: 0.0018\n",
            "Epoch 24/25\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0378 - val_loss: 0.0017\n",
            "Epoch 25/25\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0361 - val_loss: 0.0017\n",
            "Training for SNR= 7.5  sigma= 0.4216965034285822\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g762No1Tmg2"
      },
      "source": [
        "\n",
        "# Here I am using trained model\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_dl_tensor_4  = numpy.array(())\n",
        "times_per_iter_dl_tensor_4 = numpy.array(())\n",
        "\n",
        "#awgn_channel_tx = GaussianNoise(0.5,input_shape=(channel_size,))\n",
        "\n",
        "#awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [channel_size])\n",
        "#awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "#awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "#awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)\n",
        "\n",
        "train_init = tf.global_variables_initializer ()\n",
        "train_sess = tf.Session ()\n",
        "\n",
        "channel_in = []\n",
        "channel_out = []\n",
        "\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    input_message_xx = training_input_message [i:i+1]\n",
        "    #print (\"input\", input_message_xx)\n",
        "    encoded_message = encoder.predict(input_message_xx)\n",
        "    channel_in.append(encoded_message[0])\n",
        "    #encoded_message = numpy.around(encoded_message > 0.5).astype(int)\n",
        "    #print(\"encoded:\",encoded_message)\n",
        "    #print (\"encoded\", encoded_message)\n",
        "    #noised_message = awgn_channel.predict (encoded_message)\n",
        "    #noised_message = commpy.channels.awgn(encoded_message, snr)\n",
        "    noised_message = train_sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message[0]})[0].reshape([1,2*channel_size])\n",
        "    channel_out.append(noised_message[0]) \n",
        "    #noised_message = encoded_message[0] + numpy.random.normal(0, sigma, [1,channel_size])\n",
        "    #print (noised_message)\n",
        "    #awgn_channel = GaussianNoise(sigma,input_shape=(channel_size,))\n",
        "    #noised_message = awgn_channel.predict(encoded_message)\n",
        "    #noised_message = awgn_layer (encoded_message)    \n",
        "    #print(noised_message)\n",
        "    decoded_message = decoder.predict(noised_message)\n",
        "    #print (\"decoded1:\", decoded_message)\n",
        "    #decoded_message = train_sess.run ([decoder_output], feed_dict={decoder_input_x:decoded_message})\n",
        "    #print (\"decoded2:\", decoded_message)\n",
        "    #decoded_message = autoencoder.predict(input_message_xx)\n",
        "    decoded_message = numpy.around(decoded_message[0]).astype(int)\n",
        "    #print (\"decoded3:\", decoded_message)\n",
        "    #decoded_message = numpy.around(decoded_message > 0.5).astype(int)\n",
        "    #print (\"decoded:\", decoded_message)\n",
        "    #print (\".\")\n",
        "    #autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))\n",
        "    #decoded_message = autoencoder.predict(input_message_xx)\n",
        "    #print (\"output\", decoded_message)\n",
        "    if abs(decoded_message-training_input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "      #print (\"Error\")\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_dl_tensor_4=numpy.append(ber_per_iter_dl_tensor_4 ,ber)\n",
        "  times_per_iter_dl_tensor_4=numpy.append(times_per_iter_dl_tensor_4, total_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcU35gwRT84B"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1) = plt.subplots(1,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,bler_per_iter_ldpc_itpp_psk_4,'', label=\"itpp-ldpc(18,9)-qpsk(channel=9)\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor_2,'', label=\"ai-dl(2-2-1)\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor_4,'', label=\"ai-dl(4-4-2)\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_uncoded_commpy_psk_2,'', label=\"commpy-psk2-uncoded\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_uncoded_commpy_psk_4,'', label=\"commpy-psk4-uncoded\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_uncoded_itpp_psk_2,'', label=\"itpp-psk2-uncoded\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_ham_itpp_psk_4,'', label=\"itpp-ham(7,4)(input=8,channel=7)\") # plot BER vs SNR\n",
        "ax1.set_ylabel('BLER')\n",
        "ax1.set_title('Arch-2 ({},{},{})'.format(input_message_length,2*input_message_length, channel_size))\n",
        "#ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "#ax2.set_xlabel('$E_b/$N_0$')\n",
        "#ax2.set_ylabel('Decoding Time [s]')\n",
        "#ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "#            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "#            xy=(1, 0.35), xycoords='axes fraction',\n",
        "#            xytext=(-20, 20), textcoords='offset pixels',\n",
        "#            horizontalalignment='right',\n",
        "#            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(2*channel_size,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oKcLIt6XUh1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "channel_in_array = numpy.transpose(channel_in)\n",
        "\n",
        "for i in range (int(channel_size)):\n",
        "  plt.scatter(channel_in_array[i*2], channel_in_array[i*2+1])\n",
        "  plt.show()\n",
        "  plt.hist2d(channel_in_array[i*2], channel_in_array[i*2+1], cmap=plt.cm.jet)\n",
        "  plt.colorbar()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZnu97bYXlds"
      },
      "source": [
        "channel_out_array = numpy.transpose(channel_out)\n",
        "\n",
        "for i in range (int(channel_size)):\n",
        "  plt.hist2d(channel_out_array[2*i], channel_out_array[2*i+1], (50, 50), cmap=plt.cm.jet)\n",
        "  plt.colorbar()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}