{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UnCoded-LargerBlockSize.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmPt/3/jgy2H3rxx0XOARQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamviji/project/blob/master/PostMidTerm/Experiment/UnCoded_LargerBlockSize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIrI14uhAhxz",
        "outputId": "0aa64b7c-ed58-449d-a14d-fc5fae10da7d"
      },
      "source": [
        "import numpy \n",
        "import time\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior ()\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk6nCVQKAnXQ"
      },
      "source": [
        "SNR_BEGIN = 0\n",
        "SNR_END = 10\n",
        "SNR_STEP_SIZE = 0.5\n",
        "NUM_OF_INPUT_MESSAGE = 1000\n",
        "bler_per_iter_uncoded_commpy_psk_2= [0.521, 0.473, 0.436, 0.37,  0.304, 0.259, 0.187, 0.138, 0.098, 0.098, 0.052, 0.028, 0.012, 0.011, 0.009, 0.002, 0.0,  0.001, 0.,    0.0]\n",
        "bler_per_iter_uncoded_itpp_psk_2= [0.518, 0.478, 0.415, 0.355, 0.305, 0.227, 0.177, 0.149, 0.11,  0.075, 0.055, 0.023, 0.014, 0.014, 0.015, 0.001, 0.003, 0.001, 0.,    0. ]\n",
        "bler_per_iter_uncoded_commpy_psk_4 = [0.815, 0.793, 0.75,  0.714, 0.64,  0.639, 0.526, 0.49,  0.433, 0.371, 0.335, 0.236, 0.204, 0.154, 0.129, 0.08,  0.063, 0.046, 0.023, 0.018]\n",
        "bler_per_iter_uncoded_itpp_psk_4 = [0.814, 0.767, 0.729, 0.702, 0.66,  0.616, 0.563, 0.511, 0.442, 0.4,   0.294, 0.277, 0.228, 0.17,  0.114, 0.087, 0.05,  0.037, 0.022, 0.017]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oirzutQiApc1"
      },
      "source": [
        "# Helper Function\n",
        "def bits2int(a, axis=-1):\n",
        "    return numpy.right_shift(numpy.packbits(a, axis=axis), 8 - a.shape[axis]).squeeze()\n",
        "\n",
        "\n",
        "def row_bits2int(arr):\n",
        "    n = arr.shape[1]  # number of columns\n",
        "    # shift the bits of the first column to the left by n - 1\n",
        "    a = arr[:, 0] << n - 1  \n",
        "\n",
        "    for j in range(1, n):\n",
        "        # \"overlay\" with the shifted bits of the next column\n",
        "        a |= arr[:, j] << n - 1 - j  \n",
        "    return a\n",
        "\n",
        "def Snr2Sigma(snr):\n",
        "  sigma = 10 ** (- snr / 20)\n",
        "  return sigma\n",
        "\n",
        "def timer_update(i,current,time_tot,tic_incr=500):\n",
        "    last = current\n",
        "    current = time.time()\n",
        "    t_diff = current-last\n",
        "    print('SNR: {:04.3f} - Iter: {} - Last {} iterations took {:03.2f}s'.format(snr,i+1,tic_incr,t_diff))\n",
        "    return time_tot + t_diff"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeI6mq5yIsmO",
        "outputId": "2294a26a-0311-40cb-e330-d9a3d76d2e7e"
      },
      "source": [
        "# PSK-4 (QPSK) Equivalant, larger block size \n",
        "channel_size = 5\n",
        "input_message_length = 10\n",
        "num_hidden_1 = 2 * channel_size \n",
        "print (\"input_message_length=\", input_message_length, \"channel_size\", channel_size)\n",
        "\n",
        "input_message_x = tf.placeholder(\"float32\", [None, input_message_length], name=\"input_message_x\")\n",
        "awgn_noise_std_dev_x = tf.placeholder(\"float32\", name =\"awgn_noise_std_dev\")\n",
        "input_channel_x = tf.placeholder(\"float32\", [None, 2 * channel_size], name=\"input_channel_x\")\n",
        "\n",
        "weights = {\n",
        "  \"encoder_l1\" : tf.Variable (tf.random_uniform([input_message_length, input_message_length], -1, 1), name=\"encoder_l1_weights\"),\n",
        "  \"encoder_l2\" : tf.Variable (tf.random_uniform([input_message_length, num_hidden_1], -1, 1), name=\"encoder_l2_weights\"),\n",
        "\n",
        "  \"decoder_l1\" : tf.Variable (tf.random_uniform([num_hidden_1, input_message_length], -1, 1), name=\"decoder_l1_weights\"),\n",
        "  \"decoder_l2\" : tf.Variable (tf.random_uniform([input_message_length, input_message_length], -1, 1), name=\"decoder_l2_weights\"),\n",
        "}\n",
        "\n",
        "biases = {\n",
        "  \"encoder_l1\" : tf.Variable (tf.random_uniform([input_message_length], -1,1), name=\"encoder_l1_bias\"),\n",
        "  \"encoder_l2\" : tf.Variable (tf.random_uniform([num_hidden_1], -1,1), name=\"encoder_l2_bias\"),\n",
        "  \"decoder_l1\" : tf.Variable (tf.random_uniform([input_message_length], -1,1), name=\"decoder_l1_bias\"),\n",
        "  \"decoder_l2\" : tf.Variable (tf.random_uniform([input_message_length], -1,1), name=\"decoder_l2_bias\"),\n",
        "}\n",
        "\n",
        "def dl_encoder (x):\n",
        "  layer_1 = tf.nn.tanh (tf.matmul(x, weights['encoder_l1']) + biases['encoder_l1'])\n",
        "  layer_2 = tf.nn.tanh (tf.matmul(layer_1, weights['encoder_l2']) + biases['encoder_l2'])\n",
        "  #layer_2 = tf.round(layer_1)\n",
        "  layer_3 =  layer_2 / tf.sqrt(tf.reduce_mean(tf.square(layer_2)))\n",
        "  #layer_2 =  tf.nn.relu(layer_1)\n",
        "  return layer_3\n",
        "\n",
        "def dl_decoder (x):\n",
        "  layer_1 = tf.nn.tanh (tf.matmul(x, weights['decoder_l1']) + biases['decoder_l1'])\n",
        "  layer_2 = tf.nn.sigmoid (tf.matmul(layer_1, weights['decoder_l2']) + biases['decoder_l2'])\n",
        "  return layer_2\n",
        "\n",
        "def awgn_layer(x):\n",
        "  awgn_noise = tf.random.normal(tf.shape(x), stddev=awgn_noise_std_dev_x,  name=\"awgn_noise\")\n",
        "  awgn_channel_output = tf.add(x, awgn_noise, name =\"x_and_noise\")\n",
        "  return awgn_channel_output\n",
        "\n",
        "\n",
        "dl_encoder_output = dl_encoder(input_message_x)\n",
        "dl_decoder_input = awgn_layer(dl_encoder_output)\n",
        "#awgn_noise = tf.random.normal(tf.shape(dl_encoder_output), stddev=awgn_noise_std_dev,  name=\"awgn_noise\")\n",
        "#dl_decoder_input = tf.add(dl_encoder_output, awgn_noise, name =\"x_and_noise\")\n",
        "dl_decoder_output = dl_decoder (dl_decoder_input)\n",
        "dl_decoder_only_output = dl_decoder(input_channel_x)\n",
        "\n",
        "\n",
        "loss1 = tf.reduce_mean (-1 * (input_message_x*tf.log(dl_decoder_output) + (1 - input_message_x)*tf.log(1 - dl_decoder_output) ))\n",
        "lr = tf.placeholder(dtype=tf.float32,shape=[])\n",
        "opt = tf.train.AdamOptimizer(learning_rate=lr).minimize (loss1)\n",
        "\n",
        "\n",
        "awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [2*channel_size])\n",
        "awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_message_length= 10 channel_size 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZieQvmEAIvcS",
        "outputId": "fa0af4aa-df6e-4b60-9897-e591a7a05865"
      },
      "source": [
        "training_input_message = numpy.random.randint(2, size=(NUM_OF_INPUT_MESSAGE*10,input_message_length))\n",
        "print (training_input_message)\n",
        "print (len(training_input_message))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 1 ... 0 1 1]\n",
            " [0 0 1 ... 1 0 1]\n",
            " [0 1 0 ... 1 1 1]\n",
            " ...\n",
            " [1 0 0 ... 1 1 1]\n",
            " [0 0 1 ... 1 0 0]\n",
            " [0 1 0 ... 0 0 1]]\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NI7Xw41I94s",
        "outputId": "57a8ad0c-8aa6-4d98-ce96-5ee827466a00"
      },
      "source": [
        "batch_size = 500\n",
        "\n",
        "\n",
        "# Training\n",
        "train_init = tf.global_variables_initializer ()\n",
        "train_sess = tf.Session ()\n",
        "\n",
        "epochs = 70\n",
        "outer_ephocs = 1\n",
        "num_of_batches = len(training_input_message) / batch_size\n",
        "display_step = 400\n",
        "print (\"batch_size:\", batch_size, \"num_of_batcches:\", num_of_batches)\n",
        "train_sess.run(train_init)\n",
        "l = 0\n",
        "lrate = 0.1\n",
        "i = 0\n",
        "for oe in range(outer_ephocs):\n",
        "  for snr in (numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)):\n",
        "    sigma = 1.0*Snr2Sigma (snr)\n",
        "    print (\"Training for SNR=\", snr, \" sigma=\", sigma) \n",
        "    for e in range(epochs):\n",
        "      for j in range (int(num_of_batches)):\n",
        "        i = i + 1\n",
        "        k = e * epochs + j\n",
        "        x_train_batch = training_input_message [j*batch_size:(j+1)*batch_size]\n",
        "        x_train_batch_float = x_train_batch.astype(\"float32\")\n",
        "        _, l = train_sess.run ([opt, loss1], feed_dict={input_message_x:x_train_batch_float, awgn_noise_std_dev_x:sigma, lr:lrate})\n",
        "        if (l < 0.5): lrate = 0.02\n",
        "        if (l < 0.25): lrate = 0.002\n",
        "        if (l < 0.1): lrate = 0.0006\n",
        "        if k % display_step == 0:\n",
        "          print('Step %i: Minibatch Loss: %f' % (i, l))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch_size: 500 num_of_batcches: 20.0\n",
            "Training for SNR= 0.0  sigma= 1.0\n",
            "Step 1: Minibatch Loss: 0.996400\n",
            "Step 351: Minibatch Loss: 0.419077\n",
            "Step 801: Minibatch Loss: 0.427030\n",
            "Step 1151: Minibatch Loss: 0.419899\n",
            "Training for SNR= 0.5  sigma= 0.9440608762859234\n",
            "Step 1401: Minibatch Loss: 0.408038\n",
            "Step 1751: Minibatch Loss: 0.405330\n",
            "Step 2201: Minibatch Loss: 0.396790\n",
            "Step 2551: Minibatch Loss: 0.380157\n",
            "Training for SNR= 1.0  sigma= 0.8912509381337456\n",
            "Step 2801: Minibatch Loss: 0.363939\n",
            "Step 3151: Minibatch Loss: 0.364480\n",
            "Step 3601: Minibatch Loss: 0.359034\n",
            "Step 3951: Minibatch Loss: 0.354657\n",
            "Training for SNR= 1.5  sigma= 0.8413951416451951\n",
            "Step 4201: Minibatch Loss: 0.339269\n",
            "Step 4551: Minibatch Loss: 0.356134\n",
            "Step 5001: Minibatch Loss: 0.332902\n",
            "Step 5351: Minibatch Loss: 0.351567\n",
            "Training for SNR= 2.0  sigma= 0.7943282347242815\n",
            "Step 5601: Minibatch Loss: 0.326656\n",
            "Step 5951: Minibatch Loss: 0.327770\n",
            "Step 6401: Minibatch Loss: 0.324887\n",
            "Step 6751: Minibatch Loss: 0.331100\n",
            "Training for SNR= 2.5  sigma= 0.7498942093324559\n",
            "Step 7001: Minibatch Loss: 0.315838\n",
            "Step 7351: Minibatch Loss: 0.312781\n",
            "Step 7801: Minibatch Loss: 0.309687\n",
            "Step 8151: Minibatch Loss: 0.304112\n",
            "Training for SNR= 3.0  sigma= 0.7079457843841379\n",
            "Step 8401: Minibatch Loss: 0.287950\n",
            "Step 8751: Minibatch Loss: 0.287832\n",
            "Step 9201: Minibatch Loss: 0.300540\n",
            "Step 9551: Minibatch Loss: 0.297071\n",
            "Training for SNR= 3.5  sigma= 0.6683439175686147\n",
            "Step 9801: Minibatch Loss: 0.282675\n",
            "Step 10151: Minibatch Loss: 0.278540\n",
            "Step 10601: Minibatch Loss: 0.280586\n",
            "Step 10951: Minibatch Loss: 0.277710\n",
            "Training for SNR= 4.0  sigma= 0.6309573444801932\n",
            "Step 11201: Minibatch Loss: 0.260908\n",
            "Step 11551: Minibatch Loss: 0.263607\n",
            "Step 12001: Minibatch Loss: 0.266925\n",
            "Step 12351: Minibatch Loss: 0.260351\n",
            "Training for SNR= 4.5  sigma= 0.5956621435290105\n",
            "Step 12601: Minibatch Loss: 0.250969\n",
            "Step 12951: Minibatch Loss: 0.260432\n",
            "Step 13401: Minibatch Loss: 0.249297\n",
            "Step 13751: Minibatch Loss: 0.251334\n",
            "Training for SNR= 5.0  sigma= 0.5623413251903491\n",
            "Step 14001: Minibatch Loss: 0.241395\n",
            "Step 14351: Minibatch Loss: 0.238339\n",
            "Step 14801: Minibatch Loss: 0.239580\n",
            "Step 15151: Minibatch Loss: 0.241571\n",
            "Training for SNR= 5.5  sigma= 0.5308844442309884\n",
            "Step 15401: Minibatch Loss: 0.227476\n",
            "Step 15751: Minibatch Loss: 0.236662\n",
            "Step 16201: Minibatch Loss: 0.229971\n",
            "Step 16551: Minibatch Loss: 0.230173\n",
            "Training for SNR= 6.0  sigma= 0.5011872336272722\n",
            "Step 16801: Minibatch Loss: 0.224935\n",
            "Step 17151: Minibatch Loss: 0.223133\n",
            "Step 17601: Minibatch Loss: 0.226251\n",
            "Step 17951: Minibatch Loss: 0.218532\n",
            "Training for SNR= 6.5  sigma= 0.47315125896148047\n",
            "Step 18201: Minibatch Loss: 0.222471\n",
            "Step 18551: Minibatch Loss: 0.217979\n",
            "Step 19001: Minibatch Loss: 0.216232\n",
            "Step 19351: Minibatch Loss: 0.219007\n",
            "Training for SNR= 7.0  sigma= 0.44668359215096315\n",
            "Step 19601: Minibatch Loss: 0.214310\n",
            "Step 19951: Minibatch Loss: 0.218633\n",
            "Step 20401: Minibatch Loss: 0.219421\n",
            "Step 20751: Minibatch Loss: 0.215465\n",
            "Training for SNR= 7.5  sigma= 0.4216965034285822\n",
            "Step 21001: Minibatch Loss: 0.211463\n",
            "Step 21351: Minibatch Loss: 0.211996\n",
            "Step 21801: Minibatch Loss: 0.213749\n",
            "Step 22151: Minibatch Loss: 0.211354\n",
            "Training for SNR= 8.0  sigma= 0.3981071705534972\n",
            "Step 22401: Minibatch Loss: 0.168591\n",
            "Step 22751: Minibatch Loss: 0.151437\n",
            "Step 23201: Minibatch Loss: 0.145823\n",
            "Step 23551: Minibatch Loss: 0.146418\n",
            "Training for SNR= 8.5  sigma= 0.3758374042884442\n",
            "Step 23801: Minibatch Loss: 0.146789\n",
            "Step 24151: Minibatch Loss: 0.142147\n",
            "Step 24601: Minibatch Loss: 0.142994\n",
            "Step 24951: Minibatch Loss: 0.118314\n",
            "Training for SNR= 9.0  sigma= 0.35481338923357547\n",
            "Step 25201: Minibatch Loss: 0.092034\n",
            "Step 25551: Minibatch Loss: 0.084908\n",
            "Step 26001: Minibatch Loss: 0.083170\n",
            "Step 26351: Minibatch Loss: 0.079972\n",
            "Training for SNR= 9.5  sigma= 0.33496543915782767\n",
            "Step 26601: Minibatch Loss: 0.078522\n",
            "Step 26951: Minibatch Loss: 0.079555\n",
            "Step 27401: Minibatch Loss: 0.077157\n",
            "Step 27751: Minibatch Loss: 0.076164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VExzjbtgJKL4",
        "outputId": "c4391f83-c2a2-46d0-cbff-4f1b4db6511d"
      },
      "source": [
        "# Here I am using trained model\n",
        "NUM_OF_INPUT_MESSAGE_BER = NUM_OF_INPUT_MESSAGE * 2\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_dl_tensor  = numpy.array(())\n",
        "times_per_iter_dl_tensor = numpy.array(())\n",
        "\n",
        "channel_in = []\n",
        "channel_out = []\n",
        "for snr in numpy.arange (0, 10, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE_BER):\n",
        "    input_message_xx = training_input_message [i:i+1]\n",
        "    input_message_xx_float = input_message_xx.astype(\"float32\")\n",
        "    encoded_message = train_sess.run ([dl_encoder_output], feed_dict={input_message_x:input_message_xx_float})\n",
        "    #print (encoded_message[0][0])\n",
        "    channel_in.append(encoded_message[0][0])\n",
        "    #print (encoded_message[0][0])\n",
        "    awgn_channel_output_message = train_sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message[0][0]})\n",
        "    channel_out.append(awgn_channel_output_message[0]) \n",
        "    #print (awgn_channel_output_message[0])\n",
        "    decoded_message = train_sess.run ([dl_decoder_only_output], feed_dict={input_channel_x:awgn_channel_output_message})\n",
        "    #print (\"input\", input_message[i])\n",
        "    decoded_message = numpy.around(decoded_message[0][0]).astype(int)\n",
        "    #print (\"output\", decoded_message)\n",
        "    if abs(decoded_message-training_input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE_BER\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_dl_tensor=numpy.append(ber_per_iter_dl_tensor ,ber)\n",
        "  times_per_iter_dl_tensor=numpy.append(times_per_iter_dl_tensor, total_time)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 0.29s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 0.56s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 0.82s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 1.08s\n",
            "SNR: 0.000 - Iter: 1250 - Last 250.0 iterations took 1.35s\n",
            "SNR: 0.000 - Iter: 1500 - Last 250.0 iterations took 1.61s\n",
            "SNR: 0.000 - Iter: 1750 - Last 250.0 iterations took 1.89s\n",
            "SNR: 0.000 - Iter: 2000 - Last 250.0 iterations took 2.15s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.87\n",
            " -> Total Time: 9.75s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 0.55s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 0.86s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 1.15s\n",
            "SNR: 0.500 - Iter: 1250 - Last 250.0 iterations took 1.43s\n",
            "SNR: 0.500 - Iter: 1500 - Last 250.0 iterations took 1.71s\n",
            "SNR: 0.500 - Iter: 1750 - Last 250.0 iterations took 1.97s\n",
            "SNR: 0.500 - Iter: 2000 - Last 250.0 iterations took 2.23s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.85\n",
            " -> Total Time: 10.17s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 0.80s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 1.07s\n",
            "SNR: 1.000 - Iter: 1250 - Last 250.0 iterations took 1.38s\n",
            "SNR: 1.000 - Iter: 1500 - Last 250.0 iterations took 1.65s\n",
            "SNR: 1.000 - Iter: 1750 - Last 250.0 iterations took 1.92s\n",
            "SNR: 1.000 - Iter: 2000 - Last 250.0 iterations took 2.19s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.82\n",
            " -> Total Time: 9.82s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.28s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 0.56s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 0.85s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 1.14s\n",
            "SNR: 1.500 - Iter: 1250 - Last 250.0 iterations took 1.44s\n",
            "SNR: 1.500 - Iter: 1500 - Last 250.0 iterations took 1.72s\n",
            "SNR: 1.500 - Iter: 1750 - Last 250.0 iterations took 1.98s\n",
            "SNR: 1.500 - Iter: 2000 - Last 250.0 iterations took 2.26s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.80\n",
            " -> Total Time: 10.22s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.28s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 0.54s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 0.82s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 1.10s\n",
            "SNR: 2.000 - Iter: 1250 - Last 250.0 iterations took 1.38s\n",
            "SNR: 2.000 - Iter: 1500 - Last 250.0 iterations took 1.64s\n",
            "SNR: 2.000 - Iter: 1750 - Last 250.0 iterations took 1.90s\n",
            "SNR: 2.000 - Iter: 2000 - Last 250.0 iterations took 2.17s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.80\n",
            " -> Total Time: 9.83s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 0.78s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 1.04s\n",
            "SNR: 2.500 - Iter: 1250 - Last 250.0 iterations took 1.32s\n",
            "SNR: 2.500 - Iter: 1500 - Last 250.0 iterations took 1.59s\n",
            "SNR: 2.500 - Iter: 1750 - Last 250.0 iterations took 1.86s\n",
            "SNR: 2.500 - Iter: 2000 - Last 250.0 iterations took 2.12s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.76\n",
            " -> Total Time: 9.48s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 0.82s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.11s\n",
            "SNR: 3.000 - Iter: 1250 - Last 250.0 iterations took 1.41s\n",
            "SNR: 3.000 - Iter: 1500 - Last 250.0 iterations took 1.71s\n",
            "SNR: 3.000 - Iter: 1750 - Last 250.0 iterations took 1.99s\n",
            "SNR: 3.000 - Iter: 2000 - Last 250.0 iterations took 2.29s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.73\n",
            " -> Total Time: 10.11s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.29s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.58s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 0.85s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.11s\n",
            "SNR: 3.500 - Iter: 1250 - Last 250.0 iterations took 1.40s\n",
            "SNR: 3.500 - Iter: 1500 - Last 250.0 iterations took 1.67s\n",
            "SNR: 3.500 - Iter: 1750 - Last 250.0 iterations took 1.95s\n",
            "SNR: 3.500 - Iter: 2000 - Last 250.0 iterations took 2.22s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.69\n",
            " -> Total Time: 10.08s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.29s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.79s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.05s\n",
            "SNR: 4.000 - Iter: 1250 - Last 250.0 iterations took 1.35s\n",
            "SNR: 4.000 - Iter: 1500 - Last 250.0 iterations took 1.67s\n",
            "SNR: 4.000 - Iter: 1750 - Last 250.0 iterations took 1.97s\n",
            "SNR: 4.000 - Iter: 2000 - Last 250.0 iterations took 2.23s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.67\n",
            " -> Total Time: 9.88s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.54s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.82s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.11s\n",
            "SNR: 4.500 - Iter: 1250 - Last 250.0 iterations took 1.40s\n",
            "SNR: 4.500 - Iter: 1500 - Last 250.0 iterations took 1.67s\n",
            "SNR: 4.500 - Iter: 1750 - Last 250.0 iterations took 1.92s\n",
            "SNR: 4.500 - Iter: 2000 - Last 250.0 iterations took 2.18s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.62\n",
            " -> Total Time: 9.90s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.80s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 1.06s\n",
            "SNR: 5.000 - Iter: 1250 - Last 250.0 iterations took 1.33s\n",
            "SNR: 5.000 - Iter: 1500 - Last 250.0 iterations took 1.59s\n",
            "SNR: 5.000 - Iter: 1750 - Last 250.0 iterations took 1.85s\n",
            "SNR: 5.000 - Iter: 2000 - Last 250.0 iterations took 2.12s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.61\n",
            " -> Total Time: 9.54s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.28s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.55s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.84s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 1.10s\n",
            "SNR: 5.500 - Iter: 1250 - Last 250.0 iterations took 1.37s\n",
            "SNR: 5.500 - Iter: 1500 - Last 250.0 iterations took 1.64s\n",
            "SNR: 5.500 - Iter: 1750 - Last 250.0 iterations took 1.91s\n",
            "SNR: 5.500 - Iter: 2000 - Last 250.0 iterations took 2.17s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.58\n",
            " -> Total Time: 9.86s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.29s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.57s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.83s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 1.11s\n",
            "SNR: 6.000 - Iter: 1250 - Last 250.0 iterations took 1.37s\n",
            "SNR: 6.000 - Iter: 1500 - Last 250.0 iterations took 1.64s\n",
            "SNR: 6.000 - Iter: 1750 - Last 250.0 iterations took 1.90s\n",
            "SNR: 6.000 - Iter: 2000 - Last 250.0 iterations took 2.15s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.56\n",
            " -> Total Time: 9.86s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.77s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 1.04s\n",
            "SNR: 6.500 - Iter: 1250 - Last 250.0 iterations took 1.32s\n",
            "SNR: 6.500 - Iter: 1500 - Last 250.0 iterations took 1.59s\n",
            "SNR: 6.500 - Iter: 1750 - Last 250.0 iterations took 1.88s\n",
            "SNR: 6.500 - Iter: 2000 - Last 250.0 iterations took 2.16s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.54\n",
            " -> Total Time: 9.52s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.80s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 1.06s\n",
            "SNR: 7.000 - Iter: 1250 - Last 250.0 iterations took 1.32s\n",
            "SNR: 7.000 - Iter: 1500 - Last 250.0 iterations took 1.58s\n",
            "SNR: 7.000 - Iter: 1750 - Last 250.0 iterations took 1.84s\n",
            "SNR: 7.000 - Iter: 2000 - Last 250.0 iterations took 2.11s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.52\n",
            " -> Total Time: 9.52s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.77s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 1.07s\n",
            "SNR: 7.500 - Iter: 1250 - Last 250.0 iterations took 1.39s\n",
            "SNR: 7.500 - Iter: 1500 - Last 250.0 iterations took 1.67s\n",
            "SNR: 7.500 - Iter: 1750 - Last 250.0 iterations took 1.96s\n",
            "SNR: 7.500 - Iter: 2000 - Last 250.0 iterations took 2.28s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.50\n",
            " -> Total Time: 9.92s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.54s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.80s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 1.06s\n",
            "SNR: 8.000 - Iter: 1250 - Last 250.0 iterations took 1.31s\n",
            "SNR: 8.000 - Iter: 1500 - Last 250.0 iterations took 1.58s\n",
            "SNR: 8.000 - Iter: 1750 - Last 250.0 iterations took 1.83s\n",
            "SNR: 8.000 - Iter: 2000 - Last 250.0 iterations took 2.10s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.50\n",
            " -> Total Time: 9.48s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.54s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.80s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 1.08s\n",
            "SNR: 8.500 - Iter: 1250 - Last 250.0 iterations took 1.36s\n",
            "SNR: 8.500 - Iter: 1500 - Last 250.0 iterations took 1.63s\n",
            "SNR: 8.500 - Iter: 1750 - Last 250.0 iterations took 1.89s\n",
            "SNR: 8.500 - Iter: 2000 - Last 250.0 iterations took 2.17s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.49\n",
            " -> Total Time: 9.73s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.55s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.82s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 1.09s\n",
            "SNR: 9.000 - Iter: 1250 - Last 250.0 iterations took 1.37s\n",
            "SNR: 9.000 - Iter: 1500 - Last 250.0 iterations took 1.65s\n",
            "SNR: 9.000 - Iter: 1750 - Last 250.0 iterations took 1.94s\n",
            "SNR: 9.000 - Iter: 2000 - Last 250.0 iterations took 2.21s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.48\n",
            " -> Total Time: 9.89s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.79s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 1.06s\n",
            "SNR: 9.500 - Iter: 1250 - Last 250.0 iterations took 1.33s\n",
            "SNR: 9.500 - Iter: 1500 - Last 250.0 iterations took 1.59s\n",
            "SNR: 9.500 - Iter: 1750 - Last 250.0 iterations took 1.89s\n",
            "SNR: 9.500 - Iter: 2000 - Last 250.0 iterations took 2.15s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.48\n",
            " -> Total Time: 9.61s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M-Xtp4YJNl2"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1) = plt.subplots(1,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,bler_per_iter_uncoded_commpy_psk_4,'', label=\"commpy-psk-4\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_uncoded_itpp_psk_4,'', label=\"itpp-psk-4\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor,'', label=\"ai-dl(input=10,channel=5)\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_uncoded_commpy_psk_2,'', label=\"commpy-psk-2\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_uncoded_itpp_psk_2,'', label=\"itpp-psk-2\") # plot BER vs SNR\n",
        "ax1.set_ylabel('BLER')\n",
        "ax1.set_title('QPSK ({},{})'.format(input_message_length, channel_size))\n",
        "#ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "#ax2.set_xlabel('$E_b/$N_0$')\n",
        "#ax2.set_ylabel('Decoding Time [s]')\n",
        "#ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "#            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "#            xy=(1, 0.35), xycoords='axes fraction',\n",
        "#            xytext=(-20, 20), textcoords='offset pixels',\n",
        "#            horizontalalignment='right',\n",
        "#            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(2*channel_size,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show("
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}