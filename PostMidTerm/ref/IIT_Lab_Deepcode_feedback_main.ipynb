{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IIT-Lab/Deepcode/feedback_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamviji/project/blob/master/PostMidTerm/ref/IIT_Lab_Deepcode_feedback_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "6UDKVUYY70v9",
        "outputId": "d47199d9-fb90-4fb9-b352-daaa663783e6"
      },
      "source": [
        "#ref : https://github.com/IIT-Lab/Deepcode/blob/master/feedback_main.py\r\n",
        "__author__ = 'hyejikim'\r\n",
        "\r\n",
        "# Deepcode simulation. \r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "from keras.models import Sequential, Model\r\n",
        "from keras.layers import Dense, Input\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras.layers.core import Lambda\r\n",
        "from keras.layers import TimeDistributed\r\n",
        "from keras.layers import LSTM, GRU, SimpleRNN\r\n",
        "from keras.layers.wrappers import  Bidirectional\r\n",
        "from keras import regularizers\r\n",
        "from keras.engine.topology import Layer\r\n",
        "import matplotlib\r\n",
        "matplotlib.use('pdf')\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras import backend as K\r\n",
        "from keras.engine import Layer\r\n",
        "import scipy.io as sio\r\n",
        "import matplotlib, h5py, pickle, sys, time\r\n",
        "\r\n",
        "\r\n",
        "################################\r\n",
        "# GPU memory allocation \r\n",
        "from keras.backend.tensorflow_backend import set_session\r\n",
        "config = tf.ConfigProto()\r\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.6\r\n",
        "set_session(tf.Session(config=config))\r\n",
        "print ('[Test][Warining] Restrict GPU memory usage to 60%')\r\n",
        " \r\n",
        "################################\r\n",
        "# Arguments\r\n",
        "n_inp = sys.argv[1:]\r\n",
        "\r\n",
        "if '-coderate' in n_inp:\r\n",
        "    ind1 = n_inp.index('-coderate')\r\n",
        "    coderate = int(n_inp[ind1+1])\r\n",
        "else:\r\n",
        "    coderate = 3\r\n",
        " \r\n",
        "if '-tx' in n_inp:\r\n",
        "    ind1 = n_inp.index('-tx')\r\n",
        "    num_hunit_rnn_tx = int(n_inp[ind1+1])\r\n",
        "else:\r\n",
        "    num_hunit_rnn_tx = 50\r\n",
        "if '-rx' in n_inp:\r\n",
        "    ind1 = n_inp.index('-rx')\r\n",
        "    num_hunit_rnn_rx = int(n_inp[ind1+1])\r\n",
        "else:\r\n",
        "    num_hunit_rnn_rx = 50 \r\n",
        "print ('Tx hidden nodes:', num_hunit_rnn_tx)\r\n",
        "print ('Rx hidden nodes:', num_hunit_rnn_rx)\r\n",
        "\r\n",
        "\r\n",
        "if '-len' in n_inp:\r\n",
        "    ind1      = n_inp.index('-len')\r\n",
        "    bit_length = int(n_inp[ind1+1])\r\n",
        "else:\r\n",
        "    bit_length = 51 # Number of bits including one (for zero padding)\r\n",
        "print ('Block length: ', bit_length)\r\n",
        "\r\n",
        "if '-fs' in n_inp: # Noisy feedback\r\n",
        "    ind1      = n_inp.index('-fs')\r\n",
        "    fsSNR = float(n_inp[ind1+1])\r\n",
        "    feedback_sigma = 10**(-fsSNR*1.0/20)\r\n",
        "else:\r\n",
        "    fsSNR = 20 # fsSNR = 20 means noiseless feedback\r\n",
        "    feedback_sigma = 0\r\n",
        "\r\n",
        "if '-ns' in n_inp:\r\n",
        "    ind1      = n_inp.index('-ns')\r\n",
        "    nsSNR = int(n_inp[ind1+1])\r\n",
        "    noise_sigma = 10**(-nsSNR*1.0/20)\r\n",
        "else:\r\n",
        "    nsSNR = 0\r\n",
        "    noise_sigma = 10**(-nsSNR*1.0/20)\r\n",
        "print ('SNR of forward channel: ', nsSNR)\r\n",
        "print ('SNR of feedback channel: ', fsSNR)\r\n",
        "\r\n",
        "\r\n",
        "if '-k' in n_inp:\r\n",
        "\tind1 = n_inp.index('-k')\r\n",
        "\tk = int(n_inp[ind1+1])\r\n",
        "else: \r\n",
        "        k = bit_length*200000 # length of total message bits for testing.\r\n",
        "print ('Total number of bits for testing: ', k)\r\n",
        "\r\n",
        "if '-noncausal' in n_inp:\r\n",
        "    causal = False\r\n",
        "else:\r\n",
        "    causal = True        \r\n",
        "print ('Causality: ', causal)\r\n",
        "\r\n",
        " \r\n",
        "class ScaledLayer(Layer): # Power Allocation Layer\r\n",
        "    def __init__(self, **kwargs):\r\n",
        "        super(ScaledLayer, self).__init__(**kwargs)\r\n",
        "    def build(self, input_shape):\r\n",
        "        self.output_dim = input_shape[1] \r\n",
        "        self.W = self.add_weight(name = 'power_weight', shape=(1,), # Power allocation for information bit stream\r\n",
        "                                 initializer='ones', trainable=True)\r\n",
        "        self.W2 = self.add_weight(name = 'power_weight1', shape=(1,), # Power allocation for parity 1 stream\r\n",
        "                                 initializer='ones', trainable=True)\r\n",
        "        self.W3 = self.add_weight(name = 'power_weight2', shape=(1,), # Power allocation for parity 2 stream\r\n",
        "                                 initializer='ones', trainable=True)\r\n",
        "         \r\n",
        "        self.b1 = self.add_weight(name = 'b1', shape=(1,), \r\n",
        "                                 initializer='ones', trainable=True) # Power allocation for last-4 bit\r\n",
        "        self.b2 = self.add_weight(name = 'b2', shape=(1,), \r\n",
        "                                 initializer='ones', trainable=True) # Power allocation for last-3 bit\r\n",
        "        self.b3 = self.add_weight(name = 'b3', shape=(1,), \r\n",
        "                                 initializer='ones', trainable=True) # Power allocation for last-2 bit\r\n",
        "        self.b4 = self.add_weight(name = 'b4', shape=(1,), \r\n",
        "                                 initializer='ones', trainable=True) # Power allocation for last-1 bit\r\n",
        "        self.b5 = self.add_weight(name = 'b5', shape=(1,), \r\n",
        "                                 initializer='ones', trainable=True) # Power allocation for last bit\r\n",
        "  \r\n",
        "        self.g1 = self.add_weight(name = 'g1', shape=(1,), \r\n",
        "                                 initializer='ones', trainable=True) # Power allocation for 1st bit\r\n",
        "        self.g2 = self.add_weight(name = 'g2', shape=(1,), \r\n",
        "                                 initializer='ones', trainable=True) # Power allocation for 2nd bit\r\n",
        "        self.g3 = self.add_weight(name = 'g3', shape=(1,), \r\n",
        "                                 initializer='ones', trainable=True) # Power allocation for 3rd bit\r\n",
        "        self.g4 = self.add_weight(name = 'g4', shape=(1,), \r\n",
        "                                 initializer='ones', trainable=True) # Power allocation for 4th bit\r\n",
        " \r\n",
        "\r\n",
        "        super(ScaledLayer, self).build(input_shape)\r\n",
        "    def call(self, x, mask=None):\r\n",
        "        sys = tf.reshape(tf.multiply(x[:,:,0], self.W),[tf.shape(x)[0],tf.shape(x)[1],1])\r\n",
        "        par1 = tf.reshape(tf.multiply(x[:,:,1], self.W2),[tf.shape(x)[0],tf.shape(x)[1],1])\r\n",
        "        par2 = tf.reshape(tf.multiply(x[:,:,2], self.W3),[tf.shape(x)[0],tf.shape(x)[1],1])\r\n",
        " \r\n",
        "        cats = K.concatenate([K.concatenate([tf.expand_dims(tf.multiply(self.g1,sys[:,0,:]),1),tf.expand_dims(tf.multiply(self.g1,par1[:,0,:]),1),tf.expand_dims(tf.multiply(self.g1,par2[:,0,:]),1)],axis=2),\r\n",
        "                              K.concatenate([tf.expand_dims(tf.multiply(self.g2,sys[:,1,:]),1),tf.expand_dims(tf.multiply(self.g2,par1[:,1,:]),1),tf.expand_dims(tf.multiply(self.g2,par2[:,1,:]),1)],axis=2),\r\n",
        "                              K.concatenate([tf.expand_dims(tf.multiply(self.g3,sys[:,2,:]),1),tf.expand_dims(tf.multiply(self.g3,par1[:,2,:]),1),tf.expand_dims(tf.multiply(self.g3,par2[:,2,:]),1)],axis=2),\r\n",
        "                              K.concatenate([tf.expand_dims(tf.multiply(self.g4,sys[:,3,:]),1),tf.expand_dims(tf.multiply(self.g4,par1[:,3,:]),1),tf.expand_dims(tf.multiply(self.g4,par2[:,3,:]),1)],axis=2),\r\n",
        "                              K.concatenate([sys[:,4:bit_length-5,:],par1[:,4:bit_length-5,:],par2[:,4:bit_length-5,:]],axis=2),                              \r\n",
        "                              K.concatenate([tf.expand_dims(tf.multiply(self.b1,sys[:,bit_length-5,:]),1),tf.expand_dims(tf.multiply(self.b1,par1[:,bit_length-5,:]),1),tf.expand_dims(tf.multiply(self.b1,par2[:,bit_length-5,:]),1)],axis=2),\r\n",
        "                              K.concatenate([tf.expand_dims(tf.multiply(self.b2,sys[:,bit_length-4,:]),1),tf.expand_dims(tf.multiply(self.b2,par1[:,bit_length-4,:]),1),tf.expand_dims(tf.multiply(self.b2,par2[:,bit_length-4,:]),1)],axis=2),\r\n",
        "                              K.concatenate([tf.expand_dims(tf.multiply(self.b3,sys[:,bit_length-3,:]),1),tf.expand_dims(tf.multiply(self.b3,par1[:,bit_length-3,:]),1),tf.expand_dims(tf.multiply(self.b3,par2[:,bit_length-3,:]),1)],axis=2),\r\n",
        "                              K.concatenate([tf.expand_dims(tf.multiply(self.b4,sys[:,bit_length-2,:]),1),tf.expand_dims(tf.multiply(self.b4,par1[:,bit_length-2,:]),1),tf.expand_dims(tf.multiply(self.b4,par2[:,bit_length-2,:]),1)],axis=2),\r\n",
        "                              K.concatenate([tf.expand_dims(tf.multiply(self.b5,sys[:,bit_length-1,:]),1),tf.expand_dims(tf.multiply(self.b5,par1[:,bit_length-1,:]),1),tf.expand_dims(tf.multiply(self.b5,par2[:,bit_length-1,:]),1)],axis=2),\r\n",
        "                            ], axis=1)\r\n",
        " \r\n",
        "        cats_mean, cats_var = tf.nn.moments(cats,[0])\r\n",
        "        rem = bit_length-9.0\r\n",
        " \r\n",
        "        adj = bit_length*1.0/(bit_length)\r\n",
        "        den = (rem + self.g1**2 + self.g2**2 + self.g3**2 + self.g4**2 + self.b1**2 + self.b2**2 + self.b3**2 + self.b4**2 + self.b5**2)*(self.W**2+self.W2**2+self.W3**2)\r\n",
        "        return tf.sqrt(3.0*bit_length/den)*cats\r\n",
        "\r\n",
        "    def get_output_shape_for(self, input_shape):\r\n",
        "        a_shape = input_shape\r\n",
        "        return (a_shape[0], a_shape[1], a_shape[2])\r\n",
        " \r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        a_shape = input_shape\r\n",
        "        return (a_shape[0], a_shape[1], a_shape[2])\r\n",
        " \r\n",
        " \r\n",
        "# Encoder. Single Directional. One layer RNN\r\n",
        "f1 = SimpleRNN(name='simple_rnn_1', units=num_hunit_rnn_tx, activation='tanh', return_sequences=True, dropout=1.0)\r\n",
        "f3 = TimeDistributed(Dense(coderate-1, activation='sigmoid'),name = 'time_distributed_0')\r\n",
        " \r\n",
        "# Decoder. Bidirectional. Two Layered GRU with batch normalization. \r\n",
        "f4 = Bidirectional(GRU(name='bidirectional_1', units=num_hunit_rnn_rx, activation='tanh', return_sequences=True, dropout=1.0))\r\n",
        "f5 = BatchNormalization(name='batch_normalization_1')\r\n",
        "f6 = Bidirectional(GRU(name='bidirectional_2', units=num_hunit_rnn_rx, activation='tanh', return_sequences=True, dropout=1.0))\r\n",
        "f7 = BatchNormalization(name='batch_normalization_2') \r\n",
        "f8 = TimeDistributed(Dense(1, activation='sigmoid'), name='time_distributed_1')\r\n",
        " \r\n",
        "\r\n",
        "# Loss used for training: Binary crossentropy over all bits except for the zero padding\r\n",
        "def customLoss(y_true,y_pred):\r\n",
        "    y_true_50 = y_true[:,0:bit_length-1,:]\r\n",
        "    y_pred_50 = y_pred[:,0:bit_length-1,:]     \r\n",
        "    return K.binary_crossentropy(y_true_50, y_pred_50)\r\n",
        "\r\n",
        "# Errors used for training: ignoring the error on the zero padded bits\r\n",
        "def errors(y_true, y_pred):\r\n",
        "    y_true_50 = y_true[:,0:bit_length-1,:]\r\n",
        "    y_pred_50 = y_pred[:,0:bit_length-1,:]\r\n",
        "    myOtherTensor = K.not_equal(y_true_50, K.round(y_pred_50))\r\n",
        "    return K.mean(tf.cast(myOtherTensor, tf.float32))\r\n",
        " \r\n",
        "# Normalization layer of the encoder\r\n",
        "def normalize(x):\r\n",
        "    if causal == False: # Average over batches\r\n",
        "        x_mean, x_var = tf.nn.moments(x,[0])\r\n",
        "    else: # Load pre-computed mean/variance for normalization \r\n",
        "        id = str(bit_length)+'_'+str(fsSNR)+'_'+str(nsSNR)\r\n",
        "        with open('meanvar/meanvar_'+id+'.pickle') as g:  # Python 3: open(..., 'wb')\r\n",
        "            mean1, var1 = pickle.load(g)\r\n",
        "        x_mean = tf.Variable(mean1, tf.float32)\r\n",
        "        x_var = tf.Variable(var1, tf.float32)\r\n",
        "       \r\n",
        "    x = (x-x_mean)*1.0/tf.sqrt(x_var)\r\n",
        "    return x\r\n",
        " \r\n",
        "# coderate. takeNoise\r\n",
        "def takeNoise(x):\r\n",
        "    return tf.reshape(x[:,:,coderate+1:2*coderate+1],[tf.shape(x[:,:,0])[0],bit_length,coderate]) # 4 - N_i // 5 - M_i // 6 - O_i  \r\n",
        "# takeBit. BPSK modulation\r\n",
        "def takeBit(x):\r\n",
        "    return tf.reshape(2*x[:,:,0]-1,[tf.shape(x[:,:,0])[0],bit_length,1])\r\n",
        "\r\n",
        "def concat(x):\r\n",
        "    return K.concatenate(x)\r\n",
        " \r\n",
        "inputs = Input(shape=(bit_length, 2*coderate+1))\r\n",
        "x = inputs\r\n",
        "\r\n",
        "# Take input for parity generation\r\n",
        "def split_data_input_noisedelay(x):\r\n",
        "    x1 = x[:,:,0:coderate+1] # E.g., for coderate=3: 0 - b_i // 1 - N_i in Phase I // 2 - M_{i-1} in Phase II // 3 - O_{i-1} in Phase II.\r\n",
        "    return x1\r\n",
        " \r\n",
        "parity = f3(f1(Lambda(split_data_input_noisedelay)(x))) # Generate parity based on message bits and Phase I noise and delayed Phase II noise\r\n",
        "norm_parity = Lambda(normalize)(parity) # Normalize the parity\r\n",
        "codeword = Lambda(concat)([Lambda(takeBit)(x),norm_parity]) # Codeword: raw bits and normalized parity\r\n",
        "powerd_codeword = ScaledLayer(name='noload_abr')(codeword) # Codeword after Power Allocation\r\n",
        "\r\n",
        "noise = Lambda(takeNoise)(x)\r\n",
        "noisy_received = keras.layers.add([powerd_codeword,noise]) # Received value: Sum of noise & codeword \r\n",
        "predictions = f8(f7(f6(f5(f4(noisy_received))))) # Decoder output\r\n",
        " \r\n",
        "# output of model_cw is encoder's power allocated codeword\r\n",
        "model_cw = Model(inputs=inputs, outputs=powerd_codeword)\r\n",
        "optimizer= keras.optimizers.adam(lr=0.02,clipnorm=1.)\r\n",
        "model_cw.compile(optimizer=optimizer,loss=customLoss, metrics=[errors])\r\n",
        " \r\n",
        "# output of model is decoder's estimate\r\n",
        "model = Model(inputs=inputs, outputs=predictions)\r\n",
        "optimizer= keras.optimizers.adam(lr=0.02,clipnorm=1.)\r\n",
        "model.compile(optimizer=optimizer,loss=customLoss, metrics=[errors])\r\n",
        "\r\n",
        "id = str(bit_length)+'_'+str(fsSNR)+'_'+str(nsSNR)\r\n",
        " \r\n",
        "# Load model\r\n",
        "if '-fs' in n_inp:\r\n",
        "    model.load_weights('model/round3_powerabr_new_noisy_nettype_rnnrate3tx_50_rx_50_len_51_'+str(fsSNR)+'_0.h5',by_name=True)\r\n",
        "    print ('model noise', str(fsSNR),'dB')\r\n",
        "else:\r\n",
        "    model.load_weights('model/round4_powerabr_new_nettype_rnnrate3tx_50_rx_50_len_51_20_'+str(nsSNR)+'.h5',by_name=True)\r\n",
        "    print ('model', str(nsSNR),'dB')\r\n",
        "    \r\n",
        "\r\n",
        "# Generate test examples: X_train (X_test) is true label. X_train_noise (X_test_noise) is input to the neural network\r\n",
        "# Generate test examples: information bits X_train (X_test)\r\n",
        "print ('Generate test examples')\r\n",
        "X_train_raw = np.random.randint(0,2,k)\r\n",
        "X_test_raw  = np.random.randint(0,2,k)\r\n",
        "X_train = X_train_raw.reshape((k/bit_length, bit_length, 1))\r\n",
        "X_test  = X_test_raw.reshape((k/bit_length, bit_length, 1))\r\n",
        "\r\n",
        "# Generate test examples: input to the neural network X_train_noise (X_test_noise)\r\n",
        "# Input to neural network: message bits and noise sequence in Phase I(n_1,...,n_bitlength) and Phase II (m_1, o_1, m_2, o_2, ..., m_bitlength, o_bitlength)\r\n",
        "# Form the input as: X_train_noise[batch_index,i,:] = [b_i, n_i, m_{i-1}, o_{i-1}, n_i, m_i, o_i] for i = 1:bitlength\r\n",
        "\r\n",
        "X_train_noise = np.zeros([k/bit_length, bit_length, 2*coderate+1])\r\n",
        "X_train_noise[:,:,0] = X_train[:,:,0] # True message bits\r\n",
        "X_train_noise[:,bit_length-1,0] = np.zeros(X_train_noise[:,bit_length-1,0].shape) # Set the last Bit to be 0.\r\n",
        "\r\n",
        "for inx in range(1,coderate+1):\r\n",
        "    X_train_noise[:,:,coderate+inx] = noise_sigma * np.random.standard_normal(X_train_noise[:,:,coderate+inx].shape) # Noise\r\n",
        "    if inx == 1:\r\n",
        "        X_train_noise[:,:,inx] = np.roll(X_train_noise[:,:,coderate+inx], 0, axis=1) + feedback_sigma * np.random.standard_normal(X_train_noise[:,:,3].shape)  # Delayed Noise\r\n",
        "    else:\r\n",
        "        X_train_noise[:,:,inx] = np.roll(X_train_noise[:,:,coderate+inx], 1, axis=1) + feedback_sigma * np.random.standard_normal(X_train_noise[:,:,4].shape)  # Delayed Noise\r\n",
        "        X_train_noise[:,0,inx] = 0\r\n",
        "\r\n",
        "X_test_noise = np.zeros([k/bit_length, bit_length,2*coderate+1])\r\n",
        "X_test_noise[:,:,0] = X_test[:,:,0] # True message bits\r\n",
        "X_test_noise[:,bit_length-1,0] = np.zeros(X_test_noise[:,bit_length-1,0].shape) # Set the last Bit to be 0. \r\n",
        "for inx in range(1,coderate+1):\r\n",
        "    X_test_noise[:,:,coderate+inx] = noise_sigma * np.random.standard_normal(X_test_noise[:,:,coderate+inx].shape) # Noise\r\n",
        "    if inx == 1:\r\n",
        "        X_test_noise[:,:,inx] = np.roll(X_test_noise[:,:,coderate+inx], 0, axis=1) + feedback_sigma * np.random.standard_normal(X_test_noise[:,:,3].shape)  # Delayed Noise\r\n",
        "    else:\r\n",
        "        X_test_noise[:,:,inx] = np.roll(X_test_noise[:,:,coderate+inx], 1, axis=1) + feedback_sigma * np.random.standard_normal(X_test_noise[:,:,4].shape)  # Delayed Noise\r\n",
        "        X_test_noise[:,0,inx] = 0\r\n",
        "\r\n",
        "\r\n",
        "print ('-------Evaluation start-------')\r\n",
        "test_batch_size = 200\r\n",
        "codewords = model_cw.predict(X_test_noise, batch_size=test_batch_size)\r\n",
        "print ('power of codewords: ', np.var(codewords))\r\n",
        "print ('mean of codewords: ', np.mean(codewords))\r\n",
        "\r\n",
        "predicted = np.round(model.predict(X_test_noise, batch_size=test_batch_size))\r\n",
        "predicted = predicted[:,0:bit_length-1,:] # Ignore the last bit (zero padding) \r\n",
        "target = X_test[:,0:bit_length-1,:].reshape([X_test.shape[0],X_test.shape[1]-1,1]) # Ignore the last bit (zero padding)\r\n",
        "\r\n",
        "# BER\r\n",
        "c_ber = 1- sum(sum(predicted == target))*\\\r\n",
        "       1.0/(target.shape[0] * target.shape[1] *target.shape[2])\r\n",
        "# BLER\r\n",
        "tp0 = (abs(np.round(predicted)-target)).reshape([target.shape[0],target.shape[1]]) \r\n",
        "bler = sum(np.sum(tp0,axis=1)>0)*1.0/(target.shape[0])\r\n",
        "\r\n",
        "print ('BER of decoder estimate: ', c_ber[0])\r\n",
        "print ('BLER of decoder estimate: ', bler)\r\n",
        "\r\n",
        "# Interpret: generate Figure 5\r\n",
        "interpret = True\r\n",
        "\r\n",
        "if interpret == True:\r\n",
        "\r\n",
        "    r1 = X_test_noise[:,:,0] # b_i\r\n",
        "    n1 = X_test_noise[:,:,1] # N_i\r\n",
        "    n2 = X_test_noise[:,:,2] # M_{i-1}\r\n",
        "    n3 = X_test_noise[:,:,3] # O_{i-1}\r\n",
        "    p1 = codewords[:,:,1]    # Parity1_i\r\n",
        "    p2 = codewords[:,:,2]    # Parity2_i \r\n",
        "    \r\n",
        "    num_sample_points = 20 # Number of sample points\r\n",
        "    rr1 = r1[0:num_sample_points,:] # b_i\r\n",
        "    nn1 = n1[0:num_sample_points,:] # N_i\r\n",
        "    nn2 = n2[0:num_sample_points,:] # M_{i-1}\r\n",
        "    nn3 = n3[0:num_sample_points,:] # O_{i-1}\r\n",
        "    pp1 = p1[0:num_sample_points,:] # Parity1_i\r\n",
        "    pp2 = p2[0:num_sample_points,:] # Parity2_i\r\n",
        "\r\n",
        "    plt.close()\r\n",
        "    plt.plot(nn1[rr1==0],pp1[rr1==0],'r.')\r\n",
        "    plt.plot(nn1[rr1==1],pp1[rr1==1],'bx')\r\n",
        "    plt.savefig('figs/SNR'+str(nsSNR)+'plot'+str(num_sample_points)+'_PhaseI_noise_vs_parity1.png')\r\n",
        "\r\n",
        "    plt.close()\r\n",
        "    plt.plot(nn1[rr1==0],pp2[rr1==0],'r.')\r\n",
        "    plt.plot(nn1[rr1==1],pp2[rr1==1],'bx')\r\n",
        "    plt.savefig('figs/SNR'+str(nsSNR)+'plot'+str(num_sample_points)+'_PhaseI_noise_vs_parity2.png')\r\n",
        "\r\n",
        "    plt.close()\r\n",
        "    plt.plot(pp1[rr1==0],pp2[rr1==0],'r.')\r\n",
        "    plt.plot(pp1[rr1==1],pp2[rr1==1],'bx')\r\n",
        "    plt.savefig('figs/SNR'+str(nsSNR)+'plot_'+str(num_sample_points)+'_parity1_vs_parity2.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-64d40ab69acd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# GPU memory allocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_process_gpu_memory_fraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.backend.tensorflow_backend'; 'keras.backend' is not a package",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}