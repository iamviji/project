{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IIT-Lab/Deepcom-1/model_LBC_AWGN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNNVmlVO0QkZeoIw2XIvAmX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamviji/project/blob/master/PostMidTerm/ref/IIT_Lab_Deepcom_1_model_LBC_AWGN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ONjKRtfyUPy",
        "outputId": "88448a17-2734-4d11-98ac-396500a14c73"
      },
      "source": [
        "# This script trains the conv1D-based Linear Block Codes/Modulation\r\n",
        "# by ZKY 2019/02/15\r\n",
        "\r\n",
        "import os\r\n",
        "\r\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.layers import Dense, Dropout, Lambda, BatchNormalization, Input, Conv1D, TimeDistributed, Flatten, Activation\r\n",
        "from keras.models import Model\r\n",
        "from keras.callbacks import EarlyStopping, TensorBoard, History, ModelCheckpoint, ReduceLROnPlateau\r\n",
        "from keras import backend as KR\r\n",
        "import numpy as np\r\n",
        "import copy\r\n",
        "import time\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.optimizers import Adam\r\n",
        "\r\n",
        "'''\r\n",
        " --- COMMUNICATION PARAMETERS ---\r\n",
        "'''\r\n",
        "\r\n",
        "# Bits per Symbol\r\n",
        "k = 4\r\n",
        "\r\n",
        "# Number of symbols\r\n",
        "L = 10\r\n",
        "\r\n",
        "# Channel Use\r\n",
        "n = 1\r\n",
        "\r\n",
        "# Effective Throughput\r\n",
        "#  bits per symbol / channel use\r\n",
        "R = k / n\r\n",
        "\r\n",
        "# Eb/N0 used for training\r\n",
        "train_Eb_dB = 9\r\n",
        "\r\n",
        "# Noise Standard Deviation\r\n",
        "noise_sigma = np.sqrt(1 / (2 * R * 10 ** (train_Eb_dB / 10)))\r\n",
        "\r\n",
        "\r\n",
        "# Number of messages used for training, each size = k*L\r\n",
        "batch_size = 64\r\n",
        "nb_train_word = batch_size*200\r\n",
        "\r\n",
        "'''\r\n",
        " --- GENERATING INPUT DATA ---\r\n",
        "'''\r\n",
        "\r\n",
        "# Generate training binary Data\r\n",
        "train_data = np.random.randint(low=0, high=2, size=(nb_train_word, k * L))\r\n",
        "# Used as labeled data\r\n",
        "label_data = copy.copy(train_data)\r\n",
        "train_data = np.reshape(train_data, newshape=(nb_train_word, L, k))\r\n",
        "\r\n",
        "# Convert Binary Data to integer\r\n",
        "tmp_array = np.zeros(shape=k)\r\n",
        "for i in range(k):\r\n",
        "    tmp_array[i] = 2 ** i\r\n",
        "int_data = tmp_array[::-1]\r\n",
        "\r\n",
        "# Convert Integer Data to one-hot vector\r\n",
        "int_data = np.reshape(int_data, newshape=(k, 1))\r\n",
        "one_hot_data = np.dot(train_data, int_data)\r\n",
        "vec_one_hot = to_categorical(y=one_hot_data, num_classes=2 ** k)\r\n",
        "\r\n",
        "# used as Label data\r\n",
        "label_one_hot = copy.copy(vec_one_hot)\r\n",
        "\r\n",
        "'''\r\n",
        " --- NEURAL NETWORKS PARAMETERS ---\r\n",
        "'''\r\n",
        "\r\n",
        "early_stopping_patience = 100\r\n",
        "\r\n",
        "epochs = 150\r\n",
        "\r\n",
        "optimizer = Adam(lr=0.001)\r\n",
        "\r\n",
        "early_stopping = EarlyStopping(monitor='val_loss',\r\n",
        "                               patience=early_stopping_patience)\r\n",
        "\r\n",
        "\r\n",
        "# Learning Rate Control\r\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1,\r\n",
        "                              patience=5, min_lr=0.0001)\r\n",
        "\r\n",
        "# Save the best results based on Training Set\r\n",
        "modelcheckpoint = ModelCheckpoint(filepath='./' + 'model_LBC_' + str(k) + '_' + str(L) + '_' + str(n) + '_' + str(train_Eb_dB) + 'dB' + ' ' + 'AWGN' + '.h5',\r\n",
        "                                  monitor='loss',\r\n",
        "                                  verbose=1,\r\n",
        "                                  save_best_only=True,\r\n",
        "                                  save_weights_only=True,\r\n",
        "                                  mode='auto', period=1)\r\n",
        "\r\n",
        "\r\n",
        "# Define Power Norm for Tx\r\n",
        "def normalization(x):\r\n",
        "    mean = KR.mean(x ** 2)\r\n",
        "    return x / KR.sqrt(2 * mean)  # 2 = I and Q channels\r\n",
        "\r\n",
        "\r\n",
        "# Define Channel Layers including AWGN and Flat Rayleigh fading\r\n",
        "#  x: input data\r\n",
        "#  sigma: noise std\r\n",
        "def channel_layer(x, sigma):\r\n",
        "\r\n",
        "    w = KR.random_normal(KR.shape(x), mean=0.0, stddev=sigma)\r\n",
        "\r\n",
        "    return x + w\r\n",
        "\r\n",
        "\r\n",
        "model_input = Input(batch_shape=(batch_size, L, 2 ** k), name='input_bits')\r\n",
        "\r\n",
        "e = Conv1D(filters=256, strides=1, kernel_size=1, name='e_1')(model_input)\r\n",
        "e = BatchNormalization(name='e_2')(e)\r\n",
        "e = Activation('elu', name='e_3')(e)\r\n",
        "\r\n",
        "e = Conv1D(filters=256, strides=1, kernel_size=1, name='e_7')(e)\r\n",
        "e = BatchNormalization(name='e_8')(e)\r\n",
        "e = Activation('elu', name='e_9')(e)\r\n",
        "\r\n",
        "e = Conv1D(filters=2 * n, strides=1, kernel_size=1, name='e_10')(e)  # 2 = I and Q channels\r\n",
        "e = BatchNormalization(name='e_11')(e)\r\n",
        "e = Activation('linear', name='e_12')(e)\r\n",
        "\r\n",
        "e = Lambda(normalization, name='power_norm')(e)\r\n",
        "\r\n",
        "# AWGN channel\r\n",
        "y_h = Lambda(channel_layer, arguments={'sigma': noise_sigma}, name='channel_layer')(e)\r\n",
        "\r\n",
        "# Define Decoder Layers (Receiver)\r\n",
        "d = Conv1D(filters=256, strides=1, kernel_size=1, name='d_1')(y_h)\r\n",
        "d = BatchNormalization(name='d_2')(d)\r\n",
        "d = Activation('elu', name='d_3')(d)\r\n",
        "\r\n",
        "d = Conv1D(filters=256, strides=1, kernel_size=1, name='d_7')(d)\r\n",
        "d = BatchNormalization(name='d_8')(d)\r\n",
        "d = Activation('elu', name='d_9')(d)\r\n",
        "\r\n",
        "# Output One hot vector and use Softmax to soft decoding\r\n",
        "model_output = Conv1D(filters=2 ** k, strides=1, kernel_size=1, name='d_10', activation='softmax')(d)\r\n",
        "\r\n",
        "# Build System Model\r\n",
        "sys_model = Model(model_input, model_output)\r\n",
        "encoder = Model(model_input, e)\r\n",
        "\r\n",
        "# Print Model Architecture\r\n",
        "sys_model.summary()\r\n",
        "\r\n",
        "# Compile Model\r\n",
        "sys_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\r\n",
        "# print('encoder output:', '\\n', encoder.predict(vec_one_hot, batch_size=batch_size))\r\n",
        "\r\n",
        "print('starting train the NN...')\r\n",
        "start = time.clock()\r\n",
        "\r\n",
        "# TRAINING\r\n",
        "mod_history = sys_model.fit(vec_one_hot, label_one_hot,\r\n",
        "                            batch_size=batch_size,\r\n",
        "                            epochs=epochs,\r\n",
        "                            verbose=1,\r\n",
        "                            validation_split=0.3, callbacks=[modelcheckpoint,reduce_lr])\r\n",
        "\r\n",
        "end = time.clock()\r\n",
        "\r\n",
        "print('The NN has trained ' + str(end - start) + ' s')\r\n",
        "\r\n",
        "\r\n",
        "# Plot the Training Loss and Validation Loss\r\n",
        "hist_dict = mod_history.history\r\n",
        "\r\n",
        "val_loss = hist_dict['val_loss']\r\n",
        "loss = hist_dict['loss']\r\n",
        "acc = hist_dict['acc']\r\n",
        "# val_acc = hist_dict['val_acc']\r\n",
        "print('loss:',loss)\r\n",
        "print('val_loss:',val_loss)\r\n",
        "\r\n",
        "epoch = np.arange(1, epochs + 1)\r\n",
        "\r\n",
        "plt.semilogy(epoch,val_loss,label='val_loss')\r\n",
        "plt.semilogy(epoch, loss, label='loss')\r\n",
        "\r\n",
        "plt.legend(loc=0)\r\n",
        "plt.grid('true')\r\n",
        "plt.xlabel('epochs')\r\n",
        "plt.ylabel('Binary cross-entropy loss')\r\n",
        "\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_bits (InputLayer)      [(64, 10, 16)]            0         \n",
            "_________________________________________________________________\n",
            "e_1 (Conv1D)                 (64, 10, 256)             4352      \n",
            "_________________________________________________________________\n",
            "e_2 (BatchNormalization)     (64, 10, 256)             1024      \n",
            "_________________________________________________________________\n",
            "e_3 (Activation)             (64, 10, 256)             0         \n",
            "_________________________________________________________________\n",
            "e_7 (Conv1D)                 (64, 10, 256)             65792     \n",
            "_________________________________________________________________\n",
            "e_8 (BatchNormalization)     (64, 10, 256)             1024      \n",
            "_________________________________________________________________\n",
            "e_9 (Activation)             (64, 10, 256)             0         \n",
            "_________________________________________________________________\n",
            "e_10 (Conv1D)                (64, 10, 2)               514       \n",
            "_________________________________________________________________\n",
            "e_11 (BatchNormalization)    (64, 10, 2)               8         \n",
            "_________________________________________________________________\n",
            "e_12 (Activation)            (64, 10, 2)               0         \n",
            "_________________________________________________________________\n",
            "power_norm (Lambda)          (64, 10, 2)               0         \n",
            "_________________________________________________________________\n",
            "channel_layer (Lambda)       (64, 10, 2)               0         \n",
            "_________________________________________________________________\n",
            "d_1 (Conv1D)                 (64, 10, 256)             768       \n",
            "_________________________________________________________________\n",
            "d_2 (BatchNormalization)     (64, 10, 256)             1024      \n",
            "_________________________________________________________________\n",
            "d_3 (Activation)             (64, 10, 256)             0         \n",
            "_________________________________________________________________\n",
            "d_7 (Conv1D)                 (64, 10, 256)             65792     \n",
            "_________________________________________________________________\n",
            "d_8 (BatchNormalization)     (64, 10, 256)             1024      \n",
            "_________________________________________________________________\n",
            "d_9 (Activation)             (64, 10, 256)             0         \n",
            "_________________________________________________________________\n",
            "d_10 (Conv1D)                (64, 10, 16)              4112      \n",
            "=================================================================\n",
            "Total params: 145,434\n",
            "Trainable params: 143,382\n",
            "Non-trainable params: 2,052\n",
            "_________________________________________________________________\n",
            "starting train the NN...\n",
            "Epoch 1/150\n",
            "140/140 [==============================] - 7s 40ms/step - loss: 0.3386 - accuracy: 0.6057 - val_loss: 0.2312 - val_accuracy: 0.0998\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.18928, saving model to ./model_LBC_4_10_1_9dB AWGN.h5\n",
            "Epoch 2/150\n",
            "140/140 [==============================] - 5s 37ms/step - loss: 0.0515 - accuracy: 0.9649 - val_loss: 0.2455 - val_accuracy: 0.2698\n",
            "\n",
            "Epoch 00002: loss improved from 0.18928 to 0.04399, saving model to ./model_LBC_4_10_1_9dB AWGN.h5\n",
            "Epoch 3/150\n",
            "140/140 [==============================] - 5s 37ms/step - loss: 0.0294 - accuracy: 0.9692 - val_loss: 0.1779 - val_accuracy: 0.3728\n",
            "\n",
            "Epoch 00003: loss improved from 0.04399 to 0.02678, saving model to ./model_LBC_4_10_1_9dB AWGN.h5\n",
            "Epoch 4/150\n",
            "140/140 [==============================] - 5s 37ms/step - loss: 0.0217 - accuracy: 0.9696 - val_loss: 0.0406 - val_accuracy: 0.8999\n",
            "\n",
            "Epoch 00004: loss improved from 0.02678 to 0.02037, saving model to ./model_LBC_4_10_1_9dB AWGN.h5\n",
            "Epoch 5/150\n",
            "140/140 [==============================] - 5s 38ms/step - loss: 0.0170 - accuracy: 0.9745 - val_loss: 0.0167 - val_accuracy: 0.9702\n",
            "\n",
            "Epoch 00005: loss improved from 0.02037 to 0.01601, saving model to ./model_LBC_4_10_1_9dB AWGN.h5\n",
            "Epoch 6/150\n",
            "140/140 [==============================] - 5s 38ms/step - loss: 0.0146 - accuracy: 0.9750 - val_loss: 0.0124 - val_accuracy: 0.9801\n",
            "\n",
            "Epoch 00006: loss improved from 0.01601 to 0.01413, saving model to ./model_LBC_4_10_1_9dB AWGN.h5\n",
            "Epoch 7/150\n",
            "140/140 [==============================] - 5s 38ms/step - loss: 0.0130 - accuracy: 0.9761 - val_loss: 0.0108 - val_accuracy: 0.9799\n",
            "\n",
            "Epoch 00007: loss improved from 0.01413 to 0.01231, saving model to ./model_LBC_4_10_1_9dB AWGN.h5\n",
            "Epoch 8/150\n",
            "140/140 [==============================] - 5s 38ms/step - loss: 0.0119 - accuracy: 0.9756 - val_loss: 0.0093 - val_accuracy: 0.9817\n",
            "\n",
            "Epoch 00008: loss improved from 0.01231 to 0.01178, saving model to ./model_LBC_4_10_1_9dB AWGN.h5\n",
            "Epoch 9/150\n",
            "140/140 [==============================] - 5s 39ms/step - loss: 0.0112 - accuracy: 0.9767 - val_loss: 0.0099 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00009: loss improved from 0.01178 to 0.01113, saving model to ./model_LBC_4_10_1_9dB AWGN.h5\n",
            "Epoch 10/150\n",
            "140/140 [==============================] - 5s 39ms/step - loss: 0.0104 - accuracy: 0.9765 - val_loss: 0.0092 - val_accuracy: 0.9803\n",
            "\n",
            "Epoch 00010: loss improved from 0.01113 to 0.01027, saving model to ./model_LBC_4_10_1_9dB AWGN.h5\n",
            "Epoch 11/150\n",
            "140/140 [==============================] - 5s 39ms/step - loss: 0.0101 - accuracy: 0.9761 - val_loss: 0.0087 - val_accuracy: 0.9809\n",
            "\n",
            "Epoch 00011: loss improved from 0.01027 to 0.00983, saving model to ./model_LBC_4_10_1_9dB AWGN.h5\n",
            "Epoch 12/150\n",
            "140/140 [==============================] - 5s 39ms/step - loss: 0.0095 - accuracy: 0.9776 - val_loss: 0.0080 - val_accuracy: 0.9816\n",
            "\n",
            "Epoch 00012: loss improved from 0.00983 to 0.00970, saving model to ./model_LBC_4_10_1_9dB AWGN.h5\n",
            "Epoch 13/150\n",
            "140/140 [==============================] - 6s 40ms/step - loss: 0.0097 - accuracy: 0.9763 - val_loss: 0.0083 - val_accuracy: 0.9811\n",
            "\n",
            "Epoch 00013: loss improved from 0.00970 to 0.00929, saving model to ./model_LBC_4_10_1_9dB AWGN.h5\n",
            "Epoch 14/150\n",
            "140/140 [==============================] - 6s 40ms/step - loss: 0.0089 - accuracy: 0.9782 - val_loss: 0.0094 - val_accuracy: 0.9769\n",
            "\n",
            "Epoch 00014: loss improved from 0.00929 to 0.00905, saving model to ./model_LBC_4_10_1_9dB AWGN.h5\n",
            "Epoch 15/150\n",
            "140/140 [==============================] - 5s 39ms/step - loss: 0.0091 - accuracy: 0.9780 - val_loss: 0.0078 - val_accuracy: 0.9813\n",
            "\n",
            "Epoch 00015: loss improved from 0.00905 to 0.00876, saving model to ./model_LBC_4_10_1_9dB AWGN.h5\n",
            "Epoch 16/150\n",
            "140/140 [==============================] - 5s 39ms/step - loss: 0.0086 - accuracy: 0.9792 - val_loss: 0.0069 - val_accuracy: 0.9840\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.00876\n",
            "Epoch 17/150\n",
            "140/140 [==============================] - 5s 38ms/step - loss: 0.0088 - accuracy: 0.9777 - val_loss: 0.0072 - val_accuracy: 0.9818\n",
            "\n",
            "Epoch 00017: loss improved from 0.00876 to 0.00866, saving model to ./model_LBC_4_10_1_9dB AWGN.h5\n",
            "Epoch 18/150\n",
            "140/140 [==============================] - 5s 38ms/step - loss: 0.0084 - accuracy: 0.9785 - val_loss: 0.0069 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00018: loss improved from 0.00866 to 0.00800, saving model to ./model_LBC_4_10_1_9dB AWGN.h5\n",
            "Epoch 19/150\n",
            " 21/140 [===>..........................] - ETA: 4s - loss: 0.0078 - accuracy: 0.9816"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}