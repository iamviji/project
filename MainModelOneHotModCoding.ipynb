{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MainModelOneHotModCoding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamviji/project/blob/master/MainModelOneHotModCoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDSPPMfZ9czi",
        "outputId": "4b2ea614-1bdb-42d0-95d9-2810dc80b19b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "!rm -rf project\n",
        "!git clone https://github.com/iamviji/project.git\n",
        "!ls\n",
        "!ls project\n",
        "!pip install pyldpc\n",
        "!pip install scikit-commpy\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'project'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 214 (delta 1), reused 0 (delta 0), pack-reused 208\u001b[K\n",
            "Receiving objects: 100% (214/214), 22.28 MiB | 8.96 MiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n",
            "ldpc_ber_18_11.png  ldpc_ber_2_4.png  project  sample_data\n",
            "EncoderModulatorSplitArch.ipynb  MainModelKerasOneHot.ipynb\n",
            "EncoderOutputExperiment.ipynb\t MainModelModCoding.ipynb\n",
            "End2End8PSK.ipynb\t\t MainModelOneHotMethod.ipynb\n",
            "End2EndOneHotQPSK.ipynb\t\t MainModelOneHotMethodSoftMax.ipynb\n",
            "End2EndQPSK.ipynb\t\t MainModelOneHotModCoding.ipynb\n",
            "End2EndQPSKRegularized.ipynb\t MainModelWithSingleBERTraining.ipynb\n",
            "MainModel.ipynb\t\t\t README.md\n",
            "MainModelKeras.ipynb\t\t util.py\n",
            "Requirement already satisfied: pyldpc in /usr/local/lib/python3.6/dist-packages (0.7.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyldpc) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyldpc) (1.4.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from pyldpc) (0.48.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->pyldpc) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->pyldpc) (50.3.0)\n",
            "Requirement already satisfied: scikit-commpy in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->scikit-commpy) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QOuLqpdDgx2"
      },
      "source": [
        "import pyldpc\n",
        "import commpy\n",
        "import numpy \n",
        "import time\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior ()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YClXJbbr0lc7"
      },
      "source": [
        "SNR_BEGIN = 0\n",
        "SNR_END = 10\n",
        "SNR_STEP_SIZE = 0.5\n",
        "CHANEL_SIZE = 18\n",
        "NUM_OF_INPUT_MESSAGE = 1000\n",
        "LDPC_MAX_ITER = 100\n",
        "num_parity_check = 3\n",
        "num_bits_in_parity_check = 6 \n",
        "input_message_length =  0 # Caculated by channel encoder and initialized later"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvUzIMsB43i0"
      },
      "source": [
        "def timer_update(i,current,time_tot,tic_incr=500):\n",
        "    last = current\n",
        "    current = time.time()\n",
        "    t_diff = current-last\n",
        "    print('SNR: {:04.3f} - Iter: {} - Last {} iterations took {:03.2f}s'.format(snr,i+1,tic_incr,t_diff))\n",
        "    return time_tot + t_diff\n",
        "\n",
        "def Snr2Sigma(snr):\n",
        "  sigma = 10 ** (- snr / 20)\n",
        "  return sigma\n",
        "\n",
        "def pyldpc_encode (CodingMatrix, message):\n",
        "  rng = pyldpc.utils.check_random_state(seed=None)\n",
        "  d = pyldpc.utils.binaryproduct(CodingMatrix, message)\n",
        "  encoded_message = (-1) ** d\n",
        "  return encoded_message\n",
        "\n",
        "def pyldpc_decode (ParityCheckMatrix, CodingMatrix, message, snr, maxiter):\n",
        "  decoded_msg = pyldpc.decode(ParityCheckMatrix, message, snr, maxiter)\n",
        "  out_message = pyldpc.get_message(CodingMatrix, decoded_msg)\n",
        "  return out_message\n",
        "\n",
        "awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [CHANEL_SIZE])\n",
        "awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)\n",
        "\n",
        "init = tf.global_variables_initializer ()\n",
        "sess = tf.Session ()\n",
        "sess.run(init)\n",
        "\n",
        "def AWGNChannelOutput (xx, snr , s):\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  awgn_channel_output_message = s.run ([awgn_channel_output], feed_dict={noise_std_dev:sigma, channel_input:xx})\n",
        "  return awgn_channel_output_message"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jMQG-MZ_pXu",
        "outputId": "7c2767ec-acd1-43eb-828d-8a74cb32e83d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "\n",
        "ParityCheckMatrix, CodingMatrix = pyldpc.make_ldpc(CHANEL_SIZE, num_parity_check, num_bits_in_parity_check, systematic=True, sparse=True)\n",
        "input_message_length = CodingMatrix.shape[1]\n",
        "print (\"input_message_size=\", input_message_length, \"channel_size=\",CHANEL_SIZE)\n",
        "print (\"input_message_size=\", CodingMatrix.shape[1], \"channel_size=\",CodingMatrix.shape[0])\n",
        "input_message = numpy.random.randint(2, size=(NUM_OF_INPUT_MESSAGE,input_message_length))\n",
        "print (input_message)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_message_size= 11 channel_size= 18\n",
            "input_message_size= 11 channel_size= 18\n",
            "[[0 0 0 ... 0 1 0]\n",
            " [1 1 0 ... 0 0 1]\n",
            " [1 0 0 ... 0 0 1]\n",
            " ...\n",
            " [0 0 0 ... 0 1 0]\n",
            " [1 0 1 ... 1 1 1]\n",
            " [1 0 0 ... 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WKg2HU2adgZ"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fL8ptL4aeOY"
      },
      "source": [
        "This section tries to compare BER and Time performance of PYLDPC in following 3 cases\n",
        "1. SNR Noise function provided in encoder function of pyldpc library (pyldpc.encode)\n",
        "2. SNR Noise function provided by commpy library (commpy.channels.awgn) \n",
        "3. SNR Noise function implemented using tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma5zUqFv0TH2",
        "outputId": "0708dcd0-7f10-465b-c3cb-80d07c0d6738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using tensor flow based AWGN, to make sure that effect of it is same as AWGN\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_tensor  = numpy.array(())\n",
        "times_per_iter_tensor = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc_encode (CodingMatrix, input_message[i])\n",
        "    sigma = Snr2Sigma (snr)\n",
        "    awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message})[0]\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      #print (\"count=\",abs(decoded_message-input_message[i]).sum())\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_tensor=numpy.append(ber_per_iter_tensor ,ber)\n",
        "  times_per_iter_tensor=numpy.append(times_per_iter_tensor, total_time)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.65s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 3.15s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 4.75s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 6.27s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.65\n",
            " -> Total Time: 15.83s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.30s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.62s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 3.88s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 5.18s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.55\n",
            " -> Total Time: 12.98s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 1.07s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 2.13s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 3.13s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 4.23s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.49\n",
            " -> Total Time: 10.56s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.88s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.71s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.59s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 3.52s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.42\n",
            " -> Total Time: 8.70s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.61s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.27s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 2.04s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.80s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.33\n",
            " -> Total Time: 6.72s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.53s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.07s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.62s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.13s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.27\n",
            " -> Total Time: 5.36s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.49s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 1.00s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.51s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.96s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.22\n",
            " -> Total Time: 4.96s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.38s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.75s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.17s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.53s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.15\n",
            " -> Total Time: 3.83s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.37s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.71s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 1.06s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.43s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.11\n",
            " -> Total Time: 3.57s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.35s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.66s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 1.02s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.36s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.09\n",
            " -> Total Time: 3.39s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.28s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.55s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.86s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 1.15s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.06\n",
            " -> Total Time: 2.84s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.29s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.56s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.86s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 1.16s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 2.87s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.30s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.58s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.84s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 1.15s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 2.87s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.54s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.80s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 1.06s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.66s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.54s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.81s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 1.08s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.70s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.78s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 1.04s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.61s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.79s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 1.04s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.63s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.79s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 1.05s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.62s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.79s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 1.05s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.63s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.78s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 1.04s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.61s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8dIFLg76c7O",
        "outputId": "8cd847b0-1609-4325-94b6-94c44cb16e53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using commpy based AWGN \n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_awgn  = numpy.array(())\n",
        "times_per_iter_awgn = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc_encode (CodingMatrix, input_message[i])\n",
        "    awgn_channel_output_message = commpy.channels.awgn(encoded_message, snr)\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_awgn=numpy.append(ber_per_iter_awgn ,ber)\n",
        "  times_per_iter_awgn=numpy.append(times_per_iter_awgn, total_time)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.34s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 2.73s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 4.10s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 5.56s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.64\n",
            " -> Total Time: 13.73s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.23s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.49s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 3.63s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 4.74s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.54\n",
            " -> Total Time: 12.09s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.96s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 1.82s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 2.71s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 3.66s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.49\n",
            " -> Total Time: 9.15s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.81s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.50s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.23s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 2.94s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.42\n",
            " -> Total Time: 7.48s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.67s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.32s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 1.97s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.59s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.34\n",
            " -> Total Time: 6.56s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.42s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 0.90s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.26s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 1.67s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.26\n",
            " -> Total Time: 4.24s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.43s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.75s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.10s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.45s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.20\n",
            " -> Total Time: 3.74s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.23s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 0.76s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.11s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.17\n",
            " -> Total Time: 2.59s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.28s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.58s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.93s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.17s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.13\n",
            " -> Total Time: 2.96s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.46s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.69s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 0.90s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.08\n",
            " -> Total Time: 2.30s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.18s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.37s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.55s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.76s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.06\n",
            " -> Total Time: 1.86s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.35s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.52s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.69s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 1.72s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.18s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.35s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.52s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.70s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 1.74s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.49s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.67s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.63s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.33s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.50s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.66s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.65s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.33s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.49s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.65s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.62s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.33s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.49s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.65s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.63s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.46s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.63s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.55s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.47s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.63s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.57s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.46s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.62s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ihPKJJk7Jj9",
        "outputId": "30616d54-cded-46fd-d1c1-c082b8201ae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using tensor flow based AWGN, to make sure that effect of it is same as AWGN\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_pyldpc  = numpy.array(())\n",
        "times_per_iter_pyldpc = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc.encode (CodingMatrix, input_message[i], snr)\n",
        "    awgn_channel_output_message = encoded_message\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_pyldpc=numpy.append(ber_per_iter_pyldpc ,ber)\n",
        "  times_per_iter_pyldpc=numpy.append(times_per_iter_pyldpc, total_time)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.43s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 2.82s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 4.27s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 5.74s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.63\n",
            " -> Total Time: 14.27s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.23s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.41s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 3.59s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 4.81s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.56\n",
            " -> Total Time: 12.04s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.74s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 1.72s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 2.63s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 3.54s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.47\n",
            " -> Total Time: 8.64s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.82s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.60s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.40s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 3.23s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.43\n",
            " -> Total Time: 8.05s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.57s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.25s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 1.80s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.47s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.36\n",
            " -> Total Time: 6.09s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.42s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 0.94s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.37s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 1.83s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.27\n",
            " -> Total Time: 4.55s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.38s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.82s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.21s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.59s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.21\n",
            " -> Total Time: 4.00s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.28s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.57s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 0.81s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.05s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.17\n",
            " -> Total Time: 2.70s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.23s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.44s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.71s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.12\n",
            " -> Total Time: 2.34s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.21s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.40s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.63s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 0.82s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.10\n",
            " -> Total Time: 2.06s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.22s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.44s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.63s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.84s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.08\n",
            " -> Total Time: 2.14s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.18s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.34s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.50s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.67s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 1.69s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.33s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.50s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.67s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 1.66s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.47s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.64s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 1.56s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.30s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.46s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.63s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 1.53s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.33s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.50s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.66s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.64s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.47s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.63s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.59s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.30s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.45s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.60s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.50s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.46s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.61s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.53s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.30s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.45s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.61s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.50s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR4-FOJ-BkAG",
        "outputId": "385b7640-a7f9-479b-c614-4fa53ae12cbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "# Compare 3 AWGN(Tensorflow, CommPy, PYLDPC) Simulation on LDPC\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_pyldpc,'', label=\"pyldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"tensor\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_awgn,'', label=\"commpy-awgn\") # plot BER vs SNR\n",
        "\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "ax2.set_xlabel('$E_b/$N_0$')\n",
        "ax2.set_ylabel('Decoding Time [s]')\n",
        "ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "            xy=(1, 0.35), xycoords='axes fraction',\n",
        "            xytext=(-20, 20), textcoords='offset pixels',\n",
        "            horizontalalignment='right',\n",
        "            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggY5VwudaRwR"
      },
      "source": [
        "<B>Conclussion:</B>\n",
        "      It proved that tensorflow behaves similar to AWGN noise channel provided by pyldpc, commpy. But tensor flow based one takes adds little more time delay. This need to be offseted if we are comparing performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5wyTuef3s7u"
      },
      "source": [
        "class GetOutOfLoop( Exception ):\n",
        "    pass"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOeuNfeLCgfb",
        "outputId": "46d43544-6709-4a74-df1b-e70335626e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Define Model \n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "# input_message_length is initialized by ldpc encoder\n",
        "CHANEL_SIZE = 2\n",
        "num_hidden_1 = CHANEL_SIZE\n",
        "input_message_length = 4\n",
        "print (\"input_message_length=\", input_message_length)\n",
        "\n",
        "lr_x = tf.placeholder(dtype=tf.float32,shape=[])\n",
        "#batch_size_x = tf.placeholder(tf.int32,shape=[])\n",
        "input_message_x_label = tf.placeholder(\"int32\", [None], name=\"input_message_x_label\")\n",
        "input_message_x = tf.placeholder(\"float32\", [None, 2**input_message_length], name=\"input_message_x\")\n",
        "awgn_noise_std_dev_x = tf.placeholder(\"float32\", name =\"awgn_noise_std_dev\")\n",
        "input_channel_x = tf.placeholder(\"float32\", [None, CHANEL_SIZE], name=\"input_channel_x\")\n",
        "\n",
        "weights = {\n",
        "  \"encoder_l1\" : tf.Variable (tf.random_uniform([2**input_message_length, num_hidden_1], -1, 1), name=\"encoder_l1_weights\"),\n",
        "  \"decoder_l1\" : tf.Variable (tf.random_uniform([num_hidden_1, 2**input_message_length], -1, 1), name=\"decoder_l1_weights\"),\n",
        "  \"decoder_l2\" : tf.Variable (tf.random_uniform([2**input_message_length, 2**input_message_length], -1, 1), name=\"decoder_l2_weights\"),\n",
        "}\n",
        "\n",
        "biases = {\n",
        "  \"encoder_l1\" : tf.Variable (tf.random_uniform([num_hidden_1], -1,1), name=\"encoder_l1_bias\"),\n",
        "  \"decoder_l1\" : tf.Variable (tf.random_uniform([2**input_message_length], -1,1), name=\"decoder_l1_bias\"),\n",
        "  \"decoder_l2\" : tf.Variable (tf.random_uniform([2**input_message_length], -1,1), name=\"decoder_l2_bias\"),\n",
        "}\n",
        "\n",
        "def dl_encoder (x):\n",
        "  layer_1 = tf.nn.tanh (tf.matmul(x, weights['encoder_l1']) + biases['encoder_l1'])\n",
        "  #layer_2 = tf.round(layer_1)\n",
        "  #layer_1 = BatchNormalization ()(layer_1)\n",
        "  layer_2 =  layer_1 / tf.sqrt(tf.reduce_mean(tf.square(layer_1)))\n",
        "  #layer_2 =  tf.nn.relu(layer_1)\n",
        "  return layer_2\n",
        "\n",
        "def dl_decoder (x):\n",
        "  layer_1 = tf.nn.tanh (tf.matmul(x, weights['decoder_l1']) + biases['decoder_l1'])\n",
        "  layer_2 = (tf.matmul(layer_1, weights['decoder_l2']) + biases['decoder_l2'])\n",
        "  return layer_2\n",
        "\n",
        "def awgn_layer(x):\n",
        "  awgn_noise = tf.random.normal(tf.shape(x), stddev=awgn_noise_std_dev_x,  name=\"awgn_noise\")\n",
        "  awgn_channel_output = tf.add(x, awgn_noise, name =\"x_and_noise\")\n",
        "  return awgn_channel_output\n",
        "\n",
        "\n",
        "dl_encoder_output = dl_encoder(input_message_x)\n",
        "dl_decoder_input = awgn_layer(dl_encoder_output)\n",
        "#awgn_noise = tf.random.normal(tf.shape(dl_encoder_output), stddev=awgn_noise_std_dev,  name=\"awgn_noise\")\n",
        "#dl_decoder_input = tf.add(dl_encoder_output, awgn_noise, name =\"x_and_noise\")\n",
        "dl_decoder_output = dl_decoder (dl_decoder_input)\n",
        "dl_decoder_only_output = dl_decoder(input_channel_x)\n",
        "\n",
        "\n",
        "#loss1 = tf.reduce_mean (-1 * (input_message_x*tf.log(dl_decoder_output) + (1 - input_message_x)*tf.log(1 - dl_decoder_output) ))\n",
        "loss = tf.losses.sparse_softmax_cross_entropy(labels=input_message_x_label,logits=dl_decoder_output)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr_x).minimize (loss)\n",
        "\n",
        "awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [CHANEL_SIZE])\n",
        "awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_message_length= 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkW8oloyodIF",
        "outputId": "b513829a-7b6c-4d01-d16e-e89b7461ac71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "import numpy\n",
        "training_input_message = numpy.random.randint(2**input_message_length, size=(1,NUM_OF_INPUT_MESSAGE*10))\n",
        "training_input_message_one_hot = numpy.zeros((training_input_message.size, 2**input_message_length))\n",
        "training_input_message_one_hot[numpy.arange(training_input_message.size),training_input_message] = 1\n",
        "print(training_input_message_one_hot)\n",
        "print (training_input_message_one_hot.shape)\n",
        "print (training_input_message.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(10000, 16)\n",
            "(1, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxkLQFuqBT-g",
        "outputId": "9eb4272f-df17-42c6-fbc3-fe763c788bb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 100\n",
        "\n",
        "# Training\n",
        "train_init = tf.global_variables_initializer ()\n",
        "train_sess = tf.Session ()\n",
        "\n",
        "epochs = 10\n",
        "outer_ephocs = 1\n",
        "display_step = 2\n",
        "num_of_batches = training_input_message.shape[1] / batch_size\n",
        "print (\"batch_size:\", batch_size, \"num_of_batcches:\", num_of_batches)\n",
        "train_sess.run(train_init)\n",
        "l = 0\n",
        "lrate = 0.1\n",
        "i = 0\n",
        "snr_min = 9.5\n",
        "snr_max = 10.5\n",
        "snr_step_size = 0.5\n",
        "lr = 0.1\n",
        "max_iteration = epochs * num_of_batches * (snr_max - snr_min) / snr_step_size\n",
        "print (\"max iteration :\",max_iteration,\"num_of_batches:\", num_of_batches)\n",
        "try:\n",
        "  for oe in range(outer_ephocs):\n",
        "    for snr in (numpy.arange (0, 10, SNR_STEP_SIZE)):\n",
        "    #for snr in (numpy.arange (snr_min, snr_max, SNR_STEP_SIZE)):\n",
        "      sigma = 1.0*Snr2Sigma (7)\n",
        "      print (\"Training for SNR=\", snr, \" sigma=\", sigma, \"iteratin:\", oe) \n",
        "      for e in range(epochs):\n",
        "        for j in range (int(num_of_batches)):\n",
        "          i = i + 1\n",
        "          x_train_batch_one_hot = training_input_message_one_hot [j*batch_size:(j+1)*batch_size]\n",
        "          x_train_batch_one_hot = x_train_batch_one_hot.astype(\"float32\")\n",
        "          x_train_batch_label = training_input_message.reshape(training_input_message.shape[1]) [j*batch_size:(j+1)*batch_size]        \n",
        "          if (i < 100): \n",
        "            lr = 0.001\n",
        "          elif(i < 200):\n",
        "            lr = 0.0001\n",
        "          else:\n",
        "            lr = 0.00001 \n",
        "          #lr = 0.00001\n",
        "          _, l = train_sess.run ([optimizer, loss], feed_dict={input_message_x:x_train_batch_one_hot, awgn_noise_std_dev_x:sigma, lr_x:lr, input_message_x_label:x_train_batch_label.astype(\"int32\")})\n",
        "          if i % display_step == 0:          \n",
        "            print('Step %i: Minibatch Loss: %f' % (i, l ))\n",
        "          if (l < 0.05 and snr >= 9): \n",
        "            print (\"Loss=\", l)\n",
        "            raise GetOutOfLoop\n",
        "except GetOutOfLoop:\n",
        "  print(\"Early Stop\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Step 10022: Minibatch Loss: 2.331071\n",
            "Step 10024: Minibatch Loss: 2.367362\n",
            "Step 10026: Minibatch Loss: 2.427654\n",
            "Step 10028: Minibatch Loss: 2.323734\n",
            "Step 10030: Minibatch Loss: 2.372503\n",
            "Step 10032: Minibatch Loss: 2.292210\n",
            "Step 10034: Minibatch Loss: 2.310699\n",
            "Step 10036: Minibatch Loss: 2.375527\n",
            "Step 10038: Minibatch Loss: 2.285940\n",
            "Step 10040: Minibatch Loss: 2.334267\n",
            "Step 10042: Minibatch Loss: 2.302593\n",
            "Step 10044: Minibatch Loss: 2.284627\n",
            "Step 10046: Minibatch Loss: 2.380848\n",
            "Step 10048: Minibatch Loss: 2.328434\n",
            "Step 10050: Minibatch Loss: 2.408556\n",
            "Step 10052: Minibatch Loss: 2.416823\n",
            "Step 10054: Minibatch Loss: 2.358299\n",
            "Step 10056: Minibatch Loss: 2.401217\n",
            "Step 10058: Minibatch Loss: 2.320373\n",
            "Step 10060: Minibatch Loss: 2.258119\n",
            "Step 10062: Minibatch Loss: 2.435277\n",
            "Step 10064: Minibatch Loss: 2.293122\n",
            "Step 10066: Minibatch Loss: 2.394648\n",
            "Step 10068: Minibatch Loss: 2.286168\n",
            "Step 10070: Minibatch Loss: 2.373157\n",
            "Step 10072: Minibatch Loss: 2.324746\n",
            "Step 10074: Minibatch Loss: 2.353189\n",
            "Step 10076: Minibatch Loss: 2.371289\n",
            "Step 10078: Minibatch Loss: 2.342718\n",
            "Step 10080: Minibatch Loss: 2.389733\n",
            "Step 10082: Minibatch Loss: 2.391711\n",
            "Step 10084: Minibatch Loss: 2.310879\n",
            "Step 10086: Minibatch Loss: 2.330097\n",
            "Step 10088: Minibatch Loss: 2.383838\n",
            "Step 10090: Minibatch Loss: 2.316613\n",
            "Step 10092: Minibatch Loss: 2.441153\n",
            "Step 10094: Minibatch Loss: 2.350869\n",
            "Step 10096: Minibatch Loss: 2.305772\n",
            "Step 10098: Minibatch Loss: 2.380859\n",
            "Step 10100: Minibatch Loss: 2.227276\n",
            "Step 10102: Minibatch Loss: 2.357510\n",
            "Step 10104: Minibatch Loss: 2.348549\n",
            "Step 10106: Minibatch Loss: 2.374605\n",
            "Step 10108: Minibatch Loss: 2.416885\n",
            "Step 10110: Minibatch Loss: 2.323677\n",
            "Step 10112: Minibatch Loss: 2.328570\n",
            "Step 10114: Minibatch Loss: 2.413100\n",
            "Step 10116: Minibatch Loss: 2.411098\n",
            "Step 10118: Minibatch Loss: 2.358164\n",
            "Step 10120: Minibatch Loss: 2.389824\n",
            "Step 10122: Minibatch Loss: 2.404356\n",
            "Step 10124: Minibatch Loss: 2.352150\n",
            "Step 10126: Minibatch Loss: 2.390463\n",
            "Step 10128: Minibatch Loss: 2.317824\n",
            "Step 10130: Minibatch Loss: 2.327584\n",
            "Step 10132: Minibatch Loss: 2.314992\n",
            "Step 10134: Minibatch Loss: 2.351690\n",
            "Step 10136: Minibatch Loss: 2.422900\n",
            "Step 10138: Minibatch Loss: 2.295414\n",
            "Step 10140: Minibatch Loss: 2.366091\n",
            "Step 10142: Minibatch Loss: 2.321845\n",
            "Step 10144: Minibatch Loss: 2.358057\n",
            "Step 10146: Minibatch Loss: 2.353532\n",
            "Step 10148: Minibatch Loss: 2.366552\n",
            "Step 10150: Minibatch Loss: 2.387535\n",
            "Step 10152: Minibatch Loss: 2.358150\n",
            "Step 10154: Minibatch Loss: 2.379645\n",
            "Step 10156: Minibatch Loss: 2.367444\n",
            "Step 10158: Minibatch Loss: 2.363754\n",
            "Step 10160: Minibatch Loss: 2.306054\n",
            "Step 10162: Minibatch Loss: 2.415946\n",
            "Step 10164: Minibatch Loss: 2.279881\n",
            "Step 10166: Minibatch Loss: 2.398556\n",
            "Step 10168: Minibatch Loss: 2.320391\n",
            "Step 10170: Minibatch Loss: 2.451319\n",
            "Step 10172: Minibatch Loss: 2.347184\n",
            "Step 10174: Minibatch Loss: 2.313994\n",
            "Step 10176: Minibatch Loss: 2.425825\n",
            "Step 10178: Minibatch Loss: 2.358766\n",
            "Step 10180: Minibatch Loss: 2.346887\n",
            "Step 10182: Minibatch Loss: 2.364100\n",
            "Step 10184: Minibatch Loss: 2.360296\n",
            "Step 10186: Minibatch Loss: 2.321501\n",
            "Step 10188: Minibatch Loss: 2.291020\n",
            "Step 10190: Minibatch Loss: 2.376557\n",
            "Step 10192: Minibatch Loss: 2.354631\n",
            "Step 10194: Minibatch Loss: 2.322665\n",
            "Step 10196: Minibatch Loss: 2.316173\n",
            "Step 10198: Minibatch Loss: 2.356287\n",
            "Step 10200: Minibatch Loss: 2.276947\n",
            "Step 10202: Minibatch Loss: 2.317706\n",
            "Step 10204: Minibatch Loss: 2.370987\n",
            "Step 10206: Minibatch Loss: 2.333583\n",
            "Step 10208: Minibatch Loss: 2.340884\n",
            "Step 10210: Minibatch Loss: 2.323922\n",
            "Step 10212: Minibatch Loss: 2.345051\n",
            "Step 10214: Minibatch Loss: 2.427606\n",
            "Step 10216: Minibatch Loss: 2.349211\n",
            "Step 10218: Minibatch Loss: 2.341313\n",
            "Step 10220: Minibatch Loss: 2.459416\n",
            "Step 10222: Minibatch Loss: 2.325541\n",
            "Step 10224: Minibatch Loss: 2.287710\n",
            "Step 10226: Minibatch Loss: 2.365540\n",
            "Step 10228: Minibatch Loss: 2.323575\n",
            "Step 10230: Minibatch Loss: 2.358828\n",
            "Step 10232: Minibatch Loss: 2.302778\n",
            "Step 10234: Minibatch Loss: 2.272462\n",
            "Step 10236: Minibatch Loss: 2.367375\n",
            "Step 10238: Minibatch Loss: 2.332309\n",
            "Step 10240: Minibatch Loss: 2.394652\n",
            "Step 10242: Minibatch Loss: 2.293408\n",
            "Step 10244: Minibatch Loss: 2.342291\n",
            "Step 10246: Minibatch Loss: 2.373088\n",
            "Step 10248: Minibatch Loss: 2.349872\n",
            "Step 10250: Minibatch Loss: 2.340967\n",
            "Step 10252: Minibatch Loss: 2.321916\n",
            "Step 10254: Minibatch Loss: 2.321546\n",
            "Step 10256: Minibatch Loss: 2.355953\n",
            "Step 10258: Minibatch Loss: 2.321054\n",
            "Step 10260: Minibatch Loss: 2.274513\n",
            "Step 10262: Minibatch Loss: 2.396174\n",
            "Step 10264: Minibatch Loss: 2.290781\n",
            "Step 10266: Minibatch Loss: 2.415082\n",
            "Step 10268: Minibatch Loss: 2.239137\n",
            "Step 10270: Minibatch Loss: 2.390185\n",
            "Step 10272: Minibatch Loss: 2.331100\n",
            "Step 10274: Minibatch Loss: 2.291408\n",
            "Step 10276: Minibatch Loss: 2.329309\n",
            "Step 10278: Minibatch Loss: 2.340781\n",
            "Step 10280: Minibatch Loss: 2.363736\n",
            "Step 10282: Minibatch Loss: 2.325528\n",
            "Step 10284: Minibatch Loss: 2.314727\n",
            "Step 10286: Minibatch Loss: 2.336070\n",
            "Step 10288: Minibatch Loss: 2.313836\n",
            "Step 10290: Minibatch Loss: 2.356130\n",
            "Step 10292: Minibatch Loss: 2.391084\n",
            "Step 10294: Minibatch Loss: 2.269253\n",
            "Step 10296: Minibatch Loss: 2.297072\n",
            "Step 10298: Minibatch Loss: 2.352750\n",
            "Step 10300: Minibatch Loss: 2.246945\n",
            "Step 10302: Minibatch Loss: 2.363074\n",
            "Step 10304: Minibatch Loss: 2.398871\n",
            "Step 10306: Minibatch Loss: 2.285359\n",
            "Step 10308: Minibatch Loss: 2.349244\n",
            "Step 10310: Minibatch Loss: 2.311778\n",
            "Step 10312: Minibatch Loss: 2.350040\n",
            "Step 10314: Minibatch Loss: 2.390322\n",
            "Step 10316: Minibatch Loss: 2.354342\n",
            "Step 10318: Minibatch Loss: 2.353148\n",
            "Step 10320: Minibatch Loss: 2.415186\n",
            "Step 10322: Minibatch Loss: 2.342872\n",
            "Step 10324: Minibatch Loss: 2.353809\n",
            "Step 10326: Minibatch Loss: 2.376910\n",
            "Step 10328: Minibatch Loss: 2.316541\n",
            "Step 10330: Minibatch Loss: 2.321656\n",
            "Step 10332: Minibatch Loss: 2.306566\n",
            "Step 10334: Minibatch Loss: 2.322075\n",
            "Step 10336: Minibatch Loss: 2.356866\n",
            "Step 10338: Minibatch Loss: 2.306850\n",
            "Step 10340: Minibatch Loss: 2.331848\n",
            "Step 10342: Minibatch Loss: 2.280293\n",
            "Step 10344: Minibatch Loss: 2.342242\n",
            "Step 10346: Minibatch Loss: 2.316718\n",
            "Step 10348: Minibatch Loss: 2.282788\n",
            "Step 10350: Minibatch Loss: 2.316616\n",
            "Step 10352: Minibatch Loss: 2.336533\n",
            "Step 10354: Minibatch Loss: 2.361217\n",
            "Step 10356: Minibatch Loss: 2.338830\n",
            "Step 10358: Minibatch Loss: 2.369144\n",
            "Step 10360: Minibatch Loss: 2.303095\n",
            "Step 10362: Minibatch Loss: 2.377186\n",
            "Step 10364: Minibatch Loss: 2.282244\n",
            "Step 10366: Minibatch Loss: 2.393065\n",
            "Step 10368: Minibatch Loss: 2.247765\n",
            "Step 10370: Minibatch Loss: 2.346648\n",
            "Step 10372: Minibatch Loss: 2.328751\n",
            "Step 10374: Minibatch Loss: 2.316142\n",
            "Step 10376: Minibatch Loss: 2.386936\n",
            "Step 10378: Minibatch Loss: 2.268265\n",
            "Step 10380: Minibatch Loss: 2.329536\n",
            "Step 10382: Minibatch Loss: 2.337439\n",
            "Step 10384: Minibatch Loss: 2.352830\n",
            "Step 10386: Minibatch Loss: 2.258398\n",
            "Step 10388: Minibatch Loss: 2.296465\n",
            "Step 10390: Minibatch Loss: 2.384281\n",
            "Step 10392: Minibatch Loss: 2.332028\n",
            "Step 10394: Minibatch Loss: 2.353051\n",
            "Step 10396: Minibatch Loss: 2.301435\n",
            "Step 10398: Minibatch Loss: 2.367299\n",
            "Step 10400: Minibatch Loss: 2.274539\n",
            "Step 10402: Minibatch Loss: 2.316179\n",
            "Step 10404: Minibatch Loss: 2.337492\n",
            "Step 10406: Minibatch Loss: 2.258904\n",
            "Step 10408: Minibatch Loss: 2.326165\n",
            "Step 10410: Minibatch Loss: 2.360113\n",
            "Step 10412: Minibatch Loss: 2.382093\n",
            "Step 10414: Minibatch Loss: 2.320602\n",
            "Step 10416: Minibatch Loss: 2.398074\n",
            "Step 10418: Minibatch Loss: 2.413268\n",
            "Step 10420: Minibatch Loss: 2.368797\n",
            "Step 10422: Minibatch Loss: 2.333862\n",
            "Step 10424: Minibatch Loss: 2.257592\n",
            "Step 10426: Minibatch Loss: 2.361429\n",
            "Step 10428: Minibatch Loss: 2.315588\n",
            "Step 10430: Minibatch Loss: 2.312329\n",
            "Step 10432: Minibatch Loss: 2.325361\n",
            "Step 10434: Minibatch Loss: 2.254560\n",
            "Step 10436: Minibatch Loss: 2.398442\n",
            "Step 10438: Minibatch Loss: 2.285205\n",
            "Step 10440: Minibatch Loss: 2.357410\n",
            "Step 10442: Minibatch Loss: 2.311896\n",
            "Step 10444: Minibatch Loss: 2.339286\n",
            "Step 10446: Minibatch Loss: 2.347205\n",
            "Step 10448: Minibatch Loss: 2.358541\n",
            "Step 10450: Minibatch Loss: 2.336287\n",
            "Step 10452: Minibatch Loss: 2.373802\n",
            "Step 10454: Minibatch Loss: 2.338591\n",
            "Step 10456: Minibatch Loss: 2.338858\n",
            "Step 10458: Minibatch Loss: 2.276175\n",
            "Step 10460: Minibatch Loss: 2.262291\n",
            "Step 10462: Minibatch Loss: 2.404910\n",
            "Step 10464: Minibatch Loss: 2.382523\n",
            "Step 10466: Minibatch Loss: 2.406176\n",
            "Step 10468: Minibatch Loss: 2.244655\n",
            "Step 10470: Minibatch Loss: 2.379193\n",
            "Step 10472: Minibatch Loss: 2.344467\n",
            "Step 10474: Minibatch Loss: 2.375628\n",
            "Step 10476: Minibatch Loss: 2.351216\n",
            "Step 10478: Minibatch Loss: 2.329215\n",
            "Step 10480: Minibatch Loss: 2.337955\n",
            "Step 10482: Minibatch Loss: 2.337516\n",
            "Step 10484: Minibatch Loss: 2.281689\n",
            "Step 10486: Minibatch Loss: 2.337320\n",
            "Step 10488: Minibatch Loss: 2.282177\n",
            "Step 10490: Minibatch Loss: 2.355341\n",
            "Step 10492: Minibatch Loss: 2.364126\n",
            "Step 10494: Minibatch Loss: 2.274857\n",
            "Step 10496: Minibatch Loss: 2.285855\n",
            "Step 10498: Minibatch Loss: 2.279946\n",
            "Step 10500: Minibatch Loss: 2.280049\n",
            "Step 10502: Minibatch Loss: 2.310594\n",
            "Step 10504: Minibatch Loss: 2.343035\n",
            "Step 10506: Minibatch Loss: 2.261863\n",
            "Step 10508: Minibatch Loss: 2.321169\n",
            "Step 10510: Minibatch Loss: 2.314379\n",
            "Step 10512: Minibatch Loss: 2.318520\n",
            "Step 10514: Minibatch Loss: 2.355844\n",
            "Step 10516: Minibatch Loss: 2.341950\n",
            "Step 10518: Minibatch Loss: 2.352595\n",
            "Step 10520: Minibatch Loss: 2.444779\n",
            "Step 10522: Minibatch Loss: 2.341170\n",
            "Step 10524: Minibatch Loss: 2.303142\n",
            "Step 10526: Minibatch Loss: 2.295319\n",
            "Step 10528: Minibatch Loss: 2.324259\n",
            "Step 10530: Minibatch Loss: 2.364918\n",
            "Step 10532: Minibatch Loss: 2.347234\n",
            "Step 10534: Minibatch Loss: 2.236751\n",
            "Step 10536: Minibatch Loss: 2.371561\n",
            "Step 10538: Minibatch Loss: 2.317376\n",
            "Step 10540: Minibatch Loss: 2.379967\n",
            "Step 10542: Minibatch Loss: 2.259197\n",
            "Step 10544: Minibatch Loss: 2.251055\n",
            "Step 10546: Minibatch Loss: 2.340858\n",
            "Step 10548: Minibatch Loss: 2.363859\n",
            "Step 10550: Minibatch Loss: 2.332947\n",
            "Step 10552: Minibatch Loss: 2.352483\n",
            "Step 10554: Minibatch Loss: 2.372606\n",
            "Step 10556: Minibatch Loss: 2.366132\n",
            "Step 10558: Minibatch Loss: 2.328320\n",
            "Step 10560: Minibatch Loss: 2.313945\n",
            "Step 10562: Minibatch Loss: 2.368920\n",
            "Step 10564: Minibatch Loss: 2.256531\n",
            "Step 10566: Minibatch Loss: 2.356513\n",
            "Step 10568: Minibatch Loss: 2.269670\n",
            "Step 10570: Minibatch Loss: 2.412218\n",
            "Step 10572: Minibatch Loss: 2.327858\n",
            "Step 10574: Minibatch Loss: 2.318431\n",
            "Step 10576: Minibatch Loss: 2.378911\n",
            "Step 10578: Minibatch Loss: 2.353965\n",
            "Step 10580: Minibatch Loss: 2.318931\n",
            "Step 10582: Minibatch Loss: 2.348786\n",
            "Step 10584: Minibatch Loss: 2.301797\n",
            "Step 10586: Minibatch Loss: 2.375903\n",
            "Step 10588: Minibatch Loss: 2.320634\n",
            "Step 10590: Minibatch Loss: 2.304378\n",
            "Step 10592: Minibatch Loss: 2.328064\n",
            "Step 10594: Minibatch Loss: 2.259524\n",
            "Step 10596: Minibatch Loss: 2.322826\n",
            "Step 10598: Minibatch Loss: 2.372988\n",
            "Step 10600: Minibatch Loss: 2.253415\n",
            "Step 10602: Minibatch Loss: 2.428765\n",
            "Step 10604: Minibatch Loss: 2.370323\n",
            "Step 10606: Minibatch Loss: 2.271291\n",
            "Step 10608: Minibatch Loss: 2.375093\n",
            "Step 10610: Minibatch Loss: 2.288750\n",
            "Step 10612: Minibatch Loss: 2.350277\n",
            "Step 10614: Minibatch Loss: 2.346141\n",
            "Step 10616: Minibatch Loss: 2.396996\n",
            "Step 10618: Minibatch Loss: 2.318158\n",
            "Step 10620: Minibatch Loss: 2.395597\n",
            "Step 10622: Minibatch Loss: 2.346284\n",
            "Step 10624: Minibatch Loss: 2.208441\n",
            "Step 10626: Minibatch Loss: 2.324058\n",
            "Step 10628: Minibatch Loss: 2.295167\n",
            "Step 10630: Minibatch Loss: 2.304318\n",
            "Step 10632: Minibatch Loss: 2.242623\n",
            "Step 10634: Minibatch Loss: 2.243543\n",
            "Step 10636: Minibatch Loss: 2.344174\n",
            "Step 10638: Minibatch Loss: 2.325589\n",
            "Step 10640: Minibatch Loss: 2.305833\n",
            "Step 10642: Minibatch Loss: 2.259568\n",
            "Step 10644: Minibatch Loss: 2.321842\n",
            "Step 10646: Minibatch Loss: 2.416959\n",
            "Step 10648: Minibatch Loss: 2.342197\n",
            "Step 10650: Minibatch Loss: 2.331220\n",
            "Step 10652: Minibatch Loss: 2.335075\n",
            "Step 10654: Minibatch Loss: 2.323972\n",
            "Step 10656: Minibatch Loss: 2.388316\n",
            "Step 10658: Minibatch Loss: 2.365858\n",
            "Step 10660: Minibatch Loss: 2.237137\n",
            "Step 10662: Minibatch Loss: 2.395827\n",
            "Step 10664: Minibatch Loss: 2.234399\n",
            "Step 10666: Minibatch Loss: 2.371150\n",
            "Step 10668: Minibatch Loss: 2.253479\n",
            "Step 10670: Minibatch Loss: 2.397220\n",
            "Step 10672: Minibatch Loss: 2.352823\n",
            "Step 10674: Minibatch Loss: 2.350306\n",
            "Step 10676: Minibatch Loss: 2.302215\n",
            "Step 10678: Minibatch Loss: 2.354318\n",
            "Step 10680: Minibatch Loss: 2.376808\n",
            "Step 10682: Minibatch Loss: 2.353559\n",
            "Step 10684: Minibatch Loss: 2.342961\n",
            "Step 10686: Minibatch Loss: 2.320132\n",
            "Step 10688: Minibatch Loss: 2.319358\n",
            "Step 10690: Minibatch Loss: 2.320516\n",
            "Step 10692: Minibatch Loss: 2.362827\n",
            "Step 10694: Minibatch Loss: 2.274386\n",
            "Step 10696: Minibatch Loss: 2.303361\n",
            "Step 10698: Minibatch Loss: 2.295682\n",
            "Step 10700: Minibatch Loss: 2.254677\n",
            "Step 10702: Minibatch Loss: 2.340258\n",
            "Step 10704: Minibatch Loss: 2.313110\n",
            "Step 10706: Minibatch Loss: 2.255363\n",
            "Step 10708: Minibatch Loss: 2.341517\n",
            "Step 10710: Minibatch Loss: 2.329979\n",
            "Step 10712: Minibatch Loss: 2.317363\n",
            "Step 10714: Minibatch Loss: 2.380177\n",
            "Step 10716: Minibatch Loss: 2.390488\n",
            "Step 10718: Minibatch Loss: 2.336860\n",
            "Step 10720: Minibatch Loss: 2.370543\n",
            "Step 10722: Minibatch Loss: 2.309914\n",
            "Step 10724: Minibatch Loss: 2.255355\n",
            "Step 10726: Minibatch Loss: 2.345264\n",
            "Step 10728: Minibatch Loss: 2.357022\n",
            "Step 10730: Minibatch Loss: 2.266950\n",
            "Step 10732: Minibatch Loss: 2.233463\n",
            "Step 10734: Minibatch Loss: 2.309275\n",
            "Step 10736: Minibatch Loss: 2.329003\n",
            "Step 10738: Minibatch Loss: 2.279662\n",
            "Step 10740: Minibatch Loss: 2.393909\n",
            "Step 10742: Minibatch Loss: 2.266133\n",
            "Step 10744: Minibatch Loss: 2.318662\n",
            "Step 10746: Minibatch Loss: 2.313112\n",
            "Step 10748: Minibatch Loss: 2.303517\n",
            "Step 10750: Minibatch Loss: 2.406309\n",
            "Step 10752: Minibatch Loss: 2.296427\n",
            "Step 10754: Minibatch Loss: 2.301941\n",
            "Step 10756: Minibatch Loss: 2.297071\n",
            "Step 10758: Minibatch Loss: 2.270570\n",
            "Step 10760: Minibatch Loss: 2.281543\n",
            "Step 10762: Minibatch Loss: 2.374281\n",
            "Step 10764: Minibatch Loss: 2.256298\n",
            "Step 10766: Minibatch Loss: 2.369150\n",
            "Step 10768: Minibatch Loss: 2.276574\n",
            "Step 10770: Minibatch Loss: 2.327624\n",
            "Step 10772: Minibatch Loss: 2.312018\n",
            "Step 10774: Minibatch Loss: 2.333425\n",
            "Step 10776: Minibatch Loss: 2.365247\n",
            "Step 10778: Minibatch Loss: 2.287043\n",
            "Step 10780: Minibatch Loss: 2.343695\n",
            "Step 10782: Minibatch Loss: 2.413842\n",
            "Step 10784: Minibatch Loss: 2.315452\n",
            "Step 10786: Minibatch Loss: 2.326091\n",
            "Step 10788: Minibatch Loss: 2.284411\n",
            "Step 10790: Minibatch Loss: 2.383535\n",
            "Step 10792: Minibatch Loss: 2.417673\n",
            "Step 10794: Minibatch Loss: 2.281582\n",
            "Step 10796: Minibatch Loss: 2.205839\n",
            "Step 10798: Minibatch Loss: 2.283753\n",
            "Step 10800: Minibatch Loss: 2.250049\n",
            "Step 10802: Minibatch Loss: 2.380995\n",
            "Step 10804: Minibatch Loss: 2.318524\n",
            "Step 10806: Minibatch Loss: 2.230208\n",
            "Step 10808: Minibatch Loss: 2.299161\n",
            "Step 10810: Minibatch Loss: 2.299305\n",
            "Step 10812: Minibatch Loss: 2.291355\n",
            "Step 10814: Minibatch Loss: 2.386096\n",
            "Step 10816: Minibatch Loss: 2.300328\n",
            "Step 10818: Minibatch Loss: 2.319273\n",
            "Step 10820: Minibatch Loss: 2.350222\n",
            "Step 10822: Minibatch Loss: 2.296252\n",
            "Step 10824: Minibatch Loss: 2.328281\n",
            "Step 10826: Minibatch Loss: 2.353899\n",
            "Step 10828: Minibatch Loss: 2.342468\n",
            "Step 10830: Minibatch Loss: 2.351633\n",
            "Step 10832: Minibatch Loss: 2.251091\n",
            "Step 10834: Minibatch Loss: 2.263185\n",
            "Step 10836: Minibatch Loss: 2.368051\n",
            "Step 10838: Minibatch Loss: 2.342641\n",
            "Step 10840: Minibatch Loss: 2.275791\n",
            "Step 10842: Minibatch Loss: 2.261514\n",
            "Step 10844: Minibatch Loss: 2.303113\n",
            "Step 10846: Minibatch Loss: 2.324242\n",
            "Step 10848: Minibatch Loss: 2.341178\n",
            "Step 10850: Minibatch Loss: 2.311435\n",
            "Step 10852: Minibatch Loss: 2.327947\n",
            "Step 10854: Minibatch Loss: 2.216879\n",
            "Step 10856: Minibatch Loss: 2.360558\n",
            "Step 10858: Minibatch Loss: 2.316153\n",
            "Step 10860: Minibatch Loss: 2.300280\n",
            "Step 10862: Minibatch Loss: 2.386503\n",
            "Step 10864: Minibatch Loss: 2.263101\n",
            "Step 10866: Minibatch Loss: 2.324088\n",
            "Step 10868: Minibatch Loss: 2.243321\n",
            "Step 10870: Minibatch Loss: 2.332584\n",
            "Step 10872: Minibatch Loss: 2.325571\n",
            "Step 10874: Minibatch Loss: 2.299758\n",
            "Step 10876: Minibatch Loss: 2.366305\n",
            "Step 10878: Minibatch Loss: 2.312400\n",
            "Step 10880: Minibatch Loss: 2.342374\n",
            "Step 10882: Minibatch Loss: 2.345486\n",
            "Step 10884: Minibatch Loss: 2.311818\n",
            "Step 10886: Minibatch Loss: 2.320412\n",
            "Step 10888: Minibatch Loss: 2.284742\n",
            "Step 10890: Minibatch Loss: 2.337967\n",
            "Step 10892: Minibatch Loss: 2.334626\n",
            "Step 10894: Minibatch Loss: 2.245827\n",
            "Step 10896: Minibatch Loss: 2.241052\n",
            "Step 10898: Minibatch Loss: 2.283873\n",
            "Step 10900: Minibatch Loss: 2.273376\n",
            "Step 10902: Minibatch Loss: 2.375072\n",
            "Step 10904: Minibatch Loss: 2.340111\n",
            "Step 10906: Minibatch Loss: 2.299812\n",
            "Step 10908: Minibatch Loss: 2.303427\n",
            "Step 10910: Minibatch Loss: 2.281030\n",
            "Step 10912: Minibatch Loss: 2.326499\n",
            "Step 10914: Minibatch Loss: 2.387198\n",
            "Step 10916: Minibatch Loss: 2.325966\n",
            "Step 10918: Minibatch Loss: 2.369406\n",
            "Step 10920: Minibatch Loss: 2.347564\n",
            "Step 10922: Minibatch Loss: 2.355725\n",
            "Step 10924: Minibatch Loss: 2.289673\n",
            "Step 10926: Minibatch Loss: 2.349941\n",
            "Step 10928: Minibatch Loss: 2.310601\n",
            "Step 10930: Minibatch Loss: 2.301305\n",
            "Step 10932: Minibatch Loss: 2.287019\n",
            "Step 10934: Minibatch Loss: 2.340833\n",
            "Step 10936: Minibatch Loss: 2.348790\n",
            "Step 10938: Minibatch Loss: 2.285876\n",
            "Step 10940: Minibatch Loss: 2.343838\n",
            "Step 10942: Minibatch Loss: 2.213697\n",
            "Step 10944: Minibatch Loss: 2.330226\n",
            "Step 10946: Minibatch Loss: 2.292425\n",
            "Step 10948: Minibatch Loss: 2.348392\n",
            "Step 10950: Minibatch Loss: 2.367775\n",
            "Step 10952: Minibatch Loss: 2.299931\n",
            "Step 10954: Minibatch Loss: 2.364936\n",
            "Step 10956: Minibatch Loss: 2.359391\n",
            "Step 10958: Minibatch Loss: 2.315221\n",
            "Step 10960: Minibatch Loss: 2.305841\n",
            "Step 10962: Minibatch Loss: 2.402205\n",
            "Step 10964: Minibatch Loss: 2.245082\n",
            "Step 10966: Minibatch Loss: 2.303349\n",
            "Step 10968: Minibatch Loss: 2.264430\n",
            "Step 10970: Minibatch Loss: 2.328692\n",
            "Step 10972: Minibatch Loss: 2.271373\n",
            "Step 10974: Minibatch Loss: 2.268375\n",
            "Step 10976: Minibatch Loss: 2.357561\n",
            "Step 10978: Minibatch Loss: 2.350812\n",
            "Step 10980: Minibatch Loss: 2.310878\n",
            "Step 10982: Minibatch Loss: 2.348296\n",
            "Step 10984: Minibatch Loss: 2.323603\n",
            "Step 10986: Minibatch Loss: 2.271740\n",
            "Step 10988: Minibatch Loss: 2.289582\n",
            "Step 10990: Minibatch Loss: 2.335391\n",
            "Step 10992: Minibatch Loss: 2.263108\n",
            "Step 10994: Minibatch Loss: 2.309068\n",
            "Step 10996: Minibatch Loss: 2.310233\n",
            "Step 10998: Minibatch Loss: 2.261904\n",
            "Step 11000: Minibatch Loss: 2.202285\n",
            "Training for SNR= 5.5  sigma= 0.44668359215096315 iteratin: 0\n",
            "Step 11002: Minibatch Loss: 2.321033\n",
            "Step 11004: Minibatch Loss: 2.390553\n",
            "Step 11006: Minibatch Loss: 2.335819\n",
            "Step 11008: Minibatch Loss: 2.361150\n",
            "Step 11010: Minibatch Loss: 2.284102\n",
            "Step 11012: Minibatch Loss: 2.325804\n",
            "Step 11014: Minibatch Loss: 2.407852\n",
            "Step 11016: Minibatch Loss: 2.341682\n",
            "Step 11018: Minibatch Loss: 2.338108\n",
            "Step 11020: Minibatch Loss: 2.358370\n",
            "Step 11022: Minibatch Loss: 2.319456\n",
            "Step 11024: Minibatch Loss: 2.278873\n",
            "Step 11026: Minibatch Loss: 2.335380\n",
            "Step 11028: Minibatch Loss: 2.323013\n",
            "Step 11030: Minibatch Loss: 2.360462\n",
            "Step 11032: Minibatch Loss: 2.265228\n",
            "Step 11034: Minibatch Loss: 2.301493\n",
            "Step 11036: Minibatch Loss: 2.302505\n",
            "Step 11038: Minibatch Loss: 2.327704\n",
            "Step 11040: Minibatch Loss: 2.276737\n",
            "Step 11042: Minibatch Loss: 2.208185\n",
            "Step 11044: Minibatch Loss: 2.289177\n",
            "Step 11046: Minibatch Loss: 2.318821\n",
            "Step 11048: Minibatch Loss: 2.311983\n",
            "Step 11050: Minibatch Loss: 2.334544\n",
            "Step 11052: Minibatch Loss: 2.300506\n",
            "Step 11054: Minibatch Loss: 2.379970\n",
            "Step 11056: Minibatch Loss: 2.389419\n",
            "Step 11058: Minibatch Loss: 2.319555\n",
            "Step 11060: Minibatch Loss: 2.274564\n",
            "Step 11062: Minibatch Loss: 2.363467\n",
            "Step 11064: Minibatch Loss: 2.328731\n",
            "Step 11066: Minibatch Loss: 2.380297\n",
            "Step 11068: Minibatch Loss: 2.251722\n",
            "Step 11070: Minibatch Loss: 2.329681\n",
            "Step 11072: Minibatch Loss: 2.232396\n",
            "Step 11074: Minibatch Loss: 2.259740\n",
            "Step 11076: Minibatch Loss: 2.331479\n",
            "Step 11078: Minibatch Loss: 2.222982\n",
            "Step 11080: Minibatch Loss: 2.254158\n",
            "Step 11082: Minibatch Loss: 2.381389\n",
            "Step 11084: Minibatch Loss: 2.314500\n",
            "Step 11086: Minibatch Loss: 2.280184\n",
            "Step 11088: Minibatch Loss: 2.276213\n",
            "Step 11090: Minibatch Loss: 2.372946\n",
            "Step 11092: Minibatch Loss: 2.328648\n",
            "Step 11094: Minibatch Loss: 2.255335\n",
            "Step 11096: Minibatch Loss: 2.222670\n",
            "Step 11098: Minibatch Loss: 2.308847\n",
            "Step 11100: Minibatch Loss: 2.224154\n",
            "Step 11102: Minibatch Loss: 2.324845\n",
            "Step 11104: Minibatch Loss: 2.318366\n",
            "Step 11106: Minibatch Loss: 2.240427\n",
            "Step 11108: Minibatch Loss: 2.307314\n",
            "Step 11110: Minibatch Loss: 2.349733\n",
            "Step 11112: Minibatch Loss: 2.304729\n",
            "Step 11114: Minibatch Loss: 2.337919\n",
            "Step 11116: Minibatch Loss: 2.276713\n",
            "Step 11118: Minibatch Loss: 2.390359\n",
            "Step 11120: Minibatch Loss: 2.406059\n",
            "Step 11122: Minibatch Loss: 2.375748\n",
            "Step 11124: Minibatch Loss: 2.308100\n",
            "Step 11126: Minibatch Loss: 2.408178\n",
            "Step 11128: Minibatch Loss: 2.313771\n",
            "Step 11130: Minibatch Loss: 2.288363\n",
            "Step 11132: Minibatch Loss: 2.282798\n",
            "Step 11134: Minibatch Loss: 2.263527\n",
            "Step 11136: Minibatch Loss: 2.349679\n",
            "Step 11138: Minibatch Loss: 2.291838\n",
            "Step 11140: Minibatch Loss: 2.329577\n",
            "Step 11142: Minibatch Loss: 2.212852\n",
            "Step 11144: Minibatch Loss: 2.333428\n",
            "Step 11146: Minibatch Loss: 2.330229\n",
            "Step 11148: Minibatch Loss: 2.298050\n",
            "Step 11150: Minibatch Loss: 2.302722\n",
            "Step 11152: Minibatch Loss: 2.285364\n",
            "Step 11154: Minibatch Loss: 2.293670\n",
            "Step 11156: Minibatch Loss: 2.358190\n",
            "Step 11158: Minibatch Loss: 2.306721\n",
            "Step 11160: Minibatch Loss: 2.233111\n",
            "Step 11162: Minibatch Loss: 2.461316\n",
            "Step 11164: Minibatch Loss: 2.319439\n",
            "Step 11166: Minibatch Loss: 2.291166\n",
            "Step 11168: Minibatch Loss: 2.258126\n",
            "Step 11170: Minibatch Loss: 2.295549\n",
            "Step 11172: Minibatch Loss: 2.335467\n",
            "Step 11174: Minibatch Loss: 2.354119\n",
            "Step 11176: Minibatch Loss: 2.308636\n",
            "Step 11178: Minibatch Loss: 2.304932\n",
            "Step 11180: Minibatch Loss: 2.288850\n",
            "Step 11182: Minibatch Loss: 2.344138\n",
            "Step 11184: Minibatch Loss: 2.328128\n",
            "Step 11186: Minibatch Loss: 2.326831\n",
            "Step 11188: Minibatch Loss: 2.316637\n",
            "Step 11190: Minibatch Loss: 2.295448\n",
            "Step 11192: Minibatch Loss: 2.315331\n",
            "Step 11194: Minibatch Loss: 2.271940\n",
            "Step 11196: Minibatch Loss: 2.286421\n",
            "Step 11198: Minibatch Loss: 2.284572\n",
            "Step 11200: Minibatch Loss: 2.276386\n",
            "Step 11202: Minibatch Loss: 2.320840\n",
            "Step 11204: Minibatch Loss: 2.289899\n",
            "Step 11206: Minibatch Loss: 2.310869\n",
            "Step 11208: Minibatch Loss: 2.324490\n",
            "Step 11210: Minibatch Loss: 2.271265\n",
            "Step 11212: Minibatch Loss: 2.326279\n",
            "Step 11214: Minibatch Loss: 2.333196\n",
            "Step 11216: Minibatch Loss: 2.334626\n",
            "Step 11218: Minibatch Loss: 2.324779\n",
            "Step 11220: Minibatch Loss: 2.301324\n",
            "Step 11222: Minibatch Loss: 2.318248\n",
            "Step 11224: Minibatch Loss: 2.235340\n",
            "Step 11226: Minibatch Loss: 2.340503\n",
            "Step 11228: Minibatch Loss: 2.291294\n",
            "Step 11230: Minibatch Loss: 2.288870\n",
            "Step 11232: Minibatch Loss: 2.250594\n",
            "Step 11234: Minibatch Loss: 2.289409\n",
            "Step 11236: Minibatch Loss: 2.332968\n",
            "Step 11238: Minibatch Loss: 2.266597\n",
            "Step 11240: Minibatch Loss: 2.265593\n",
            "Step 11242: Minibatch Loss: 2.221889\n",
            "Step 11244: Minibatch Loss: 2.294121\n",
            "Step 11246: Minibatch Loss: 2.327770\n",
            "Step 11248: Minibatch Loss: 2.208209\n",
            "Step 11250: Minibatch Loss: 2.292067\n",
            "Step 11252: Minibatch Loss: 2.269071\n",
            "Step 11254: Minibatch Loss: 2.267326\n",
            "Step 11256: Minibatch Loss: 2.349288\n",
            "Step 11258: Minibatch Loss: 2.332508\n",
            "Step 11260: Minibatch Loss: 2.184338\n",
            "Step 11262: Minibatch Loss: 2.334364\n",
            "Step 11264: Minibatch Loss: 2.287274\n",
            "Step 11266: Minibatch Loss: 2.376739\n",
            "Step 11268: Minibatch Loss: 2.284078\n",
            "Step 11270: Minibatch Loss: 2.331229\n",
            "Step 11272: Minibatch Loss: 2.279571\n",
            "Step 11274: Minibatch Loss: 2.400798\n",
            "Step 11276: Minibatch Loss: 2.337019\n",
            "Step 11278: Minibatch Loss: 2.330030\n",
            "Step 11280: Minibatch Loss: 2.276388\n",
            "Step 11282: Minibatch Loss: 2.329191\n",
            "Step 11284: Minibatch Loss: 2.304634\n",
            "Step 11286: Minibatch Loss: 2.288057\n",
            "Step 11288: Minibatch Loss: 2.222500\n",
            "Step 11290: Minibatch Loss: 2.316110\n",
            "Step 11292: Minibatch Loss: 2.316321\n",
            "Step 11294: Minibatch Loss: 2.265749\n",
            "Step 11296: Minibatch Loss: 2.249518\n",
            "Step 11298: Minibatch Loss: 2.295205\n",
            "Step 11300: Minibatch Loss: 2.228664\n",
            "Step 11302: Minibatch Loss: 2.322437\n",
            "Step 11304: Minibatch Loss: 2.283145\n",
            "Step 11306: Minibatch Loss: 2.231964\n",
            "Step 11308: Minibatch Loss: 2.325399\n",
            "Step 11310: Minibatch Loss: 2.251206\n",
            "Step 11312: Minibatch Loss: 2.401998\n",
            "Step 11314: Minibatch Loss: 2.404846\n",
            "Step 11316: Minibatch Loss: 2.381677\n",
            "Step 11318: Minibatch Loss: 2.297665\n",
            "Step 11320: Minibatch Loss: 2.374637\n",
            "Step 11322: Minibatch Loss: 2.281552\n",
            "Step 11324: Minibatch Loss: 2.302921\n",
            "Step 11326: Minibatch Loss: 2.337481\n",
            "Step 11328: Minibatch Loss: 2.331323\n",
            "Step 11330: Minibatch Loss: 2.344835\n",
            "Step 11332: Minibatch Loss: 2.285958\n",
            "Step 11334: Minibatch Loss: 2.263927\n",
            "Step 11336: Minibatch Loss: 2.358177\n",
            "Step 11338: Minibatch Loss: 2.281130\n",
            "Step 11340: Minibatch Loss: 2.339435\n",
            "Step 11342: Minibatch Loss: 2.227266\n",
            "Step 11344: Minibatch Loss: 2.301427\n",
            "Step 11346: Minibatch Loss: 2.291604\n",
            "Step 11348: Minibatch Loss: 2.248870\n",
            "Step 11350: Minibatch Loss: 2.287148\n",
            "Step 11352: Minibatch Loss: 2.251316\n",
            "Step 11354: Minibatch Loss: 2.339523\n",
            "Step 11356: Minibatch Loss: 2.367600\n",
            "Step 11358: Minibatch Loss: 2.303962\n",
            "Step 11360: Minibatch Loss: 2.266257\n",
            "Step 11362: Minibatch Loss: 2.375356\n",
            "Step 11364: Minibatch Loss: 2.238763\n",
            "Step 11366: Minibatch Loss: 2.375042\n",
            "Step 11368: Minibatch Loss: 2.243264\n",
            "Step 11370: Minibatch Loss: 2.361995\n",
            "Step 11372: Minibatch Loss: 2.265480\n",
            "Step 11374: Minibatch Loss: 2.280980\n",
            "Step 11376: Minibatch Loss: 2.269536\n",
            "Step 11378: Minibatch Loss: 2.255413\n",
            "Step 11380: Minibatch Loss: 2.327025\n",
            "Step 11382: Minibatch Loss: 2.374082\n",
            "Step 11384: Minibatch Loss: 2.218163\n",
            "Step 11386: Minibatch Loss: 2.293121\n",
            "Step 11388: Minibatch Loss: 2.319567\n",
            "Step 11390: Minibatch Loss: 2.366225\n",
            "Step 11392: Minibatch Loss: 2.329535\n",
            "Step 11394: Minibatch Loss: 2.259606\n",
            "Step 11396: Minibatch Loss: 2.230858\n",
            "Step 11398: Minibatch Loss: 2.224224\n",
            "Step 11400: Minibatch Loss: 2.181177\n",
            "Step 11402: Minibatch Loss: 2.299273\n",
            "Step 11404: Minibatch Loss: 2.352750\n",
            "Step 11406: Minibatch Loss: 2.235949\n",
            "Step 11408: Minibatch Loss: 2.275416\n",
            "Step 11410: Minibatch Loss: 2.231009\n",
            "Step 11412: Minibatch Loss: 2.323557\n",
            "Step 11414: Minibatch Loss: 2.370760\n",
            "Step 11416: Minibatch Loss: 2.367123\n",
            "Step 11418: Minibatch Loss: 2.331894\n",
            "Step 11420: Minibatch Loss: 2.290583\n",
            "Step 11422: Minibatch Loss: 2.292954\n",
            "Step 11424: Minibatch Loss: 2.308449\n",
            "Step 11426: Minibatch Loss: 2.305001\n",
            "Step 11428: Minibatch Loss: 2.298424\n",
            "Step 11430: Minibatch Loss: 2.321786\n",
            "Step 11432: Minibatch Loss: 2.247196\n",
            "Step 11434: Minibatch Loss: 2.256977\n",
            "Step 11436: Minibatch Loss: 2.352069\n",
            "Step 11438: Minibatch Loss: 2.243660\n",
            "Step 11440: Minibatch Loss: 2.312726\n",
            "Step 11442: Minibatch Loss: 2.200573\n",
            "Step 11444: Minibatch Loss: 2.286897\n",
            "Step 11446: Minibatch Loss: 2.304558\n",
            "Step 11448: Minibatch Loss: 2.321321\n",
            "Step 11450: Minibatch Loss: 2.305306\n",
            "Step 11452: Minibatch Loss: 2.296027\n",
            "Step 11454: Minibatch Loss: 2.265465\n",
            "Step 11456: Minibatch Loss: 2.327877\n",
            "Step 11458: Minibatch Loss: 2.275353\n",
            "Step 11460: Minibatch Loss: 2.214152\n",
            "Step 11462: Minibatch Loss: 2.410931\n",
            "Step 11464: Minibatch Loss: 2.320762\n",
            "Step 11466: Minibatch Loss: 2.333070\n",
            "Step 11468: Minibatch Loss: 2.231843\n",
            "Step 11470: Minibatch Loss: 2.310215\n",
            "Step 11472: Minibatch Loss: 2.314904\n",
            "Step 11474: Minibatch Loss: 2.312972\n",
            "Step 11476: Minibatch Loss: 2.278771\n",
            "Step 11478: Minibatch Loss: 2.270791\n",
            "Step 11480: Minibatch Loss: 2.400338\n",
            "Step 11482: Minibatch Loss: 2.352719\n",
            "Step 11484: Minibatch Loss: 2.375783\n",
            "Step 11486: Minibatch Loss: 2.245906\n",
            "Step 11488: Minibatch Loss: 2.309728\n",
            "Step 11490: Minibatch Loss: 2.308489\n",
            "Step 11492: Minibatch Loss: 2.360466\n",
            "Step 11494: Minibatch Loss: 2.291363\n",
            "Step 11496: Minibatch Loss: 2.285028\n",
            "Step 11498: Minibatch Loss: 2.249229\n",
            "Step 11500: Minibatch Loss: 2.246461\n",
            "Step 11502: Minibatch Loss: 2.293025\n",
            "Step 11504: Minibatch Loss: 2.300385\n",
            "Step 11506: Minibatch Loss: 2.241689\n",
            "Step 11508: Minibatch Loss: 2.241122\n",
            "Step 11510: Minibatch Loss: 2.316064\n",
            "Step 11512: Minibatch Loss: 2.358641\n",
            "Step 11514: Minibatch Loss: 2.284312\n",
            "Step 11516: Minibatch Loss: 2.320778\n",
            "Step 11518: Minibatch Loss: 2.295434\n",
            "Step 11520: Minibatch Loss: 2.350501\n",
            "Step 11522: Minibatch Loss: 2.334000\n",
            "Step 11524: Minibatch Loss: 2.264574\n",
            "Step 11526: Minibatch Loss: 2.325689\n",
            "Step 11528: Minibatch Loss: 2.320001\n",
            "Step 11530: Minibatch Loss: 2.333159\n",
            "Step 11532: Minibatch Loss: 2.237744\n",
            "Step 11534: Minibatch Loss: 2.226354\n",
            "Step 11536: Minibatch Loss: 2.304019\n",
            "Step 11538: Minibatch Loss: 2.203783\n",
            "Step 11540: Minibatch Loss: 2.232486\n",
            "Step 11542: Minibatch Loss: 2.254197\n",
            "Step 11544: Minibatch Loss: 2.263074\n",
            "Step 11546: Minibatch Loss: 2.389233\n",
            "Step 11548: Minibatch Loss: 2.277321\n",
            "Step 11550: Minibatch Loss: 2.337165\n",
            "Step 11552: Minibatch Loss: 2.265127\n",
            "Step 11554: Minibatch Loss: 2.269952\n",
            "Step 11556: Minibatch Loss: 2.313323\n",
            "Step 11558: Minibatch Loss: 2.274733\n",
            "Step 11560: Minibatch Loss: 2.282115\n",
            "Step 11562: Minibatch Loss: 2.294601\n",
            "Step 11564: Minibatch Loss: 2.271199\n",
            "Step 11566: Minibatch Loss: 2.336760\n",
            "Step 11568: Minibatch Loss: 2.204797\n",
            "Step 11570: Minibatch Loss: 2.337729\n",
            "Step 11572: Minibatch Loss: 2.236676\n",
            "Step 11574: Minibatch Loss: 2.265272\n",
            "Step 11576: Minibatch Loss: 2.315788\n",
            "Step 11578: Minibatch Loss: 2.256602\n",
            "Step 11580: Minibatch Loss: 2.251326\n",
            "Step 11582: Minibatch Loss: 2.298690\n",
            "Step 11584: Minibatch Loss: 2.324378\n",
            "Step 11586: Minibatch Loss: 2.263691\n",
            "Step 11588: Minibatch Loss: 2.359903\n",
            "Step 11590: Minibatch Loss: 2.353381\n",
            "Step 11592: Minibatch Loss: 2.344731\n",
            "Step 11594: Minibatch Loss: 2.292367\n",
            "Step 11596: Minibatch Loss: 2.286972\n",
            "Step 11598: Minibatch Loss: 2.324691\n",
            "Step 11600: Minibatch Loss: 2.231828\n",
            "Step 11602: Minibatch Loss: 2.340752\n",
            "Step 11604: Minibatch Loss: 2.273033\n",
            "Step 11606: Minibatch Loss: 2.215854\n",
            "Step 11608: Minibatch Loss: 2.292686\n",
            "Step 11610: Minibatch Loss: 2.219994\n",
            "Step 11612: Minibatch Loss: 2.302691\n",
            "Step 11614: Minibatch Loss: 2.316734\n",
            "Step 11616: Minibatch Loss: 2.349861\n",
            "Step 11618: Minibatch Loss: 2.287008\n",
            "Step 11620: Minibatch Loss: 2.336335\n",
            "Step 11622: Minibatch Loss: 2.235932\n",
            "Step 11624: Minibatch Loss: 2.222949\n",
            "Step 11626: Minibatch Loss: 2.350046\n",
            "Step 11628: Minibatch Loss: 2.312798\n",
            "Step 11630: Minibatch Loss: 2.340736\n",
            "Step 11632: Minibatch Loss: 2.267609\n",
            "Step 11634: Minibatch Loss: 2.256217\n",
            "Step 11636: Minibatch Loss: 2.363093\n",
            "Step 11638: Minibatch Loss: 2.270109\n",
            "Step 11640: Minibatch Loss: 2.267840\n",
            "Step 11642: Minibatch Loss: 2.283039\n",
            "Step 11644: Minibatch Loss: 2.272321\n",
            "Step 11646: Minibatch Loss: 2.267432\n",
            "Step 11648: Minibatch Loss: 2.326011\n",
            "Step 11650: Minibatch Loss: 2.288565\n",
            "Step 11652: Minibatch Loss: 2.283293\n",
            "Step 11654: Minibatch Loss: 2.248812\n",
            "Step 11656: Minibatch Loss: 2.356707\n",
            "Step 11658: Minibatch Loss: 2.302117\n",
            "Step 11660: Minibatch Loss: 2.201444\n",
            "Step 11662: Minibatch Loss: 2.327284\n",
            "Step 11664: Minibatch Loss: 2.242854\n",
            "Step 11666: Minibatch Loss: 2.312030\n",
            "Step 11668: Minibatch Loss: 2.223740\n",
            "Step 11670: Minibatch Loss: 2.286000\n",
            "Step 11672: Minibatch Loss: 2.270130\n",
            "Step 11674: Minibatch Loss: 2.355494\n",
            "Step 11676: Minibatch Loss: 2.320682\n",
            "Step 11678: Minibatch Loss: 2.328090\n",
            "Step 11680: Minibatch Loss: 2.344674\n",
            "Step 11682: Minibatch Loss: 2.284196\n",
            "Step 11684: Minibatch Loss: 2.280578\n",
            "Step 11686: Minibatch Loss: 2.261269\n",
            "Step 11688: Minibatch Loss: 2.310753\n",
            "Step 11690: Minibatch Loss: 2.285246\n",
            "Step 11692: Minibatch Loss: 2.319868\n",
            "Step 11694: Minibatch Loss: 2.250706\n",
            "Step 11696: Minibatch Loss: 2.210814\n",
            "Step 11698: Minibatch Loss: 2.230714\n",
            "Step 11700: Minibatch Loss: 2.201283\n",
            "Step 11702: Minibatch Loss: 2.307113\n",
            "Step 11704: Minibatch Loss: 2.328400\n",
            "Step 11706: Minibatch Loss: 2.198162\n",
            "Step 11708: Minibatch Loss: 2.302894\n",
            "Step 11710: Minibatch Loss: 2.249983\n",
            "Step 11712: Minibatch Loss: 2.321622\n",
            "Step 11714: Minibatch Loss: 2.322458\n",
            "Step 11716: Minibatch Loss: 2.249499\n",
            "Step 11718: Minibatch Loss: 2.338388\n",
            "Step 11720: Minibatch Loss: 2.343765\n",
            "Step 11722: Minibatch Loss: 2.335297\n",
            "Step 11724: Minibatch Loss: 2.248839\n",
            "Step 11726: Minibatch Loss: 2.309577\n",
            "Step 11728: Minibatch Loss: 2.312015\n",
            "Step 11730: Minibatch Loss: 2.309865\n",
            "Step 11732: Minibatch Loss: 2.242323\n",
            "Step 11734: Minibatch Loss: 2.207731\n",
            "Step 11736: Minibatch Loss: 2.351713\n",
            "Step 11738: Minibatch Loss: 2.235516\n",
            "Step 11740: Minibatch Loss: 2.294813\n",
            "Step 11742: Minibatch Loss: 2.202809\n",
            "Step 11744: Minibatch Loss: 2.319751\n",
            "Step 11746: Minibatch Loss: 2.284031\n",
            "Step 11748: Minibatch Loss: 2.265412\n",
            "Step 11750: Minibatch Loss: 2.276670\n",
            "Step 11752: Minibatch Loss: 2.240530\n",
            "Step 11754: Minibatch Loss: 2.299645\n",
            "Step 11756: Minibatch Loss: 2.288820\n",
            "Step 11758: Minibatch Loss: 2.274294\n",
            "Step 11760: Minibatch Loss: 2.265054\n",
            "Step 11762: Minibatch Loss: 2.320963\n",
            "Step 11764: Minibatch Loss: 2.251889\n",
            "Step 11766: Minibatch Loss: 2.306415\n",
            "Step 11768: Minibatch Loss: 2.233584\n",
            "Step 11770: Minibatch Loss: 2.386062\n",
            "Step 11772: Minibatch Loss: 2.294343\n",
            "Step 11774: Minibatch Loss: 2.261573\n",
            "Step 11776: Minibatch Loss: 2.236996\n",
            "Step 11778: Minibatch Loss: 2.294676\n",
            "Step 11780: Minibatch Loss: 2.309212\n",
            "Step 11782: Minibatch Loss: 2.332620\n",
            "Step 11784: Minibatch Loss: 2.328458\n",
            "Step 11786: Minibatch Loss: 2.306326\n",
            "Step 11788: Minibatch Loss: 2.280692\n",
            "Step 11790: Minibatch Loss: 2.262301\n",
            "Step 11792: Minibatch Loss: 2.306338\n",
            "Step 11794: Minibatch Loss: 2.314959\n",
            "Step 11796: Minibatch Loss: 2.245515\n",
            "Step 11798: Minibatch Loss: 2.312752\n",
            "Step 11800: Minibatch Loss: 2.216018\n",
            "Step 11802: Minibatch Loss: 2.319240\n",
            "Step 11804: Minibatch Loss: 2.279528\n",
            "Step 11806: Minibatch Loss: 2.212603\n",
            "Step 11808: Minibatch Loss: 2.303548\n",
            "Step 11810: Minibatch Loss: 2.219613\n",
            "Step 11812: Minibatch Loss: 2.254661\n",
            "Step 11814: Minibatch Loss: 2.365346\n",
            "Step 11816: Minibatch Loss: 2.325803\n",
            "Step 11818: Minibatch Loss: 2.360039\n",
            "Step 11820: Minibatch Loss: 2.324373\n",
            "Step 11822: Minibatch Loss: 2.291937\n",
            "Step 11824: Minibatch Loss: 2.303376\n",
            "Step 11826: Minibatch Loss: 2.301423\n",
            "Step 11828: Minibatch Loss: 2.269693\n",
            "Step 11830: Minibatch Loss: 2.301254\n",
            "Step 11832: Minibatch Loss: 2.184287\n",
            "Step 11834: Minibatch Loss: 2.258250\n",
            "Step 11836: Minibatch Loss: 2.337768\n",
            "Step 11838: Minibatch Loss: 2.271965\n",
            "Step 11840: Minibatch Loss: 2.281801\n",
            "Step 11842: Minibatch Loss: 2.214028\n",
            "Step 11844: Minibatch Loss: 2.226626\n",
            "Step 11846: Minibatch Loss: 2.255853\n",
            "Step 11848: Minibatch Loss: 2.250299\n",
            "Step 11850: Minibatch Loss: 2.266224\n",
            "Step 11852: Minibatch Loss: 2.278405\n",
            "Step 11854: Minibatch Loss: 2.243315\n",
            "Step 11856: Minibatch Loss: 2.288977\n",
            "Step 11858: Minibatch Loss: 2.271551\n",
            "Step 11860: Minibatch Loss: 2.271410\n",
            "Step 11862: Minibatch Loss: 2.319196\n",
            "Step 11864: Minibatch Loss: 2.235895\n",
            "Step 11866: Minibatch Loss: 2.300465\n",
            "Step 11868: Minibatch Loss: 2.216293\n",
            "Step 11870: Minibatch Loss: 2.314532\n",
            "Step 11872: Minibatch Loss: 2.265472\n",
            "Step 11874: Minibatch Loss: 2.337071\n",
            "Step 11876: Minibatch Loss: 2.249346\n",
            "Step 11878: Minibatch Loss: 2.272167\n",
            "Step 11880: Minibatch Loss: 2.302570\n",
            "Step 11882: Minibatch Loss: 2.315985\n",
            "Step 11884: Minibatch Loss: 2.256696\n",
            "Step 11886: Minibatch Loss: 2.299318\n",
            "Step 11888: Minibatch Loss: 2.211513\n",
            "Step 11890: Minibatch Loss: 2.284250\n",
            "Step 11892: Minibatch Loss: 2.342799\n",
            "Step 11894: Minibatch Loss: 2.209198\n",
            "Step 11896: Minibatch Loss: 2.216961\n",
            "Step 11898: Minibatch Loss: 2.280962\n",
            "Step 11900: Minibatch Loss: 2.275329\n",
            "Step 11902: Minibatch Loss: 2.286105\n",
            "Step 11904: Minibatch Loss: 2.306819\n",
            "Step 11906: Minibatch Loss: 2.199177\n",
            "Step 11908: Minibatch Loss: 2.273009\n",
            "Step 11910: Minibatch Loss: 2.241997\n",
            "Step 11912: Minibatch Loss: 2.293762\n",
            "Step 11914: Minibatch Loss: 2.380715\n",
            "Step 11916: Minibatch Loss: 2.331833\n",
            "Step 11918: Minibatch Loss: 2.329268\n",
            "Step 11920: Minibatch Loss: 2.371285\n",
            "Step 11922: Minibatch Loss: 2.241659\n",
            "Step 11924: Minibatch Loss: 2.246601\n",
            "Step 11926: Minibatch Loss: 2.291205\n",
            "Step 11928: Minibatch Loss: 2.295550\n",
            "Step 11930: Minibatch Loss: 2.230457\n",
            "Step 11932: Minibatch Loss: 2.232792\n",
            "Step 11934: Minibatch Loss: 2.267013\n",
            "Step 11936: Minibatch Loss: 2.306343\n",
            "Step 11938: Minibatch Loss: 2.277191\n",
            "Step 11940: Minibatch Loss: 2.318352\n",
            "Step 11942: Minibatch Loss: 2.243697\n",
            "Step 11944: Minibatch Loss: 2.254671\n",
            "Step 11946: Minibatch Loss: 2.292949\n",
            "Step 11948: Minibatch Loss: 2.280402\n",
            "Step 11950: Minibatch Loss: 2.223413\n",
            "Step 11952: Minibatch Loss: 2.254596\n",
            "Step 11954: Minibatch Loss: 2.315763\n",
            "Step 11956: Minibatch Loss: 2.296217\n",
            "Step 11958: Minibatch Loss: 2.242694\n",
            "Step 11960: Minibatch Loss: 2.236198\n",
            "Step 11962: Minibatch Loss: 2.344679\n",
            "Step 11964: Minibatch Loss: 2.224400\n",
            "Step 11966: Minibatch Loss: 2.388569\n",
            "Step 11968: Minibatch Loss: 2.195599\n",
            "Step 11970: Minibatch Loss: 2.286790\n",
            "Step 11972: Minibatch Loss: 2.333257\n",
            "Step 11974: Minibatch Loss: 2.307906\n",
            "Step 11976: Minibatch Loss: 2.330812\n",
            "Step 11978: Minibatch Loss: 2.219135\n",
            "Step 11980: Minibatch Loss: 2.267549\n",
            "Step 11982: Minibatch Loss: 2.272586\n",
            "Step 11984: Minibatch Loss: 2.338897\n",
            "Step 11986: Minibatch Loss: 2.259396\n",
            "Step 11988: Minibatch Loss: 2.221916\n",
            "Step 11990: Minibatch Loss: 2.294415\n",
            "Step 11992: Minibatch Loss: 2.273813\n",
            "Step 11994: Minibatch Loss: 2.228524\n",
            "Step 11996: Minibatch Loss: 2.190732\n",
            "Step 11998: Minibatch Loss: 2.242238\n",
            "Step 12000: Minibatch Loss: 2.263735\n",
            "Training for SNR= 6.0  sigma= 0.44668359215096315 iteratin: 0\n",
            "Step 12002: Minibatch Loss: 2.262317\n",
            "Step 12004: Minibatch Loss: 2.305424\n",
            "Step 12006: Minibatch Loss: 2.259801\n",
            "Step 12008: Minibatch Loss: 2.298047\n",
            "Step 12010: Minibatch Loss: 2.207640\n",
            "Step 12012: Minibatch Loss: 2.274566\n",
            "Step 12014: Minibatch Loss: 2.278575\n",
            "Step 12016: Minibatch Loss: 2.264760\n",
            "Step 12018: Minibatch Loss: 2.294000\n",
            "Step 12020: Minibatch Loss: 2.326168\n",
            "Step 12022: Minibatch Loss: 2.281201\n",
            "Step 12024: Minibatch Loss: 2.319691\n",
            "Step 12026: Minibatch Loss: 2.330961\n",
            "Step 12028: Minibatch Loss: 2.233794\n",
            "Step 12030: Minibatch Loss: 2.281795\n",
            "Step 12032: Minibatch Loss: 2.231405\n",
            "Step 12034: Minibatch Loss: 2.255218\n",
            "Step 12036: Minibatch Loss: 2.295680\n",
            "Step 12038: Minibatch Loss: 2.208409\n",
            "Step 12040: Minibatch Loss: 2.309233\n",
            "Step 12042: Minibatch Loss: 2.206442\n",
            "Step 12044: Minibatch Loss: 2.268257\n",
            "Step 12046: Minibatch Loss: 2.352342\n",
            "Step 12048: Minibatch Loss: 2.282391\n",
            "Step 12050: Minibatch Loss: 2.271353\n",
            "Step 12052: Minibatch Loss: 2.157006\n",
            "Step 12054: Minibatch Loss: 2.298310\n",
            "Step 12056: Minibatch Loss: 2.281421\n",
            "Step 12058: Minibatch Loss: 2.261937\n",
            "Step 12060: Minibatch Loss: 2.220849\n",
            "Step 12062: Minibatch Loss: 2.290548\n",
            "Step 12064: Minibatch Loss: 2.257734\n",
            "Step 12066: Minibatch Loss: 2.322604\n",
            "Step 12068: Minibatch Loss: 2.200864\n",
            "Step 12070: Minibatch Loss: 2.294515\n",
            "Step 12072: Minibatch Loss: 2.291607\n",
            "Step 12074: Minibatch Loss: 2.246455\n",
            "Step 12076: Minibatch Loss: 2.293070\n",
            "Step 12078: Minibatch Loss: 2.272130\n",
            "Step 12080: Minibatch Loss: 2.306734\n",
            "Step 12082: Minibatch Loss: 2.349813\n",
            "Step 12084: Minibatch Loss: 2.269928\n",
            "Step 12086: Minibatch Loss: 2.284860\n",
            "Step 12088: Minibatch Loss: 2.251733\n",
            "Step 12090: Minibatch Loss: 2.220479\n",
            "Step 12092: Minibatch Loss: 2.262583\n",
            "Step 12094: Minibatch Loss: 2.270370\n",
            "Step 12096: Minibatch Loss: 2.228468\n",
            "Step 12098: Minibatch Loss: 2.271396\n",
            "Step 12100: Minibatch Loss: 2.203850\n",
            "Step 12102: Minibatch Loss: 2.264911\n",
            "Step 12104: Minibatch Loss: 2.271942\n",
            "Step 12106: Minibatch Loss: 2.214575\n",
            "Step 12108: Minibatch Loss: 2.347574\n",
            "Step 12110: Minibatch Loss: 2.292045\n",
            "Step 12112: Minibatch Loss: 2.227067\n",
            "Step 12114: Minibatch Loss: 2.379250\n",
            "Step 12116: Minibatch Loss: 2.299527\n",
            "Step 12118: Minibatch Loss: 2.281073\n",
            "Step 12120: Minibatch Loss: 2.287801\n",
            "Step 12122: Minibatch Loss: 2.309854\n",
            "Step 12124: Minibatch Loss: 2.221991\n",
            "Step 12126: Minibatch Loss: 2.275173\n",
            "Step 12128: Minibatch Loss: 2.265035\n",
            "Step 12130: Minibatch Loss: 2.261685\n",
            "Step 12132: Minibatch Loss: 2.200747\n",
            "Step 12134: Minibatch Loss: 2.198600\n",
            "Step 12136: Minibatch Loss: 2.366554\n",
            "Step 12138: Minibatch Loss: 2.242659\n",
            "Step 12140: Minibatch Loss: 2.289419\n",
            "Step 12142: Minibatch Loss: 2.286239\n",
            "Step 12144: Minibatch Loss: 2.324203\n",
            "Step 12146: Minibatch Loss: 2.304732\n",
            "Step 12148: Minibatch Loss: 2.245429\n",
            "Step 12150: Minibatch Loss: 2.281549\n",
            "Step 12152: Minibatch Loss: 2.270120\n",
            "Step 12154: Minibatch Loss: 2.243906\n",
            "Step 12156: Minibatch Loss: 2.345645\n",
            "Step 12158: Minibatch Loss: 2.317836\n",
            "Step 12160: Minibatch Loss: 2.264802\n",
            "Step 12162: Minibatch Loss: 2.288475\n",
            "Step 12164: Minibatch Loss: 2.184798\n",
            "Step 12166: Minibatch Loss: 2.259549\n",
            "Step 12168: Minibatch Loss: 2.249998\n",
            "Step 12170: Minibatch Loss: 2.339454\n",
            "Step 12172: Minibatch Loss: 2.339915\n",
            "Step 12174: Minibatch Loss: 2.268055\n",
            "Step 12176: Minibatch Loss: 2.234007\n",
            "Step 12178: Minibatch Loss: 2.248323\n",
            "Step 12180: Minibatch Loss: 2.310887\n",
            "Step 12182: Minibatch Loss: 2.301725\n",
            "Step 12184: Minibatch Loss: 2.225016\n",
            "Step 12186: Minibatch Loss: 2.293104\n",
            "Step 12188: Minibatch Loss: 2.308915\n",
            "Step 12190: Minibatch Loss: 2.300330\n",
            "Step 12192: Minibatch Loss: 2.249342\n",
            "Step 12194: Minibatch Loss: 2.215413\n",
            "Step 12196: Minibatch Loss: 2.197064\n",
            "Step 12198: Minibatch Loss: 2.300404\n",
            "Step 12200: Minibatch Loss: 2.236346\n",
            "Step 12202: Minibatch Loss: 2.272991\n",
            "Step 12204: Minibatch Loss: 2.254769\n",
            "Step 12206: Minibatch Loss: 2.212887\n",
            "Step 12208: Minibatch Loss: 2.219848\n",
            "Step 12210: Minibatch Loss: 2.215446\n",
            "Step 12212: Minibatch Loss: 2.297994\n",
            "Step 12214: Minibatch Loss: 2.278308\n",
            "Step 12216: Minibatch Loss: 2.247199\n",
            "Step 12218: Minibatch Loss: 2.338315\n",
            "Step 12220: Minibatch Loss: 2.314414\n",
            "Step 12222: Minibatch Loss: 2.294161\n",
            "Step 12224: Minibatch Loss: 2.258201\n",
            "Step 12226: Minibatch Loss: 2.325803\n",
            "Step 12228: Minibatch Loss: 2.241680\n",
            "Step 12230: Minibatch Loss: 2.281438\n",
            "Step 12232: Minibatch Loss: 2.199816\n",
            "Step 12234: Minibatch Loss: 2.217794\n",
            "Step 12236: Minibatch Loss: 2.265422\n",
            "Step 12238: Minibatch Loss: 2.233653\n",
            "Step 12240: Minibatch Loss: 2.255911\n",
            "Step 12242: Minibatch Loss: 2.192903\n",
            "Step 12244: Minibatch Loss: 2.288398\n",
            "Step 12246: Minibatch Loss: 2.295118\n",
            "Step 12248: Minibatch Loss: 2.239873\n",
            "Step 12250: Minibatch Loss: 2.277834\n",
            "Step 12252: Minibatch Loss: 2.268110\n",
            "Step 12254: Minibatch Loss: 2.281726\n",
            "Step 12256: Minibatch Loss: 2.278309\n",
            "Step 12258: Minibatch Loss: 2.313936\n",
            "Step 12260: Minibatch Loss: 2.228188\n",
            "Step 12262: Minibatch Loss: 2.292745\n",
            "Step 12264: Minibatch Loss: 2.196945\n",
            "Step 12266: Minibatch Loss: 2.341211\n",
            "Step 12268: Minibatch Loss: 2.233710\n",
            "Step 12270: Minibatch Loss: 2.288390\n",
            "Step 12272: Minibatch Loss: 2.255656\n",
            "Step 12274: Minibatch Loss: 2.257904\n",
            "Step 12276: Minibatch Loss: 2.309494\n",
            "Step 12278: Minibatch Loss: 2.236658\n",
            "Step 12280: Minibatch Loss: 2.247017\n",
            "Step 12282: Minibatch Loss: 2.308380\n",
            "Step 12284: Minibatch Loss: 2.230119\n",
            "Step 12286: Minibatch Loss: 2.253902\n",
            "Step 12288: Minibatch Loss: 2.280002\n",
            "Step 12290: Minibatch Loss: 2.287983\n",
            "Step 12292: Minibatch Loss: 2.292286\n",
            "Step 12294: Minibatch Loss: 2.206673\n",
            "Step 12296: Minibatch Loss: 2.234886\n",
            "Step 12298: Minibatch Loss: 2.302098\n",
            "Step 12300: Minibatch Loss: 2.209628\n",
            "Step 12302: Minibatch Loss: 2.240000\n",
            "Step 12304: Minibatch Loss: 2.258021\n",
            "Step 12306: Minibatch Loss: 2.241697\n",
            "Step 12308: Minibatch Loss: 2.226848\n",
            "Step 12310: Minibatch Loss: 2.197111\n",
            "Step 12312: Minibatch Loss: 2.291011\n",
            "Step 12314: Minibatch Loss: 2.304981\n",
            "Step 12316: Minibatch Loss: 2.251704\n",
            "Step 12318: Minibatch Loss: 2.367924\n",
            "Step 12320: Minibatch Loss: 2.326161\n",
            "Step 12322: Minibatch Loss: 2.293360\n",
            "Step 12324: Minibatch Loss: 2.310707\n",
            "Step 12326: Minibatch Loss: 2.274417\n",
            "Step 12328: Minibatch Loss: 2.255718\n",
            "Step 12330: Minibatch Loss: 2.215971\n",
            "Step 12332: Minibatch Loss: 2.241411\n",
            "Step 12334: Minibatch Loss: 2.219356\n",
            "Step 12336: Minibatch Loss: 2.365512\n",
            "Step 12338: Minibatch Loss: 2.231670\n",
            "Step 12340: Minibatch Loss: 2.206926\n",
            "Step 12342: Minibatch Loss: 2.152103\n",
            "Step 12344: Minibatch Loss: 2.243553\n",
            "Step 12346: Minibatch Loss: 2.272301\n",
            "Step 12348: Minibatch Loss: 2.253493\n",
            "Step 12350: Minibatch Loss: 2.259177\n",
            "Step 12352: Minibatch Loss: 2.251940\n",
            "Step 12354: Minibatch Loss: 2.288975\n",
            "Step 12356: Minibatch Loss: 2.285960\n",
            "Step 12358: Minibatch Loss: 2.241266\n",
            "Step 12360: Minibatch Loss: 2.190524\n",
            "Step 12362: Minibatch Loss: 2.330695\n",
            "Step 12364: Minibatch Loss: 2.180283\n",
            "Step 12366: Minibatch Loss: 2.393534\n",
            "Step 12368: Minibatch Loss: 2.220185\n",
            "Step 12370: Minibatch Loss: 2.290397\n",
            "Step 12372: Minibatch Loss: 2.279056\n",
            "Step 12374: Minibatch Loss: 2.274171\n",
            "Step 12376: Minibatch Loss: 2.298023\n",
            "Step 12378: Minibatch Loss: 2.241357\n",
            "Step 12380: Minibatch Loss: 2.257577\n",
            "Step 12382: Minibatch Loss: 2.262673\n",
            "Step 12384: Minibatch Loss: 2.276449\n",
            "Step 12386: Minibatch Loss: 2.196343\n",
            "Step 12388: Minibatch Loss: 2.199030\n",
            "Step 12390: Minibatch Loss: 2.218323\n",
            "Step 12392: Minibatch Loss: 2.284892\n",
            "Step 12394: Minibatch Loss: 2.218433\n",
            "Step 12396: Minibatch Loss: 2.225645\n",
            "Step 12398: Minibatch Loss: 2.346704\n",
            "Step 12400: Minibatch Loss: 2.244535\n",
            "Step 12402: Minibatch Loss: 2.294642\n",
            "Step 12404: Minibatch Loss: 2.254876\n",
            "Step 12406: Minibatch Loss: 2.227936\n",
            "Step 12408: Minibatch Loss: 2.267740\n",
            "Step 12410: Minibatch Loss: 2.268870\n",
            "Step 12412: Minibatch Loss: 2.274213\n",
            "Step 12414: Minibatch Loss: 2.280649\n",
            "Step 12416: Minibatch Loss: 2.273881\n",
            "Step 12418: Minibatch Loss: 2.262453\n",
            "Step 12420: Minibatch Loss: 2.266589\n",
            "Step 12422: Minibatch Loss: 2.294238\n",
            "Step 12424: Minibatch Loss: 2.188952\n",
            "Step 12426: Minibatch Loss: 2.343526\n",
            "Step 12428: Minibatch Loss: 2.276718\n",
            "Step 12430: Minibatch Loss: 2.247081\n",
            "Step 12432: Minibatch Loss: 2.252788\n",
            "Step 12434: Minibatch Loss: 2.252833\n",
            "Step 12436: Minibatch Loss: 2.323300\n",
            "Step 12438: Minibatch Loss: 2.205149\n",
            "Step 12440: Minibatch Loss: 2.266452\n",
            "Step 12442: Minibatch Loss: 2.181724\n",
            "Step 12444: Minibatch Loss: 2.303628\n",
            "Step 12446: Minibatch Loss: 2.249925\n",
            "Step 12448: Minibatch Loss: 2.297062\n",
            "Step 12450: Minibatch Loss: 2.225414\n",
            "Step 12452: Minibatch Loss: 2.264284\n",
            "Step 12454: Minibatch Loss: 2.295221\n",
            "Step 12456: Minibatch Loss: 2.261913\n",
            "Step 12458: Minibatch Loss: 2.279003\n",
            "Step 12460: Minibatch Loss: 2.202296\n",
            "Step 12462: Minibatch Loss: 2.332838\n",
            "Step 12464: Minibatch Loss: 2.214352\n",
            "Step 12466: Minibatch Loss: 2.257941\n",
            "Step 12468: Minibatch Loss: 2.180789\n",
            "Step 12470: Minibatch Loss: 2.312970\n",
            "Step 12472: Minibatch Loss: 2.288490\n",
            "Step 12474: Minibatch Loss: 2.233279\n",
            "Step 12476: Minibatch Loss: 2.225226\n",
            "Step 12478: Minibatch Loss: 2.318246\n",
            "Step 12480: Minibatch Loss: 2.275946\n",
            "Step 12482: Minibatch Loss: 2.284937\n",
            "Step 12484: Minibatch Loss: 2.281415\n",
            "Step 12486: Minibatch Loss: 2.222028\n",
            "Step 12488: Minibatch Loss: 2.275485\n",
            "Step 12490: Minibatch Loss: 2.269998\n",
            "Step 12492: Minibatch Loss: 2.272043\n",
            "Step 12494: Minibatch Loss: 2.215321\n",
            "Step 12496: Minibatch Loss: 2.244328\n",
            "Step 12498: Minibatch Loss: 2.308579\n",
            "Step 12500: Minibatch Loss: 2.195174\n",
            "Step 12502: Minibatch Loss: 2.315911\n",
            "Step 12504: Minibatch Loss: 2.281405\n",
            "Step 12506: Minibatch Loss: 2.227532\n",
            "Step 12508: Minibatch Loss: 2.217258\n",
            "Step 12510: Minibatch Loss: 2.229544\n",
            "Step 12512: Minibatch Loss: 2.268132\n",
            "Step 12514: Minibatch Loss: 2.266619\n",
            "Step 12516: Minibatch Loss: 2.281753\n",
            "Step 12518: Minibatch Loss: 2.296445\n",
            "Step 12520: Minibatch Loss: 2.280473\n",
            "Step 12522: Minibatch Loss: 2.282445\n",
            "Step 12524: Minibatch Loss: 2.221949\n",
            "Step 12526: Minibatch Loss: 2.300192\n",
            "Step 12528: Minibatch Loss: 2.245780\n",
            "Step 12530: Minibatch Loss: 2.246986\n",
            "Step 12532: Minibatch Loss: 2.268336\n",
            "Step 12534: Minibatch Loss: 2.208571\n",
            "Step 12536: Minibatch Loss: 2.277785\n",
            "Step 12538: Minibatch Loss: 2.204265\n",
            "Step 12540: Minibatch Loss: 2.297763\n",
            "Step 12542: Minibatch Loss: 2.255838\n",
            "Step 12544: Minibatch Loss: 2.245772\n",
            "Step 12546: Minibatch Loss: 2.293633\n",
            "Step 12548: Minibatch Loss: 2.282008\n",
            "Step 12550: Minibatch Loss: 2.244552\n",
            "Step 12552: Minibatch Loss: 2.222880\n",
            "Step 12554: Minibatch Loss: 2.284980\n",
            "Step 12556: Minibatch Loss: 2.317491\n",
            "Step 12558: Minibatch Loss: 2.283236\n",
            "Step 12560: Minibatch Loss: 2.190753\n",
            "Step 12562: Minibatch Loss: 2.314882\n",
            "Step 12564: Minibatch Loss: 2.201753\n",
            "Step 12566: Minibatch Loss: 2.257353\n",
            "Step 12568: Minibatch Loss: 2.161430\n",
            "Step 12570: Minibatch Loss: 2.309400\n",
            "Step 12572: Minibatch Loss: 2.225990\n",
            "Step 12574: Minibatch Loss: 2.272872\n",
            "Step 12576: Minibatch Loss: 2.249975\n",
            "Step 12578: Minibatch Loss: 2.284004\n",
            "Step 12580: Minibatch Loss: 2.198733\n",
            "Step 12582: Minibatch Loss: 2.289115\n",
            "Step 12584: Minibatch Loss: 2.259768\n",
            "Step 12586: Minibatch Loss: 2.273517\n",
            "Step 12588: Minibatch Loss: 2.255777\n",
            "Step 12590: Minibatch Loss: 2.297629\n",
            "Step 12592: Minibatch Loss: 2.255240\n",
            "Step 12594: Minibatch Loss: 2.264087\n",
            "Step 12596: Minibatch Loss: 2.162875\n",
            "Step 12598: Minibatch Loss: 2.269144\n",
            "Step 12600: Minibatch Loss: 2.233309\n",
            "Step 12602: Minibatch Loss: 2.256772\n",
            "Step 12604: Minibatch Loss: 2.280607\n",
            "Step 12606: Minibatch Loss: 2.235963\n",
            "Step 12608: Minibatch Loss: 2.289331\n",
            "Step 12610: Minibatch Loss: 2.239387\n",
            "Step 12612: Minibatch Loss: 2.273040\n",
            "Step 12614: Minibatch Loss: 2.225790\n",
            "Step 12616: Minibatch Loss: 2.267424\n",
            "Step 12618: Minibatch Loss: 2.262729\n",
            "Step 12620: Minibatch Loss: 2.265748\n",
            "Step 12622: Minibatch Loss: 2.227529\n",
            "Step 12624: Minibatch Loss: 2.208302\n",
            "Step 12626: Minibatch Loss: 2.273787\n",
            "Step 12628: Minibatch Loss: 2.225994\n",
            "Step 12630: Minibatch Loss: 2.223320\n",
            "Step 12632: Minibatch Loss: 2.248796\n",
            "Step 12634: Minibatch Loss: 2.238543\n",
            "Step 12636: Minibatch Loss: 2.289942\n",
            "Step 12638: Minibatch Loss: 2.222751\n",
            "Step 12640: Minibatch Loss: 2.230876\n",
            "Step 12642: Minibatch Loss: 2.205645\n",
            "Step 12644: Minibatch Loss: 2.249523\n",
            "Step 12646: Minibatch Loss: 2.322224\n",
            "Step 12648: Minibatch Loss: 2.191485\n",
            "Step 12650: Minibatch Loss: 2.275084\n",
            "Step 12652: Minibatch Loss: 2.203783\n",
            "Step 12654: Minibatch Loss: 2.243689\n",
            "Step 12656: Minibatch Loss: 2.215978\n",
            "Step 12658: Minibatch Loss: 2.192241\n",
            "Step 12660: Minibatch Loss: 2.238070\n",
            "Step 12662: Minibatch Loss: 2.297947\n",
            "Step 12664: Minibatch Loss: 2.209389\n",
            "Step 12666: Minibatch Loss: 2.270591\n",
            "Step 12668: Minibatch Loss: 2.180054\n",
            "Step 12670: Minibatch Loss: 2.271150\n",
            "Step 12672: Minibatch Loss: 2.273083\n",
            "Step 12674: Minibatch Loss: 2.247978\n",
            "Step 12676: Minibatch Loss: 2.211320\n",
            "Step 12678: Minibatch Loss: 2.243536\n",
            "Step 12680: Minibatch Loss: 2.256341\n",
            "Step 12682: Minibatch Loss: 2.262781\n",
            "Step 12684: Minibatch Loss: 2.211560\n",
            "Step 12686: Minibatch Loss: 2.222294\n",
            "Step 12688: Minibatch Loss: 2.247702\n",
            "Step 12690: Minibatch Loss: 2.282524\n",
            "Step 12692: Minibatch Loss: 2.219453\n",
            "Step 12694: Minibatch Loss: 2.181311\n",
            "Step 12696: Minibatch Loss: 2.209383\n",
            "Step 12698: Minibatch Loss: 2.254235\n",
            "Step 12700: Minibatch Loss: 2.200301\n",
            "Step 12702: Minibatch Loss: 2.298464\n",
            "Step 12704: Minibatch Loss: 2.203987\n",
            "Step 12706: Minibatch Loss: 2.187268\n",
            "Step 12708: Minibatch Loss: 2.241064\n",
            "Step 12710: Minibatch Loss: 2.195941\n",
            "Step 12712: Minibatch Loss: 2.224908\n",
            "Step 12714: Minibatch Loss: 2.269782\n",
            "Step 12716: Minibatch Loss: 2.242809\n",
            "Step 12718: Minibatch Loss: 2.249340\n",
            "Step 12720: Minibatch Loss: 2.260669\n",
            "Step 12722: Minibatch Loss: 2.257649\n",
            "Step 12724: Minibatch Loss: 2.190248\n",
            "Step 12726: Minibatch Loss: 2.320210\n",
            "Step 12728: Minibatch Loss: 2.222385\n",
            "Step 12730: Minibatch Loss: 2.301071\n",
            "Step 12732: Minibatch Loss: 2.212968\n",
            "Step 12734: Minibatch Loss: 2.183451\n",
            "Step 12736: Minibatch Loss: 2.279526\n",
            "Step 12738: Minibatch Loss: 2.180700\n",
            "Step 12740: Minibatch Loss: 2.242295\n",
            "Step 12742: Minibatch Loss: 2.212583\n",
            "Step 12744: Minibatch Loss: 2.268685\n",
            "Step 12746: Minibatch Loss: 2.241299\n",
            "Step 12748: Minibatch Loss: 2.243656\n",
            "Step 12750: Minibatch Loss: 2.249895\n",
            "Step 12752: Minibatch Loss: 2.222856\n",
            "Step 12754: Minibatch Loss: 2.278650\n",
            "Step 12756: Minibatch Loss: 2.292218\n",
            "Step 12758: Minibatch Loss: 2.260980\n",
            "Step 12760: Minibatch Loss: 2.230529\n",
            "Step 12762: Minibatch Loss: 2.304057\n",
            "Step 12764: Minibatch Loss: 2.182551\n",
            "Step 12766: Minibatch Loss: 2.256978\n",
            "Step 12768: Minibatch Loss: 2.191525\n",
            "Step 12770: Minibatch Loss: 2.318991\n",
            "Step 12772: Minibatch Loss: 2.217193\n",
            "Step 12774: Minibatch Loss: 2.285159\n",
            "Step 12776: Minibatch Loss: 2.269610\n",
            "Step 12778: Minibatch Loss: 2.182522\n",
            "Step 12780: Minibatch Loss: 2.260117\n",
            "Step 12782: Minibatch Loss: 2.270095\n",
            "Step 12784: Minibatch Loss: 2.261053\n",
            "Step 12786: Minibatch Loss: 2.247905\n",
            "Step 12788: Minibatch Loss: 2.237870\n",
            "Step 12790: Minibatch Loss: 2.227721\n",
            "Step 12792: Minibatch Loss: 2.270814\n",
            "Step 12794: Minibatch Loss: 2.171601\n",
            "Step 12796: Minibatch Loss: 2.177415\n",
            "Step 12798: Minibatch Loss: 2.218220\n",
            "Step 12800: Minibatch Loss: 2.205255\n",
            "Step 12802: Minibatch Loss: 2.301395\n",
            "Step 12804: Minibatch Loss: 2.244956\n",
            "Step 12806: Minibatch Loss: 2.216039\n",
            "Step 12808: Minibatch Loss: 2.303699\n",
            "Step 12810: Minibatch Loss: 2.267755\n",
            "Step 12812: Minibatch Loss: 2.283210\n",
            "Step 12814: Minibatch Loss: 2.315462\n",
            "Step 12816: Minibatch Loss: 2.315519\n",
            "Step 12818: Minibatch Loss: 2.233503\n",
            "Step 12820: Minibatch Loss: 2.320855\n",
            "Step 12822: Minibatch Loss: 2.271379\n",
            "Step 12824: Minibatch Loss: 2.222296\n",
            "Step 12826: Minibatch Loss: 2.314577\n",
            "Step 12828: Minibatch Loss: 2.237568\n",
            "Step 12830: Minibatch Loss: 2.254950\n",
            "Step 12832: Minibatch Loss: 2.198390\n",
            "Step 12834: Minibatch Loss: 2.252537\n",
            "Step 12836: Minibatch Loss: 2.251260\n",
            "Step 12838: Minibatch Loss: 2.191379\n",
            "Step 12840: Minibatch Loss: 2.232096\n",
            "Step 12842: Minibatch Loss: 2.199029\n",
            "Step 12844: Minibatch Loss: 2.267008\n",
            "Step 12846: Minibatch Loss: 2.191234\n",
            "Step 12848: Minibatch Loss: 2.246220\n",
            "Step 12850: Minibatch Loss: 2.220164\n",
            "Step 12852: Minibatch Loss: 2.292971\n",
            "Step 12854: Minibatch Loss: 2.168216\n",
            "Step 12856: Minibatch Loss: 2.292357\n",
            "Step 12858: Minibatch Loss: 2.239922\n",
            "Step 12860: Minibatch Loss: 2.204233\n",
            "Step 12862: Minibatch Loss: 2.250873\n",
            "Step 12864: Minibatch Loss: 2.194209\n",
            "Step 12866: Minibatch Loss: 2.271925\n",
            "Step 12868: Minibatch Loss: 2.156239\n",
            "Step 12870: Minibatch Loss: 2.298113\n",
            "Step 12872: Minibatch Loss: 2.255114\n",
            "Step 12874: Minibatch Loss: 2.273624\n",
            "Step 12876: Minibatch Loss: 2.303872\n",
            "Step 12878: Minibatch Loss: 2.238476\n",
            "Step 12880: Minibatch Loss: 2.220164\n",
            "Step 12882: Minibatch Loss: 2.285575\n",
            "Step 12884: Minibatch Loss: 2.274667\n",
            "Step 12886: Minibatch Loss: 2.239641\n",
            "Step 12888: Minibatch Loss: 2.301240\n",
            "Step 12890: Minibatch Loss: 2.300984\n",
            "Step 12892: Minibatch Loss: 2.283654\n",
            "Step 12894: Minibatch Loss: 2.277080\n",
            "Step 12896: Minibatch Loss: 2.105149\n",
            "Step 12898: Minibatch Loss: 2.227265\n",
            "Step 12900: Minibatch Loss: 2.261057\n",
            "Step 12902: Minibatch Loss: 2.227986\n",
            "Step 12904: Minibatch Loss: 2.277655\n",
            "Step 12906: Minibatch Loss: 2.206481\n",
            "Step 12908: Minibatch Loss: 2.284002\n",
            "Step 12910: Minibatch Loss: 2.263464\n",
            "Step 12912: Minibatch Loss: 2.255204\n",
            "Step 12914: Minibatch Loss: 2.252144\n",
            "Step 12916: Minibatch Loss: 2.183371\n",
            "Step 12918: Minibatch Loss: 2.257729\n",
            "Step 12920: Minibatch Loss: 2.274698\n",
            "Step 12922: Minibatch Loss: 2.206017\n",
            "Step 12924: Minibatch Loss: 2.246977\n",
            "Step 12926: Minibatch Loss: 2.268308\n",
            "Step 12928: Minibatch Loss: 2.214024\n",
            "Step 12930: Minibatch Loss: 2.207446\n",
            "Step 12932: Minibatch Loss: 2.221408\n",
            "Step 12934: Minibatch Loss: 2.213685\n",
            "Step 12936: Minibatch Loss: 2.338594\n",
            "Step 12938: Minibatch Loss: 2.190178\n",
            "Step 12940: Minibatch Loss: 2.252049\n",
            "Step 12942: Minibatch Loss: 2.234644\n",
            "Step 12944: Minibatch Loss: 2.264133\n",
            "Step 12946: Minibatch Loss: 2.210473\n",
            "Step 12948: Minibatch Loss: 2.216927\n",
            "Step 12950: Minibatch Loss: 2.203715\n",
            "Step 12952: Minibatch Loss: 2.212561\n",
            "Step 12954: Minibatch Loss: 2.292508\n",
            "Step 12956: Minibatch Loss: 2.261016\n",
            "Step 12958: Minibatch Loss: 2.302834\n",
            "Step 12960: Minibatch Loss: 2.227418\n",
            "Step 12962: Minibatch Loss: 2.232457\n",
            "Step 12964: Minibatch Loss: 2.202107\n",
            "Step 12966: Minibatch Loss: 2.293799\n",
            "Step 12968: Minibatch Loss: 2.171029\n",
            "Step 12970: Minibatch Loss: 2.273946\n",
            "Step 12972: Minibatch Loss: 2.295756\n",
            "Step 12974: Minibatch Loss: 2.221435\n",
            "Step 12976: Minibatch Loss: 2.205791\n",
            "Step 12978: Minibatch Loss: 2.230906\n",
            "Step 12980: Minibatch Loss: 2.277082\n",
            "Step 12982: Minibatch Loss: 2.212208\n",
            "Step 12984: Minibatch Loss: 2.231989\n",
            "Step 12986: Minibatch Loss: 2.291554\n",
            "Step 12988: Minibatch Loss: 2.176322\n",
            "Step 12990: Minibatch Loss: 2.227052\n",
            "Step 12992: Minibatch Loss: 2.315465\n",
            "Step 12994: Minibatch Loss: 2.178196\n",
            "Step 12996: Minibatch Loss: 2.158146\n",
            "Step 12998: Minibatch Loss: 2.226966\n",
            "Step 13000: Minibatch Loss: 2.179397\n",
            "Training for SNR= 6.5  sigma= 0.44668359215096315 iteratin: 0\n",
            "Step 13002: Minibatch Loss: 2.273118\n",
            "Step 13004: Minibatch Loss: 2.306142\n",
            "Step 13006: Minibatch Loss: 2.168905\n",
            "Step 13008: Minibatch Loss: 2.266827\n",
            "Step 13010: Minibatch Loss: 2.256559\n",
            "Step 13012: Minibatch Loss: 2.225392\n",
            "Step 13014: Minibatch Loss: 2.225324\n",
            "Step 13016: Minibatch Loss: 2.266071\n",
            "Step 13018: Minibatch Loss: 2.349538\n",
            "Step 13020: Minibatch Loss: 2.229316\n",
            "Step 13022: Minibatch Loss: 2.325702\n",
            "Step 13024: Minibatch Loss: 2.183797\n",
            "Step 13026: Minibatch Loss: 2.269724\n",
            "Step 13028: Minibatch Loss: 2.252827\n",
            "Step 13030: Minibatch Loss: 2.232290\n",
            "Step 13032: Minibatch Loss: 2.209915\n",
            "Step 13034: Minibatch Loss: 2.237125\n",
            "Step 13036: Minibatch Loss: 2.316485\n",
            "Step 13038: Minibatch Loss: 2.177915\n",
            "Step 13040: Minibatch Loss: 2.279797\n",
            "Step 13042: Minibatch Loss: 2.173983\n",
            "Step 13044: Minibatch Loss: 2.283355\n",
            "Step 13046: Minibatch Loss: 2.257812\n",
            "Step 13048: Minibatch Loss: 2.217474\n",
            "Step 13050: Minibatch Loss: 2.248389\n",
            "Step 13052: Minibatch Loss: 2.248283\n",
            "Step 13054: Minibatch Loss: 2.280887\n",
            "Step 13056: Minibatch Loss: 2.325920\n",
            "Step 13058: Minibatch Loss: 2.270001\n",
            "Step 13060: Minibatch Loss: 2.211411\n",
            "Step 13062: Minibatch Loss: 2.290934\n",
            "Step 13064: Minibatch Loss: 2.161587\n",
            "Step 13066: Minibatch Loss: 2.227183\n",
            "Step 13068: Minibatch Loss: 2.170438\n",
            "Step 13070: Minibatch Loss: 2.253711\n",
            "Step 13072: Minibatch Loss: 2.316631\n",
            "Step 13074: Minibatch Loss: 2.245559\n",
            "Step 13076: Minibatch Loss: 2.208934\n",
            "Step 13078: Minibatch Loss: 2.203599\n",
            "Step 13080: Minibatch Loss: 2.161350\n",
            "Step 13082: Minibatch Loss: 2.271988\n",
            "Step 13084: Minibatch Loss: 2.245516\n",
            "Step 13086: Minibatch Loss: 2.221899\n",
            "Step 13088: Minibatch Loss: 2.264206\n",
            "Step 13090: Minibatch Loss: 2.297018\n",
            "Step 13092: Minibatch Loss: 2.265393\n",
            "Step 13094: Minibatch Loss: 2.161239\n",
            "Step 13096: Minibatch Loss: 2.174467\n",
            "Step 13098: Minibatch Loss: 2.215971\n",
            "Step 13100: Minibatch Loss: 2.181180\n",
            "Step 13102: Minibatch Loss: 2.257022\n",
            "Step 13104: Minibatch Loss: 2.256386\n",
            "Step 13106: Minibatch Loss: 2.185252\n",
            "Step 13108: Minibatch Loss: 2.254420\n",
            "Step 13110: Minibatch Loss: 2.211289\n",
            "Step 13112: Minibatch Loss: 2.295920\n",
            "Step 13114: Minibatch Loss: 2.275268\n",
            "Step 13116: Minibatch Loss: 2.221412\n",
            "Step 13118: Minibatch Loss: 2.306324\n",
            "Step 13120: Minibatch Loss: 2.264068\n",
            "Step 13122: Minibatch Loss: 2.196634\n",
            "Step 13124: Minibatch Loss: 2.248728\n",
            "Step 13126: Minibatch Loss: 2.208011\n",
            "Step 13128: Minibatch Loss: 2.241061\n",
            "Step 13130: Minibatch Loss: 2.228062\n",
            "Step 13132: Minibatch Loss: 2.228805\n",
            "Step 13134: Minibatch Loss: 2.214719\n",
            "Step 13136: Minibatch Loss: 2.299428\n",
            "Step 13138: Minibatch Loss: 2.176650\n",
            "Step 13140: Minibatch Loss: 2.196444\n",
            "Step 13142: Minibatch Loss: 2.142496\n",
            "Step 13144: Minibatch Loss: 2.250569\n",
            "Step 13146: Minibatch Loss: 2.278799\n",
            "Step 13148: Minibatch Loss: 2.254394\n",
            "Step 13150: Minibatch Loss: 2.166259\n",
            "Step 13152: Minibatch Loss: 2.221388\n",
            "Step 13154: Minibatch Loss: 2.178404\n",
            "Step 13156: Minibatch Loss: 2.207759\n",
            "Step 13158: Minibatch Loss: 2.214752\n",
            "Step 13160: Minibatch Loss: 2.131320\n",
            "Step 13162: Minibatch Loss: 2.253263\n",
            "Step 13164: Minibatch Loss: 2.289032\n",
            "Step 13166: Minibatch Loss: 2.280027\n",
            "Step 13168: Minibatch Loss: 2.219834\n",
            "Step 13170: Minibatch Loss: 2.270686\n",
            "Step 13172: Minibatch Loss: 2.188483\n",
            "Step 13174: Minibatch Loss: 2.191426\n",
            "Step 13176: Minibatch Loss: 2.231577\n",
            "Step 13178: Minibatch Loss: 2.213184\n",
            "Step 13180: Minibatch Loss: 2.257724\n",
            "Step 13182: Minibatch Loss: 2.201596\n",
            "Step 13184: Minibatch Loss: 2.272327\n",
            "Step 13186: Minibatch Loss: 2.237440\n",
            "Step 13188: Minibatch Loss: 2.231401\n",
            "Step 13190: Minibatch Loss: 2.234131\n",
            "Step 13192: Minibatch Loss: 2.218394\n",
            "Step 13194: Minibatch Loss: 2.232021\n",
            "Step 13196: Minibatch Loss: 2.182166\n",
            "Step 13198: Minibatch Loss: 2.243913\n",
            "Step 13200: Minibatch Loss: 2.168236\n",
            "Step 13202: Minibatch Loss: 2.269414\n",
            "Step 13204: Minibatch Loss: 2.225407\n",
            "Step 13206: Minibatch Loss: 2.186292\n",
            "Step 13208: Minibatch Loss: 2.283330\n",
            "Step 13210: Minibatch Loss: 2.180981\n",
            "Step 13212: Minibatch Loss: 2.226316\n",
            "Step 13214: Minibatch Loss: 2.372824\n",
            "Step 13216: Minibatch Loss: 2.270606\n",
            "Step 13218: Minibatch Loss: 2.247859\n",
            "Step 13220: Minibatch Loss: 2.258250\n",
            "Step 13222: Minibatch Loss: 2.236151\n",
            "Step 13224: Minibatch Loss: 2.282972\n",
            "Step 13226: Minibatch Loss: 2.213078\n",
            "Step 13228: Minibatch Loss: 2.208961\n",
            "Step 13230: Minibatch Loss: 2.238558\n",
            "Step 13232: Minibatch Loss: 2.157931\n",
            "Step 13234: Minibatch Loss: 2.235645\n",
            "Step 13236: Minibatch Loss: 2.337049\n",
            "Step 13238: Minibatch Loss: 2.129805\n",
            "Step 13240: Minibatch Loss: 2.191735\n",
            "Step 13242: Minibatch Loss: 2.154832\n",
            "Step 13244: Minibatch Loss: 2.245063\n",
            "Step 13246: Minibatch Loss: 2.221058\n",
            "Step 13248: Minibatch Loss: 2.249368\n",
            "Step 13250: Minibatch Loss: 2.162275\n",
            "Step 13252: Minibatch Loss: 2.138992\n",
            "Step 13254: Minibatch Loss: 2.234411\n",
            "Step 13256: Minibatch Loss: 2.213597\n",
            "Step 13258: Minibatch Loss: 2.276369\n",
            "Step 13260: Minibatch Loss: 2.227546\n",
            "Step 13262: Minibatch Loss: 2.233273\n",
            "Step 13264: Minibatch Loss: 2.144485\n",
            "Step 13266: Minibatch Loss: 2.274526\n",
            "Step 13268: Minibatch Loss: 2.176356\n",
            "Step 13270: Minibatch Loss: 2.250138\n",
            "Step 13272: Minibatch Loss: 2.227144\n",
            "Step 13274: Minibatch Loss: 2.234142\n",
            "Step 13276: Minibatch Loss: 2.234606\n",
            "Step 13278: Minibatch Loss: 2.264630\n",
            "Step 13280: Minibatch Loss: 2.216517\n",
            "Step 13282: Minibatch Loss: 2.302562\n",
            "Step 13284: Minibatch Loss: 2.239439\n",
            "Step 13286: Minibatch Loss: 2.245676\n",
            "Step 13288: Minibatch Loss: 2.196225\n",
            "Step 13290: Minibatch Loss: 2.188144\n",
            "Step 13292: Minibatch Loss: 2.241014\n",
            "Step 13294: Minibatch Loss: 2.230907\n",
            "Step 13296: Minibatch Loss: 2.154927\n",
            "Step 13298: Minibatch Loss: 2.242542\n",
            "Step 13300: Minibatch Loss: 2.136035\n",
            "Step 13302: Minibatch Loss: 2.221509\n",
            "Step 13304: Minibatch Loss: 2.254496\n",
            "Step 13306: Minibatch Loss: 2.137323\n",
            "Step 13308: Minibatch Loss: 2.198847\n",
            "Step 13310: Minibatch Loss: 2.153793\n",
            "Step 13312: Minibatch Loss: 2.275431\n",
            "Step 13314: Minibatch Loss: 2.205000\n",
            "Step 13316: Minibatch Loss: 2.205451\n",
            "Step 13318: Minibatch Loss: 2.250858\n",
            "Step 13320: Minibatch Loss: 2.268144\n",
            "Step 13322: Minibatch Loss: 2.274741\n",
            "Step 13324: Minibatch Loss: 2.231751\n",
            "Step 13326: Minibatch Loss: 2.228353\n",
            "Step 13328: Minibatch Loss: 2.153769\n",
            "Step 13330: Minibatch Loss: 2.200389\n",
            "Step 13332: Minibatch Loss: 2.155348\n",
            "Step 13334: Minibatch Loss: 2.144829\n",
            "Step 13336: Minibatch Loss: 2.293579\n",
            "Step 13338: Minibatch Loss: 2.228010\n",
            "Step 13340: Minibatch Loss: 2.253117\n",
            "Step 13342: Minibatch Loss: 2.260010\n",
            "Step 13344: Minibatch Loss: 2.219459\n",
            "Step 13346: Minibatch Loss: 2.202864\n",
            "Step 13348: Minibatch Loss: 2.230482\n",
            "Step 13350: Minibatch Loss: 2.297217\n",
            "Step 13352: Minibatch Loss: 2.215886\n",
            "Step 13354: Minibatch Loss: 2.302281\n",
            "Step 13356: Minibatch Loss: 2.233547\n",
            "Step 13358: Minibatch Loss: 2.192670\n",
            "Step 13360: Minibatch Loss: 2.164166\n",
            "Step 13362: Minibatch Loss: 2.266439\n",
            "Step 13364: Minibatch Loss: 2.153487\n",
            "Step 13366: Minibatch Loss: 2.219584\n",
            "Step 13368: Minibatch Loss: 2.125703\n",
            "Step 13370: Minibatch Loss: 2.247284\n",
            "Step 13372: Minibatch Loss: 2.217067\n",
            "Step 13374: Minibatch Loss: 2.266204\n",
            "Step 13376: Minibatch Loss: 2.301962\n",
            "Step 13378: Minibatch Loss: 2.183014\n",
            "Step 13380: Minibatch Loss: 2.276694\n",
            "Step 13382: Minibatch Loss: 2.214930\n",
            "Step 13384: Minibatch Loss: 2.221248\n",
            "Step 13386: Minibatch Loss: 2.202300\n",
            "Step 13388: Minibatch Loss: 2.300488\n",
            "Step 13390: Minibatch Loss: 2.256828\n",
            "Step 13392: Minibatch Loss: 2.264347\n",
            "Step 13394: Minibatch Loss: 2.214118\n",
            "Step 13396: Minibatch Loss: 2.166403\n",
            "Step 13398: Minibatch Loss: 2.206851\n",
            "Step 13400: Minibatch Loss: 2.154919\n",
            "Step 13402: Minibatch Loss: 2.283172\n",
            "Step 13404: Minibatch Loss: 2.221054\n",
            "Step 13406: Minibatch Loss: 2.167609\n",
            "Step 13408: Minibatch Loss: 2.257374\n",
            "Step 13410: Minibatch Loss: 2.199501\n",
            "Step 13412: Minibatch Loss: 2.228074\n",
            "Step 13414: Minibatch Loss: 2.301186\n",
            "Step 13416: Minibatch Loss: 2.278919\n",
            "Step 13418: Minibatch Loss: 2.235680\n",
            "Step 13420: Minibatch Loss: 2.268983\n",
            "Step 13422: Minibatch Loss: 2.170063\n",
            "Step 13424: Minibatch Loss: 2.240668\n",
            "Step 13426: Minibatch Loss: 2.339080\n",
            "Step 13428: Minibatch Loss: 2.198982\n",
            "Step 13430: Minibatch Loss: 2.251186\n",
            "Step 13432: Minibatch Loss: 2.177454\n",
            "Step 13434: Minibatch Loss: 2.174373\n",
            "Step 13436: Minibatch Loss: 2.309294\n",
            "Step 13438: Minibatch Loss: 2.218922\n",
            "Step 13440: Minibatch Loss: 2.188229\n",
            "Step 13442: Minibatch Loss: 2.164977\n",
            "Step 13444: Minibatch Loss: 2.249139\n",
            "Step 13446: Minibatch Loss: 2.166023\n",
            "Step 13448: Minibatch Loss: 2.195756\n",
            "Step 13450: Minibatch Loss: 2.174938\n",
            "Step 13452: Minibatch Loss: 2.201133\n",
            "Step 13454: Minibatch Loss: 2.211225\n",
            "Step 13456: Minibatch Loss: 2.228257\n",
            "Step 13458: Minibatch Loss: 2.231254\n",
            "Step 13460: Minibatch Loss: 2.185301\n",
            "Step 13462: Minibatch Loss: 2.318806\n",
            "Step 13464: Minibatch Loss: 2.159410\n",
            "Step 13466: Minibatch Loss: 2.279623\n",
            "Step 13468: Minibatch Loss: 2.134310\n",
            "Step 13470: Minibatch Loss: 2.292861\n",
            "Step 13472: Minibatch Loss: 2.228788\n",
            "Step 13474: Minibatch Loss: 2.237670\n",
            "Step 13476: Minibatch Loss: 2.211204\n",
            "Step 13478: Minibatch Loss: 2.218553\n",
            "Step 13480: Minibatch Loss: 2.200309\n",
            "Step 13482: Minibatch Loss: 2.261794\n",
            "Step 13484: Minibatch Loss: 2.173795\n",
            "Step 13486: Minibatch Loss: 2.188608\n",
            "Step 13488: Minibatch Loss: 2.188683\n",
            "Step 13490: Minibatch Loss: 2.262155\n",
            "Step 13492: Minibatch Loss: 2.222962\n",
            "Step 13494: Minibatch Loss: 2.204826\n",
            "Step 13496: Minibatch Loss: 2.183409\n",
            "Step 13498: Minibatch Loss: 2.249908\n",
            "Step 13500: Minibatch Loss: 2.216640\n",
            "Step 13502: Minibatch Loss: 2.232292\n",
            "Step 13504: Minibatch Loss: 2.243317\n",
            "Step 13506: Minibatch Loss: 2.203273\n",
            "Step 13508: Minibatch Loss: 2.200932\n",
            "Step 13510: Minibatch Loss: 2.201856\n",
            "Step 13512: Minibatch Loss: 2.251348\n",
            "Step 13514: Minibatch Loss: 2.273597\n",
            "Step 13516: Minibatch Loss: 2.285859\n",
            "Step 13518: Minibatch Loss: 2.237135\n",
            "Step 13520: Minibatch Loss: 2.261269\n",
            "Step 13522: Minibatch Loss: 2.233934\n",
            "Step 13524: Minibatch Loss: 2.217649\n",
            "Step 13526: Minibatch Loss: 2.257431\n",
            "Step 13528: Minibatch Loss: 2.205316\n",
            "Step 13530: Minibatch Loss: 2.225388\n",
            "Step 13532: Minibatch Loss: 2.229643\n",
            "Step 13534: Minibatch Loss: 2.213362\n",
            "Step 13536: Minibatch Loss: 2.289418\n",
            "Step 13538: Minibatch Loss: 2.193624\n",
            "Step 13540: Minibatch Loss: 2.205344\n",
            "Step 13542: Minibatch Loss: 2.143206\n",
            "Step 13544: Minibatch Loss: 2.276446\n",
            "Step 13546: Minibatch Loss: 2.223521\n",
            "Step 13548: Minibatch Loss: 2.206618\n",
            "Step 13550: Minibatch Loss: 2.219673\n",
            "Step 13552: Minibatch Loss: 2.217755\n",
            "Step 13554: Minibatch Loss: 2.239986\n",
            "Step 13556: Minibatch Loss: 2.245409\n",
            "Step 13558: Minibatch Loss: 2.229230\n",
            "Step 13560: Minibatch Loss: 2.198279\n",
            "Step 13562: Minibatch Loss: 2.258711\n",
            "Step 13564: Minibatch Loss: 2.231967\n",
            "Step 13566: Minibatch Loss: 2.206779\n",
            "Step 13568: Minibatch Loss: 2.067376\n",
            "Step 13570: Minibatch Loss: 2.247567\n",
            "Step 13572: Minibatch Loss: 2.280965\n",
            "Step 13574: Minibatch Loss: 2.212298\n",
            "Step 13576: Minibatch Loss: 2.226745\n",
            "Step 13578: Minibatch Loss: 2.210398\n",
            "Step 13580: Minibatch Loss: 2.231298\n",
            "Step 13582: Minibatch Loss: 2.315652\n",
            "Step 13584: Minibatch Loss: 2.257024\n",
            "Step 13586: Minibatch Loss: 2.194688\n",
            "Step 13588: Minibatch Loss: 2.247928\n",
            "Step 13590: Minibatch Loss: 2.240370\n",
            "Step 13592: Minibatch Loss: 2.213154\n",
            "Step 13594: Minibatch Loss: 2.167390\n",
            "Step 13596: Minibatch Loss: 2.162767\n",
            "Step 13598: Minibatch Loss: 2.207640\n",
            "Step 13600: Minibatch Loss: 2.176771\n",
            "Step 13602: Minibatch Loss: 2.240210\n",
            "Step 13604: Minibatch Loss: 2.146776\n",
            "Step 13606: Minibatch Loss: 2.115422\n",
            "Step 13608: Minibatch Loss: 2.205521\n",
            "Step 13610: Minibatch Loss: 2.129896\n",
            "Step 13612: Minibatch Loss: 2.259526\n",
            "Step 13614: Minibatch Loss: 2.251517\n",
            "Step 13616: Minibatch Loss: 2.187425\n",
            "Step 13618: Minibatch Loss: 2.265435\n",
            "Step 13620: Minibatch Loss: 2.234040\n",
            "Step 13622: Minibatch Loss: 2.145679\n",
            "Step 13624: Minibatch Loss: 2.172549\n",
            "Step 13626: Minibatch Loss: 2.275057\n",
            "Step 13628: Minibatch Loss: 2.222167\n",
            "Step 13630: Minibatch Loss: 2.242754\n",
            "Step 13632: Minibatch Loss: 2.186007\n",
            "Step 13634: Minibatch Loss: 2.200066\n",
            "Step 13636: Minibatch Loss: 2.243351\n",
            "Step 13638: Minibatch Loss: 2.156700\n",
            "Step 13640: Minibatch Loss: 2.226372\n",
            "Step 13642: Minibatch Loss: 2.133242\n",
            "Step 13644: Minibatch Loss: 2.181502\n",
            "Step 13646: Minibatch Loss: 2.207212\n",
            "Step 13648: Minibatch Loss: 2.193710\n",
            "Step 13650: Minibatch Loss: 2.226307\n",
            "Step 13652: Minibatch Loss: 2.169996\n",
            "Step 13654: Minibatch Loss: 2.261075\n",
            "Step 13656: Minibatch Loss: 2.259407\n",
            "Step 13658: Minibatch Loss: 2.162139\n",
            "Step 13660: Minibatch Loss: 2.222705\n",
            "Step 13662: Minibatch Loss: 2.234357\n",
            "Step 13664: Minibatch Loss: 2.170257\n",
            "Step 13666: Minibatch Loss: 2.266936\n",
            "Step 13668: Minibatch Loss: 2.211092\n",
            "Step 13670: Minibatch Loss: 2.257958\n",
            "Step 13672: Minibatch Loss: 2.214126\n",
            "Step 13674: Minibatch Loss: 2.255674\n",
            "Step 13676: Minibatch Loss: 2.153185\n",
            "Step 13678: Minibatch Loss: 2.161731\n",
            "Step 13680: Minibatch Loss: 2.240469\n",
            "Step 13682: Minibatch Loss: 2.233743\n",
            "Step 13684: Minibatch Loss: 2.166425\n",
            "Step 13686: Minibatch Loss: 2.218919\n",
            "Step 13688: Minibatch Loss: 2.201643\n",
            "Step 13690: Minibatch Loss: 2.223684\n",
            "Step 13692: Minibatch Loss: 2.275092\n",
            "Step 13694: Minibatch Loss: 2.253178\n",
            "Step 13696: Minibatch Loss: 2.236639\n",
            "Step 13698: Minibatch Loss: 2.224399\n",
            "Step 13700: Minibatch Loss: 2.147918\n",
            "Step 13702: Minibatch Loss: 2.250801\n",
            "Step 13704: Minibatch Loss: 2.236414\n",
            "Step 13706: Minibatch Loss: 2.208308\n",
            "Step 13708: Minibatch Loss: 2.260221\n",
            "Step 13710: Minibatch Loss: 2.197286\n",
            "Step 13712: Minibatch Loss: 2.235891\n",
            "Step 13714: Minibatch Loss: 2.230043\n",
            "Step 13716: Minibatch Loss: 2.300136\n",
            "Step 13718: Minibatch Loss: 2.278597\n",
            "Step 13720: Minibatch Loss: 2.229532\n",
            "Step 13722: Minibatch Loss: 2.218874\n",
            "Step 13724: Minibatch Loss: 2.175504\n",
            "Step 13726: Minibatch Loss: 2.227554\n",
            "Step 13728: Minibatch Loss: 2.275563\n",
            "Step 13730: Minibatch Loss: 2.208197\n",
            "Step 13732: Minibatch Loss: 2.216744\n",
            "Step 13734: Minibatch Loss: 2.190922\n",
            "Step 13736: Minibatch Loss: 2.265118\n",
            "Step 13738: Minibatch Loss: 2.160195\n",
            "Step 13740: Minibatch Loss: 2.217941\n",
            "Step 13742: Minibatch Loss: 2.154731\n",
            "Step 13744: Minibatch Loss: 2.178989\n",
            "Step 13746: Minibatch Loss: 2.232210\n",
            "Step 13748: Minibatch Loss: 2.186069\n",
            "Step 13750: Minibatch Loss: 2.281159\n",
            "Step 13752: Minibatch Loss: 2.098904\n",
            "Step 13754: Minibatch Loss: 2.210522\n",
            "Step 13756: Minibatch Loss: 2.208852\n",
            "Step 13758: Minibatch Loss: 2.172543\n",
            "Step 13760: Minibatch Loss: 2.173012\n",
            "Step 13762: Minibatch Loss: 2.265502\n",
            "Step 13764: Minibatch Loss: 2.150556\n",
            "Step 13766: Minibatch Loss: 2.249048\n",
            "Step 13768: Minibatch Loss: 2.120924\n",
            "Step 13770: Minibatch Loss: 2.189888\n",
            "Step 13772: Minibatch Loss: 2.173015\n",
            "Step 13774: Minibatch Loss: 2.150618\n",
            "Step 13776: Minibatch Loss: 2.220701\n",
            "Step 13778: Minibatch Loss: 2.241473\n",
            "Step 13780: Minibatch Loss: 2.277095\n",
            "Step 13782: Minibatch Loss: 2.173899\n",
            "Step 13784: Minibatch Loss: 2.211458\n",
            "Step 13786: Minibatch Loss: 2.213189\n",
            "Step 13788: Minibatch Loss: 2.258230\n",
            "Step 13790: Minibatch Loss: 2.232738\n",
            "Step 13792: Minibatch Loss: 2.250100\n",
            "Step 13794: Minibatch Loss: 2.158368\n",
            "Step 13796: Minibatch Loss: 2.184574\n",
            "Step 13798: Minibatch Loss: 2.222391\n",
            "Step 13800: Minibatch Loss: 2.180792\n",
            "Step 13802: Minibatch Loss: 2.208046\n",
            "Step 13804: Minibatch Loss: 2.249317\n",
            "Step 13806: Minibatch Loss: 2.174571\n",
            "Step 13808: Minibatch Loss: 2.265280\n",
            "Step 13810: Minibatch Loss: 2.234066\n",
            "Step 13812: Minibatch Loss: 2.185312\n",
            "Step 13814: Minibatch Loss: 2.209074\n",
            "Step 13816: Minibatch Loss: 2.209810\n",
            "Step 13818: Minibatch Loss: 2.270420\n",
            "Step 13820: Minibatch Loss: 2.281741\n",
            "Step 13822: Minibatch Loss: 2.162592\n",
            "Step 13824: Minibatch Loss: 2.221399\n",
            "Step 13826: Minibatch Loss: 2.246411\n",
            "Step 13828: Minibatch Loss: 2.184200\n",
            "Step 13830: Minibatch Loss: 2.231763\n",
            "Step 13832: Minibatch Loss: 2.182976\n",
            "Step 13834: Minibatch Loss: 2.197548\n",
            "Step 13836: Minibatch Loss: 2.216071\n",
            "Step 13838: Minibatch Loss: 2.198277\n",
            "Step 13840: Minibatch Loss: 2.165673\n",
            "Step 13842: Minibatch Loss: 2.198384\n",
            "Step 13844: Minibatch Loss: 2.178486\n",
            "Step 13846: Minibatch Loss: 2.282763\n",
            "Step 13848: Minibatch Loss: 2.225339\n",
            "Step 13850: Minibatch Loss: 2.165607\n",
            "Step 13852: Minibatch Loss: 2.179822\n",
            "Step 13854: Minibatch Loss: 2.298674\n",
            "Step 13856: Minibatch Loss: 2.234203\n",
            "Step 13858: Minibatch Loss: 2.192620\n",
            "Step 13860: Minibatch Loss: 2.197329\n",
            "Step 13862: Minibatch Loss: 2.222661\n",
            "Step 13864: Minibatch Loss: 2.147555\n",
            "Step 13866: Minibatch Loss: 2.243023\n",
            "Step 13868: Minibatch Loss: 2.158452\n",
            "Step 13870: Minibatch Loss: 2.265973\n",
            "Step 13872: Minibatch Loss: 2.253810\n",
            "Step 13874: Minibatch Loss: 2.277015\n",
            "Step 13876: Minibatch Loss: 2.235379\n",
            "Step 13878: Minibatch Loss: 2.184435\n",
            "Step 13880: Minibatch Loss: 2.258913\n",
            "Step 13882: Minibatch Loss: 2.281749\n",
            "Step 13884: Minibatch Loss: 2.235491\n",
            "Step 13886: Minibatch Loss: 2.202114\n",
            "Step 13888: Minibatch Loss: 2.178299\n",
            "Step 13890: Minibatch Loss: 2.219826\n",
            "Step 13892: Minibatch Loss: 2.223895\n",
            "Step 13894: Minibatch Loss: 2.192121\n",
            "Step 13896: Minibatch Loss: 2.230450\n",
            "Step 13898: Minibatch Loss: 2.208288\n",
            "Step 13900: Minibatch Loss: 2.125988\n",
            "Step 13902: Minibatch Loss: 2.205409\n",
            "Step 13904: Minibatch Loss: 2.278502\n",
            "Step 13906: Minibatch Loss: 2.144430\n",
            "Step 13908: Minibatch Loss: 2.230862\n",
            "Step 13910: Minibatch Loss: 2.160260\n",
            "Step 13912: Minibatch Loss: 2.215936\n",
            "Step 13914: Minibatch Loss: 2.242701\n",
            "Step 13916: Minibatch Loss: 2.169294\n",
            "Step 13918: Minibatch Loss: 2.250211\n",
            "Step 13920: Minibatch Loss: 2.250241\n",
            "Step 13922: Minibatch Loss: 2.158681\n",
            "Step 13924: Minibatch Loss: 2.216587\n",
            "Step 13926: Minibatch Loss: 2.194318\n",
            "Step 13928: Minibatch Loss: 2.191227\n",
            "Step 13930: Minibatch Loss: 2.206756\n",
            "Step 13932: Minibatch Loss: 2.202389\n",
            "Step 13934: Minibatch Loss: 2.222940\n",
            "Step 13936: Minibatch Loss: 2.301850\n",
            "Step 13938: Minibatch Loss: 2.138189\n",
            "Step 13940: Minibatch Loss: 2.226854\n",
            "Step 13942: Minibatch Loss: 2.140274\n",
            "Step 13944: Minibatch Loss: 2.213595\n",
            "Step 13946: Minibatch Loss: 2.146473\n",
            "Step 13948: Minibatch Loss: 2.182689\n",
            "Step 13950: Minibatch Loss: 2.160823\n",
            "Step 13952: Minibatch Loss: 2.206905\n",
            "Step 13954: Minibatch Loss: 2.256831\n",
            "Step 13956: Minibatch Loss: 2.308218\n",
            "Step 13958: Minibatch Loss: 2.248015\n",
            "Step 13960: Minibatch Loss: 2.208175\n",
            "Step 13962: Minibatch Loss: 2.281700\n",
            "Step 13964: Minibatch Loss: 2.161333\n",
            "Step 13966: Minibatch Loss: 2.192292\n",
            "Step 13968: Minibatch Loss: 2.191558\n",
            "Step 13970: Minibatch Loss: 2.255219\n",
            "Step 13972: Minibatch Loss: 2.171725\n",
            "Step 13974: Minibatch Loss: 2.199938\n",
            "Step 13976: Minibatch Loss: 2.168867\n",
            "Step 13978: Minibatch Loss: 2.171108\n",
            "Step 13980: Minibatch Loss: 2.197932\n",
            "Step 13982: Minibatch Loss: 2.171480\n",
            "Step 13984: Minibatch Loss: 2.186762\n",
            "Step 13986: Minibatch Loss: 2.229868\n",
            "Step 13988: Minibatch Loss: 2.194459\n",
            "Step 13990: Minibatch Loss: 2.189722\n",
            "Step 13992: Minibatch Loss: 2.197534\n",
            "Step 13994: Minibatch Loss: 2.158359\n",
            "Step 13996: Minibatch Loss: 2.139573\n",
            "Step 13998: Minibatch Loss: 2.200402\n",
            "Step 14000: Minibatch Loss: 2.117186\n",
            "Training for SNR= 7.0  sigma= 0.44668359215096315 iteratin: 0\n",
            "Step 14002: Minibatch Loss: 2.234599\n",
            "Step 14004: Minibatch Loss: 2.197965\n",
            "Step 14006: Minibatch Loss: 2.155042\n",
            "Step 14008: Minibatch Loss: 2.324340\n",
            "Step 14010: Minibatch Loss: 2.147842\n",
            "Step 14012: Minibatch Loss: 2.194600\n",
            "Step 14014: Minibatch Loss: 2.268965\n",
            "Step 14016: Minibatch Loss: 2.188304\n",
            "Step 14018: Minibatch Loss: 2.250765\n",
            "Step 14020: Minibatch Loss: 2.232838\n",
            "Step 14022: Minibatch Loss: 2.151920\n",
            "Step 14024: Minibatch Loss: 2.216552\n",
            "Step 14026: Minibatch Loss: 2.254683\n",
            "Step 14028: Minibatch Loss: 2.134264\n",
            "Step 14030: Minibatch Loss: 2.165517\n",
            "Step 14032: Minibatch Loss: 2.151856\n",
            "Step 14034: Minibatch Loss: 2.203004\n",
            "Step 14036: Minibatch Loss: 2.205431\n",
            "Step 14038: Minibatch Loss: 2.149175\n",
            "Step 14040: Minibatch Loss: 2.242868\n",
            "Step 14042: Minibatch Loss: 2.185720\n",
            "Step 14044: Minibatch Loss: 2.240893\n",
            "Step 14046: Minibatch Loss: 2.244979\n",
            "Step 14048: Minibatch Loss: 2.157190\n",
            "Step 14050: Minibatch Loss: 2.180374\n",
            "Step 14052: Minibatch Loss: 2.231555\n",
            "Step 14054: Minibatch Loss: 2.234227\n",
            "Step 14056: Minibatch Loss: 2.251508\n",
            "Step 14058: Minibatch Loss: 2.174355\n",
            "Step 14060: Minibatch Loss: 2.112038\n",
            "Step 14062: Minibatch Loss: 2.203430\n",
            "Step 14064: Minibatch Loss: 2.105599\n",
            "Step 14066: Minibatch Loss: 2.283530\n",
            "Step 14068: Minibatch Loss: 2.092162\n",
            "Step 14070: Minibatch Loss: 2.232910\n",
            "Step 14072: Minibatch Loss: 2.207854\n",
            "Step 14074: Minibatch Loss: 2.187172\n",
            "Step 14076: Minibatch Loss: 2.186799\n",
            "Step 14078: Minibatch Loss: 2.233725\n",
            "Step 14080: Minibatch Loss: 2.238300\n",
            "Step 14082: Minibatch Loss: 2.248905\n",
            "Step 14084: Minibatch Loss: 2.175701\n",
            "Step 14086: Minibatch Loss: 2.244312\n",
            "Step 14088: Minibatch Loss: 2.205120\n",
            "Step 14090: Minibatch Loss: 2.243192\n",
            "Step 14092: Minibatch Loss: 2.208364\n",
            "Step 14094: Minibatch Loss: 2.133500\n",
            "Step 14096: Minibatch Loss: 2.097806\n",
            "Step 14098: Minibatch Loss: 2.196500\n",
            "Step 14100: Minibatch Loss: 2.109869\n",
            "Step 14102: Minibatch Loss: 2.215975\n",
            "Step 14104: Minibatch Loss: 2.159025\n",
            "Step 14106: Minibatch Loss: 2.143672\n",
            "Step 14108: Minibatch Loss: 2.183864\n",
            "Step 14110: Minibatch Loss: 2.179187\n",
            "Step 14112: Minibatch Loss: 2.228292\n",
            "Step 14114: Minibatch Loss: 2.285531\n",
            "Step 14116: Minibatch Loss: 2.199174\n",
            "Step 14118: Minibatch Loss: 2.245896\n",
            "Step 14120: Minibatch Loss: 2.263329\n",
            "Step 14122: Minibatch Loss: 2.194484\n",
            "Step 14124: Minibatch Loss: 2.146067\n",
            "Step 14126: Minibatch Loss: 2.267348\n",
            "Step 14128: Minibatch Loss: 2.209290\n",
            "Step 14130: Minibatch Loss: 2.218126\n",
            "Step 14132: Minibatch Loss: 2.138374\n",
            "Step 14134: Minibatch Loss: 2.160236\n",
            "Step 14136: Minibatch Loss: 2.242386\n",
            "Step 14138: Minibatch Loss: 2.148959\n",
            "Step 14140: Minibatch Loss: 2.208714\n",
            "Step 14142: Minibatch Loss: 2.112920\n",
            "Step 14144: Minibatch Loss: 2.234752\n",
            "Step 14146: Minibatch Loss: 2.138812\n",
            "Step 14148: Minibatch Loss: 2.148470\n",
            "Step 14150: Minibatch Loss: 2.110286\n",
            "Step 14152: Minibatch Loss: 2.153861\n",
            "Step 14154: Minibatch Loss: 2.199740\n",
            "Step 14156: Minibatch Loss: 2.251814\n",
            "Step 14158: Minibatch Loss: 2.206291\n",
            "Step 14160: Minibatch Loss: 2.161550\n",
            "Step 14162: Minibatch Loss: 2.226557\n",
            "Step 14164: Minibatch Loss: 2.125207\n",
            "Step 14166: Minibatch Loss: 2.226977\n",
            "Step 14168: Minibatch Loss: 2.183451\n",
            "Step 14170: Minibatch Loss: 2.242831\n",
            "Step 14172: Minibatch Loss: 2.175720\n",
            "Step 14174: Minibatch Loss: 2.194747\n",
            "Step 14176: Minibatch Loss: 2.180206\n",
            "Step 14178: Minibatch Loss: 2.197427\n",
            "Step 14180: Minibatch Loss: 2.259279\n",
            "Step 14182: Minibatch Loss: 2.241437\n",
            "Step 14184: Minibatch Loss: 2.139284\n",
            "Step 14186: Minibatch Loss: 2.212287\n",
            "Step 14188: Minibatch Loss: 2.163095\n",
            "Step 14190: Minibatch Loss: 2.194180\n",
            "Step 14192: Minibatch Loss: 2.265002\n",
            "Step 14194: Minibatch Loss: 2.194752\n",
            "Step 14196: Minibatch Loss: 2.172065\n",
            "Step 14198: Minibatch Loss: 2.139702\n",
            "Step 14200: Minibatch Loss: 2.077171\n",
            "Step 14202: Minibatch Loss: 2.210563\n",
            "Step 14204: Minibatch Loss: 2.235326\n",
            "Step 14206: Minibatch Loss: 2.234594\n",
            "Step 14208: Minibatch Loss: 2.178604\n",
            "Step 14210: Minibatch Loss: 2.120739\n",
            "Step 14212: Minibatch Loss: 2.212913\n",
            "Step 14214: Minibatch Loss: 2.265888\n",
            "Step 14216: Minibatch Loss: 2.180299\n",
            "Step 14218: Minibatch Loss: 2.218809\n",
            "Step 14220: Minibatch Loss: 2.266819\n",
            "Step 14222: Minibatch Loss: 2.191501\n",
            "Step 14224: Minibatch Loss: 2.199515\n",
            "Step 14226: Minibatch Loss: 2.236632\n",
            "Step 14228: Minibatch Loss: 2.199085\n",
            "Step 14230: Minibatch Loss: 2.256490\n",
            "Step 14232: Minibatch Loss: 2.113577\n",
            "Step 14234: Minibatch Loss: 2.166156\n",
            "Step 14236: Minibatch Loss: 2.196436\n",
            "Step 14238: Minibatch Loss: 2.131162\n",
            "Step 14240: Minibatch Loss: 2.182837\n",
            "Step 14242: Minibatch Loss: 2.169241\n",
            "Step 14244: Minibatch Loss: 2.228855\n",
            "Step 14246: Minibatch Loss: 2.190577\n",
            "Step 14248: Minibatch Loss: 2.148850\n",
            "Step 14250: Minibatch Loss: 2.186878\n",
            "Step 14252: Minibatch Loss: 2.236740\n",
            "Step 14254: Minibatch Loss: 2.223795\n",
            "Step 14256: Minibatch Loss: 2.192977\n",
            "Step 14258: Minibatch Loss: 2.155724\n",
            "Step 14260: Minibatch Loss: 2.213856\n",
            "Step 14262: Minibatch Loss: 2.228926\n",
            "Step 14264: Minibatch Loss: 2.138467\n",
            "Step 14266: Minibatch Loss: 2.213754\n",
            "Step 14268: Minibatch Loss: 2.122967\n",
            "Step 14270: Minibatch Loss: 2.224664\n",
            "Step 14272: Minibatch Loss: 2.161813\n",
            "Step 14274: Minibatch Loss: 2.158612\n",
            "Step 14276: Minibatch Loss: 2.150362\n",
            "Step 14278: Minibatch Loss: 2.157822\n",
            "Step 14280: Minibatch Loss: 2.250116\n",
            "Step 14282: Minibatch Loss: 2.245778\n",
            "Step 14284: Minibatch Loss: 2.117403\n",
            "Step 14286: Minibatch Loss: 2.152211\n",
            "Step 14288: Minibatch Loss: 2.174155\n",
            "Step 14290: Minibatch Loss: 2.233594\n",
            "Step 14292: Minibatch Loss: 2.208225\n",
            "Step 14294: Minibatch Loss: 2.176898\n",
            "Step 14296: Minibatch Loss: 2.153346\n",
            "Step 14298: Minibatch Loss: 2.215003\n",
            "Step 14300: Minibatch Loss: 2.135774\n",
            "Step 14302: Minibatch Loss: 2.224140\n",
            "Step 14304: Minibatch Loss: 2.216326\n",
            "Step 14306: Minibatch Loss: 2.191446\n",
            "Step 14308: Minibatch Loss: 2.191349\n",
            "Step 14310: Minibatch Loss: 2.150245\n",
            "Step 14312: Minibatch Loss: 2.239580\n",
            "Step 14314: Minibatch Loss: 2.192231\n",
            "Step 14316: Minibatch Loss: 2.210507\n",
            "Step 14318: Minibatch Loss: 2.282839\n",
            "Step 14320: Minibatch Loss: 2.219191\n",
            "Step 14322: Minibatch Loss: 2.253596\n",
            "Step 14324: Minibatch Loss: 2.171925\n",
            "Step 14326: Minibatch Loss: 2.196336\n",
            "Step 14328: Minibatch Loss: 2.240956\n",
            "Step 14330: Minibatch Loss: 2.207616\n",
            "Step 14332: Minibatch Loss: 2.151740\n",
            "Step 14334: Minibatch Loss: 2.143002\n",
            "Step 14336: Minibatch Loss: 2.189448\n",
            "Step 14338: Minibatch Loss: 2.145729\n",
            "Step 14340: Minibatch Loss: 2.204367\n",
            "Step 14342: Minibatch Loss: 2.167246\n",
            "Step 14344: Minibatch Loss: 2.229868\n",
            "Step 14346: Minibatch Loss: 2.206255\n",
            "Step 14348: Minibatch Loss: 2.205351\n",
            "Step 14350: Minibatch Loss: 2.182396\n",
            "Step 14352: Minibatch Loss: 2.122649\n",
            "Step 14354: Minibatch Loss: 2.177021\n",
            "Step 14356: Minibatch Loss: 2.215544\n",
            "Step 14358: Minibatch Loss: 2.178586\n",
            "Step 14360: Minibatch Loss: 2.118311\n",
            "Step 14362: Minibatch Loss: 2.211868\n",
            "Step 14364: Minibatch Loss: 2.144353\n",
            "Step 14366: Minibatch Loss: 2.224777\n",
            "Step 14368: Minibatch Loss: 2.104528\n",
            "Step 14370: Minibatch Loss: 2.247632\n",
            "Step 14372: Minibatch Loss: 2.228499\n",
            "Step 14374: Minibatch Loss: 2.246751\n",
            "Step 14376: Minibatch Loss: 2.171105\n",
            "Step 14378: Minibatch Loss: 2.194421\n",
            "Step 14380: Minibatch Loss: 2.188619\n",
            "Step 14382: Minibatch Loss: 2.159831\n",
            "Step 14384: Minibatch Loss: 2.148816\n",
            "Step 14386: Minibatch Loss: 2.213712\n",
            "Step 14388: Minibatch Loss: 2.167428\n",
            "Step 14390: Minibatch Loss: 2.237235\n",
            "Step 14392: Minibatch Loss: 2.164452\n",
            "Step 14394: Minibatch Loss: 2.127479\n",
            "Step 14396: Minibatch Loss: 2.139181\n",
            "Step 14398: Minibatch Loss: 2.143043\n",
            "Step 14400: Minibatch Loss: 2.158607\n",
            "Step 14402: Minibatch Loss: 2.228428\n",
            "Step 14404: Minibatch Loss: 2.178759\n",
            "Step 14406: Minibatch Loss: 2.191166\n",
            "Step 14408: Minibatch Loss: 2.125624\n",
            "Step 14410: Minibatch Loss: 2.188782\n",
            "Step 14412: Minibatch Loss: 2.265091\n",
            "Step 14414: Minibatch Loss: 2.201129\n",
            "Step 14416: Minibatch Loss: 2.147159\n",
            "Step 14418: Minibatch Loss: 2.150682\n",
            "Step 14420: Minibatch Loss: 2.200043\n",
            "Step 14422: Minibatch Loss: 2.121508\n",
            "Step 14424: Minibatch Loss: 2.176184\n",
            "Step 14426: Minibatch Loss: 2.172127\n",
            "Step 14428: Minibatch Loss: 2.175971\n",
            "Step 14430: Minibatch Loss: 2.179598\n",
            "Step 14432: Minibatch Loss: 2.101815\n",
            "Step 14434: Minibatch Loss: 2.172066\n",
            "Step 14436: Minibatch Loss: 2.316434\n",
            "Step 14438: Minibatch Loss: 2.113483\n",
            "Step 14440: Minibatch Loss: 2.129234\n",
            "Step 14442: Minibatch Loss: 2.159951\n",
            "Step 14444: Minibatch Loss: 2.244999\n",
            "Step 14446: Minibatch Loss: 2.192391\n",
            "Step 14448: Minibatch Loss: 2.148473\n",
            "Step 14450: Minibatch Loss: 2.123065\n",
            "Step 14452: Minibatch Loss: 2.158109\n",
            "Step 14454: Minibatch Loss: 2.190413\n",
            "Step 14456: Minibatch Loss: 2.220577\n",
            "Step 14458: Minibatch Loss: 2.179346\n",
            "Step 14460: Minibatch Loss: 2.161085\n",
            "Step 14462: Minibatch Loss: 2.229161\n",
            "Step 14464: Minibatch Loss: 2.125885\n",
            "Step 14466: Minibatch Loss: 2.247877\n",
            "Step 14468: Minibatch Loss: 2.146830\n",
            "Step 14470: Minibatch Loss: 2.248644\n",
            "Step 14472: Minibatch Loss: 2.197070\n",
            "Step 14474: Minibatch Loss: 2.226427\n",
            "Step 14476: Minibatch Loss: 2.095655\n",
            "Step 14478: Minibatch Loss: 2.216024\n",
            "Step 14480: Minibatch Loss: 2.200316\n",
            "Step 14482: Minibatch Loss: 2.216460\n",
            "Step 14484: Minibatch Loss: 2.146298\n",
            "Step 14486: Minibatch Loss: 2.214553\n",
            "Step 14488: Minibatch Loss: 2.237605\n",
            "Step 14490: Minibatch Loss: 2.205739\n",
            "Step 14492: Minibatch Loss: 2.224057\n",
            "Step 14494: Minibatch Loss: 2.173732\n",
            "Step 14496: Minibatch Loss: 2.096972\n",
            "Step 14498: Minibatch Loss: 2.192118\n",
            "Step 14500: Minibatch Loss: 2.123006\n",
            "Step 14502: Minibatch Loss: 2.199687\n",
            "Step 14504: Minibatch Loss: 2.212257\n",
            "Step 14506: Minibatch Loss: 2.136553\n",
            "Step 14508: Minibatch Loss: 2.198422\n",
            "Step 14510: Minibatch Loss: 2.135291\n",
            "Step 14512: Minibatch Loss: 2.251330\n",
            "Step 14514: Minibatch Loss: 2.179875\n",
            "Step 14516: Minibatch Loss: 2.158380\n",
            "Step 14518: Minibatch Loss: 2.217748\n",
            "Step 14520: Minibatch Loss: 2.203689\n",
            "Step 14522: Minibatch Loss: 2.161611\n",
            "Step 14524: Minibatch Loss: 2.191262\n",
            "Step 14526: Minibatch Loss: 2.262603\n",
            "Step 14528: Minibatch Loss: 2.155396\n",
            "Step 14530: Minibatch Loss: 2.169050\n",
            "Step 14532: Minibatch Loss: 2.097456\n",
            "Step 14534: Minibatch Loss: 2.103756\n",
            "Step 14536: Minibatch Loss: 2.239786\n",
            "Step 14538: Minibatch Loss: 2.114030\n",
            "Step 14540: Minibatch Loss: 2.194285\n",
            "Step 14542: Minibatch Loss: 2.119281\n",
            "Step 14544: Minibatch Loss: 2.143264\n",
            "Step 14546: Minibatch Loss: 2.173757\n",
            "Step 14548: Minibatch Loss: 2.140705\n",
            "Step 14550: Minibatch Loss: 2.155131\n",
            "Step 14552: Minibatch Loss: 2.167362\n",
            "Step 14554: Minibatch Loss: 2.174136\n",
            "Step 14556: Minibatch Loss: 2.166706\n",
            "Step 14558: Minibatch Loss: 2.220766\n",
            "Step 14560: Minibatch Loss: 2.144185\n",
            "Step 14562: Minibatch Loss: 2.220467\n",
            "Step 14564: Minibatch Loss: 2.123405\n",
            "Step 14566: Minibatch Loss: 2.269892\n",
            "Step 14568: Minibatch Loss: 2.110713\n",
            "Step 14570: Minibatch Loss: 2.252918\n",
            "Step 14572: Minibatch Loss: 2.175132\n",
            "Step 14574: Minibatch Loss: 2.168245\n",
            "Step 14576: Minibatch Loss: 2.222140\n",
            "Step 14578: Minibatch Loss: 2.187886\n",
            "Step 14580: Minibatch Loss: 2.210903\n",
            "Step 14582: Minibatch Loss: 2.152692\n",
            "Step 14584: Minibatch Loss: 2.197646\n",
            "Step 14586: Minibatch Loss: 2.173996\n",
            "Step 14588: Minibatch Loss: 2.161782\n",
            "Step 14590: Minibatch Loss: 2.189615\n",
            "Step 14592: Minibatch Loss: 2.204850\n",
            "Step 14594: Minibatch Loss: 2.146159\n",
            "Step 14596: Minibatch Loss: 2.096873\n",
            "Step 14598: Minibatch Loss: 2.186091\n",
            "Step 14600: Minibatch Loss: 2.102501\n",
            "Step 14602: Minibatch Loss: 2.203051\n",
            "Step 14604: Minibatch Loss: 2.172390\n",
            "Step 14606: Minibatch Loss: 2.129210\n",
            "Step 14608: Minibatch Loss: 2.246641\n",
            "Step 14610: Minibatch Loss: 2.211689\n",
            "Step 14612: Minibatch Loss: 2.222601\n",
            "Step 14614: Minibatch Loss: 2.172828\n",
            "Step 14616: Minibatch Loss: 2.200441\n",
            "Step 14618: Minibatch Loss: 2.231471\n",
            "Step 14620: Minibatch Loss: 2.184514\n",
            "Step 14622: Minibatch Loss: 2.144246\n",
            "Step 14624: Minibatch Loss: 2.210386\n",
            "Step 14626: Minibatch Loss: 2.169764\n",
            "Step 14628: Minibatch Loss: 2.177293\n",
            "Step 14630: Minibatch Loss: 2.183809\n",
            "Step 14632: Minibatch Loss: 2.120796\n",
            "Step 14634: Minibatch Loss: 2.135424\n",
            "Step 14636: Minibatch Loss: 2.261293\n",
            "Step 14638: Minibatch Loss: 2.117803\n",
            "Step 14640: Minibatch Loss: 2.180151\n",
            "Step 14642: Minibatch Loss: 2.175304\n",
            "Step 14644: Minibatch Loss: 2.118862\n",
            "Step 14646: Minibatch Loss: 2.108659\n",
            "Step 14648: Minibatch Loss: 2.183211\n",
            "Step 14650: Minibatch Loss: 2.193246\n",
            "Step 14652: Minibatch Loss: 2.192803\n",
            "Step 14654: Minibatch Loss: 2.193103\n",
            "Step 14656: Minibatch Loss: 2.180435\n",
            "Step 14658: Minibatch Loss: 2.183728\n",
            "Step 14660: Minibatch Loss: 2.150677\n",
            "Step 14662: Minibatch Loss: 2.215176\n",
            "Step 14664: Minibatch Loss: 2.134067\n",
            "Step 14666: Minibatch Loss: 2.266117\n",
            "Step 14668: Minibatch Loss: 2.079503\n",
            "Step 14670: Minibatch Loss: 2.263965\n",
            "Step 14672: Minibatch Loss: 2.156148\n",
            "Step 14674: Minibatch Loss: 2.157231\n",
            "Step 14676: Minibatch Loss: 2.205547\n",
            "Step 14678: Minibatch Loss: 2.198341\n",
            "Step 14680: Minibatch Loss: 2.149235\n",
            "Step 14682: Minibatch Loss: 2.215320\n",
            "Step 14684: Minibatch Loss: 2.176625\n",
            "Step 14686: Minibatch Loss: 2.201562\n",
            "Step 14688: Minibatch Loss: 2.158382\n",
            "Step 14690: Minibatch Loss: 2.195082\n",
            "Step 14692: Minibatch Loss: 2.213117\n",
            "Step 14694: Minibatch Loss: 2.139824\n",
            "Step 14696: Minibatch Loss: 2.172866\n",
            "Step 14698: Minibatch Loss: 2.163769\n",
            "Step 14700: Minibatch Loss: 2.074466\n",
            "Step 14702: Minibatch Loss: 2.210162\n",
            "Step 14704: Minibatch Loss: 2.219093\n",
            "Step 14706: Minibatch Loss: 2.142814\n",
            "Step 14708: Minibatch Loss: 2.189509\n",
            "Step 14710: Minibatch Loss: 2.157426\n",
            "Step 14712: Minibatch Loss: 2.180330\n",
            "Step 14714: Minibatch Loss: 2.183695\n",
            "Step 14716: Minibatch Loss: 2.191529\n",
            "Step 14718: Minibatch Loss: 2.182743\n",
            "Step 14720: Minibatch Loss: 2.223602\n",
            "Step 14722: Minibatch Loss: 2.121169\n",
            "Step 14724: Minibatch Loss: 2.209778\n",
            "Step 14726: Minibatch Loss: 2.232079\n",
            "Step 14728: Minibatch Loss: 2.126385\n",
            "Step 14730: Minibatch Loss: 2.242078\n",
            "Step 14732: Minibatch Loss: 2.127290\n",
            "Step 14734: Minibatch Loss: 2.042271\n",
            "Step 14736: Minibatch Loss: 2.220891\n",
            "Step 14738: Minibatch Loss: 2.110274\n",
            "Step 14740: Minibatch Loss: 2.174251\n",
            "Step 14742: Minibatch Loss: 2.103426\n",
            "Step 14744: Minibatch Loss: 2.140231\n",
            "Step 14746: Minibatch Loss: 2.184699\n",
            "Step 14748: Minibatch Loss: 2.164615\n",
            "Step 14750: Minibatch Loss: 2.201147\n",
            "Step 14752: Minibatch Loss: 2.174227\n",
            "Step 14754: Minibatch Loss: 2.144376\n",
            "Step 14756: Minibatch Loss: 2.183542\n",
            "Step 14758: Minibatch Loss: 2.168751\n",
            "Step 14760: Minibatch Loss: 2.135989\n",
            "Step 14762: Minibatch Loss: 2.235817\n",
            "Step 14764: Minibatch Loss: 2.155643\n",
            "Step 14766: Minibatch Loss: 2.247121\n",
            "Step 14768: Minibatch Loss: 2.112188\n",
            "Step 14770: Minibatch Loss: 2.138898\n",
            "Step 14772: Minibatch Loss: 2.218201\n",
            "Step 14774: Minibatch Loss: 2.157140\n",
            "Step 14776: Minibatch Loss: 2.153874\n",
            "Step 14778: Minibatch Loss: 2.160813\n",
            "Step 14780: Minibatch Loss: 2.182232\n",
            "Step 14782: Minibatch Loss: 2.199618\n",
            "Step 14784: Minibatch Loss: 2.179434\n",
            "Step 14786: Minibatch Loss: 2.135278\n",
            "Step 14788: Minibatch Loss: 2.234267\n",
            "Step 14790: Minibatch Loss: 2.222637\n",
            "Step 14792: Minibatch Loss: 2.195167\n",
            "Step 14794: Minibatch Loss: 2.158259\n",
            "Step 14796: Minibatch Loss: 2.103268\n",
            "Step 14798: Minibatch Loss: 2.131179\n",
            "Step 14800: Minibatch Loss: 2.093277\n",
            "Step 14802: Minibatch Loss: 2.205596\n",
            "Step 14804: Minibatch Loss: 2.192810\n",
            "Step 14806: Minibatch Loss: 2.182745\n",
            "Step 14808: Minibatch Loss: 2.187212\n",
            "Step 14810: Minibatch Loss: 2.143413\n",
            "Step 14812: Minibatch Loss: 2.232056\n",
            "Step 14814: Minibatch Loss: 2.200346\n",
            "Step 14816: Minibatch Loss: 2.129089\n",
            "Step 14818: Minibatch Loss: 2.232846\n",
            "Step 14820: Minibatch Loss: 2.235757\n",
            "Step 14822: Minibatch Loss: 2.172405\n",
            "Step 14824: Minibatch Loss: 2.166552\n",
            "Step 14826: Minibatch Loss: 2.177465\n",
            "Step 14828: Minibatch Loss: 2.145780\n",
            "Step 14830: Minibatch Loss: 2.180438\n",
            "Step 14832: Minibatch Loss: 2.088276\n",
            "Step 14834: Minibatch Loss: 2.175816\n",
            "Step 14836: Minibatch Loss: 2.273754\n",
            "Step 14838: Minibatch Loss: 2.120344\n",
            "Step 14840: Minibatch Loss: 2.142504\n",
            "Step 14842: Minibatch Loss: 2.129297\n",
            "Step 14844: Minibatch Loss: 2.157875\n",
            "Step 14846: Minibatch Loss: 2.150733\n",
            "Step 14848: Minibatch Loss: 2.134766\n",
            "Step 14850: Minibatch Loss: 2.180526\n",
            "Step 14852: Minibatch Loss: 2.192039\n",
            "Step 14854: Minibatch Loss: 2.157732\n",
            "Step 14856: Minibatch Loss: 2.121034\n",
            "Step 14858: Minibatch Loss: 2.166823\n",
            "Step 14860: Minibatch Loss: 2.126232\n",
            "Step 14862: Minibatch Loss: 2.172894\n",
            "Step 14864: Minibatch Loss: 2.071592\n",
            "Step 14866: Minibatch Loss: 2.174732\n",
            "Step 14868: Minibatch Loss: 2.104351\n",
            "Step 14870: Minibatch Loss: 2.234421\n",
            "Step 14872: Minibatch Loss: 2.178442\n",
            "Step 14874: Minibatch Loss: 2.177505\n",
            "Step 14876: Minibatch Loss: 2.161138\n",
            "Step 14878: Minibatch Loss: 2.185357\n",
            "Step 14880: Minibatch Loss: 2.100864\n",
            "Step 14882: Minibatch Loss: 2.208846\n",
            "Step 14884: Minibatch Loss: 2.168185\n",
            "Step 14886: Minibatch Loss: 2.160827\n",
            "Step 14888: Minibatch Loss: 2.158153\n",
            "Step 14890: Minibatch Loss: 2.193959\n",
            "Step 14892: Minibatch Loss: 2.152799\n",
            "Step 14894: Minibatch Loss: 2.229589\n",
            "Step 14896: Minibatch Loss: 2.083687\n",
            "Step 14898: Minibatch Loss: 2.161985\n",
            "Step 14900: Minibatch Loss: 2.174113\n",
            "Step 14902: Minibatch Loss: 2.203936\n",
            "Step 14904: Minibatch Loss: 2.223716\n",
            "Step 14906: Minibatch Loss: 2.072663\n",
            "Step 14908: Minibatch Loss: 2.151650\n",
            "Step 14910: Minibatch Loss: 2.123736\n",
            "Step 14912: Minibatch Loss: 2.179664\n",
            "Step 14914: Minibatch Loss: 2.240933\n",
            "Step 14916: Minibatch Loss: 2.171407\n",
            "Step 14918: Minibatch Loss: 2.211046\n",
            "Step 14920: Minibatch Loss: 2.204630\n",
            "Step 14922: Minibatch Loss: 2.154647\n",
            "Step 14924: Minibatch Loss: 2.191615\n",
            "Step 14926: Minibatch Loss: 2.198546\n",
            "Step 14928: Minibatch Loss: 2.160461\n",
            "Step 14930: Minibatch Loss: 2.220656\n",
            "Step 14932: Minibatch Loss: 2.128726\n",
            "Step 14934: Minibatch Loss: 2.145874\n",
            "Step 14936: Minibatch Loss: 2.265116\n",
            "Step 14938: Minibatch Loss: 2.161896\n",
            "Step 14940: Minibatch Loss: 2.203663\n",
            "Step 14942: Minibatch Loss: 2.096584\n",
            "Step 14944: Minibatch Loss: 2.225558\n",
            "Step 14946: Minibatch Loss: 2.104353\n",
            "Step 14948: Minibatch Loss: 2.101583\n",
            "Step 14950: Minibatch Loss: 2.177462\n",
            "Step 14952: Minibatch Loss: 2.168057\n",
            "Step 14954: Minibatch Loss: 2.198640\n",
            "Step 14956: Minibatch Loss: 2.173041\n",
            "Step 14958: Minibatch Loss: 2.188493\n",
            "Step 14960: Minibatch Loss: 2.104462\n",
            "Step 14962: Minibatch Loss: 2.242018\n",
            "Step 14964: Minibatch Loss: 2.138702\n",
            "Step 14966: Minibatch Loss: 2.246083\n",
            "Step 14968: Minibatch Loss: 2.081576\n",
            "Step 14970: Minibatch Loss: 2.196063\n",
            "Step 14972: Minibatch Loss: 2.139061\n",
            "Step 14974: Minibatch Loss: 2.145996\n",
            "Step 14976: Minibatch Loss: 2.232822\n",
            "Step 14978: Minibatch Loss: 2.124028\n",
            "Step 14980: Minibatch Loss: 2.152033\n",
            "Step 14982: Minibatch Loss: 2.192543\n",
            "Step 14984: Minibatch Loss: 2.199552\n",
            "Step 14986: Minibatch Loss: 2.156531\n",
            "Step 14988: Minibatch Loss: 2.154773\n",
            "Step 14990: Minibatch Loss: 2.133472\n",
            "Step 14992: Minibatch Loss: 2.184201\n",
            "Step 14994: Minibatch Loss: 2.140026\n",
            "Step 14996: Minibatch Loss: 2.096473\n",
            "Step 14998: Minibatch Loss: 2.180114\n",
            "Step 15000: Minibatch Loss: 2.093244\n",
            "Training for SNR= 7.5  sigma= 0.44668359215096315 iteratin: 0\n",
            "Step 15002: Minibatch Loss: 2.187567\n",
            "Step 15004: Minibatch Loss: 2.216701\n",
            "Step 15006: Minibatch Loss: 2.164588\n",
            "Step 15008: Minibatch Loss: 2.158549\n",
            "Step 15010: Minibatch Loss: 2.130114\n",
            "Step 15012: Minibatch Loss: 2.161186\n",
            "Step 15014: Minibatch Loss: 2.126729\n",
            "Step 15016: Minibatch Loss: 2.126948\n",
            "Step 15018: Minibatch Loss: 2.171192\n",
            "Step 15020: Minibatch Loss: 2.179404\n",
            "Step 15022: Minibatch Loss: 2.137588\n",
            "Step 15024: Minibatch Loss: 2.166390\n",
            "Step 15026: Minibatch Loss: 2.216127\n",
            "Step 15028: Minibatch Loss: 2.184911\n",
            "Step 15030: Minibatch Loss: 2.170454\n",
            "Step 15032: Minibatch Loss: 2.124766\n",
            "Step 15034: Minibatch Loss: 2.118265\n",
            "Step 15036: Minibatch Loss: 2.249564\n",
            "Step 15038: Minibatch Loss: 2.120508\n",
            "Step 15040: Minibatch Loss: 2.161795\n",
            "Step 15042: Minibatch Loss: 2.111507\n",
            "Step 15044: Minibatch Loss: 2.134737\n",
            "Step 15046: Minibatch Loss: 2.191994\n",
            "Step 15048: Minibatch Loss: 2.135853\n",
            "Step 15050: Minibatch Loss: 2.130183\n",
            "Step 15052: Minibatch Loss: 2.127700\n",
            "Step 15054: Minibatch Loss: 2.145094\n",
            "Step 15056: Minibatch Loss: 2.180513\n",
            "Step 15058: Minibatch Loss: 2.122816\n",
            "Step 15060: Minibatch Loss: 2.149917\n",
            "Step 15062: Minibatch Loss: 2.166266\n",
            "Step 15064: Minibatch Loss: 2.076175\n",
            "Step 15066: Minibatch Loss: 2.246422\n",
            "Step 15068: Minibatch Loss: 2.097636\n",
            "Step 15070: Minibatch Loss: 2.169903\n",
            "Step 15072: Minibatch Loss: 2.143779\n",
            "Step 15074: Minibatch Loss: 2.136255\n",
            "Step 15076: Minibatch Loss: 2.136187\n",
            "Step 15078: Minibatch Loss: 2.132213\n",
            "Step 15080: Minibatch Loss: 2.171664\n",
            "Step 15082: Minibatch Loss: 2.176750\n",
            "Step 15084: Minibatch Loss: 2.128119\n",
            "Step 15086: Minibatch Loss: 2.168870\n",
            "Step 15088: Minibatch Loss: 2.136533\n",
            "Step 15090: Minibatch Loss: 2.169836\n",
            "Step 15092: Minibatch Loss: 2.165185\n",
            "Step 15094: Minibatch Loss: 2.145405\n",
            "Step 15096: Minibatch Loss: 2.091874\n",
            "Step 15098: Minibatch Loss: 2.206907\n",
            "Step 15100: Minibatch Loss: 2.096410\n",
            "Step 15102: Minibatch Loss: 2.210456\n",
            "Step 15104: Minibatch Loss: 2.179210\n",
            "Step 15106: Minibatch Loss: 2.097826\n",
            "Step 15108: Minibatch Loss: 2.180563\n",
            "Step 15110: Minibatch Loss: 2.134363\n",
            "Step 15112: Minibatch Loss: 2.245526\n",
            "Step 15114: Minibatch Loss: 2.238113\n",
            "Step 15116: Minibatch Loss: 2.130285\n",
            "Step 15118: Minibatch Loss: 2.202441\n",
            "Step 15120: Minibatch Loss: 2.158962\n",
            "Step 15122: Minibatch Loss: 2.116675\n",
            "Step 15124: Minibatch Loss: 2.124301\n",
            "Step 15126: Minibatch Loss: 2.139190\n",
            "Step 15128: Minibatch Loss: 2.120058\n",
            "Step 15130: Minibatch Loss: 2.146914\n",
            "Step 15132: Minibatch Loss: 2.104504\n",
            "Step 15134: Minibatch Loss: 2.070868\n",
            "Step 15136: Minibatch Loss: 2.193558\n",
            "Step 15138: Minibatch Loss: 2.154441\n",
            "Step 15140: Minibatch Loss: 2.222261\n",
            "Step 15142: Minibatch Loss: 2.122246\n",
            "Step 15144: Minibatch Loss: 2.224009\n",
            "Step 15146: Minibatch Loss: 2.151548\n",
            "Step 15148: Minibatch Loss: 2.183033\n",
            "Step 15150: Minibatch Loss: 2.177318\n",
            "Step 15152: Minibatch Loss: 2.147205\n",
            "Step 15154: Minibatch Loss: 2.104658\n",
            "Step 15156: Minibatch Loss: 2.220894\n",
            "Step 15158: Minibatch Loss: 2.140069\n",
            "Step 15160: Minibatch Loss: 2.116010\n",
            "Step 15162: Minibatch Loss: 2.235837\n",
            "Step 15164: Minibatch Loss: 2.154572\n",
            "Step 15166: Minibatch Loss: 2.178103\n",
            "Step 15168: Minibatch Loss: 2.095524\n",
            "Step 15170: Minibatch Loss: 2.194472\n",
            "Step 15172: Minibatch Loss: 2.206630\n",
            "Step 15174: Minibatch Loss: 2.150526\n",
            "Step 15176: Minibatch Loss: 2.171286\n",
            "Step 15178: Minibatch Loss: 2.163254\n",
            "Step 15180: Minibatch Loss: 2.136997\n",
            "Step 15182: Minibatch Loss: 2.169786\n",
            "Step 15184: Minibatch Loss: 2.140012\n",
            "Step 15186: Minibatch Loss: 2.206530\n",
            "Step 15188: Minibatch Loss: 2.136979\n",
            "Step 15190: Minibatch Loss: 2.148847\n",
            "Step 15192: Minibatch Loss: 2.176929\n",
            "Step 15194: Minibatch Loss: 2.154779\n",
            "Step 15196: Minibatch Loss: 2.094254\n",
            "Step 15198: Minibatch Loss: 2.176490\n",
            "Step 15200: Minibatch Loss: 2.106130\n",
            "Step 15202: Minibatch Loss: 2.230188\n",
            "Step 15204: Minibatch Loss: 2.214970\n",
            "Step 15206: Minibatch Loss: 2.154276\n",
            "Step 15208: Minibatch Loss: 2.223394\n",
            "Step 15210: Minibatch Loss: 2.142395\n",
            "Step 15212: Minibatch Loss: 2.154727\n",
            "Step 15214: Minibatch Loss: 2.163701\n",
            "Step 15216: Minibatch Loss: 2.143012\n",
            "Step 15218: Minibatch Loss: 2.171862\n",
            "Step 15220: Minibatch Loss: 2.166971\n",
            "Step 15222: Minibatch Loss: 2.157025\n",
            "Step 15224: Minibatch Loss: 2.179640\n",
            "Step 15226: Minibatch Loss: 2.156564\n",
            "Step 15228: Minibatch Loss: 2.127797\n",
            "Step 15230: Minibatch Loss: 2.132117\n",
            "Step 15232: Minibatch Loss: 2.094003\n",
            "Step 15234: Minibatch Loss: 2.143388\n",
            "Step 15236: Minibatch Loss: 2.199773\n",
            "Step 15238: Minibatch Loss: 2.069154\n",
            "Step 15240: Minibatch Loss: 2.148697\n",
            "Step 15242: Minibatch Loss: 2.096817\n",
            "Step 15244: Minibatch Loss: 2.135136\n",
            "Step 15246: Minibatch Loss: 2.123398\n",
            "Step 15248: Minibatch Loss: 2.117334\n",
            "Step 15250: Minibatch Loss: 2.097653\n",
            "Step 15252: Minibatch Loss: 2.144156\n",
            "Step 15254: Minibatch Loss: 2.146124\n",
            "Step 15256: Minibatch Loss: 2.205144\n",
            "Step 15258: Minibatch Loss: 2.140997\n",
            "Step 15260: Minibatch Loss: 2.101411\n",
            "Step 15262: Minibatch Loss: 2.246194\n",
            "Step 15264: Minibatch Loss: 2.062492\n",
            "Step 15266: Minibatch Loss: 2.217448\n",
            "Step 15268: Minibatch Loss: 2.074689\n",
            "Step 15270: Minibatch Loss: 2.164069\n",
            "Step 15272: Minibatch Loss: 2.135054\n",
            "Step 15274: Minibatch Loss: 2.108460\n",
            "Step 15276: Minibatch Loss: 2.192928\n",
            "Step 15278: Minibatch Loss: 2.126672\n",
            "Step 15280: Minibatch Loss: 2.202261\n",
            "Step 15282: Minibatch Loss: 2.165700\n",
            "Step 15284: Minibatch Loss: 2.145964\n",
            "Step 15286: Minibatch Loss: 2.151511\n",
            "Step 15288: Minibatch Loss: 2.162169\n",
            "Step 15290: Minibatch Loss: 2.194683\n",
            "Step 15292: Minibatch Loss: 2.170587\n",
            "Step 15294: Minibatch Loss: 2.175343\n",
            "Step 15296: Minibatch Loss: 2.132250\n",
            "Step 15298: Minibatch Loss: 2.163692\n",
            "Step 15300: Minibatch Loss: 2.050748\n",
            "Step 15302: Minibatch Loss: 2.185643\n",
            "Step 15304: Minibatch Loss: 2.121933\n",
            "Step 15306: Minibatch Loss: 2.147300\n",
            "Step 15308: Minibatch Loss: 2.212223\n",
            "Step 15310: Minibatch Loss: 2.171742\n",
            "Step 15312: Minibatch Loss: 2.219072\n",
            "Step 15314: Minibatch Loss: 2.207424\n",
            "Step 15316: Minibatch Loss: 2.137365\n",
            "Step 15318: Minibatch Loss: 2.144461\n",
            "Step 15320: Minibatch Loss: 2.202917\n",
            "Step 15322: Minibatch Loss: 2.193243\n",
            "Step 15324: Minibatch Loss: 2.159153\n",
            "Step 15326: Minibatch Loss: 2.181956\n",
            "Step 15328: Minibatch Loss: 2.123595\n",
            "Step 15330: Minibatch Loss: 2.214377\n",
            "Step 15332: Minibatch Loss: 2.174442\n",
            "Step 15334: Minibatch Loss: 2.116740\n",
            "Step 15336: Minibatch Loss: 2.149636\n",
            "Step 15338: Minibatch Loss: 2.104573\n",
            "Step 15340: Minibatch Loss: 2.146843\n",
            "Step 15342: Minibatch Loss: 2.059064\n",
            "Step 15344: Minibatch Loss: 2.184451\n",
            "Step 15346: Minibatch Loss: 2.163025\n",
            "Step 15348: Minibatch Loss: 2.174623\n",
            "Step 15350: Minibatch Loss: 2.103760\n",
            "Step 15352: Minibatch Loss: 2.140229\n",
            "Step 15354: Minibatch Loss: 2.137241\n",
            "Step 15356: Minibatch Loss: 2.136163\n",
            "Step 15358: Minibatch Loss: 2.173013\n",
            "Step 15360: Minibatch Loss: 2.063304\n",
            "Step 15362: Minibatch Loss: 2.200157\n",
            "Step 15364: Minibatch Loss: 2.101951\n",
            "Step 15366: Minibatch Loss: 2.135865\n",
            "Step 15368: Minibatch Loss: 2.060418\n",
            "Step 15370: Minibatch Loss: 2.189170\n",
            "Step 15372: Minibatch Loss: 2.134987\n",
            "Step 15374: Minibatch Loss: 2.169138\n",
            "Step 15376: Minibatch Loss: 2.133591\n",
            "Step 15378: Minibatch Loss: 2.153894\n",
            "Step 15380: Minibatch Loss: 2.103795\n",
            "Step 15382: Minibatch Loss: 2.181233\n",
            "Step 15384: Minibatch Loss: 2.125199\n",
            "Step 15386: Minibatch Loss: 2.133292\n",
            "Step 15388: Minibatch Loss: 2.105747\n",
            "Step 15390: Minibatch Loss: 2.197856\n",
            "Step 15392: Minibatch Loss: 2.148431\n",
            "Step 15394: Minibatch Loss: 2.064501\n",
            "Step 15396: Minibatch Loss: 2.121100\n",
            "Step 15398: Minibatch Loss: 2.128038\n",
            "Step 15400: Minibatch Loss: 2.101402\n",
            "Step 15402: Minibatch Loss: 2.141475\n",
            "Step 15404: Minibatch Loss: 2.176037\n",
            "Step 15406: Minibatch Loss: 2.084924\n",
            "Step 15408: Minibatch Loss: 2.129463\n",
            "Step 15410: Minibatch Loss: 2.138485\n",
            "Step 15412: Minibatch Loss: 2.266150\n",
            "Step 15414: Minibatch Loss: 2.241940\n",
            "Step 15416: Minibatch Loss: 2.143133\n",
            "Step 15418: Minibatch Loss: 2.218798\n",
            "Step 15420: Minibatch Loss: 2.156669\n",
            "Step 15422: Minibatch Loss: 2.184687\n",
            "Step 15424: Minibatch Loss: 2.190056\n",
            "Step 15426: Minibatch Loss: 2.160853\n",
            "Step 15428: Minibatch Loss: 2.211045\n",
            "Step 15430: Minibatch Loss: 2.148762\n",
            "Step 15432: Minibatch Loss: 2.117666\n",
            "Step 15434: Minibatch Loss: 2.122032\n",
            "Step 15436: Minibatch Loss: 2.140695\n",
            "Step 15438: Minibatch Loss: 2.082156\n",
            "Step 15440: Minibatch Loss: 2.144867\n",
            "Step 15442: Minibatch Loss: 2.066692\n",
            "Step 15444: Minibatch Loss: 2.202034\n",
            "Step 15446: Minibatch Loss: 2.172561\n",
            "Step 15448: Minibatch Loss: 2.114259\n",
            "Step 15450: Minibatch Loss: 2.124102\n",
            "Step 15452: Minibatch Loss: 2.143113\n",
            "Step 15454: Minibatch Loss: 2.157481\n",
            "Step 15456: Minibatch Loss: 2.159016\n",
            "Step 15458: Minibatch Loss: 2.124349\n",
            "Step 15460: Minibatch Loss: 2.146638\n",
            "Step 15462: Minibatch Loss: 2.167250\n",
            "Step 15464: Minibatch Loss: 2.096684\n",
            "Step 15466: Minibatch Loss: 2.207713\n",
            "Step 15468: Minibatch Loss: 2.032954\n",
            "Step 15470: Minibatch Loss: 2.185121\n",
            "Step 15472: Minibatch Loss: 2.200691\n",
            "Step 15474: Minibatch Loss: 2.192626\n",
            "Step 15476: Minibatch Loss: 2.126441\n",
            "Step 15478: Minibatch Loss: 2.097664\n",
            "Step 15480: Minibatch Loss: 2.127942\n",
            "Step 15482: Minibatch Loss: 2.195898\n",
            "Step 15484: Minibatch Loss: 2.127367\n",
            "Step 15486: Minibatch Loss: 2.135263\n",
            "Step 15488: Minibatch Loss: 2.155698\n",
            "Step 15490: Minibatch Loss: 2.158007\n",
            "Step 15492: Minibatch Loss: 2.215393\n",
            "Step 15494: Minibatch Loss: 2.148349\n",
            "Step 15496: Minibatch Loss: 2.068691\n",
            "Step 15498: Minibatch Loss: 2.183063\n",
            "Step 15500: Minibatch Loss: 2.142085\n",
            "Step 15502: Minibatch Loss: 2.152631\n",
            "Step 15504: Minibatch Loss: 2.194582\n",
            "Step 15506: Minibatch Loss: 2.135726\n",
            "Step 15508: Minibatch Loss: 2.125986\n",
            "Step 15510: Minibatch Loss: 2.125047\n",
            "Step 15512: Minibatch Loss: 2.216979\n",
            "Step 15514: Minibatch Loss: 2.164290\n",
            "Step 15516: Minibatch Loss: 2.213101\n",
            "Step 15518: Minibatch Loss: 2.191737\n",
            "Step 15520: Minibatch Loss: 2.203591\n",
            "Step 15522: Minibatch Loss: 2.111489\n",
            "Step 15524: Minibatch Loss: 2.203793\n",
            "Step 15526: Minibatch Loss: 2.167876\n",
            "Step 15528: Minibatch Loss: 2.141125\n",
            "Step 15530: Minibatch Loss: 2.143501\n",
            "Step 15532: Minibatch Loss: 2.054521\n",
            "Step 15534: Minibatch Loss: 2.105871\n",
            "Step 15536: Minibatch Loss: 2.202034\n",
            "Step 15538: Minibatch Loss: 2.073361\n",
            "Step 15540: Minibatch Loss: 2.181852\n",
            "Step 15542: Minibatch Loss: 2.112918\n",
            "Step 15544: Minibatch Loss: 2.151619\n",
            "Step 15546: Minibatch Loss: 2.095275\n",
            "Step 15548: Minibatch Loss: 2.085064\n",
            "Step 15550: Minibatch Loss: 2.137795\n",
            "Step 15552: Minibatch Loss: 2.201464\n",
            "Step 15554: Minibatch Loss: 2.090749\n",
            "Step 15556: Minibatch Loss: 2.166689\n",
            "Step 15558: Minibatch Loss: 2.159167\n",
            "Step 15560: Minibatch Loss: 2.124393\n",
            "Step 15562: Minibatch Loss: 2.211751\n",
            "Step 15564: Minibatch Loss: 2.074669\n",
            "Step 15566: Minibatch Loss: 2.182267\n",
            "Step 15568: Minibatch Loss: 2.047702\n",
            "Step 15570: Minibatch Loss: 2.151030\n",
            "Step 15572: Minibatch Loss: 2.191460\n",
            "Step 15574: Minibatch Loss: 2.161869\n",
            "Step 15576: Minibatch Loss: 2.141994\n",
            "Step 15578: Minibatch Loss: 2.088746\n",
            "Step 15580: Minibatch Loss: 2.191993\n",
            "Step 15582: Minibatch Loss: 2.176368\n",
            "Step 15584: Minibatch Loss: 2.176428\n",
            "Step 15586: Minibatch Loss: 2.150379\n",
            "Step 15588: Minibatch Loss: 2.145080\n",
            "Step 15590: Minibatch Loss: 2.147497\n",
            "Step 15592: Minibatch Loss: 2.214314\n",
            "Step 15594: Minibatch Loss: 2.148930\n",
            "Step 15596: Minibatch Loss: 2.128965\n",
            "Step 15598: Minibatch Loss: 2.176253\n",
            "Step 15600: Minibatch Loss: 2.091960\n",
            "Step 15602: Minibatch Loss: 2.240558\n",
            "Step 15604: Minibatch Loss: 2.193134\n",
            "Step 15606: Minibatch Loss: 2.133288\n",
            "Step 15608: Minibatch Loss: 2.092654\n",
            "Step 15610: Minibatch Loss: 2.117043\n",
            "Step 15612: Minibatch Loss: 2.186236\n",
            "Step 15614: Minibatch Loss: 2.196913\n",
            "Step 15616: Minibatch Loss: 2.197750\n",
            "Step 15618: Minibatch Loss: 2.102384\n",
            "Step 15620: Minibatch Loss: 2.232643\n",
            "Step 15622: Minibatch Loss: 2.138806\n",
            "Step 15624: Minibatch Loss: 2.185648\n",
            "Step 15626: Minibatch Loss: 2.165298\n",
            "Step 15628: Minibatch Loss: 2.137828\n",
            "Step 15630: Minibatch Loss: 2.124906\n",
            "Step 15632: Minibatch Loss: 2.105747\n",
            "Step 15634: Minibatch Loss: 2.160030\n",
            "Step 15636: Minibatch Loss: 2.163688\n",
            "Step 15638: Minibatch Loss: 2.073883\n",
            "Step 15640: Minibatch Loss: 2.127398\n",
            "Step 15642: Minibatch Loss: 2.102365\n",
            "Step 15644: Minibatch Loss: 2.144895\n",
            "Step 15646: Minibatch Loss: 2.140843\n",
            "Step 15648: Minibatch Loss: 2.134125\n",
            "Step 15650: Minibatch Loss: 2.113400\n",
            "Step 15652: Minibatch Loss: 2.115822\n",
            "Step 15654: Minibatch Loss: 2.142104\n",
            "Step 15656: Minibatch Loss: 2.143739\n",
            "Step 15658: Minibatch Loss: 2.132557\n",
            "Step 15660: Minibatch Loss: 2.084538\n",
            "Step 15662: Minibatch Loss: 2.124610\n",
            "Step 15664: Minibatch Loss: 2.030698\n",
            "Step 15666: Minibatch Loss: 2.143562\n",
            "Step 15668: Minibatch Loss: 2.029570\n",
            "Step 15670: Minibatch Loss: 2.133880\n",
            "Step 15672: Minibatch Loss: 2.155612\n",
            "Step 15674: Minibatch Loss: 2.179138\n",
            "Step 15676: Minibatch Loss: 2.131081\n",
            "Step 15678: Minibatch Loss: 2.104658\n",
            "Step 15680: Minibatch Loss: 2.150350\n",
            "Step 15682: Minibatch Loss: 2.174827\n",
            "Step 15684: Minibatch Loss: 2.158013\n",
            "Step 15686: Minibatch Loss: 2.125270\n",
            "Step 15688: Minibatch Loss: 2.179659\n",
            "Step 15690: Minibatch Loss: 2.180809\n",
            "Step 15692: Minibatch Loss: 2.153235\n",
            "Step 15694: Minibatch Loss: 2.096546\n",
            "Step 15696: Minibatch Loss: 2.116162\n",
            "Step 15698: Minibatch Loss: 2.121243\n",
            "Step 15700: Minibatch Loss: 2.099566\n",
            "Step 15702: Minibatch Loss: 2.180937\n",
            "Step 15704: Minibatch Loss: 2.172120\n",
            "Step 15706: Minibatch Loss: 2.115742\n",
            "Step 15708: Minibatch Loss: 2.154451\n",
            "Step 15710: Minibatch Loss: 2.069959\n",
            "Step 15712: Minibatch Loss: 2.144134\n",
            "Step 15714: Minibatch Loss: 2.147427\n",
            "Step 15716: Minibatch Loss: 2.108356\n",
            "Step 15718: Minibatch Loss: 2.153746\n",
            "Step 15720: Minibatch Loss: 2.221508\n",
            "Step 15722: Minibatch Loss: 2.049619\n",
            "Step 15724: Minibatch Loss: 2.133677\n",
            "Step 15726: Minibatch Loss: 2.205326\n",
            "Step 15728: Minibatch Loss: 2.122842\n",
            "Step 15730: Minibatch Loss: 2.183517\n",
            "Step 15732: Minibatch Loss: 2.081900\n",
            "Step 15734: Minibatch Loss: 2.073632\n",
            "Step 15736: Minibatch Loss: 2.204502\n",
            "Step 15738: Minibatch Loss: 2.100973\n",
            "Step 15740: Minibatch Loss: 2.131843\n",
            "Step 15742: Minibatch Loss: 2.110086\n",
            "Step 15744: Minibatch Loss: 2.118682\n",
            "Step 15746: Minibatch Loss: 2.102222\n",
            "Step 15748: Minibatch Loss: 2.124478\n",
            "Step 15750: Minibatch Loss: 2.103140\n",
            "Step 15752: Minibatch Loss: 2.111360\n",
            "Step 15754: Minibatch Loss: 2.194448\n",
            "Step 15756: Minibatch Loss: 2.238138\n",
            "Step 15758: Minibatch Loss: 2.187652\n",
            "Step 15760: Minibatch Loss: 2.110221\n",
            "Step 15762: Minibatch Loss: 2.144893\n",
            "Step 15764: Minibatch Loss: 2.047823\n",
            "Step 15766: Minibatch Loss: 2.164444\n",
            "Step 15768: Minibatch Loss: 2.069119\n",
            "Step 15770: Minibatch Loss: 2.170203\n",
            "Step 15772: Minibatch Loss: 2.067617\n",
            "Step 15774: Minibatch Loss: 2.114661\n",
            "Step 15776: Minibatch Loss: 2.062411\n",
            "Step 15778: Minibatch Loss: 2.199823\n",
            "Step 15780: Minibatch Loss: 2.152878\n",
            "Step 15782: Minibatch Loss: 2.135165\n",
            "Step 15784: Minibatch Loss: 2.165603\n",
            "Step 15786: Minibatch Loss: 2.095652\n",
            "Step 15788: Minibatch Loss: 2.085839\n",
            "Step 15790: Minibatch Loss: 2.213986\n",
            "Step 15792: Minibatch Loss: 2.153145\n",
            "Step 15794: Minibatch Loss: 2.129686\n",
            "Step 15796: Minibatch Loss: 2.131768\n",
            "Step 15798: Minibatch Loss: 2.148731\n",
            "Step 15800: Minibatch Loss: 2.075842\n",
            "Step 15802: Minibatch Loss: 2.164075\n",
            "Step 15804: Minibatch Loss: 2.156073\n",
            "Step 15806: Minibatch Loss: 2.100435\n",
            "Step 15808: Minibatch Loss: 2.112326\n",
            "Step 15810: Minibatch Loss: 2.080239\n",
            "Step 15812: Minibatch Loss: 2.120728\n",
            "Step 15814: Minibatch Loss: 2.115925\n",
            "Step 15816: Minibatch Loss: 2.134199\n",
            "Step 15818: Minibatch Loss: 2.225198\n",
            "Step 15820: Minibatch Loss: 2.160035\n",
            "Step 15822: Minibatch Loss: 2.057979\n",
            "Step 15824: Minibatch Loss: 2.101354\n",
            "Step 15826: Minibatch Loss: 2.135917\n",
            "Step 15828: Minibatch Loss: 2.180743\n",
            "Step 15830: Minibatch Loss: 2.151927\n",
            "Step 15832: Minibatch Loss: 2.103651\n",
            "Step 15834: Minibatch Loss: 2.108232\n",
            "Step 15836: Minibatch Loss: 2.188444\n",
            "Step 15838: Minibatch Loss: 2.113258\n",
            "Step 15840: Minibatch Loss: 2.187190\n",
            "Step 15842: Minibatch Loss: 2.094618\n",
            "Step 15844: Minibatch Loss: 2.120140\n",
            "Step 15846: Minibatch Loss: 2.083844\n",
            "Step 15848: Minibatch Loss: 2.122774\n",
            "Step 15850: Minibatch Loss: 2.150617\n",
            "Step 15852: Minibatch Loss: 2.100011\n",
            "Step 15854: Minibatch Loss: 2.082455\n",
            "Step 15856: Minibatch Loss: 2.215077\n",
            "Step 15858: Minibatch Loss: 2.124902\n",
            "Step 15860: Minibatch Loss: 2.125151\n",
            "Step 15862: Minibatch Loss: 2.127200\n",
            "Step 15864: Minibatch Loss: 2.093133\n",
            "Step 15866: Minibatch Loss: 2.187528\n",
            "Step 15868: Minibatch Loss: 2.042457\n",
            "Step 15870: Minibatch Loss: 2.136157\n",
            "Step 15872: Minibatch Loss: 2.165266\n",
            "Step 15874: Minibatch Loss: 2.174420\n",
            "Step 15876: Minibatch Loss: 2.099286\n",
            "Step 15878: Minibatch Loss: 2.107663\n",
            "Step 15880: Minibatch Loss: 2.170955\n",
            "Step 15882: Minibatch Loss: 2.156831\n",
            "Step 15884: Minibatch Loss: 2.130733\n",
            "Step 15886: Minibatch Loss: 2.113818\n",
            "Step 15888: Minibatch Loss: 2.128013\n",
            "Step 15890: Minibatch Loss: 2.120528\n",
            "Step 15892: Minibatch Loss: 2.169935\n",
            "Step 15894: Minibatch Loss: 2.103852\n",
            "Step 15896: Minibatch Loss: 2.040576\n",
            "Step 15898: Minibatch Loss: 2.151843\n",
            "Step 15900: Minibatch Loss: 2.135217\n",
            "Step 15902: Minibatch Loss: 2.190736\n",
            "Step 15904: Minibatch Loss: 2.093140\n",
            "Step 15906: Minibatch Loss: 2.101358\n",
            "Step 15908: Minibatch Loss: 2.111930\n",
            "Step 15910: Minibatch Loss: 2.090366\n",
            "Step 15912: Minibatch Loss: 2.187834\n",
            "Step 15914: Minibatch Loss: 2.170837\n",
            "Step 15916: Minibatch Loss: 2.112690\n",
            "Step 15918: Minibatch Loss: 2.176219\n",
            "Step 15920: Minibatch Loss: 2.152184\n",
            "Step 15922: Minibatch Loss: 2.060752\n",
            "Step 15924: Minibatch Loss: 2.155113\n",
            "Step 15926: Minibatch Loss: 2.119835\n",
            "Step 15928: Minibatch Loss: 2.099714\n",
            "Step 15930: Minibatch Loss: 2.164838\n",
            "Step 15932: Minibatch Loss: 2.101369\n",
            "Step 15934: Minibatch Loss: 2.063047\n",
            "Step 15936: Minibatch Loss: 2.248053\n",
            "Step 15938: Minibatch Loss: 2.017390\n",
            "Step 15940: Minibatch Loss: 2.124977\n",
            "Step 15942: Minibatch Loss: 2.078071\n",
            "Step 15944: Minibatch Loss: 2.129988\n",
            "Step 15946: Minibatch Loss: 2.172822\n",
            "Step 15948: Minibatch Loss: 2.042613\n",
            "Step 15950: Minibatch Loss: 2.127589\n",
            "Step 15952: Minibatch Loss: 2.007214\n",
            "Step 15954: Minibatch Loss: 2.142129\n",
            "Step 15956: Minibatch Loss: 2.176072\n",
            "Step 15958: Minibatch Loss: 2.160947\n",
            "Step 15960: Minibatch Loss: 2.095329\n",
            "Step 15962: Minibatch Loss: 2.157618\n",
            "Step 15964: Minibatch Loss: 2.134398\n",
            "Step 15966: Minibatch Loss: 2.182981\n",
            "Step 15968: Minibatch Loss: 2.115311\n",
            "Step 15970: Minibatch Loss: 2.266994\n",
            "Step 15972: Minibatch Loss: 2.117008\n",
            "Step 15974: Minibatch Loss: 2.104592\n",
            "Step 15976: Minibatch Loss: 2.083476\n",
            "Step 15978: Minibatch Loss: 2.145012\n",
            "Step 15980: Minibatch Loss: 2.086335\n",
            "Step 15982: Minibatch Loss: 2.118455\n",
            "Step 15984: Minibatch Loss: 2.184983\n",
            "Step 15986: Minibatch Loss: 2.145469\n",
            "Step 15988: Minibatch Loss: 2.160553\n",
            "Step 15990: Minibatch Loss: 2.148921\n",
            "Step 15992: Minibatch Loss: 2.100161\n",
            "Step 15994: Minibatch Loss: 2.092610\n",
            "Step 15996: Minibatch Loss: 2.125212\n",
            "Step 15998: Minibatch Loss: 2.124534\n",
            "Step 16000: Minibatch Loss: 2.068302\n",
            "Training for SNR= 8.0  sigma= 0.44668359215096315 iteratin: 0\n",
            "Step 16002: Minibatch Loss: 2.182822\n",
            "Step 16004: Minibatch Loss: 2.154385\n",
            "Step 16006: Minibatch Loss: 2.074191\n",
            "Step 16008: Minibatch Loss: 2.142377\n",
            "Step 16010: Minibatch Loss: 2.079410\n",
            "Step 16012: Minibatch Loss: 2.120896\n",
            "Step 16014: Minibatch Loss: 2.141464\n",
            "Step 16016: Minibatch Loss: 2.120188\n",
            "Step 16018: Minibatch Loss: 2.186630\n",
            "Step 16020: Minibatch Loss: 2.139607\n",
            "Step 16022: Minibatch Loss: 2.103791\n",
            "Step 16024: Minibatch Loss: 2.161245\n",
            "Step 16026: Minibatch Loss: 2.173576\n",
            "Step 16028: Minibatch Loss: 2.129964\n",
            "Step 16030: Minibatch Loss: 2.089741\n",
            "Step 16032: Minibatch Loss: 2.124335\n",
            "Step 16034: Minibatch Loss: 2.054627\n",
            "Step 16036: Minibatch Loss: 2.158931\n",
            "Step 16038: Minibatch Loss: 2.114413\n",
            "Step 16040: Minibatch Loss: 2.154106\n",
            "Step 16042: Minibatch Loss: 2.051381\n",
            "Step 16044: Minibatch Loss: 2.143653\n",
            "Step 16046: Minibatch Loss: 2.098353\n",
            "Step 16048: Minibatch Loss: 2.096592\n",
            "Step 16050: Minibatch Loss: 2.158295\n",
            "Step 16052: Minibatch Loss: 2.077308\n",
            "Step 16054: Minibatch Loss: 2.188242\n",
            "Step 16056: Minibatch Loss: 2.112611\n",
            "Step 16058: Minibatch Loss: 2.131357\n",
            "Step 16060: Minibatch Loss: 2.117228\n",
            "Step 16062: Minibatch Loss: 2.152101\n",
            "Step 16064: Minibatch Loss: 2.052131\n",
            "Step 16066: Minibatch Loss: 2.180741\n",
            "Step 16068: Minibatch Loss: 2.077785\n",
            "Step 16070: Minibatch Loss: 2.128533\n",
            "Step 16072: Minibatch Loss: 2.163607\n",
            "Step 16074: Minibatch Loss: 2.124233\n",
            "Step 16076: Minibatch Loss: 2.166977\n",
            "Step 16078: Minibatch Loss: 2.121357\n",
            "Step 16080: Minibatch Loss: 2.111836\n",
            "Step 16082: Minibatch Loss: 2.145297\n",
            "Step 16084: Minibatch Loss: 2.099275\n",
            "Step 16086: Minibatch Loss: 2.189366\n",
            "Step 16088: Minibatch Loss: 2.065730\n",
            "Step 16090: Minibatch Loss: 2.137186\n",
            "Step 16092: Minibatch Loss: 2.128610\n",
            "Step 16094: Minibatch Loss: 2.109801\n",
            "Step 16096: Minibatch Loss: 2.121542\n",
            "Step 16098: Minibatch Loss: 2.134672\n",
            "Step 16100: Minibatch Loss: 2.059811\n",
            "Step 16102: Minibatch Loss: 2.184971\n",
            "Step 16104: Minibatch Loss: 2.131192\n",
            "Step 16106: Minibatch Loss: 2.111568\n",
            "Step 16108: Minibatch Loss: 2.075910\n",
            "Step 16110: Minibatch Loss: 2.131296\n",
            "Step 16112: Minibatch Loss: 2.145149\n",
            "Step 16114: Minibatch Loss: 2.099429\n",
            "Step 16116: Minibatch Loss: 2.098742\n",
            "Step 16118: Minibatch Loss: 2.189785\n",
            "Step 16120: Minibatch Loss: 2.131109\n",
            "Step 16122: Minibatch Loss: 2.188749\n",
            "Step 16124: Minibatch Loss: 2.100622\n",
            "Step 16126: Minibatch Loss: 2.100954\n",
            "Step 16128: Minibatch Loss: 2.118435\n",
            "Step 16130: Minibatch Loss: 2.146947\n",
            "Step 16132: Minibatch Loss: 2.073215\n",
            "Step 16134: Minibatch Loss: 2.078037\n",
            "Step 16136: Minibatch Loss: 2.194576\n",
            "Step 16138: Minibatch Loss: 2.058727\n",
            "Step 16140: Minibatch Loss: 2.145314\n",
            "Step 16142: Minibatch Loss: 2.051460\n",
            "Step 16144: Minibatch Loss: 2.078202\n",
            "Step 16146: Minibatch Loss: 2.127940\n",
            "Step 16148: Minibatch Loss: 2.137337\n",
            "Step 16150: Minibatch Loss: 2.080312\n",
            "Step 16152: Minibatch Loss: 2.107981\n",
            "Step 16154: Minibatch Loss: 2.077547\n",
            "Step 16156: Minibatch Loss: 2.132662\n",
            "Step 16158: Minibatch Loss: 2.155048\n",
            "Step 16160: Minibatch Loss: 2.092744\n",
            "Step 16162: Minibatch Loss: 2.153023\n",
            "Step 16164: Minibatch Loss: 2.098082\n",
            "Step 16166: Minibatch Loss: 2.134589\n",
            "Step 16168: Minibatch Loss: 2.049851\n",
            "Step 16170: Minibatch Loss: 2.165210\n",
            "Step 16172: Minibatch Loss: 2.130667\n",
            "Step 16174: Minibatch Loss: 2.076461\n",
            "Step 16176: Minibatch Loss: 2.144979\n",
            "Step 16178: Minibatch Loss: 2.124185\n",
            "Step 16180: Minibatch Loss: 2.071139\n",
            "Step 16182: Minibatch Loss: 2.084779\n",
            "Step 16184: Minibatch Loss: 2.111896\n",
            "Step 16186: Minibatch Loss: 2.200397\n",
            "Step 16188: Minibatch Loss: 2.108744\n",
            "Step 16190: Minibatch Loss: 2.126523\n",
            "Step 16192: Minibatch Loss: 2.059687\n",
            "Step 16194: Minibatch Loss: 2.094256\n",
            "Step 16196: Minibatch Loss: 2.069502\n",
            "Step 16198: Minibatch Loss: 2.098052\n",
            "Step 16200: Minibatch Loss: 2.106652\n",
            "Step 16202: Minibatch Loss: 2.152846\n",
            "Step 16204: Minibatch Loss: 2.171953\n",
            "Step 16206: Minibatch Loss: 2.082927\n",
            "Step 16208: Minibatch Loss: 2.154910\n",
            "Step 16210: Minibatch Loss: 2.100469\n",
            "Step 16212: Minibatch Loss: 2.123553\n",
            "Step 16214: Minibatch Loss: 2.098481\n",
            "Step 16216: Minibatch Loss: 2.121212\n",
            "Step 16218: Minibatch Loss: 2.167361\n",
            "Step 16220: Minibatch Loss: 2.166543\n",
            "Step 16222: Minibatch Loss: 2.130265\n",
            "Step 16224: Minibatch Loss: 2.104229\n",
            "Step 16226: Minibatch Loss: 2.181336\n",
            "Step 16228: Minibatch Loss: 2.083571\n",
            "Step 16230: Minibatch Loss: 2.108063\n",
            "Step 16232: Minibatch Loss: 2.002641\n",
            "Step 16234: Minibatch Loss: 2.040956\n",
            "Step 16236: Minibatch Loss: 2.190037\n",
            "Step 16238: Minibatch Loss: 2.042121\n",
            "Step 16240: Minibatch Loss: 2.092719\n",
            "Step 16242: Minibatch Loss: 2.059840\n",
            "Step 16244: Minibatch Loss: 2.152522\n",
            "Step 16246: Minibatch Loss: 2.060667\n",
            "Step 16248: Minibatch Loss: 2.110156\n",
            "Step 16250: Minibatch Loss: 2.074635\n",
            "Step 16252: Minibatch Loss: 2.079615\n",
            "Step 16254: Minibatch Loss: 2.138417\n",
            "Step 16256: Minibatch Loss: 2.160577\n",
            "Step 16258: Minibatch Loss: 2.106479\n",
            "Step 16260: Minibatch Loss: 2.119564\n",
            "Step 16262: Minibatch Loss: 2.152140\n",
            "Step 16264: Minibatch Loss: 2.040857\n",
            "Step 16266: Minibatch Loss: 2.113801\n",
            "Step 16268: Minibatch Loss: 2.092656\n",
            "Step 16270: Minibatch Loss: 2.148433\n",
            "Step 16272: Minibatch Loss: 2.112870\n",
            "Step 16274: Minibatch Loss: 2.144118\n",
            "Step 16276: Minibatch Loss: 2.077645\n",
            "Step 16278: Minibatch Loss: 2.115571\n",
            "Step 16280: Minibatch Loss: 2.167857\n",
            "Step 16282: Minibatch Loss: 2.106136\n",
            "Step 16284: Minibatch Loss: 2.106945\n",
            "Step 16286: Minibatch Loss: 2.137894\n",
            "Step 16288: Minibatch Loss: 2.066123\n",
            "Step 16290: Minibatch Loss: 2.161568\n",
            "Step 16292: Minibatch Loss: 2.107857\n",
            "Step 16294: Minibatch Loss: 2.150490\n",
            "Step 16296: Minibatch Loss: 2.076203\n",
            "Step 16298: Minibatch Loss: 2.151640\n",
            "Step 16300: Minibatch Loss: 2.050815\n",
            "Step 16302: Minibatch Loss: 2.180101\n",
            "Step 16304: Minibatch Loss: 2.157097\n",
            "Step 16306: Minibatch Loss: 2.139354\n",
            "Step 16308: Minibatch Loss: 2.110872\n",
            "Step 16310: Minibatch Loss: 2.034450\n",
            "Step 16312: Minibatch Loss: 2.219332\n",
            "Step 16314: Minibatch Loss: 2.106363\n",
            "Step 16316: Minibatch Loss: 2.091269\n",
            "Step 16318: Minibatch Loss: 2.181592\n",
            "Step 16320: Minibatch Loss: 2.192021\n",
            "Step 16322: Minibatch Loss: 2.096484\n",
            "Step 16324: Minibatch Loss: 2.080073\n",
            "Step 16326: Minibatch Loss: 2.148453\n",
            "Step 16328: Minibatch Loss: 2.092206\n",
            "Step 16330: Minibatch Loss: 2.098295\n",
            "Step 16332: Minibatch Loss: 2.089837\n",
            "Step 16334: Minibatch Loss: 2.086022\n",
            "Step 16336: Minibatch Loss: 2.170110\n",
            "Step 16338: Minibatch Loss: 2.044470\n",
            "Step 16340: Minibatch Loss: 2.085931\n",
            "Step 16342: Minibatch Loss: 2.091426\n",
            "Step 16344: Minibatch Loss: 2.120176\n",
            "Step 16346: Minibatch Loss: 2.067956\n",
            "Step 16348: Minibatch Loss: 2.117789\n",
            "Step 16350: Minibatch Loss: 2.070532\n",
            "Step 16352: Minibatch Loss: 2.124708\n",
            "Step 16354: Minibatch Loss: 2.112647\n",
            "Step 16356: Minibatch Loss: 2.172248\n",
            "Step 16358: Minibatch Loss: 2.140692\n",
            "Step 16360: Minibatch Loss: 2.075790\n",
            "Step 16362: Minibatch Loss: 2.163442\n",
            "Step 16364: Minibatch Loss: 2.042033\n",
            "Step 16366: Minibatch Loss: 2.160808\n",
            "Step 16368: Minibatch Loss: 2.020895\n",
            "Step 16370: Minibatch Loss: 2.131038\n",
            "Step 16372: Minibatch Loss: 2.153114\n",
            "Step 16374: Minibatch Loss: 2.115378\n",
            "Step 16376: Minibatch Loss: 2.061367\n",
            "Step 16378: Minibatch Loss: 2.098692\n",
            "Step 16380: Minibatch Loss: 2.125432\n",
            "Step 16382: Minibatch Loss: 2.142575\n",
            "Step 16384: Minibatch Loss: 2.119665\n",
            "Step 16386: Minibatch Loss: 2.130662\n",
            "Step 16388: Minibatch Loss: 2.131012\n",
            "Step 16390: Minibatch Loss: 2.107948\n",
            "Step 16392: Minibatch Loss: 2.103443\n",
            "Step 16394: Minibatch Loss: 2.092466\n",
            "Step 16396: Minibatch Loss: 2.034352\n",
            "Step 16398: Minibatch Loss: 2.103321\n",
            "Step 16400: Minibatch Loss: 2.070356\n",
            "Step 16402: Minibatch Loss: 2.143067\n",
            "Step 16404: Minibatch Loss: 2.085597\n",
            "Step 16406: Minibatch Loss: 2.096431\n",
            "Step 16408: Minibatch Loss: 2.111888\n",
            "Step 16410: Minibatch Loss: 2.130546\n",
            "Step 16412: Minibatch Loss: 2.168656\n",
            "Step 16414: Minibatch Loss: 2.190511\n",
            "Step 16416: Minibatch Loss: 2.042253\n",
            "Step 16418: Minibatch Loss: 2.092576\n",
            "Step 16420: Minibatch Loss: 2.093886\n",
            "Step 16422: Minibatch Loss: 2.115003\n",
            "Step 16424: Minibatch Loss: 2.129824\n",
            "Step 16426: Minibatch Loss: 2.110637\n",
            "Step 16428: Minibatch Loss: 2.096242\n",
            "Step 16430: Minibatch Loss: 2.118128\n",
            "Step 16432: Minibatch Loss: 2.077465\n",
            "Step 16434: Minibatch Loss: 2.078614\n",
            "Step 16436: Minibatch Loss: 2.174145\n",
            "Step 16438: Minibatch Loss: 2.112972\n",
            "Step 16440: Minibatch Loss: 2.118879\n",
            "Step 16442: Minibatch Loss: 2.009512\n",
            "Step 16444: Minibatch Loss: 2.124508\n",
            "Step 16446: Minibatch Loss: 2.113344\n",
            "Step 16448: Minibatch Loss: 2.081409\n",
            "Step 16450: Minibatch Loss: 2.146474\n",
            "Step 16452: Minibatch Loss: 2.090665\n",
            "Step 16454: Minibatch Loss: 2.112986\n",
            "Step 16456: Minibatch Loss: 2.131246\n",
            "Step 16458: Minibatch Loss: 2.156774\n",
            "Step 16460: Minibatch Loss: 2.091851\n",
            "Step 16462: Minibatch Loss: 2.160573\n",
            "Step 16464: Minibatch Loss: 2.068558\n",
            "Step 16466: Minibatch Loss: 2.128218\n",
            "Step 16468: Minibatch Loss: 2.057790\n",
            "Step 16470: Minibatch Loss: 2.157789\n",
            "Step 16472: Minibatch Loss: 2.108453\n",
            "Step 16474: Minibatch Loss: 2.115463\n",
            "Step 16476: Minibatch Loss: 2.088943\n",
            "Step 16478: Minibatch Loss: 2.075628\n",
            "Step 16480: Minibatch Loss: 2.102563\n",
            "Step 16482: Minibatch Loss: 2.116700\n",
            "Step 16484: Minibatch Loss: 2.095116\n",
            "Step 16486: Minibatch Loss: 2.144933\n",
            "Step 16488: Minibatch Loss: 2.095126\n",
            "Step 16490: Minibatch Loss: 2.111841\n",
            "Step 16492: Minibatch Loss: 2.121511\n",
            "Step 16494: Minibatch Loss: 2.094370\n",
            "Step 16496: Minibatch Loss: 2.034095\n",
            "Step 16498: Minibatch Loss: 2.128939\n",
            "Step 16500: Minibatch Loss: 2.077176\n",
            "Step 16502: Minibatch Loss: 2.111645\n",
            "Step 16504: Minibatch Loss: 1.996148\n",
            "Step 16506: Minibatch Loss: 2.094308\n",
            "Step 16508: Minibatch Loss: 2.105175\n",
            "Step 16510: Minibatch Loss: 2.126382\n",
            "Step 16512: Minibatch Loss: 2.103984\n",
            "Step 16514: Minibatch Loss: 2.105099\n",
            "Step 16516: Minibatch Loss: 2.027047\n",
            "Step 16518: Minibatch Loss: 2.152102\n",
            "Step 16520: Minibatch Loss: 2.146140\n",
            "Step 16522: Minibatch Loss: 2.104026\n",
            "Step 16524: Minibatch Loss: 2.102730\n",
            "Step 16526: Minibatch Loss: 2.136949\n",
            "Step 16528: Minibatch Loss: 2.084208\n",
            "Step 16530: Minibatch Loss: 2.129974\n",
            "Step 16532: Minibatch Loss: 2.131668\n",
            "Step 16534: Minibatch Loss: 2.083170\n",
            "Step 16536: Minibatch Loss: 2.182287\n",
            "Step 16538: Minibatch Loss: 2.065299\n",
            "Step 16540: Minibatch Loss: 2.171972\n",
            "Step 16542: Minibatch Loss: 2.055320\n",
            "Step 16544: Minibatch Loss: 2.130617\n",
            "Step 16546: Minibatch Loss: 2.083952\n",
            "Step 16548: Minibatch Loss: 2.077813\n",
            "Step 16550: Minibatch Loss: 2.135077\n",
            "Step 16552: Minibatch Loss: 2.078202\n",
            "Step 16554: Minibatch Loss: 2.077301\n",
            "Step 16556: Minibatch Loss: 2.114268\n",
            "Step 16558: Minibatch Loss: 2.083332\n",
            "Step 16560: Minibatch Loss: 2.111435\n",
            "Step 16562: Minibatch Loss: 2.210692\n",
            "Step 16564: Minibatch Loss: 2.036278\n",
            "Step 16566: Minibatch Loss: 2.134166\n",
            "Step 16568: Minibatch Loss: 2.022786\n",
            "Step 16570: Minibatch Loss: 2.117767\n",
            "Step 16572: Minibatch Loss: 2.073839\n",
            "Step 16574: Minibatch Loss: 2.060950\n",
            "Step 16576: Minibatch Loss: 2.122663\n",
            "Step 16578: Minibatch Loss: 2.113011\n",
            "Step 16580: Minibatch Loss: 2.099300\n",
            "Step 16582: Minibatch Loss: 2.169788\n",
            "Step 16584: Minibatch Loss: 2.034903\n",
            "Step 16586: Minibatch Loss: 2.119501\n",
            "Step 16588: Minibatch Loss: 2.115596\n",
            "Step 16590: Minibatch Loss: 2.176072\n",
            "Step 16592: Minibatch Loss: 2.094549\n",
            "Step 16594: Minibatch Loss: 2.023581\n",
            "Step 16596: Minibatch Loss: 2.062563\n",
            "Step 16598: Minibatch Loss: 2.143557\n",
            "Step 16600: Minibatch Loss: 2.091603\n",
            "Step 16602: Minibatch Loss: 2.165967\n",
            "Step 16604: Minibatch Loss: 2.052965\n",
            "Step 16606: Minibatch Loss: 2.098262\n",
            "Step 16608: Minibatch Loss: 2.116978\n",
            "Step 16610: Minibatch Loss: 2.078017\n",
            "Step 16612: Minibatch Loss: 2.143219\n",
            "Step 16614: Minibatch Loss: 2.040134\n",
            "Step 16616: Minibatch Loss: 2.108445\n",
            "Step 16618: Minibatch Loss: 2.099320\n",
            "Step 16620: Minibatch Loss: 2.138535\n",
            "Step 16622: Minibatch Loss: 2.117141\n",
            "Step 16624: Minibatch Loss: 2.073319\n",
            "Step 16626: Minibatch Loss: 2.093513\n",
            "Step 16628: Minibatch Loss: 2.116911\n",
            "Step 16630: Minibatch Loss: 2.144006\n",
            "Step 16632: Minibatch Loss: 2.049542\n",
            "Step 16634: Minibatch Loss: 2.021477\n",
            "Step 16636: Minibatch Loss: 2.170988\n",
            "Step 16638: Minibatch Loss: 2.092357\n",
            "Step 16640: Minibatch Loss: 2.097871\n",
            "Step 16642: Minibatch Loss: 2.036315\n",
            "Step 16644: Minibatch Loss: 2.156583\n",
            "Step 16646: Minibatch Loss: 2.105809\n",
            "Step 16648: Minibatch Loss: 2.053407\n",
            "Step 16650: Minibatch Loss: 2.041466\n",
            "Step 16652: Minibatch Loss: 2.085530\n",
            "Step 16654: Minibatch Loss: 2.156581\n",
            "Step 16656: Minibatch Loss: 2.157184\n",
            "Step 16658: Minibatch Loss: 2.176123\n",
            "Step 16660: Minibatch Loss: 2.041134\n",
            "Step 16662: Minibatch Loss: 2.097818\n",
            "Step 16664: Minibatch Loss: 2.060887\n",
            "Step 16666: Minibatch Loss: 2.146925\n",
            "Step 16668: Minibatch Loss: 2.069044\n",
            "Step 16670: Minibatch Loss: 2.121568\n",
            "Step 16672: Minibatch Loss: 2.096237\n",
            "Step 16674: Minibatch Loss: 2.112195\n",
            "Step 16676: Minibatch Loss: 2.108835\n",
            "Step 16678: Minibatch Loss: 2.164308\n",
            "Step 16680: Minibatch Loss: 2.121766\n",
            "Step 16682: Minibatch Loss: 2.120938\n",
            "Step 16684: Minibatch Loss: 2.067407\n",
            "Step 16686: Minibatch Loss: 2.114957\n",
            "Step 16688: Minibatch Loss: 2.063284\n",
            "Step 16690: Minibatch Loss: 2.138063\n",
            "Step 16692: Minibatch Loss: 2.112959\n",
            "Step 16694: Minibatch Loss: 2.090013\n",
            "Step 16696: Minibatch Loss: 2.126718\n",
            "Step 16698: Minibatch Loss: 2.035783\n",
            "Step 16700: Minibatch Loss: 2.107302\n",
            "Step 16702: Minibatch Loss: 2.120173\n",
            "Step 16704: Minibatch Loss: 2.076524\n",
            "Step 16706: Minibatch Loss: 2.114496\n",
            "Step 16708: Minibatch Loss: 2.108456\n",
            "Step 16710: Minibatch Loss: 2.083755\n",
            "Step 16712: Minibatch Loss: 2.069567\n",
            "Step 16714: Minibatch Loss: 2.099550\n",
            "Step 16716: Minibatch Loss: 2.084371\n",
            "Step 16718: Minibatch Loss: 2.095964\n",
            "Step 16720: Minibatch Loss: 2.174449\n",
            "Step 16722: Minibatch Loss: 2.081656\n",
            "Step 16724: Minibatch Loss: 2.101876\n",
            "Step 16726: Minibatch Loss: 2.176771\n",
            "Step 16728: Minibatch Loss: 2.063186\n",
            "Step 16730: Minibatch Loss: 2.066574\n",
            "Step 16732: Minibatch Loss: 2.096207\n",
            "Step 16734: Minibatch Loss: 2.070070\n",
            "Step 16736: Minibatch Loss: 2.188099\n",
            "Step 16738: Minibatch Loss: 2.058812\n",
            "Step 16740: Minibatch Loss: 2.101209\n",
            "Step 16742: Minibatch Loss: 2.035229\n",
            "Step 16744: Minibatch Loss: 2.042040\n",
            "Step 16746: Minibatch Loss: 2.022915\n",
            "Step 16748: Minibatch Loss: 2.092732\n",
            "Step 16750: Minibatch Loss: 2.077514\n",
            "Step 16752: Minibatch Loss: 2.081517\n",
            "Step 16754: Minibatch Loss: 2.064040\n",
            "Step 16756: Minibatch Loss: 2.096352\n",
            "Step 16758: Minibatch Loss: 2.163908\n",
            "Step 16760: Minibatch Loss: 2.073501\n",
            "Step 16762: Minibatch Loss: 2.096367\n",
            "Step 16764: Minibatch Loss: 2.038548\n",
            "Step 16766: Minibatch Loss: 2.128504\n",
            "Step 16768: Minibatch Loss: 2.052997\n",
            "Step 16770: Minibatch Loss: 2.115083\n",
            "Step 16772: Minibatch Loss: 2.154990\n",
            "Step 16774: Minibatch Loss: 2.088057\n",
            "Step 16776: Minibatch Loss: 2.108813\n",
            "Step 16778: Minibatch Loss: 2.045767\n",
            "Step 16780: Minibatch Loss: 2.108934\n",
            "Step 16782: Minibatch Loss: 2.131467\n",
            "Step 16784: Minibatch Loss: 2.134723\n",
            "Step 16786: Minibatch Loss: 2.108712\n",
            "Step 16788: Minibatch Loss: 2.039707\n",
            "Step 16790: Minibatch Loss: 2.123287\n",
            "Step 16792: Minibatch Loss: 2.074330\n",
            "Step 16794: Minibatch Loss: 2.178742\n",
            "Step 16796: Minibatch Loss: 2.050567\n",
            "Step 16798: Minibatch Loss: 2.128589\n",
            "Step 16800: Minibatch Loss: 2.059076\n",
            "Step 16802: Minibatch Loss: 2.189022\n",
            "Step 16804: Minibatch Loss: 2.107563\n",
            "Step 16806: Minibatch Loss: 2.113515\n",
            "Step 16808: Minibatch Loss: 2.126478\n",
            "Step 16810: Minibatch Loss: 2.104888\n",
            "Step 16812: Minibatch Loss: 2.132746\n",
            "Step 16814: Minibatch Loss: 2.101661\n",
            "Step 16816: Minibatch Loss: 2.095294\n",
            "Step 16818: Minibatch Loss: 2.168431\n",
            "Step 16820: Minibatch Loss: 2.099772\n",
            "Step 16822: Minibatch Loss: 2.070407\n",
            "Step 16824: Minibatch Loss: 2.147331\n",
            "Step 16826: Minibatch Loss: 2.111992\n",
            "Step 16828: Minibatch Loss: 2.070214\n",
            "Step 16830: Minibatch Loss: 2.085654\n",
            "Step 16832: Minibatch Loss: 2.061598\n",
            "Step 16834: Minibatch Loss: 2.120020\n",
            "Step 16836: Minibatch Loss: 2.158443\n",
            "Step 16838: Minibatch Loss: 2.059429\n",
            "Step 16840: Minibatch Loss: 2.054027\n",
            "Step 16842: Minibatch Loss: 2.021816\n",
            "Step 16844: Minibatch Loss: 2.085818\n",
            "Step 16846: Minibatch Loss: 2.132221\n",
            "Step 16848: Minibatch Loss: 2.109118\n",
            "Step 16850: Minibatch Loss: 2.020557\n",
            "Step 16852: Minibatch Loss: 2.111005\n",
            "Step 16854: Minibatch Loss: 2.156693\n",
            "Step 16856: Minibatch Loss: 2.077702\n",
            "Step 16858: Minibatch Loss: 2.098648\n",
            "Step 16860: Minibatch Loss: 2.104223\n",
            "Step 16862: Minibatch Loss: 2.107551\n",
            "Step 16864: Minibatch Loss: 2.056117\n",
            "Step 16866: Minibatch Loss: 2.110793\n",
            "Step 16868: Minibatch Loss: 1.995869\n",
            "Step 16870: Minibatch Loss: 2.134431\n",
            "Step 16872: Minibatch Loss: 2.082180\n",
            "Step 16874: Minibatch Loss: 2.116122\n",
            "Step 16876: Minibatch Loss: 2.111569\n",
            "Step 16878: Minibatch Loss: 2.082551\n",
            "Step 16880: Minibatch Loss: 2.144034\n",
            "Step 16882: Minibatch Loss: 2.235093\n",
            "Step 16884: Minibatch Loss: 2.111401\n",
            "Step 16886: Minibatch Loss: 2.106295\n",
            "Step 16888: Minibatch Loss: 2.024882\n",
            "Step 16890: Minibatch Loss: 2.103152\n",
            "Step 16892: Minibatch Loss: 2.047238\n",
            "Step 16894: Minibatch Loss: 2.089042\n",
            "Step 16896: Minibatch Loss: 2.039078\n",
            "Step 16898: Minibatch Loss: 2.064226\n",
            "Step 16900: Minibatch Loss: 2.116483\n",
            "Step 16902: Minibatch Loss: 2.123195\n",
            "Step 16904: Minibatch Loss: 2.133142\n",
            "Step 16906: Minibatch Loss: 2.121788\n",
            "Step 16908: Minibatch Loss: 2.141451\n",
            "Step 16910: Minibatch Loss: 2.057045\n",
            "Step 16912: Minibatch Loss: 2.133500\n",
            "Step 16914: Minibatch Loss: 2.130817\n",
            "Step 16916: Minibatch Loss: 2.038922\n",
            "Step 16918: Minibatch Loss: 2.071704\n",
            "Step 16920: Minibatch Loss: 2.095491\n",
            "Step 16922: Minibatch Loss: 2.096637\n",
            "Step 16924: Minibatch Loss: 2.055773\n",
            "Step 16926: Minibatch Loss: 2.155070\n",
            "Step 16928: Minibatch Loss: 2.114923\n",
            "Step 16930: Minibatch Loss: 2.121360\n",
            "Step 16932: Minibatch Loss: 2.063561\n",
            "Step 16934: Minibatch Loss: 2.086913\n",
            "Step 16936: Minibatch Loss: 2.173154\n",
            "Step 16938: Minibatch Loss: 2.098419\n",
            "Step 16940: Minibatch Loss: 2.076838\n",
            "Step 16942: Minibatch Loss: 2.040345\n",
            "Step 16944: Minibatch Loss: 2.098805\n",
            "Step 16946: Minibatch Loss: 2.082792\n",
            "Step 16948: Minibatch Loss: 2.105239\n",
            "Step 16950: Minibatch Loss: 2.132108\n",
            "Step 16952: Minibatch Loss: 2.049679\n",
            "Step 16954: Minibatch Loss: 2.073156\n",
            "Step 16956: Minibatch Loss: 2.148228\n",
            "Step 16958: Minibatch Loss: 2.049041\n",
            "Step 16960: Minibatch Loss: 2.064578\n",
            "Step 16962: Minibatch Loss: 2.098790\n",
            "Step 16964: Minibatch Loss: 1.969830\n",
            "Step 16966: Minibatch Loss: 2.141799\n",
            "Step 16968: Minibatch Loss: 2.010390\n",
            "Step 16970: Minibatch Loss: 2.119316\n",
            "Step 16972: Minibatch Loss: 2.098261\n",
            "Step 16974: Minibatch Loss: 2.115705\n",
            "Step 16976: Minibatch Loss: 2.102564\n",
            "Step 16978: Minibatch Loss: 2.094058\n",
            "Step 16980: Minibatch Loss: 2.133671\n",
            "Step 16982: Minibatch Loss: 2.092903\n",
            "Step 16984: Minibatch Loss: 2.053354\n",
            "Step 16986: Minibatch Loss: 2.126563\n",
            "Step 16988: Minibatch Loss: 2.077704\n",
            "Step 16990: Minibatch Loss: 2.120442\n",
            "Step 16992: Minibatch Loss: 2.111232\n",
            "Step 16994: Minibatch Loss: 2.029087\n",
            "Step 16996: Minibatch Loss: 1.997527\n",
            "Step 16998: Minibatch Loss: 2.081539\n",
            "Step 17000: Minibatch Loss: 2.056165\n",
            "Training for SNR= 8.5  sigma= 0.44668359215096315 iteratin: 0\n",
            "Step 17002: Minibatch Loss: 2.139160\n",
            "Step 17004: Minibatch Loss: 2.085858\n",
            "Step 17006: Minibatch Loss: 2.055155\n",
            "Step 17008: Minibatch Loss: 2.076680\n",
            "Step 17010: Minibatch Loss: 2.091193\n",
            "Step 17012: Minibatch Loss: 2.170261\n",
            "Step 17014: Minibatch Loss: 2.132715\n",
            "Step 17016: Minibatch Loss: 2.002489\n",
            "Step 17018: Minibatch Loss: 2.086938\n",
            "Step 17020: Minibatch Loss: 2.090643\n",
            "Step 17022: Minibatch Loss: 2.081122\n",
            "Step 17024: Minibatch Loss: 2.078738\n",
            "Step 17026: Minibatch Loss: 2.106506\n",
            "Step 17028: Minibatch Loss: 2.044830\n",
            "Step 17030: Minibatch Loss: 2.093723\n",
            "Step 17032: Minibatch Loss: 2.060950\n",
            "Step 17034: Minibatch Loss: 2.054955\n",
            "Step 17036: Minibatch Loss: 2.100285\n",
            "Step 17038: Minibatch Loss: 2.002492\n",
            "Step 17040: Minibatch Loss: 2.039467\n",
            "Step 17042: Minibatch Loss: 1.985940\n",
            "Step 17044: Minibatch Loss: 2.117080\n",
            "Step 17046: Minibatch Loss: 2.049730\n",
            "Step 17048: Minibatch Loss: 2.099130\n",
            "Step 17050: Minibatch Loss: 2.039306\n",
            "Step 17052: Minibatch Loss: 2.060035\n",
            "Step 17054: Minibatch Loss: 2.136369\n",
            "Step 17056: Minibatch Loss: 2.077798\n",
            "Step 17058: Minibatch Loss: 2.106509\n",
            "Step 17060: Minibatch Loss: 2.048031\n",
            "Step 17062: Minibatch Loss: 2.125380\n",
            "Step 17064: Minibatch Loss: 2.063236\n",
            "Step 17066: Minibatch Loss: 2.132972\n",
            "Step 17068: Minibatch Loss: 2.003749\n",
            "Step 17070: Minibatch Loss: 2.100623\n",
            "Step 17072: Minibatch Loss: 2.073951\n",
            "Step 17074: Minibatch Loss: 2.082038\n",
            "Step 17076: Minibatch Loss: 2.082502\n",
            "Step 17078: Minibatch Loss: 2.095742\n",
            "Step 17080: Minibatch Loss: 2.147267\n",
            "Step 17082: Minibatch Loss: 2.063818\n",
            "Step 17084: Minibatch Loss: 2.092399\n",
            "Step 17086: Minibatch Loss: 2.101407\n",
            "Step 17088: Minibatch Loss: 2.075985\n",
            "Step 17090: Minibatch Loss: 2.114013\n",
            "Step 17092: Minibatch Loss: 2.131239\n",
            "Step 17094: Minibatch Loss: 2.083537\n",
            "Step 17096: Minibatch Loss: 2.072399\n",
            "Step 17098: Minibatch Loss: 2.092120\n",
            "Step 17100: Minibatch Loss: 2.070811\n",
            "Step 17102: Minibatch Loss: 2.136852\n",
            "Step 17104: Minibatch Loss: 2.078746\n",
            "Step 17106: Minibatch Loss: 2.080267\n",
            "Step 17108: Minibatch Loss: 2.104179\n",
            "Step 17110: Minibatch Loss: 2.045831\n",
            "Step 17112: Minibatch Loss: 2.118793\n",
            "Step 17114: Minibatch Loss: 2.133221\n",
            "Step 17116: Minibatch Loss: 2.092391\n",
            "Step 17118: Minibatch Loss: 2.137285\n",
            "Step 17120: Minibatch Loss: 2.110055\n",
            "Step 17122: Minibatch Loss: 2.065425\n",
            "Step 17124: Minibatch Loss: 2.120321\n",
            "Step 17126: Minibatch Loss: 2.141719\n",
            "Step 17128: Minibatch Loss: 2.105242\n",
            "Step 17130: Minibatch Loss: 2.084315\n",
            "Step 17132: Minibatch Loss: 2.078059\n",
            "Step 17134: Minibatch Loss: 2.091943\n",
            "Step 17136: Minibatch Loss: 2.153043\n",
            "Step 17138: Minibatch Loss: 2.001448\n",
            "Step 17140: Minibatch Loss: 2.078629\n",
            "Step 17142: Minibatch Loss: 2.066745\n",
            "Step 17144: Minibatch Loss: 2.058035\n",
            "Step 17146: Minibatch Loss: 2.034282\n",
            "Step 17148: Minibatch Loss: 2.039470\n",
            "Step 17150: Minibatch Loss: 2.123135\n",
            "Step 17152: Minibatch Loss: 2.100014\n",
            "Step 17154: Minibatch Loss: 2.102968\n",
            "Step 17156: Minibatch Loss: 2.069271\n",
            "Step 17158: Minibatch Loss: 2.035008\n",
            "Step 17160: Minibatch Loss: 2.079965\n",
            "Step 17162: Minibatch Loss: 2.060592\n",
            "Step 17164: Minibatch Loss: 2.035951\n",
            "Step 17166: Minibatch Loss: 2.144773\n",
            "Step 17168: Minibatch Loss: 2.028605\n",
            "Step 17170: Minibatch Loss: 2.112739\n",
            "Step 17172: Minibatch Loss: 2.143169\n",
            "Step 17174: Minibatch Loss: 2.036262\n",
            "Step 17176: Minibatch Loss: 2.023478\n",
            "Step 17178: Minibatch Loss: 2.085953\n",
            "Step 17180: Minibatch Loss: 2.100422\n",
            "Step 17182: Minibatch Loss: 2.102004\n",
            "Step 17184: Minibatch Loss: 2.101543\n",
            "Step 17186: Minibatch Loss: 2.090665\n",
            "Step 17188: Minibatch Loss: 2.089307\n",
            "Step 17190: Minibatch Loss: 2.081083\n",
            "Step 17192: Minibatch Loss: 2.058514\n",
            "Step 17194: Minibatch Loss: 2.100042\n",
            "Step 17196: Minibatch Loss: 2.091647\n",
            "Step 17198: Minibatch Loss: 2.093968\n",
            "Step 17200: Minibatch Loss: 2.070926\n",
            "Step 17202: Minibatch Loss: 2.150484\n",
            "Step 17204: Minibatch Loss: 2.126039\n",
            "Step 17206: Minibatch Loss: 2.074824\n",
            "Step 17208: Minibatch Loss: 2.025915\n",
            "Step 17210: Minibatch Loss: 2.041873\n",
            "Step 17212: Minibatch Loss: 2.117036\n",
            "Step 17214: Minibatch Loss: 2.123192\n",
            "Step 17216: Minibatch Loss: 2.044892\n",
            "Step 17218: Minibatch Loss: 2.081079\n",
            "Step 17220: Minibatch Loss: 2.130425\n",
            "Step 17222: Minibatch Loss: 2.036611\n",
            "Step 17224: Minibatch Loss: 2.179220\n",
            "Step 17226: Minibatch Loss: 2.090835\n",
            "Step 17228: Minibatch Loss: 2.037289\n",
            "Step 17230: Minibatch Loss: 2.046649\n",
            "Step 17232: Minibatch Loss: 2.048034\n",
            "Step 17234: Minibatch Loss: 2.059746\n",
            "Step 17236: Minibatch Loss: 2.156769\n",
            "Step 17238: Minibatch Loss: 2.049569\n",
            "Step 17240: Minibatch Loss: 2.084408\n",
            "Step 17242: Minibatch Loss: 2.013466\n",
            "Step 17244: Minibatch Loss: 2.082486\n",
            "Step 17246: Minibatch Loss: 2.148296\n",
            "Step 17248: Minibatch Loss: 2.071267\n",
            "Step 17250: Minibatch Loss: 2.093966\n",
            "Step 17252: Minibatch Loss: 2.077336\n",
            "Step 17254: Minibatch Loss: 2.070381\n",
            "Step 17256: Minibatch Loss: 2.093779\n",
            "Step 17258: Minibatch Loss: 2.071428\n",
            "Step 17260: Minibatch Loss: 2.082508\n",
            "Step 17262: Minibatch Loss: 2.104107\n",
            "Step 17264: Minibatch Loss: 2.046870\n",
            "Step 17266: Minibatch Loss: 2.134226\n",
            "Step 17268: Minibatch Loss: 2.010458\n",
            "Step 17270: Minibatch Loss: 2.085210\n",
            "Step 17272: Minibatch Loss: 2.105309\n",
            "Step 17274: Minibatch Loss: 2.016547\n",
            "Step 17276: Minibatch Loss: 2.065816\n",
            "Step 17278: Minibatch Loss: 2.109957\n",
            "Step 17280: Minibatch Loss: 2.145716\n",
            "Step 17282: Minibatch Loss: 2.068321\n",
            "Step 17284: Minibatch Loss: 2.049762\n",
            "Step 17286: Minibatch Loss: 2.138138\n",
            "Step 17288: Minibatch Loss: 2.064537\n",
            "Step 17290: Minibatch Loss: 2.110779\n",
            "Step 17292: Minibatch Loss: 2.073489\n",
            "Step 17294: Minibatch Loss: 2.092461\n",
            "Step 17296: Minibatch Loss: 2.077911\n",
            "Step 17298: Minibatch Loss: 2.096491\n",
            "Step 17300: Minibatch Loss: 2.007018\n",
            "Step 17302: Minibatch Loss: 2.146337\n",
            "Step 17304: Minibatch Loss: 2.137832\n",
            "Step 17306: Minibatch Loss: 2.035466\n",
            "Step 17308: Minibatch Loss: 2.138524\n",
            "Step 17310: Minibatch Loss: 2.077127\n",
            "Step 17312: Minibatch Loss: 2.129245\n",
            "Step 17314: Minibatch Loss: 2.082835\n",
            "Step 17316: Minibatch Loss: 2.089011\n",
            "Step 17318: Minibatch Loss: 2.046170\n",
            "Step 17320: Minibatch Loss: 2.113291\n",
            "Step 17322: Minibatch Loss: 1.996536\n",
            "Step 17324: Minibatch Loss: 2.091201\n",
            "Step 17326: Minibatch Loss: 2.090540\n",
            "Step 17328: Minibatch Loss: 2.051974\n",
            "Step 17330: Minibatch Loss: 2.091539\n",
            "Step 17332: Minibatch Loss: 2.012991\n",
            "Step 17334: Minibatch Loss: 2.047350\n",
            "Step 17336: Minibatch Loss: 2.109144\n",
            "Step 17338: Minibatch Loss: 2.107485\n",
            "Step 17340: Minibatch Loss: 2.126044\n",
            "Step 17342: Minibatch Loss: 2.037755\n",
            "Step 17344: Minibatch Loss: 2.041749\n",
            "Step 17346: Minibatch Loss: 2.077381\n",
            "Step 17348: Minibatch Loss: 2.104173\n",
            "Step 17350: Minibatch Loss: 2.028699\n",
            "Step 17352: Minibatch Loss: 2.074021\n",
            "Step 17354: Minibatch Loss: 2.149994\n",
            "Step 17356: Minibatch Loss: 2.086180\n",
            "Step 17358: Minibatch Loss: 2.059958\n",
            "Step 17360: Minibatch Loss: 2.024670\n",
            "Step 17362: Minibatch Loss: 2.067603\n",
            "Step 17364: Minibatch Loss: 2.026858\n",
            "Step 17366: Minibatch Loss: 2.088522\n",
            "Step 17368: Minibatch Loss: 2.034764\n",
            "Step 17370: Minibatch Loss: 2.135064\n",
            "Step 17372: Minibatch Loss: 2.067051\n",
            "Step 17374: Minibatch Loss: 2.091211\n",
            "Step 17376: Minibatch Loss: 2.107966\n",
            "Step 17378: Minibatch Loss: 2.100433\n",
            "Step 17380: Minibatch Loss: 2.106223\n",
            "Step 17382: Minibatch Loss: 2.099280\n",
            "Step 17384: Minibatch Loss: 2.038517\n",
            "Step 17386: Minibatch Loss: 2.030493\n",
            "Step 17388: Minibatch Loss: 2.016699\n",
            "Step 17390: Minibatch Loss: 2.127650\n",
            "Step 17392: Minibatch Loss: 2.043415\n",
            "Step 17394: Minibatch Loss: 2.052010\n",
            "Step 17396: Minibatch Loss: 2.060948\n",
            "Step 17398: Minibatch Loss: 2.040287\n",
            "Step 17400: Minibatch Loss: 2.058195\n",
            "Step 17402: Minibatch Loss: 2.087642\n",
            "Step 17404: Minibatch Loss: 2.138080\n",
            "Step 17406: Minibatch Loss: 2.055133\n",
            "Step 17408: Minibatch Loss: 2.061610\n",
            "Step 17410: Minibatch Loss: 2.019651\n",
            "Step 17412: Minibatch Loss: 2.144182\n",
            "Step 17414: Minibatch Loss: 2.057387\n",
            "Step 17416: Minibatch Loss: 2.027017\n",
            "Step 17418: Minibatch Loss: 2.009436\n",
            "Step 17420: Minibatch Loss: 2.135900\n",
            "Step 17422: Minibatch Loss: 2.129940\n",
            "Step 17424: Minibatch Loss: 2.084811\n",
            "Step 17426: Minibatch Loss: 2.059695\n",
            "Step 17428: Minibatch Loss: 2.016596\n",
            "Step 17430: Minibatch Loss: 2.076659\n",
            "Step 17432: Minibatch Loss: 2.076796\n",
            "Step 17434: Minibatch Loss: 2.039874\n",
            "Step 17436: Minibatch Loss: 2.143954\n",
            "Step 17438: Minibatch Loss: 2.032745\n",
            "Step 17440: Minibatch Loss: 2.103103\n",
            "Step 17442: Minibatch Loss: 2.024292\n",
            "Step 17444: Minibatch Loss: 2.038020\n",
            "Step 17446: Minibatch Loss: 2.035792\n",
            "Step 17448: Minibatch Loss: 2.047286\n",
            "Step 17450: Minibatch Loss: 2.096603\n",
            "Step 17452: Minibatch Loss: 2.086727\n",
            "Step 17454: Minibatch Loss: 2.073669\n",
            "Step 17456: Minibatch Loss: 2.133965\n",
            "Step 17458: Minibatch Loss: 2.010221\n",
            "Step 17460: Minibatch Loss: 2.104477\n",
            "Step 17462: Minibatch Loss: 2.106956\n",
            "Step 17464: Minibatch Loss: 2.012996\n",
            "Step 17466: Minibatch Loss: 2.120451\n",
            "Step 17468: Minibatch Loss: 2.025603\n",
            "Step 17470: Minibatch Loss: 2.084198\n",
            "Step 17472: Minibatch Loss: 2.072682\n",
            "Step 17474: Minibatch Loss: 2.087487\n",
            "Step 17476: Minibatch Loss: 2.135842\n",
            "Step 17478: Minibatch Loss: 2.018585\n",
            "Step 17480: Minibatch Loss: 2.063307\n",
            "Step 17482: Minibatch Loss: 2.141738\n",
            "Step 17484: Minibatch Loss: 2.093062\n",
            "Step 17486: Minibatch Loss: 2.090144\n",
            "Step 17488: Minibatch Loss: 2.113438\n",
            "Step 17490: Minibatch Loss: 2.115781\n",
            "Step 17492: Minibatch Loss: 2.032353\n",
            "Step 17494: Minibatch Loss: 2.106226\n",
            "Step 17496: Minibatch Loss: 2.012776\n",
            "Step 17498: Minibatch Loss: 2.103082\n",
            "Step 17500: Minibatch Loss: 2.011907\n",
            "Step 17502: Minibatch Loss: 2.119765\n",
            "Step 17504: Minibatch Loss: 2.118575\n",
            "Step 17506: Minibatch Loss: 2.106926\n",
            "Step 17508: Minibatch Loss: 2.107768\n",
            "Step 17510: Minibatch Loss: 2.104410\n",
            "Step 17512: Minibatch Loss: 2.093748\n",
            "Step 17514: Minibatch Loss: 2.030502\n",
            "Step 17516: Minibatch Loss: 2.073791\n",
            "Step 17518: Minibatch Loss: 2.085847\n",
            "Step 17520: Minibatch Loss: 2.090575\n",
            "Step 17522: Minibatch Loss: 2.022276\n",
            "Step 17524: Minibatch Loss: 2.054632\n",
            "Step 17526: Minibatch Loss: 2.083735\n",
            "Step 17528: Minibatch Loss: 2.038647\n",
            "Step 17530: Minibatch Loss: 2.112927\n",
            "Step 17532: Minibatch Loss: 2.037429\n",
            "Step 17534: Minibatch Loss: 2.050923\n",
            "Step 17536: Minibatch Loss: 2.152417\n",
            "Step 17538: Minibatch Loss: 2.044605\n",
            "Step 17540: Minibatch Loss: 2.048872\n",
            "Step 17542: Minibatch Loss: 2.004452\n",
            "Step 17544: Minibatch Loss: 2.050266\n",
            "Step 17546: Minibatch Loss: 2.017785\n",
            "Step 17548: Minibatch Loss: 2.077526\n",
            "Step 17550: Minibatch Loss: 2.036372\n",
            "Step 17552: Minibatch Loss: 2.021012\n",
            "Step 17554: Minibatch Loss: 2.049193\n",
            "Step 17556: Minibatch Loss: 2.113120\n",
            "Step 17558: Minibatch Loss: 2.070172\n",
            "Step 17560: Minibatch Loss: 1.966468\n",
            "Step 17562: Minibatch Loss: 2.088334\n",
            "Step 17564: Minibatch Loss: 2.006357\n",
            "Step 17566: Minibatch Loss: 2.060292\n",
            "Step 17568: Minibatch Loss: 2.017007\n",
            "Step 17570: Minibatch Loss: 2.082611\n",
            "Step 17572: Minibatch Loss: 2.100013\n",
            "Step 17574: Minibatch Loss: 2.129449\n",
            "Step 17576: Minibatch Loss: 2.034042\n",
            "Step 17578: Minibatch Loss: 2.073644\n",
            "Step 17580: Minibatch Loss: 2.108613\n",
            "Step 17582: Minibatch Loss: 2.112521\n",
            "Step 17584: Minibatch Loss: 2.087918\n",
            "Step 17586: Minibatch Loss: 2.084402\n",
            "Step 17588: Minibatch Loss: 2.020180\n",
            "Step 17590: Minibatch Loss: 2.069021\n",
            "Step 17592: Minibatch Loss: 2.048305\n",
            "Step 17594: Minibatch Loss: 2.065963\n",
            "Step 17596: Minibatch Loss: 2.001993\n",
            "Step 17598: Minibatch Loss: 2.084593\n",
            "Step 17600: Minibatch Loss: 2.024646\n",
            "Step 17602: Minibatch Loss: 2.083375\n",
            "Step 17604: Minibatch Loss: 2.062562\n",
            "Step 17606: Minibatch Loss: 2.103809\n",
            "Step 17608: Minibatch Loss: 2.082616\n",
            "Step 17610: Minibatch Loss: 2.071073\n",
            "Step 17612: Minibatch Loss: 2.110146\n",
            "Step 17614: Minibatch Loss: 2.151602\n",
            "Step 17616: Minibatch Loss: 2.031835\n",
            "Step 17618: Minibatch Loss: 2.092394\n",
            "Step 17620: Minibatch Loss: 2.096230\n",
            "Step 17622: Minibatch Loss: 2.023106\n",
            "Step 17624: Minibatch Loss: 2.036558\n",
            "Step 17626: Minibatch Loss: 2.077958\n",
            "Step 17628: Minibatch Loss: 2.056726\n",
            "Step 17630: Minibatch Loss: 2.030742\n",
            "Step 17632: Minibatch Loss: 2.027089\n",
            "Step 17634: Minibatch Loss: 1.986217\n",
            "Step 17636: Minibatch Loss: 2.145900\n",
            "Step 17638: Minibatch Loss: 2.030543\n",
            "Step 17640: Minibatch Loss: 2.079201\n",
            "Step 17642: Minibatch Loss: 1.995047\n",
            "Step 17644: Minibatch Loss: 2.081203\n",
            "Step 17646: Minibatch Loss: 2.041517\n",
            "Step 17648: Minibatch Loss: 2.048215\n",
            "Step 17650: Minibatch Loss: 2.034745\n",
            "Step 17652: Minibatch Loss: 2.031195\n",
            "Step 17654: Minibatch Loss: 2.110187\n",
            "Step 17656: Minibatch Loss: 2.053658\n",
            "Step 17658: Minibatch Loss: 2.057163\n",
            "Step 17660: Minibatch Loss: 2.086894\n",
            "Step 17662: Minibatch Loss: 2.144890\n",
            "Step 17664: Minibatch Loss: 2.093365\n",
            "Step 17666: Minibatch Loss: 2.146931\n",
            "Step 17668: Minibatch Loss: 2.022236\n",
            "Step 17670: Minibatch Loss: 2.096654\n",
            "Step 17672: Minibatch Loss: 2.094235\n",
            "Step 17674: Minibatch Loss: 2.054332\n",
            "Step 17676: Minibatch Loss: 2.105108\n",
            "Step 17678: Minibatch Loss: 2.026134\n",
            "Step 17680: Minibatch Loss: 2.085355\n",
            "Step 17682: Minibatch Loss: 2.109976\n",
            "Step 17684: Minibatch Loss: 2.130861\n",
            "Step 17686: Minibatch Loss: 2.091284\n",
            "Step 17688: Minibatch Loss: 1.999870\n",
            "Step 17690: Minibatch Loss: 2.116316\n",
            "Step 17692: Minibatch Loss: 2.101372\n",
            "Step 17694: Minibatch Loss: 2.010895\n",
            "Step 17696: Minibatch Loss: 1.936682\n",
            "Step 17698: Minibatch Loss: 2.075294\n",
            "Step 17700: Minibatch Loss: 2.025552\n",
            "Step 17702: Minibatch Loss: 2.090397\n",
            "Step 17704: Minibatch Loss: 2.018965\n",
            "Step 17706: Minibatch Loss: 2.082140\n",
            "Step 17708: Minibatch Loss: 2.065233\n",
            "Step 17710: Minibatch Loss: 2.018701\n",
            "Step 17712: Minibatch Loss: 2.093157\n",
            "Step 17714: Minibatch Loss: 2.065614\n",
            "Step 17716: Minibatch Loss: 2.026156\n",
            "Step 17718: Minibatch Loss: 2.084390\n",
            "Step 17720: Minibatch Loss: 2.082163\n",
            "Step 17722: Minibatch Loss: 2.057044\n",
            "Step 17724: Minibatch Loss: 2.076993\n",
            "Step 17726: Minibatch Loss: 2.128667\n",
            "Step 17728: Minibatch Loss: 2.116597\n",
            "Step 17730: Minibatch Loss: 2.025439\n",
            "Step 17732: Minibatch Loss: 2.007648\n",
            "Step 17734: Minibatch Loss: 1.986152\n",
            "Step 17736: Minibatch Loss: 2.134451\n",
            "Step 17738: Minibatch Loss: 2.073388\n",
            "Step 17740: Minibatch Loss: 2.044247\n",
            "Step 17742: Minibatch Loss: 1.976218\n",
            "Step 17744: Minibatch Loss: 2.055522\n",
            "Step 17746: Minibatch Loss: 2.001936\n",
            "Step 17748: Minibatch Loss: 2.019409\n",
            "Step 17750: Minibatch Loss: 2.044850\n",
            "Step 17752: Minibatch Loss: 2.048905\n",
            "Step 17754: Minibatch Loss: 2.023554\n",
            "Step 17756: Minibatch Loss: 2.135621\n",
            "Step 17758: Minibatch Loss: 2.049462\n",
            "Step 17760: Minibatch Loss: 2.065331\n",
            "Step 17762: Minibatch Loss: 2.072092\n",
            "Step 17764: Minibatch Loss: 1.982507\n",
            "Step 17766: Minibatch Loss: 2.127634\n",
            "Step 17768: Minibatch Loss: 1.989381\n",
            "Step 17770: Minibatch Loss: 2.088442\n",
            "Step 17772: Minibatch Loss: 2.037143\n",
            "Step 17774: Minibatch Loss: 2.067334\n",
            "Step 17776: Minibatch Loss: 2.055920\n",
            "Step 17778: Minibatch Loss: 2.077984\n",
            "Step 17780: Minibatch Loss: 2.043154\n",
            "Step 17782: Minibatch Loss: 2.027921\n",
            "Step 17784: Minibatch Loss: 2.066074\n",
            "Step 17786: Minibatch Loss: 2.099328\n",
            "Step 17788: Minibatch Loss: 2.098959\n",
            "Step 17790: Minibatch Loss: 2.055710\n",
            "Step 17792: Minibatch Loss: 2.030319\n",
            "Step 17794: Minibatch Loss: 2.054676\n",
            "Step 17796: Minibatch Loss: 2.027215\n",
            "Step 17798: Minibatch Loss: 2.078734\n",
            "Step 17800: Minibatch Loss: 2.007773\n",
            "Step 17802: Minibatch Loss: 2.153429\n",
            "Step 17804: Minibatch Loss: 2.019318\n",
            "Step 17806: Minibatch Loss: 2.066207\n",
            "Step 17808: Minibatch Loss: 2.058349\n",
            "Step 17810: Minibatch Loss: 2.019825\n",
            "Step 17812: Minibatch Loss: 2.114872\n",
            "Step 17814: Minibatch Loss: 2.143848\n",
            "Step 17816: Minibatch Loss: 2.064249\n",
            "Step 17818: Minibatch Loss: 2.055850\n",
            "Step 17820: Minibatch Loss: 2.096013\n",
            "Step 17822: Minibatch Loss: 2.065197\n",
            "Step 17824: Minibatch Loss: 2.079563\n",
            "Step 17826: Minibatch Loss: 2.096574\n",
            "Step 17828: Minibatch Loss: 2.043133\n",
            "Step 17830: Minibatch Loss: 2.061951\n",
            "Step 17832: Minibatch Loss: 2.050051\n",
            "Step 17834: Minibatch Loss: 2.067888\n",
            "Step 17836: Minibatch Loss: 2.137630\n",
            "Step 17838: Minibatch Loss: 2.034623\n",
            "Step 17840: Minibatch Loss: 2.042000\n",
            "Step 17842: Minibatch Loss: 2.070085\n",
            "Step 17844: Minibatch Loss: 2.058961\n",
            "Step 17846: Minibatch Loss: 2.020858\n",
            "Step 17848: Minibatch Loss: 2.069138\n",
            "Step 17850: Minibatch Loss: 2.002974\n",
            "Step 17852: Minibatch Loss: 2.094182\n",
            "Step 17854: Minibatch Loss: 2.080857\n",
            "Step 17856: Minibatch Loss: 2.078521\n",
            "Step 17858: Minibatch Loss: 2.056332\n",
            "Step 17860: Minibatch Loss: 1.996311\n",
            "Step 17862: Minibatch Loss: 2.079861\n",
            "Step 17864: Minibatch Loss: 2.016746\n",
            "Step 17866: Minibatch Loss: 2.088790\n",
            "Step 17868: Minibatch Loss: 1.994812\n",
            "Step 17870: Minibatch Loss: 2.042176\n",
            "Step 17872: Minibatch Loss: 2.079692\n",
            "Step 17874: Minibatch Loss: 2.080483\n",
            "Step 17876: Minibatch Loss: 2.049348\n",
            "Step 17878: Minibatch Loss: 2.051664\n",
            "Step 17880: Minibatch Loss: 2.067779\n",
            "Step 17882: Minibatch Loss: 2.102088\n",
            "Step 17884: Minibatch Loss: 2.111490\n",
            "Step 17886: Minibatch Loss: 2.080621\n",
            "Step 17888: Minibatch Loss: 2.038770\n",
            "Step 17890: Minibatch Loss: 2.053187\n",
            "Step 17892: Minibatch Loss: 2.035224\n",
            "Step 17894: Minibatch Loss: 2.073258\n",
            "Step 17896: Minibatch Loss: 1.978108\n",
            "Step 17898: Minibatch Loss: 2.103351\n",
            "Step 17900: Minibatch Loss: 2.031494\n",
            "Step 17902: Minibatch Loss: 2.112001\n",
            "Step 17904: Minibatch Loss: 2.067920\n",
            "Step 17906: Minibatch Loss: 2.065692\n",
            "Step 17908: Minibatch Loss: 2.056175\n",
            "Step 17910: Minibatch Loss: 2.060019\n",
            "Step 17912: Minibatch Loss: 2.089177\n",
            "Step 17914: Minibatch Loss: 2.141290\n",
            "Step 17916: Minibatch Loss: 2.025731\n",
            "Step 17918: Minibatch Loss: 2.127584\n",
            "Step 17920: Minibatch Loss: 2.096017\n",
            "Step 17922: Minibatch Loss: 2.050870\n",
            "Step 17924: Minibatch Loss: 2.101544\n",
            "Step 17926: Minibatch Loss: 2.085141\n",
            "Step 17928: Minibatch Loss: 2.068388\n",
            "Step 17930: Minibatch Loss: 2.061058\n",
            "Step 17932: Minibatch Loss: 2.054275\n",
            "Step 17934: Minibatch Loss: 2.039472\n",
            "Step 17936: Minibatch Loss: 2.120460\n",
            "Step 17938: Minibatch Loss: 1.997939\n",
            "Step 17940: Minibatch Loss: 2.045878\n",
            "Step 17942: Minibatch Loss: 2.016606\n",
            "Step 17944: Minibatch Loss: 2.055209\n",
            "Step 17946: Minibatch Loss: 2.107549\n",
            "Step 17948: Minibatch Loss: 2.047680\n",
            "Step 17950: Minibatch Loss: 1.975985\n",
            "Step 17952: Minibatch Loss: 2.055022\n",
            "Step 17954: Minibatch Loss: 2.066561\n",
            "Step 17956: Minibatch Loss: 2.060297\n",
            "Step 17958: Minibatch Loss: 2.056436\n",
            "Step 17960: Minibatch Loss: 1.973970\n",
            "Step 17962: Minibatch Loss: 2.086390\n",
            "Step 17964: Minibatch Loss: 1.958947\n",
            "Step 17966: Minibatch Loss: 2.066587\n",
            "Step 17968: Minibatch Loss: 1.999239\n",
            "Step 17970: Minibatch Loss: 2.100123\n",
            "Step 17972: Minibatch Loss: 2.082521\n",
            "Step 17974: Minibatch Loss: 2.057694\n",
            "Step 17976: Minibatch Loss: 2.055847\n",
            "Step 17978: Minibatch Loss: 2.022870\n",
            "Step 17980: Minibatch Loss: 2.104782\n",
            "Step 17982: Minibatch Loss: 2.057938\n",
            "Step 17984: Minibatch Loss: 2.079224\n",
            "Step 17986: Minibatch Loss: 2.084082\n",
            "Step 17988: Minibatch Loss: 2.020293\n",
            "Step 17990: Minibatch Loss: 2.047055\n",
            "Step 17992: Minibatch Loss: 2.056874\n",
            "Step 17994: Minibatch Loss: 2.010662\n",
            "Step 17996: Minibatch Loss: 2.045660\n",
            "Step 17998: Minibatch Loss: 2.089622\n",
            "Step 18000: Minibatch Loss: 2.032868\n",
            "Training for SNR= 9.0  sigma= 0.44668359215096315 iteratin: 0\n",
            "Step 18002: Minibatch Loss: 2.065307\n",
            "Step 18004: Minibatch Loss: 2.013254\n",
            "Step 18006: Minibatch Loss: 2.110492\n",
            "Step 18008: Minibatch Loss: 2.011279\n",
            "Step 18010: Minibatch Loss: 2.082918\n",
            "Step 18012: Minibatch Loss: 2.107284\n",
            "Step 18014: Minibatch Loss: 2.120639\n",
            "Step 18016: Minibatch Loss: 1.984935\n",
            "Step 18018: Minibatch Loss: 2.065467\n",
            "Step 18020: Minibatch Loss: 2.117120\n",
            "Step 18022: Minibatch Loss: 2.006819\n",
            "Step 18024: Minibatch Loss: 2.049703\n",
            "Step 18026: Minibatch Loss: 2.065976\n",
            "Step 18028: Minibatch Loss: 2.031172\n",
            "Step 18030: Minibatch Loss: 2.021656\n",
            "Step 18032: Minibatch Loss: 2.043012\n",
            "Step 18034: Minibatch Loss: 1.947963\n",
            "Step 18036: Minibatch Loss: 2.108481\n",
            "Step 18038: Minibatch Loss: 1.966430\n",
            "Step 18040: Minibatch Loss: 2.071291\n",
            "Step 18042: Minibatch Loss: 2.048849\n",
            "Step 18044: Minibatch Loss: 2.033242\n",
            "Step 18046: Minibatch Loss: 1.995719\n",
            "Step 18048: Minibatch Loss: 2.070837\n",
            "Step 18050: Minibatch Loss: 2.040227\n",
            "Step 18052: Minibatch Loss: 2.053053\n",
            "Step 18054: Minibatch Loss: 2.069389\n",
            "Step 18056: Minibatch Loss: 2.061385\n",
            "Step 18058: Minibatch Loss: 2.079490\n",
            "Step 18060: Minibatch Loss: 2.099057\n",
            "Step 18062: Minibatch Loss: 2.098280\n",
            "Step 18064: Minibatch Loss: 1.947168\n",
            "Step 18066: Minibatch Loss: 2.100401\n",
            "Step 18068: Minibatch Loss: 2.023535\n",
            "Step 18070: Minibatch Loss: 2.096062\n",
            "Step 18072: Minibatch Loss: 2.112928\n",
            "Step 18074: Minibatch Loss: 2.021311\n",
            "Step 18076: Minibatch Loss: 2.051542\n",
            "Step 18078: Minibatch Loss: 2.044664\n",
            "Step 18080: Minibatch Loss: 2.048277\n",
            "Step 18082: Minibatch Loss: 2.047259\n",
            "Step 18084: Minibatch Loss: 2.042827\n",
            "Step 18086: Minibatch Loss: 2.066891\n",
            "Step 18088: Minibatch Loss: 2.068309\n",
            "Step 18090: Minibatch Loss: 2.033141\n",
            "Step 18092: Minibatch Loss: 2.014151\n",
            "Step 18094: Minibatch Loss: 1.994821\n",
            "Step 18096: Minibatch Loss: 1.993517\n",
            "Step 18098: Minibatch Loss: 2.065275\n",
            "Step 18100: Minibatch Loss: 1.988033\n",
            "Step 18102: Minibatch Loss: 2.012312\n",
            "Step 18104: Minibatch Loss: 2.121045\n",
            "Step 18106: Minibatch Loss: 2.018588\n",
            "Step 18108: Minibatch Loss: 2.022933\n",
            "Step 18110: Minibatch Loss: 2.038854\n",
            "Step 18112: Minibatch Loss: 2.111765\n",
            "Step 18114: Minibatch Loss: 2.053547\n",
            "Step 18116: Minibatch Loss: 2.077181\n",
            "Step 18118: Minibatch Loss: 2.067889\n",
            "Step 18120: Minibatch Loss: 2.063553\n",
            "Step 18122: Minibatch Loss: 1.995276\n",
            "Step 18124: Minibatch Loss: 2.104985\n",
            "Step 18126: Minibatch Loss: 2.075568\n",
            "Step 18128: Minibatch Loss: 2.016821\n",
            "Step 18130: Minibatch Loss: 2.074347\n",
            "Step 18132: Minibatch Loss: 2.051183\n",
            "Step 18134: Minibatch Loss: 1.997941\n",
            "Step 18136: Minibatch Loss: 2.117851\n",
            "Step 18138: Minibatch Loss: 2.031905\n",
            "Step 18140: Minibatch Loss: 2.050147\n",
            "Step 18142: Minibatch Loss: 1.976964\n",
            "Step 18144: Minibatch Loss: 2.047310\n",
            "Step 18146: Minibatch Loss: 2.060909\n",
            "Step 18148: Minibatch Loss: 2.054617\n",
            "Step 18150: Minibatch Loss: 2.057471\n",
            "Step 18152: Minibatch Loss: 2.043712\n",
            "Step 18154: Minibatch Loss: 2.072667\n",
            "Step 18156: Minibatch Loss: 2.075015\n",
            "Step 18158: Minibatch Loss: 2.039245\n",
            "Step 18160: Minibatch Loss: 2.010672\n",
            "Step 18162: Minibatch Loss: 2.089454\n",
            "Step 18164: Minibatch Loss: 1.965924\n",
            "Step 18166: Minibatch Loss: 2.086171\n",
            "Step 18168: Minibatch Loss: 1.955672\n",
            "Step 18170: Minibatch Loss: 2.076640\n",
            "Step 18172: Minibatch Loss: 2.076538\n",
            "Step 18174: Minibatch Loss: 2.008591\n",
            "Step 18176: Minibatch Loss: 1.964413\n",
            "Step 18178: Minibatch Loss: 2.072773\n",
            "Step 18180: Minibatch Loss: 2.055418\n",
            "Step 18182: Minibatch Loss: 2.064813\n",
            "Step 18184: Minibatch Loss: 2.092036\n",
            "Step 18186: Minibatch Loss: 2.033432\n",
            "Step 18188: Minibatch Loss: 2.092074\n",
            "Step 18190: Minibatch Loss: 2.028435\n",
            "Step 18192: Minibatch Loss: 2.028450\n",
            "Step 18194: Minibatch Loss: 2.065161\n",
            "Step 18196: Minibatch Loss: 2.015028\n",
            "Step 18198: Minibatch Loss: 2.037228\n",
            "Step 18200: Minibatch Loss: 2.021748\n",
            "Step 18202: Minibatch Loss: 2.086149\n",
            "Step 18204: Minibatch Loss: 2.075591\n",
            "Step 18206: Minibatch Loss: 2.023261\n",
            "Step 18208: Minibatch Loss: 2.012501\n",
            "Step 18210: Minibatch Loss: 2.073070\n",
            "Step 18212: Minibatch Loss: 2.041617\n",
            "Step 18214: Minibatch Loss: 2.023746\n",
            "Step 18216: Minibatch Loss: 2.038900\n",
            "Step 18218: Minibatch Loss: 2.049130\n",
            "Step 18220: Minibatch Loss: 2.109647\n",
            "Step 18222: Minibatch Loss: 2.068598\n",
            "Step 18224: Minibatch Loss: 2.082174\n",
            "Step 18226: Minibatch Loss: 2.101578\n",
            "Step 18228: Minibatch Loss: 2.066874\n",
            "Step 18230: Minibatch Loss: 2.001516\n",
            "Step 18232: Minibatch Loss: 1.985253\n",
            "Step 18234: Minibatch Loss: 1.983193\n",
            "Step 18236: Minibatch Loss: 2.111033\n",
            "Step 18238: Minibatch Loss: 1.994068\n",
            "Step 18240: Minibatch Loss: 2.088916\n",
            "Step 18242: Minibatch Loss: 2.003112\n",
            "Step 18244: Minibatch Loss: 2.048497\n",
            "Step 18246: Minibatch Loss: 2.055411\n",
            "Step 18248: Minibatch Loss: 2.089027\n",
            "Step 18250: Minibatch Loss: 2.056089\n",
            "Step 18252: Minibatch Loss: 2.009496\n",
            "Step 18254: Minibatch Loss: 2.040259\n",
            "Step 18256: Minibatch Loss: 2.029119\n",
            "Step 18258: Minibatch Loss: 2.072004\n",
            "Step 18260: Minibatch Loss: 2.049781\n",
            "Step 18262: Minibatch Loss: 2.071088\n",
            "Step 18264: Minibatch Loss: 2.029914\n",
            "Step 18266: Minibatch Loss: 2.106283\n",
            "Step 18268: Minibatch Loss: 1.970048\n",
            "Step 18270: Minibatch Loss: 2.112558\n",
            "Step 18272: Minibatch Loss: 2.105863\n",
            "Step 18274: Minibatch Loss: 2.033967\n",
            "Step 18276: Minibatch Loss: 2.031843\n",
            "Step 18278: Minibatch Loss: 2.053193\n",
            "Step 18280: Minibatch Loss: 2.127879\n",
            "Step 18282: Minibatch Loss: 2.089318\n",
            "Step 18284: Minibatch Loss: 2.085677\n",
            "Step 18286: Minibatch Loss: 2.006818\n",
            "Step 18288: Minibatch Loss: 1.994613\n",
            "Step 18290: Minibatch Loss: 2.023616\n",
            "Step 18292: Minibatch Loss: 2.044527\n",
            "Step 18294: Minibatch Loss: 2.091594\n",
            "Step 18296: Minibatch Loss: 1.989995\n",
            "Step 18298: Minibatch Loss: 2.045194\n",
            "Step 18300: Minibatch Loss: 2.022085\n",
            "Step 18302: Minibatch Loss: 2.090972\n",
            "Step 18304: Minibatch Loss: 2.074607\n",
            "Step 18306: Minibatch Loss: 2.017128\n",
            "Step 18308: Minibatch Loss: 2.084295\n",
            "Step 18310: Minibatch Loss: 2.002718\n",
            "Step 18312: Minibatch Loss: 2.058608\n",
            "Step 18314: Minibatch Loss: 1.984048\n",
            "Step 18316: Minibatch Loss: 2.011457\n",
            "Step 18318: Minibatch Loss: 2.113699\n",
            "Step 18320: Minibatch Loss: 2.071835\n",
            "Step 18322: Minibatch Loss: 2.021656\n",
            "Step 18324: Minibatch Loss: 2.099134\n",
            "Step 18326: Minibatch Loss: 2.043516\n",
            "Step 18328: Minibatch Loss: 1.971062\n",
            "Step 18330: Minibatch Loss: 2.001227\n",
            "Step 18332: Minibatch Loss: 1.998640\n",
            "Step 18334: Minibatch Loss: 1.973961\n",
            "Step 18336: Minibatch Loss: 2.105049\n",
            "Step 18338: Minibatch Loss: 1.990638\n",
            "Step 18340: Minibatch Loss: 1.990330\n",
            "Step 18342: Minibatch Loss: 1.940313\n",
            "Step 18344: Minibatch Loss: 2.035112\n",
            "Step 18346: Minibatch Loss: 2.039638\n",
            "Step 18348: Minibatch Loss: 2.068824\n",
            "Step 18350: Minibatch Loss: 2.012505\n",
            "Step 18352: Minibatch Loss: 1.958920\n",
            "Step 18354: Minibatch Loss: 2.080097\n",
            "Step 18356: Minibatch Loss: 2.005110\n",
            "Step 18358: Minibatch Loss: 2.084333\n",
            "Step 18360: Minibatch Loss: 2.043621\n",
            "Step 18362: Minibatch Loss: 2.067729\n",
            "Step 18364: Minibatch Loss: 1.994499\n",
            "Step 18366: Minibatch Loss: 2.035416\n",
            "Step 18368: Minibatch Loss: 2.000407\n",
            "Step 18370: Minibatch Loss: 2.024833\n",
            "Step 18372: Minibatch Loss: 2.034849\n",
            "Step 18374: Minibatch Loss: 2.018861\n",
            "Step 18376: Minibatch Loss: 2.009571\n",
            "Step 18378: Minibatch Loss: 2.020032\n",
            "Step 18380: Minibatch Loss: 2.009171\n",
            "Step 18382: Minibatch Loss: 2.039530\n",
            "Step 18384: Minibatch Loss: 2.109524\n",
            "Step 18386: Minibatch Loss: 2.043845\n",
            "Step 18388: Minibatch Loss: 2.007402\n",
            "Step 18390: Minibatch Loss: 2.069392\n",
            "Step 18392: Minibatch Loss: 2.055723\n",
            "Step 18394: Minibatch Loss: 2.021086\n",
            "Step 18396: Minibatch Loss: 1.980275\n",
            "Step 18398: Minibatch Loss: 2.021223\n",
            "Step 18400: Minibatch Loss: 1.966619\n",
            "Step 18402: Minibatch Loss: 2.109973\n",
            "Step 18404: Minibatch Loss: 2.079478\n",
            "Step 18406: Minibatch Loss: 2.001468\n",
            "Step 18408: Minibatch Loss: 2.058171\n",
            "Step 18410: Minibatch Loss: 1.996399\n",
            "Step 18412: Minibatch Loss: 2.067328\n",
            "Step 18414: Minibatch Loss: 2.106303\n",
            "Step 18416: Minibatch Loss: 1.933895\n",
            "Step 18418: Minibatch Loss: 2.033561\n",
            "Step 18420: Minibatch Loss: 2.038237\n",
            "Step 18422: Minibatch Loss: 1.958335\n",
            "Step 18424: Minibatch Loss: 2.050945\n",
            "Step 18426: Minibatch Loss: 2.077890\n",
            "Step 18428: Minibatch Loss: 2.001087\n",
            "Step 18430: Minibatch Loss: 2.034468\n",
            "Step 18432: Minibatch Loss: 2.007685\n",
            "Step 18434: Minibatch Loss: 2.030093\n",
            "Step 18436: Minibatch Loss: 2.079921\n",
            "Step 18438: Minibatch Loss: 1.979387\n",
            "Step 18440: Minibatch Loss: 2.072243\n",
            "Step 18442: Minibatch Loss: 2.022504\n",
            "Step 18444: Minibatch Loss: 2.014084\n",
            "Step 18446: Minibatch Loss: 2.050573\n",
            "Step 18448: Minibatch Loss: 2.005028\n",
            "Step 18450: Minibatch Loss: 2.062828\n",
            "Step 18452: Minibatch Loss: 2.117023\n",
            "Step 18454: Minibatch Loss: 2.043067\n",
            "Step 18456: Minibatch Loss: 2.122145\n",
            "Step 18458: Minibatch Loss: 2.034052\n",
            "Step 18460: Minibatch Loss: 2.020038\n",
            "Step 18462: Minibatch Loss: 2.073340\n",
            "Step 18464: Minibatch Loss: 1.935553\n",
            "Step 18466: Minibatch Loss: 2.027501\n",
            "Step 18468: Minibatch Loss: 1.961686\n",
            "Step 18470: Minibatch Loss: 2.108124\n",
            "Step 18472: Minibatch Loss: 2.062239\n",
            "Step 18474: Minibatch Loss: 2.031025\n",
            "Step 18476: Minibatch Loss: 1.994586\n",
            "Step 18478: Minibatch Loss: 2.010750\n",
            "Step 18480: Minibatch Loss: 2.069926\n",
            "Step 18482: Minibatch Loss: 2.044761\n",
            "Step 18484: Minibatch Loss: 2.082934\n",
            "Step 18486: Minibatch Loss: 2.027815\n",
            "Step 18488: Minibatch Loss: 2.003286\n",
            "Step 18490: Minibatch Loss: 2.085351\n",
            "Step 18492: Minibatch Loss: 2.038758\n",
            "Step 18494: Minibatch Loss: 2.017467\n",
            "Step 18496: Minibatch Loss: 1.990039\n",
            "Step 18498: Minibatch Loss: 2.020563\n",
            "Step 18500: Minibatch Loss: 1.989419\n",
            "Step 18502: Minibatch Loss: 2.095589\n",
            "Step 18504: Minibatch Loss: 2.085802\n",
            "Step 18506: Minibatch Loss: 2.019778\n",
            "Step 18508: Minibatch Loss: 2.021286\n",
            "Step 18510: Minibatch Loss: 2.002899\n",
            "Step 18512: Minibatch Loss: 2.022391\n",
            "Step 18514: Minibatch Loss: 2.064003\n",
            "Step 18516: Minibatch Loss: 2.054385\n",
            "Step 18518: Minibatch Loss: 2.081958\n",
            "Step 18520: Minibatch Loss: 2.148359\n",
            "Step 18522: Minibatch Loss: 1.994602\n",
            "Step 18524: Minibatch Loss: 2.086524\n",
            "Step 18526: Minibatch Loss: 1.993933\n",
            "Step 18528: Minibatch Loss: 2.006693\n",
            "Step 18530: Minibatch Loss: 2.021113\n",
            "Step 18532: Minibatch Loss: 2.032512\n",
            "Step 18534: Minibatch Loss: 1.958915\n",
            "Step 18536: Minibatch Loss: 2.090865\n",
            "Step 18538: Minibatch Loss: 1.960257\n",
            "Step 18540: Minibatch Loss: 2.043269\n",
            "Step 18542: Minibatch Loss: 2.022237\n",
            "Step 18544: Minibatch Loss: 2.052989\n",
            "Step 18546: Minibatch Loss: 1.985544\n",
            "Step 18548: Minibatch Loss: 2.031050\n",
            "Step 18550: Minibatch Loss: 2.003347\n",
            "Step 18552: Minibatch Loss: 1.965914\n",
            "Step 18554: Minibatch Loss: 2.073697\n",
            "Step 18556: Minibatch Loss: 2.052936\n",
            "Step 18558: Minibatch Loss: 2.086395\n",
            "Step 18560: Minibatch Loss: 2.009187\n",
            "Step 18562: Minibatch Loss: 2.053994\n",
            "Step 18564: Minibatch Loss: 2.042684\n",
            "Step 18566: Minibatch Loss: 2.119515\n",
            "Step 18568: Minibatch Loss: 1.953009\n",
            "Step 18570: Minibatch Loss: 2.040105\n",
            "Step 18572: Minibatch Loss: 2.058867\n",
            "Step 18574: Minibatch Loss: 2.062099\n",
            "Step 18576: Minibatch Loss: 2.026128\n",
            "Step 18578: Minibatch Loss: 2.069439\n",
            "Step 18580: Minibatch Loss: 2.011744\n",
            "Step 18582: Minibatch Loss: 2.012793\n",
            "Step 18584: Minibatch Loss: 2.105014\n",
            "Step 18586: Minibatch Loss: 2.083504\n",
            "Step 18588: Minibatch Loss: 2.037062\n",
            "Step 18590: Minibatch Loss: 2.133901\n",
            "Step 18592: Minibatch Loss: 2.028787\n",
            "Step 18594: Minibatch Loss: 1.992235\n",
            "Step 18596: Minibatch Loss: 2.032258\n",
            "Step 18598: Minibatch Loss: 2.021385\n",
            "Step 18600: Minibatch Loss: 2.056685\n",
            "Step 18602: Minibatch Loss: 2.070827\n",
            "Step 18604: Minibatch Loss: 2.017734\n",
            "Step 18606: Minibatch Loss: 2.017155\n",
            "Step 18608: Minibatch Loss: 2.021232\n",
            "Step 18610: Minibatch Loss: 2.024996\n",
            "Step 18612: Minibatch Loss: 2.109004\n",
            "Step 18614: Minibatch Loss: 2.046754\n",
            "Step 18616: Minibatch Loss: 2.056249\n",
            "Step 18618: Minibatch Loss: 2.031886\n",
            "Step 18620: Minibatch Loss: 2.051210\n",
            "Step 18622: Minibatch Loss: 2.022584\n",
            "Step 18624: Minibatch Loss: 2.038110\n",
            "Step 18626: Minibatch Loss: 2.056094\n",
            "Step 18628: Minibatch Loss: 1.995724\n",
            "Step 18630: Minibatch Loss: 2.037756\n",
            "Step 18632: Minibatch Loss: 1.961437\n",
            "Step 18634: Minibatch Loss: 2.029534\n",
            "Step 18636: Minibatch Loss: 2.087285\n",
            "Step 18638: Minibatch Loss: 1.989158\n",
            "Step 18640: Minibatch Loss: 2.012924\n",
            "Step 18642: Minibatch Loss: 2.035496\n",
            "Step 18644: Minibatch Loss: 2.068115\n",
            "Step 18646: Minibatch Loss: 1.974020\n",
            "Step 18648: Minibatch Loss: 2.045761\n",
            "Step 18650: Minibatch Loss: 1.996838\n",
            "Step 18652: Minibatch Loss: 2.021776\n",
            "Step 18654: Minibatch Loss: 2.033119\n",
            "Step 18656: Minibatch Loss: 2.076501\n",
            "Step 18658: Minibatch Loss: 2.040058\n",
            "Step 18660: Minibatch Loss: 2.059804\n",
            "Step 18662: Minibatch Loss: 2.011521\n",
            "Step 18664: Minibatch Loss: 2.002721\n",
            "Step 18666: Minibatch Loss: 2.087460\n",
            "Step 18668: Minibatch Loss: 1.989973\n",
            "Step 18670: Minibatch Loss: 2.041859\n",
            "Step 18672: Minibatch Loss: 2.087023\n",
            "Step 18674: Minibatch Loss: 2.051470\n",
            "Step 18676: Minibatch Loss: 2.070681\n",
            "Step 18678: Minibatch Loss: 2.012256\n",
            "Step 18680: Minibatch Loss: 2.085063\n",
            "Step 18682: Minibatch Loss: 1.990944\n",
            "Step 18684: Minibatch Loss: 2.071690\n",
            "Step 18686: Minibatch Loss: 2.033472\n",
            "Step 18688: Minibatch Loss: 2.064862\n",
            "Step 18690: Minibatch Loss: 2.060560\n",
            "Step 18692: Minibatch Loss: 2.001281\n",
            "Step 18694: Minibatch Loss: 2.006038\n",
            "Step 18696: Minibatch Loss: 1.968546\n",
            "Step 18698: Minibatch Loss: 2.020931\n",
            "Step 18700: Minibatch Loss: 2.034698\n",
            "Step 18702: Minibatch Loss: 2.065772\n",
            "Step 18704: Minibatch Loss: 1.980703\n",
            "Step 18706: Minibatch Loss: 2.029705\n",
            "Step 18708: Minibatch Loss: 2.055995\n",
            "Step 18710: Minibatch Loss: 1.998620\n",
            "Step 18712: Minibatch Loss: 2.041178\n",
            "Step 18714: Minibatch Loss: 2.061440\n",
            "Step 18716: Minibatch Loss: 1.980513\n",
            "Step 18718: Minibatch Loss: 2.090245\n",
            "Step 18720: Minibatch Loss: 2.069922\n",
            "Step 18722: Minibatch Loss: 2.002434\n",
            "Step 18724: Minibatch Loss: 2.035593\n",
            "Step 18726: Minibatch Loss: 2.014381\n",
            "Step 18728: Minibatch Loss: 1.994943\n",
            "Step 18730: Minibatch Loss: 2.043837\n",
            "Step 18732: Minibatch Loss: 2.010987\n",
            "Step 18734: Minibatch Loss: 1.925300\n",
            "Step 18736: Minibatch Loss: 2.079427\n",
            "Step 18738: Minibatch Loss: 2.011703\n",
            "Step 18740: Minibatch Loss: 1.989537\n",
            "Step 18742: Minibatch Loss: 2.003427\n",
            "Step 18744: Minibatch Loss: 2.051908\n",
            "Step 18746: Minibatch Loss: 2.012574\n",
            "Step 18748: Minibatch Loss: 2.071344\n",
            "Step 18750: Minibatch Loss: 1.979256\n",
            "Step 18752: Minibatch Loss: 1.990995\n",
            "Step 18754: Minibatch Loss: 2.062675\n",
            "Step 18756: Minibatch Loss: 2.012164\n",
            "Step 18758: Minibatch Loss: 2.027893\n",
            "Step 18760: Minibatch Loss: 1.987590\n",
            "Step 18762: Minibatch Loss: 2.039868\n",
            "Step 18764: Minibatch Loss: 2.022264\n",
            "Step 18766: Minibatch Loss: 2.073769\n",
            "Step 18768: Minibatch Loss: 2.002599\n",
            "Step 18770: Minibatch Loss: 2.076802\n",
            "Step 18772: Minibatch Loss: 2.010510\n",
            "Step 18774: Minibatch Loss: 2.014974\n",
            "Step 18776: Minibatch Loss: 1.985954\n",
            "Step 18778: Minibatch Loss: 2.027893\n",
            "Step 18780: Minibatch Loss: 2.050656\n",
            "Step 18782: Minibatch Loss: 2.081410\n",
            "Step 18784: Minibatch Loss: 2.001623\n",
            "Step 18786: Minibatch Loss: 1.978877\n",
            "Step 18788: Minibatch Loss: 1.995104\n",
            "Step 18790: Minibatch Loss: 2.087351\n",
            "Step 18792: Minibatch Loss: 2.049986\n",
            "Step 18794: Minibatch Loss: 1.969134\n",
            "Step 18796: Minibatch Loss: 1.948457\n",
            "Step 18798: Minibatch Loss: 2.037197\n",
            "Step 18800: Minibatch Loss: 1.947945\n",
            "Step 18802: Minibatch Loss: 2.081510\n",
            "Step 18804: Minibatch Loss: 2.103475\n",
            "Step 18806: Minibatch Loss: 1.975076\n",
            "Step 18808: Minibatch Loss: 2.078725\n",
            "Step 18810: Minibatch Loss: 1.995076\n",
            "Step 18812: Minibatch Loss: 2.105131\n",
            "Step 18814: Minibatch Loss: 2.027829\n",
            "Step 18816: Minibatch Loss: 2.022339\n",
            "Step 18818: Minibatch Loss: 2.114261\n",
            "Step 18820: Minibatch Loss: 2.059780\n",
            "Step 18822: Minibatch Loss: 2.044343\n",
            "Step 18824: Minibatch Loss: 2.070310\n",
            "Step 18826: Minibatch Loss: 2.129908\n",
            "Step 18828: Minibatch Loss: 2.000732\n",
            "Step 18830: Minibatch Loss: 2.016100\n",
            "Step 18832: Minibatch Loss: 1.983432\n",
            "Step 18834: Minibatch Loss: 2.011127\n",
            "Step 18836: Minibatch Loss: 2.081847\n",
            "Step 18838: Minibatch Loss: 1.971338\n",
            "Step 18840: Minibatch Loss: 2.032160\n",
            "Step 18842: Minibatch Loss: 1.973829\n",
            "Step 18844: Minibatch Loss: 2.024899\n",
            "Step 18846: Minibatch Loss: 1.959508\n",
            "Step 18848: Minibatch Loss: 2.026480\n",
            "Step 18850: Minibatch Loss: 1.981942\n",
            "Step 18852: Minibatch Loss: 2.040334\n",
            "Step 18854: Minibatch Loss: 2.052872\n",
            "Step 18856: Minibatch Loss: 2.052283\n",
            "Step 18858: Minibatch Loss: 2.022340\n",
            "Step 18860: Minibatch Loss: 2.003294\n",
            "Step 18862: Minibatch Loss: 2.029304\n",
            "Step 18864: Minibatch Loss: 1.996773\n",
            "Step 18866: Minibatch Loss: 2.055459\n",
            "Step 18868: Minibatch Loss: 1.959438\n",
            "Step 18870: Minibatch Loss: 2.068682\n",
            "Step 18872: Minibatch Loss: 2.060785\n",
            "Step 18874: Minibatch Loss: 2.048357\n",
            "Step 18876: Minibatch Loss: 1.959575\n",
            "Step 18878: Minibatch Loss: 2.009651\n",
            "Step 18880: Minibatch Loss: 2.089890\n",
            "Step 18882: Minibatch Loss: 2.012870\n",
            "Step 18884: Minibatch Loss: 2.025006\n",
            "Step 18886: Minibatch Loss: 1.987939\n",
            "Step 18888: Minibatch Loss: 2.013156\n",
            "Step 18890: Minibatch Loss: 2.042994\n",
            "Step 18892: Minibatch Loss: 2.021924\n",
            "Step 18894: Minibatch Loss: 1.976759\n",
            "Step 18896: Minibatch Loss: 1.964383\n",
            "Step 18898: Minibatch Loss: 2.041068\n",
            "Step 18900: Minibatch Loss: 2.037909\n",
            "Step 18902: Minibatch Loss: 2.036589\n",
            "Step 18904: Minibatch Loss: 2.033750\n",
            "Step 18906: Minibatch Loss: 2.030847\n",
            "Step 18908: Minibatch Loss: 2.120801\n",
            "Step 18910: Minibatch Loss: 1.999692\n",
            "Step 18912: Minibatch Loss: 2.030261\n",
            "Step 18914: Minibatch Loss: 1.985618\n",
            "Step 18916: Minibatch Loss: 1.968194\n",
            "Step 18918: Minibatch Loss: 2.036947\n",
            "Step 18920: Minibatch Loss: 1.964936\n",
            "Step 18922: Minibatch Loss: 2.055889\n",
            "Step 18924: Minibatch Loss: 2.037023\n",
            "Step 18926: Minibatch Loss: 1.998761\n",
            "Step 18928: Minibatch Loss: 2.049759\n",
            "Step 18930: Minibatch Loss: 2.019217\n",
            "Step 18932: Minibatch Loss: 2.007645\n",
            "Step 18934: Minibatch Loss: 2.008449\n",
            "Step 18936: Minibatch Loss: 2.095872\n",
            "Step 18938: Minibatch Loss: 1.965323\n",
            "Step 18940: Minibatch Loss: 2.019845\n",
            "Step 18942: Minibatch Loss: 1.930199\n",
            "Step 18944: Minibatch Loss: 2.018916\n",
            "Step 18946: Minibatch Loss: 1.991688\n",
            "Step 18948: Minibatch Loss: 1.970933\n",
            "Step 18950: Minibatch Loss: 1.972529\n",
            "Step 18952: Minibatch Loss: 2.022198\n",
            "Step 18954: Minibatch Loss: 2.033690\n",
            "Step 18956: Minibatch Loss: 2.063094\n",
            "Step 18958: Minibatch Loss: 1.994878\n",
            "Step 18960: Minibatch Loss: 1.973249\n",
            "Step 18962: Minibatch Loss: 2.028823\n",
            "Step 18964: Minibatch Loss: 1.978108\n",
            "Step 18966: Minibatch Loss: 2.049356\n",
            "Step 18968: Minibatch Loss: 1.954646\n",
            "Step 18970: Minibatch Loss: 2.011620\n",
            "Step 18972: Minibatch Loss: 1.998566\n",
            "Step 18974: Minibatch Loss: 1.982352\n",
            "Step 18976: Minibatch Loss: 2.025354\n",
            "Step 18978: Minibatch Loss: 2.024944\n",
            "Step 18980: Minibatch Loss: 2.010777\n",
            "Step 18982: Minibatch Loss: 2.003143\n",
            "Step 18984: Minibatch Loss: 2.033590\n",
            "Step 18986: Minibatch Loss: 2.028995\n",
            "Step 18988: Minibatch Loss: 1.990350\n",
            "Step 18990: Minibatch Loss: 1.982011\n",
            "Step 18992: Minibatch Loss: 1.974703\n",
            "Step 18994: Minibatch Loss: 2.009442\n",
            "Step 18996: Minibatch Loss: 1.961030\n",
            "Step 18998: Minibatch Loss: 2.024801\n",
            "Step 19000: Minibatch Loss: 2.001395\n",
            "Training for SNR= 9.5  sigma= 0.44668359215096315 iteratin: 0\n",
            "Step 19002: Minibatch Loss: 2.053969\n",
            "Step 19004: Minibatch Loss: 2.012935\n",
            "Step 19006: Minibatch Loss: 2.005258\n",
            "Step 19008: Minibatch Loss: 2.062086\n",
            "Step 19010: Minibatch Loss: 1.937124\n",
            "Step 19012: Minibatch Loss: 2.039796\n",
            "Step 19014: Minibatch Loss: 2.072229\n",
            "Step 19016: Minibatch Loss: 1.951272\n",
            "Step 19018: Minibatch Loss: 2.063502\n",
            "Step 19020: Minibatch Loss: 2.059297\n",
            "Step 19022: Minibatch Loss: 1.942761\n",
            "Step 19024: Minibatch Loss: 2.050393\n",
            "Step 19026: Minibatch Loss: 2.047771\n",
            "Step 19028: Minibatch Loss: 1.991573\n",
            "Step 19030: Minibatch Loss: 2.024636\n",
            "Step 19032: Minibatch Loss: 2.022102\n",
            "Step 19034: Minibatch Loss: 2.006628\n",
            "Step 19036: Minibatch Loss: 2.073268\n",
            "Step 19038: Minibatch Loss: 1.991169\n",
            "Step 19040: Minibatch Loss: 2.075370\n",
            "Step 19042: Minibatch Loss: 2.009538\n",
            "Step 19044: Minibatch Loss: 2.037685\n",
            "Step 19046: Minibatch Loss: 1.989315\n",
            "Step 19048: Minibatch Loss: 1.916806\n",
            "Step 19050: Minibatch Loss: 1.938958\n",
            "Step 19052: Minibatch Loss: 2.002571\n",
            "Step 19054: Minibatch Loss: 2.002890\n",
            "Step 19056: Minibatch Loss: 2.081407\n",
            "Step 19058: Minibatch Loss: 1.997114\n",
            "Step 19060: Minibatch Loss: 1.940611\n",
            "Step 19062: Minibatch Loss: 2.082788\n",
            "Step 19064: Minibatch Loss: 1.975306\n",
            "Step 19066: Minibatch Loss: 2.005346\n",
            "Step 19068: Minibatch Loss: 1.992826\n",
            "Step 19070: Minibatch Loss: 2.018514\n",
            "Step 19072: Minibatch Loss: 1.994367\n",
            "Step 19074: Minibatch Loss: 2.049886\n",
            "Step 19076: Minibatch Loss: 2.025575\n",
            "Step 19078: Minibatch Loss: 2.000864\n",
            "Step 19080: Minibatch Loss: 2.017050\n",
            "Step 19082: Minibatch Loss: 2.053528\n",
            "Step 19084: Minibatch Loss: 1.997296\n",
            "Step 19086: Minibatch Loss: 2.050527\n",
            "Step 19088: Minibatch Loss: 1.961221\n",
            "Step 19090: Minibatch Loss: 2.046238\n",
            "Step 19092: Minibatch Loss: 2.011196\n",
            "Step 19094: Minibatch Loss: 1.965794\n",
            "Step 19096: Minibatch Loss: 1.975825\n",
            "Step 19098: Minibatch Loss: 2.017487\n",
            "Step 19100: Minibatch Loss: 1.936561\n",
            "Step 19102: Minibatch Loss: 2.026402\n",
            "Step 19104: Minibatch Loss: 2.045554\n",
            "Step 19106: Minibatch Loss: 2.044492\n",
            "Step 19108: Minibatch Loss: 2.067800\n",
            "Step 19110: Minibatch Loss: 2.020442\n",
            "Step 19112: Minibatch Loss: 2.016133\n",
            "Step 19114: Minibatch Loss: 1.954720\n",
            "Step 19116: Minibatch Loss: 1.942103\n",
            "Step 19118: Minibatch Loss: 2.106612\n",
            "Step 19120: Minibatch Loss: 2.062250\n",
            "Step 19122: Minibatch Loss: 1.976010\n",
            "Step 19124: Minibatch Loss: 2.092573\n",
            "Step 19126: Minibatch Loss: 2.056400\n",
            "Step 19128: Minibatch Loss: 2.046147\n",
            "Step 19130: Minibatch Loss: 2.024628\n",
            "Step 19132: Minibatch Loss: 2.002969\n",
            "Step 19134: Minibatch Loss: 2.037265\n",
            "Step 19136: Minibatch Loss: 2.033504\n",
            "Step 19138: Minibatch Loss: 1.958062\n",
            "Step 19140: Minibatch Loss: 2.004932\n",
            "Step 19142: Minibatch Loss: 1.961752\n",
            "Step 19144: Minibatch Loss: 2.061222\n",
            "Step 19146: Minibatch Loss: 2.014415\n",
            "Step 19148: Minibatch Loss: 1.993513\n",
            "Step 19150: Minibatch Loss: 1.972091\n",
            "Step 19152: Minibatch Loss: 2.012851\n",
            "Step 19154: Minibatch Loss: 2.068495\n",
            "Step 19156: Minibatch Loss: 2.102920\n",
            "Step 19158: Minibatch Loss: 1.989205\n",
            "Step 19160: Minibatch Loss: 1.998252\n",
            "Step 19162: Minibatch Loss: 2.054342\n",
            "Step 19164: Minibatch Loss: 1.983726\n",
            "Step 19166: Minibatch Loss: 2.057148\n",
            "Step 19168: Minibatch Loss: 1.973269\n",
            "Step 19170: Minibatch Loss: 1.996605\n",
            "Step 19172: Minibatch Loss: 2.057032\n",
            "Step 19174: Minibatch Loss: 1.967101\n",
            "Step 19176: Minibatch Loss: 2.067790\n",
            "Step 19178: Minibatch Loss: 2.017202\n",
            "Step 19180: Minibatch Loss: 1.982198\n",
            "Step 19182: Minibatch Loss: 2.015311\n",
            "Step 19184: Minibatch Loss: 2.038435\n",
            "Step 19186: Minibatch Loss: 2.002283\n",
            "Step 19188: Minibatch Loss: 2.019621\n",
            "Step 19190: Minibatch Loss: 2.050906\n",
            "Step 19192: Minibatch Loss: 2.048252\n",
            "Step 19194: Minibatch Loss: 1.936463\n",
            "Step 19196: Minibatch Loss: 2.023401\n",
            "Step 19198: Minibatch Loss: 2.050565\n",
            "Step 19200: Minibatch Loss: 1.976651\n",
            "Step 19202: Minibatch Loss: 2.029114\n",
            "Step 19204: Minibatch Loss: 2.089764\n",
            "Step 19206: Minibatch Loss: 1.958361\n",
            "Step 19208: Minibatch Loss: 1.980645\n",
            "Step 19210: Minibatch Loss: 1.985805\n",
            "Step 19212: Minibatch Loss: 1.973200\n",
            "Step 19214: Minibatch Loss: 2.000740\n",
            "Step 19216: Minibatch Loss: 1.968898\n",
            "Step 19218: Minibatch Loss: 1.988916\n",
            "Step 19220: Minibatch Loss: 2.017329\n",
            "Step 19222: Minibatch Loss: 2.037897\n",
            "Step 19224: Minibatch Loss: 2.025178\n",
            "Step 19226: Minibatch Loss: 2.019557\n",
            "Step 19228: Minibatch Loss: 1.956474\n",
            "Step 19230: Minibatch Loss: 2.015968\n",
            "Step 19232: Minibatch Loss: 1.962967\n",
            "Step 19234: Minibatch Loss: 1.991410\n",
            "Step 19236: Minibatch Loss: 2.113383\n",
            "Step 19238: Minibatch Loss: 1.959322\n",
            "Step 19240: Minibatch Loss: 2.064847\n",
            "Step 19242: Minibatch Loss: 1.942979\n",
            "Step 19244: Minibatch Loss: 1.972792\n",
            "Step 19246: Minibatch Loss: 1.916270\n",
            "Step 19248: Minibatch Loss: 2.002236\n",
            "Step 19250: Minibatch Loss: 2.001361\n",
            "Step 19252: Minibatch Loss: 2.042744\n",
            "Step 19254: Minibatch Loss: 2.015222\n",
            "Step 19256: Minibatch Loss: 2.054130\n",
            "Step 19258: Minibatch Loss: 2.009708\n",
            "Step 19260: Minibatch Loss: 2.031019\n",
            "Step 19262: Minibatch Loss: 2.024845\n",
            "Step 19264: Minibatch Loss: 1.903528\n",
            "Step 19266: Minibatch Loss: 2.055635\n",
            "Step 19268: Minibatch Loss: 1.913757\n",
            "Step 19270: Minibatch Loss: 2.003860\n",
            "Step 19272: Minibatch Loss: 2.050371\n",
            "Step 19274: Minibatch Loss: 2.066565\n",
            "Step 19276: Minibatch Loss: 2.020894\n",
            "Step 19278: Minibatch Loss: 2.012625\n",
            "Step 19280: Minibatch Loss: 2.025147\n",
            "Step 19282: Minibatch Loss: 1.990391\n",
            "Step 19284: Minibatch Loss: 1.986066\n",
            "Step 19286: Minibatch Loss: 2.041861\n",
            "Step 19288: Minibatch Loss: 1.968048\n",
            "Step 19290: Minibatch Loss: 2.110595\n",
            "Step 19292: Minibatch Loss: 1.978092\n",
            "Step 19294: Minibatch Loss: 1.977774\n",
            "Step 19296: Minibatch Loss: 1.997495\n",
            "Step 19298: Minibatch Loss: 1.977417\n",
            "Step 19300: Minibatch Loss: 2.014094\n",
            "Step 19302: Minibatch Loss: 2.024121\n",
            "Step 19304: Minibatch Loss: 2.046404\n",
            "Step 19306: Minibatch Loss: 2.010550\n",
            "Step 19308: Minibatch Loss: 1.998009\n",
            "Step 19310: Minibatch Loss: 1.971174\n",
            "Step 19312: Minibatch Loss: 2.055261\n",
            "Step 19314: Minibatch Loss: 2.058962\n",
            "Step 19316: Minibatch Loss: 2.069876\n",
            "Step 19318: Minibatch Loss: 2.095030\n",
            "Step 19320: Minibatch Loss: 2.007126\n",
            "Step 19322: Minibatch Loss: 2.072881\n",
            "Step 19324: Minibatch Loss: 2.044019\n",
            "Step 19326: Minibatch Loss: 2.026582\n",
            "Step 19328: Minibatch Loss: 1.982665\n",
            "Step 19330: Minibatch Loss: 2.018651\n",
            "Step 19332: Minibatch Loss: 2.003899\n",
            "Step 19334: Minibatch Loss: 1.951227\n",
            "Step 19336: Minibatch Loss: 2.065604\n",
            "Step 19338: Minibatch Loss: 1.943459\n",
            "Step 19340: Minibatch Loss: 1.997000\n",
            "Step 19342: Minibatch Loss: 1.952605\n",
            "Step 19344: Minibatch Loss: 2.003872\n",
            "Step 19346: Minibatch Loss: 1.996267\n",
            "Step 19348: Minibatch Loss: 2.016889\n",
            "Step 19350: Minibatch Loss: 1.969839\n",
            "Step 19352: Minibatch Loss: 2.042473\n",
            "Step 19354: Minibatch Loss: 1.991629\n",
            "Step 19356: Minibatch Loss: 2.053963\n",
            "Step 19358: Minibatch Loss: 1.965228\n",
            "Step 19360: Minibatch Loss: 1.960898\n",
            "Step 19362: Minibatch Loss: 2.033038\n",
            "Step 19364: Minibatch Loss: 2.002896\n",
            "Step 19366: Minibatch Loss: 2.019781\n",
            "Step 19368: Minibatch Loss: 1.962894\n",
            "Step 19370: Minibatch Loss: 1.961659\n",
            "Step 19372: Minibatch Loss: 1.956340\n",
            "Step 19374: Minibatch Loss: 1.968212\n",
            "Step 19376: Minibatch Loss: 2.026906\n",
            "Step 19378: Minibatch Loss: 2.010179\n",
            "Step 19380: Minibatch Loss: 2.054551\n",
            "Step 19382: Minibatch Loss: 2.012242\n",
            "Step 19384: Minibatch Loss: 1.958915\n",
            "Step 19386: Minibatch Loss: 2.002866\n",
            "Step 19388: Minibatch Loss: 2.037261\n",
            "Step 19390: Minibatch Loss: 2.009170\n",
            "Step 19392: Minibatch Loss: 1.962851\n",
            "Step 19394: Minibatch Loss: 1.960652\n",
            "Step 19396: Minibatch Loss: 1.909518\n",
            "Step 19398: Minibatch Loss: 2.048442\n",
            "Step 19400: Minibatch Loss: 2.030082\n",
            "Step 19402: Minibatch Loss: 2.027375\n",
            "Step 19404: Minibatch Loss: 1.993970\n",
            "Step 19406: Minibatch Loss: 1.991478\n",
            "Step 19408: Minibatch Loss: 2.033170\n",
            "Step 19410: Minibatch Loss: 1.942227\n",
            "Step 19412: Minibatch Loss: 2.099846\n",
            "Step 19414: Minibatch Loss: 2.002330\n",
            "Step 19416: Minibatch Loss: 2.023280\n",
            "Step 19418: Minibatch Loss: 2.022234\n",
            "Step 19420: Minibatch Loss: 1.987642\n",
            "Step 19422: Minibatch Loss: 2.038836\n",
            "Step 19424: Minibatch Loss: 2.020220\n",
            "Step 19426: Minibatch Loss: 2.066554\n",
            "Step 19428: Minibatch Loss: 2.035398\n",
            "Step 19430: Minibatch Loss: 1.979749\n",
            "Step 19432: Minibatch Loss: 1.952148\n",
            "Step 19434: Minibatch Loss: 1.965077\n",
            "Step 19436: Minibatch Loss: 2.074509\n",
            "Step 19438: Minibatch Loss: 1.973681\n",
            "Step 19440: Minibatch Loss: 1.977462\n",
            "Step 19442: Minibatch Loss: 1.934914\n",
            "Step 19444: Minibatch Loss: 1.974703\n",
            "Step 19446: Minibatch Loss: 2.000962\n",
            "Step 19448: Minibatch Loss: 1.944413\n",
            "Step 19450: Minibatch Loss: 1.985802\n",
            "Step 19452: Minibatch Loss: 1.938909\n",
            "Step 19454: Minibatch Loss: 2.016375\n",
            "Step 19456: Minibatch Loss: 2.009244\n",
            "Step 19458: Minibatch Loss: 2.043735\n",
            "Step 19460: Minibatch Loss: 1.961958\n",
            "Step 19462: Minibatch Loss: 2.082792\n",
            "Step 19464: Minibatch Loss: 1.915808\n",
            "Step 19466: Minibatch Loss: 2.013088\n",
            "Step 19468: Minibatch Loss: 1.907660\n",
            "Step 19470: Minibatch Loss: 2.035658\n",
            "Step 19472: Minibatch Loss: 2.015559\n",
            "Step 19474: Minibatch Loss: 2.018506\n",
            "Step 19476: Minibatch Loss: 2.025318\n",
            "Step 19478: Minibatch Loss: 2.009264\n",
            "Step 19480: Minibatch Loss: 2.040454\n",
            "Step 19482: Minibatch Loss: 2.044636\n",
            "Step 19484: Minibatch Loss: 1.969911\n",
            "Step 19486: Minibatch Loss: 2.034223\n",
            "Step 19488: Minibatch Loss: 2.018805\n",
            "Step 19490: Minibatch Loss: 1.966117\n",
            "Step 19492: Minibatch Loss: 2.092262\n",
            "Step 19494: Minibatch Loss: 1.994271\n",
            "Step 19496: Minibatch Loss: 1.945741\n",
            "Step 19498: Minibatch Loss: 2.033072\n",
            "Step 19500: Minibatch Loss: 2.016366\n",
            "Step 19502: Minibatch Loss: 1.988636\n",
            "Step 19504: Minibatch Loss: 2.042959\n",
            "Step 19506: Minibatch Loss: 1.998175\n",
            "Step 19508: Minibatch Loss: 2.014969\n",
            "Step 19510: Minibatch Loss: 1.954163\n",
            "Step 19512: Minibatch Loss: 2.079726\n",
            "Step 19514: Minibatch Loss: 2.012450\n",
            "Step 19516: Minibatch Loss: 1.970686\n",
            "Step 19518: Minibatch Loss: 2.009742\n",
            "Step 19520: Minibatch Loss: 2.030390\n",
            "Step 19522: Minibatch Loss: 1.937325\n",
            "Step 19524: Minibatch Loss: 1.981491\n",
            "Step 19526: Minibatch Loss: 2.012584\n",
            "Step 19528: Minibatch Loss: 1.946876\n",
            "Step 19530: Minibatch Loss: 1.984443\n",
            "Step 19532: Minibatch Loss: 1.996529\n",
            "Step 19534: Minibatch Loss: 1.980969\n",
            "Step 19536: Minibatch Loss: 2.074918\n",
            "Step 19538: Minibatch Loss: 1.986480\n",
            "Step 19540: Minibatch Loss: 1.977306\n",
            "Step 19542: Minibatch Loss: 1.924981\n",
            "Step 19544: Minibatch Loss: 2.036668\n",
            "Step 19546: Minibatch Loss: 1.959889\n",
            "Step 19548: Minibatch Loss: 1.954845\n",
            "Step 19550: Minibatch Loss: 1.937893\n",
            "Step 19552: Minibatch Loss: 2.006869\n",
            "Step 19554: Minibatch Loss: 2.014869\n",
            "Step 19556: Minibatch Loss: 2.025202\n",
            "Step 19558: Minibatch Loss: 2.016089\n",
            "Step 19560: Minibatch Loss: 1.939336\n",
            "Step 19562: Minibatch Loss: 2.021130\n",
            "Step 19564: Minibatch Loss: 1.932235\n",
            "Step 19566: Minibatch Loss: 2.048509\n",
            "Step 19568: Minibatch Loss: 1.932808\n",
            "Step 19570: Minibatch Loss: 1.994572\n",
            "Step 19572: Minibatch Loss: 2.006788\n",
            "Step 19574: Minibatch Loss: 1.996216\n",
            "Step 19576: Minibatch Loss: 1.974884\n",
            "Step 19578: Minibatch Loss: 2.005231\n",
            "Step 19580: Minibatch Loss: 1.963976\n",
            "Step 19582: Minibatch Loss: 2.104951\n",
            "Step 19584: Minibatch Loss: 2.040375\n",
            "Step 19586: Minibatch Loss: 2.018630\n",
            "Step 19588: Minibatch Loss: 1.988915\n",
            "Step 19590: Minibatch Loss: 1.993439\n",
            "Step 19592: Minibatch Loss: 2.008213\n",
            "Step 19594: Minibatch Loss: 2.010100\n",
            "Step 19596: Minibatch Loss: 1.922446\n",
            "Step 19598: Minibatch Loss: 1.955683\n",
            "Step 19600: Minibatch Loss: 1.958390\n",
            "Step 19602: Minibatch Loss: 2.045933\n",
            "Step 19604: Minibatch Loss: 2.040675\n",
            "Step 19606: Minibatch Loss: 1.980478\n",
            "Step 19608: Minibatch Loss: 2.017154\n",
            "Step 19610: Minibatch Loss: 1.976524\n",
            "Step 19612: Minibatch Loss: 2.020183\n",
            "Step 19614: Minibatch Loss: 2.011543\n",
            "Step 19616: Minibatch Loss: 1.996721\n",
            "Step 19618: Minibatch Loss: 2.004636\n",
            "Step 19620: Minibatch Loss: 2.027357\n",
            "Step 19622: Minibatch Loss: 1.982251\n",
            "Step 19624: Minibatch Loss: 2.068688\n",
            "Step 19626: Minibatch Loss: 1.996291\n",
            "Step 19628: Minibatch Loss: 1.978021\n",
            "Step 19630: Minibatch Loss: 1.970737\n",
            "Step 19632: Minibatch Loss: 2.065289\n",
            "Step 19634: Minibatch Loss: 1.965996\n",
            "Step 19636: Minibatch Loss: 2.035303\n",
            "Step 19638: Minibatch Loss: 1.934395\n",
            "Step 19640: Minibatch Loss: 2.039757\n",
            "Step 19642: Minibatch Loss: 1.929794\n",
            "Step 19644: Minibatch Loss: 2.030651\n",
            "Step 19646: Minibatch Loss: 1.963263\n",
            "Step 19648: Minibatch Loss: 1.965555\n",
            "Step 19650: Minibatch Loss: 1.971720\n",
            "Step 19652: Minibatch Loss: 1.947433\n",
            "Step 19654: Minibatch Loss: 2.005507\n",
            "Step 19656: Minibatch Loss: 2.037971\n",
            "Step 19658: Minibatch Loss: 1.952259\n",
            "Step 19660: Minibatch Loss: 1.932520\n",
            "Step 19662: Minibatch Loss: 2.009546\n",
            "Step 19664: Minibatch Loss: 1.931787\n",
            "Step 19666: Minibatch Loss: 2.032693\n",
            "Step 19668: Minibatch Loss: 1.942764\n",
            "Step 19670: Minibatch Loss: 1.979376\n",
            "Step 19672: Minibatch Loss: 1.999102\n",
            "Step 19674: Minibatch Loss: 1.992700\n",
            "Step 19676: Minibatch Loss: 1.980574\n",
            "Step 19678: Minibatch Loss: 1.970900\n",
            "Step 19680: Minibatch Loss: 2.050292\n",
            "Step 19682: Minibatch Loss: 2.000561\n",
            "Step 19684: Minibatch Loss: 2.040225\n",
            "Step 19686: Minibatch Loss: 2.030996\n",
            "Step 19688: Minibatch Loss: 2.020099\n",
            "Step 19690: Minibatch Loss: 1.991017\n",
            "Step 19692: Minibatch Loss: 1.978786\n",
            "Step 19694: Minibatch Loss: 1.972254\n",
            "Step 19696: Minibatch Loss: 1.984316\n",
            "Step 19698: Minibatch Loss: 1.951509\n",
            "Step 19700: Minibatch Loss: 1.932420\n",
            "Step 19702: Minibatch Loss: 1.996333\n",
            "Step 19704: Minibatch Loss: 1.964955\n",
            "Step 19706: Minibatch Loss: 2.012605\n",
            "Step 19708: Minibatch Loss: 1.980430\n",
            "Step 19710: Minibatch Loss: 1.991658\n",
            "Step 19712: Minibatch Loss: 2.013432\n",
            "Step 19714: Minibatch Loss: 1.975532\n",
            "Step 19716: Minibatch Loss: 1.932370\n",
            "Step 19718: Minibatch Loss: 2.036738\n",
            "Step 19720: Minibatch Loss: 2.054339\n",
            "Step 19722: Minibatch Loss: 1.981758\n",
            "Step 19724: Minibatch Loss: 2.032683\n",
            "Step 19726: Minibatch Loss: 2.011200\n",
            "Step 19728: Minibatch Loss: 1.974751\n",
            "Step 19730: Minibatch Loss: 2.047869\n",
            "Step 19732: Minibatch Loss: 1.957793\n",
            "Step 19734: Minibatch Loss: 1.932012\n",
            "Step 19736: Minibatch Loss: 2.077179\n",
            "Step 19738: Minibatch Loss: 1.966275\n",
            "Step 19740: Minibatch Loss: 1.999864\n",
            "Step 19742: Minibatch Loss: 1.919340\n",
            "Step 19744: Minibatch Loss: 2.015177\n",
            "Step 19746: Minibatch Loss: 2.073957\n",
            "Step 19748: Minibatch Loss: 1.957256\n",
            "Step 19750: Minibatch Loss: 1.941194\n",
            "Step 19752: Minibatch Loss: 1.968211\n",
            "Step 19754: Minibatch Loss: 1.985338\n",
            "Step 19756: Minibatch Loss: 1.982137\n",
            "Step 19758: Minibatch Loss: 2.021319\n",
            "Step 19760: Minibatch Loss: 1.924501\n",
            "Step 19762: Minibatch Loss: 2.019395\n",
            "Step 19764: Minibatch Loss: 1.907813\n",
            "Step 19766: Minibatch Loss: 2.064717\n",
            "Step 19768: Minibatch Loss: 1.913192\n",
            "Step 19770: Minibatch Loss: 1.967454\n",
            "Step 19772: Minibatch Loss: 1.984289\n",
            "Step 19774: Minibatch Loss: 1.962730\n",
            "Step 19776: Minibatch Loss: 1.966063\n",
            "Step 19778: Minibatch Loss: 1.986403\n",
            "Step 19780: Minibatch Loss: 1.968390\n",
            "Step 19782: Minibatch Loss: 1.956548\n",
            "Step 19784: Minibatch Loss: 2.000234\n",
            "Step 19786: Minibatch Loss: 2.009859\n",
            "Step 19788: Minibatch Loss: 1.992151\n",
            "Step 19790: Minibatch Loss: 1.984425\n",
            "Step 19792: Minibatch Loss: 2.037885\n",
            "Step 19794: Minibatch Loss: 1.958619\n",
            "Step 19796: Minibatch Loss: 1.954717\n",
            "Step 19798: Minibatch Loss: 1.980407\n",
            "Step 19800: Minibatch Loss: 1.979898\n",
            "Step 19802: Minibatch Loss: 2.045013\n",
            "Step 19804: Minibatch Loss: 1.990405\n",
            "Step 19806: Minibatch Loss: 2.005974\n",
            "Step 19808: Minibatch Loss: 2.051971\n",
            "Step 19810: Minibatch Loss: 1.982482\n",
            "Step 19812: Minibatch Loss: 2.009716\n",
            "Step 19814: Minibatch Loss: 1.996590\n",
            "Step 19816: Minibatch Loss: 1.984859\n",
            "Step 19818: Minibatch Loss: 2.047837\n",
            "Step 19820: Minibatch Loss: 1.992020\n",
            "Step 19822: Minibatch Loss: 1.971465\n",
            "Step 19824: Minibatch Loss: 2.044911\n",
            "Step 19826: Minibatch Loss: 1.995772\n",
            "Step 19828: Minibatch Loss: 2.001790\n",
            "Step 19830: Minibatch Loss: 1.957965\n",
            "Step 19832: Minibatch Loss: 1.961826\n",
            "Step 19834: Minibatch Loss: 1.929838\n",
            "Step 19836: Minibatch Loss: 2.074227\n",
            "Step 19838: Minibatch Loss: 1.935643\n",
            "Step 19840: Minibatch Loss: 2.024243\n",
            "Step 19842: Minibatch Loss: 1.946620\n",
            "Step 19844: Minibatch Loss: 2.031218\n",
            "Step 19846: Minibatch Loss: 1.953055\n",
            "Step 19848: Minibatch Loss: 1.932998\n",
            "Step 19850: Minibatch Loss: 1.965153\n",
            "Step 19852: Minibatch Loss: 1.978100\n",
            "Step 19854: Minibatch Loss: 1.984922\n",
            "Step 19856: Minibatch Loss: 1.977234\n",
            "Step 19858: Minibatch Loss: 2.018593\n",
            "Step 19860: Minibatch Loss: 1.998722\n",
            "Step 19862: Minibatch Loss: 1.982247\n",
            "Step 19864: Minibatch Loss: 1.959188\n",
            "Step 19866: Minibatch Loss: 1.999989\n",
            "Step 19868: Minibatch Loss: 1.904282\n",
            "Step 19870: Minibatch Loss: 1.982160\n",
            "Step 19872: Minibatch Loss: 2.025105\n",
            "Step 19874: Minibatch Loss: 2.044480\n",
            "Step 19876: Minibatch Loss: 2.061388\n",
            "Step 19878: Minibatch Loss: 2.041672\n",
            "Step 19880: Minibatch Loss: 1.994293\n",
            "Step 19882: Minibatch Loss: 1.977531\n",
            "Step 19884: Minibatch Loss: 1.979294\n",
            "Step 19886: Minibatch Loss: 1.983131\n",
            "Step 19888: Minibatch Loss: 2.048464\n",
            "Step 19890: Minibatch Loss: 2.047439\n",
            "Step 19892: Minibatch Loss: 1.936788\n",
            "Step 19894: Minibatch Loss: 1.950042\n",
            "Step 19896: Minibatch Loss: 1.968815\n",
            "Step 19898: Minibatch Loss: 2.037784\n",
            "Step 19900: Minibatch Loss: 1.988123\n",
            "Step 19902: Minibatch Loss: 2.034308\n",
            "Step 19904: Minibatch Loss: 2.050098\n",
            "Step 19906: Minibatch Loss: 1.990748\n",
            "Step 19908: Minibatch Loss: 2.039715\n",
            "Step 19910: Minibatch Loss: 1.963627\n",
            "Step 19912: Minibatch Loss: 2.062025\n",
            "Step 19914: Minibatch Loss: 1.947466\n",
            "Step 19916: Minibatch Loss: 1.984389\n",
            "Step 19918: Minibatch Loss: 2.011271\n",
            "Step 19920: Minibatch Loss: 1.998766\n",
            "Step 19922: Minibatch Loss: 1.977972\n",
            "Step 19924: Minibatch Loss: 1.989362\n",
            "Step 19926: Minibatch Loss: 1.989878\n",
            "Step 19928: Minibatch Loss: 1.967743\n",
            "Step 19930: Minibatch Loss: 1.991741\n",
            "Step 19932: Minibatch Loss: 1.954076\n",
            "Step 19934: Minibatch Loss: 1.892873\n",
            "Step 19936: Minibatch Loss: 2.033176\n",
            "Step 19938: Minibatch Loss: 1.981928\n",
            "Step 19940: Minibatch Loss: 2.026022\n",
            "Step 19942: Minibatch Loss: 1.946059\n",
            "Step 19944: Minibatch Loss: 2.011080\n",
            "Step 19946: Minibatch Loss: 1.999133\n",
            "Step 19948: Minibatch Loss: 1.940712\n",
            "Step 19950: Minibatch Loss: 1.916412\n",
            "Step 19952: Minibatch Loss: 1.955647\n",
            "Step 19954: Minibatch Loss: 2.024953\n",
            "Step 19956: Minibatch Loss: 1.992242\n",
            "Step 19958: Minibatch Loss: 2.081664\n",
            "Step 19960: Minibatch Loss: 1.928013\n",
            "Step 19962: Minibatch Loss: 2.020130\n",
            "Step 19964: Minibatch Loss: 1.916414\n",
            "Step 19966: Minibatch Loss: 1.994298\n",
            "Step 19968: Minibatch Loss: 2.001768\n",
            "Step 19970: Minibatch Loss: 1.972665\n",
            "Step 19972: Minibatch Loss: 2.004719\n",
            "Step 19974: Minibatch Loss: 1.932210\n",
            "Step 19976: Minibatch Loss: 2.021058\n",
            "Step 19978: Minibatch Loss: 1.973476\n",
            "Step 19980: Minibatch Loss: 1.996270\n",
            "Step 19982: Minibatch Loss: 2.022883\n",
            "Step 19984: Minibatch Loss: 2.052393\n",
            "Step 19986: Minibatch Loss: 1.992017\n",
            "Step 19988: Minibatch Loss: 2.009224\n",
            "Step 19990: Minibatch Loss: 2.007422\n",
            "Step 19992: Minibatch Loss: 1.993795\n",
            "Step 19994: Minibatch Loss: 2.002543\n",
            "Step 19996: Minibatch Loss: 1.927058\n",
            "Step 19998: Minibatch Loss: 1.996032\n",
            "Step 20000: Minibatch Loss: 1.966216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHByzQbTUqbv",
        "outputId": "62fa1c3e-87e2-4623-f8dd-1765b7143614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using trained model\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_dl_tensor  = numpy.array(())\n",
        "times_per_iter_dl_tensor = numpy.array(())\n",
        "\n",
        "channel_x = []\n",
        "channel_y = []\n",
        "channel_n_x = []\n",
        "channel_n_y = []\n",
        "\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  lrate = 0.001\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    input_message_xx = training_input_message_one_hot [i:i+1]\n",
        "    input_message_xx = input_message_xx.astype(\"float32\")\n",
        "    #,input_message_x_label:training_input_message [i]\n",
        "    encoded_message = train_sess.run ([dl_encoder_output], feed_dict={input_message_x:input_message_xx })\n",
        "    encoded_message = encoded_message[0][0]\n",
        "    #encoded_message = numpy.around(encoded_message[0][0]> 0).astype(int)\n",
        "    #print (encoded_message)\n",
        "    channel_x.append(encoded_message[0])\n",
        "    channel_y.append(encoded_message[1])\n",
        "    awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message})\n",
        "    #print (awgn_channel_output_message)\n",
        "    decoded_message = train_sess.run ([dl_decoder_only_output], feed_dict={input_channel_x:awgn_channel_output_message})\n",
        "    #print (\"input\", input_message[i])\n",
        "    #decoded_message = numpy.around(decoded_message[0][0]> 0).astype(int)\n",
        "    #rint (\"output\", decoded_message)\n",
        "    #print (\"output\", numpy.argmax(training_input_message_one_hot[i]), numpy.argmax(decoded_message[0][0]))\n",
        "    if (numpy.argmax(training_input_message_one_hot[i]) != numpy.argmax(decoded_message[0][0])):\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_dl_tensor=numpy.append(ber_per_iter_dl_tensor ,ber)\n",
        "  times_per_iter_dl_tensor=numpy.append(times_per_iter_dl_tensor, total_time)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.84\n",
            " -> Total Time: 2.48s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.81\n",
            " -> Total Time: 2.52s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.81\n",
            " -> Total Time: 2.48s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.78\n",
            " -> Total Time: 2.46s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.81\n",
            " -> Total Time: 2.40s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.78\n",
            " -> Total Time: 2.41s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.78\n",
            " -> Total Time: 2.47s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.78\n",
            " -> Total Time: 2.50s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.75\n",
            " -> Total Time: 2.42s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.77\n",
            " -> Total Time: 2.44s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.74\n",
            " -> Total Time: 2.48s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 1.01s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.73\n",
            " -> Total Time: 2.50s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.73\n",
            " -> Total Time: 2.45s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.73\n",
            " -> Total Time: 2.45s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.73\n",
            " -> Total Time: 2.45s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.70\n",
            " -> Total Time: 2.42s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.71\n",
            " -> Total Time: 2.42s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.72\n",
            " -> Total Time: 2.49s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.69\n",
            " -> Total Time: 2.43s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.69\n",
            " -> Total Time: 2.44s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iafclazel4RD",
        "outputId": "c0893581-f958-4df3-e05c-4bc3acf6155d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.scatter(channel_x, channel_y)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syUQij3fuxRm",
        "outputId": "99de3476-7aac-409e-f6f9-6490e75bbb84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1) = plt.subplots(1,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"ldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor,'', label=\"ai-dl\") # plot BER vs SNR\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "#ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "#ax2.set_xlabel('$E_b/$N_0$')\n",
        "#ax2.set_ylabel('Decoding Time [s]')\n",
        "#ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "#            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "#            xy=(1, 0.35), xycoords='axes fraction',\n",
        "#            xytext=(-20, 20), textcoords='offset pixels',\n",
        "#            horizontalalignment='right',\n",
        "#            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpIbD_FUw-iL",
        "outputId": "ef9ae511-e988-4947-b84d-fec137114acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "import numpy\n",
        "training_input_message = numpy.random.randint(2**input_message_length, size=(1,NUM_OF_INPUT_MESSAGE))\n",
        "training_input_message_one_hot = numpy.zeros((training_input_message.size, 2**input_message_length))\n",
        "training_input_message_one_hot[numpy.arange(training_input_message.size),training_input_message] = 1\n",
        "print(training_input_message_one_hot)\n",
        "print (training_input_message_one_hot.shape)\n",
        "print (training_input_message.shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n",
            "(1000, 16)\n",
            "(1, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "farYxiF5xJFj",
        "outputId": "8f0bb709-2f48-427c-e7be-3658291c1e7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using trained model\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_dl_tensor  = numpy.array(())\n",
        "times_per_iter_dl_tensor = numpy.array(())\n",
        "\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  lrate = 0.001\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    input_message_xx = training_input_message_one_hot [i:i+1]\n",
        "    input_message_xx = input_message_xx.astype(\"float32\")\n",
        "    #,input_message_x_label:training_input_message [i]\n",
        "    encoded_message = train_sess.run ([dl_encoder_output], feed_dict={input_message_x:input_message_xx })\n",
        "    encoded_message = encoded_message[0][0]\n",
        "    #encoded_message = numpy.around(encoded_message[0][0]> 0).astype(int)\n",
        "    #print (encoded_message[0][0])\n",
        "    awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message})\n",
        "    #print (awgn_channel_output_message)\n",
        "    decoded_message = train_sess.run ([dl_decoder_only_output], feed_dict={input_channel_x:awgn_channel_output_message})\n",
        "    #print (\"input\", input_message[i])\n",
        "    #decoded_message = numpy.around(decoded_message[0][0]> 0).astype(int)\n",
        "    #rint (\"output\", decoded_message)\n",
        "    #print (\"output\", numpy.argmax(training_input_message_one_hot[i]), numpy.argmax(decoded_message[0][0]))\n",
        "    if (numpy.argmax(training_input_message_one_hot[i]) != numpy.argmax(decoded_message[0][0])):\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_dl_tensor=numpy.append(ber_per_iter_dl_tensor ,ber)\n",
        "  times_per_iter_dl_tensor=numpy.append(times_per_iter_dl_tensor, total_time)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.81\n",
            " -> Total Time: 2.42s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.82\n",
            " -> Total Time: 2.45s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.82\n",
            " -> Total Time: 2.41s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.80\n",
            " -> Total Time: 2.41s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.78\n",
            " -> Total Time: 2.42s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.79\n",
            " -> Total Time: 2.51s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.78\n",
            " -> Total Time: 2.44s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.78\n",
            " -> Total Time: 2.44s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.78\n",
            " -> Total Time: 2.45s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.75\n",
            " -> Total Time: 2.45s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.77\n",
            " -> Total Time: 2.47s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.73\n",
            " -> Total Time: 2.41s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.74\n",
            " -> Total Time: 2.40s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.73\n",
            " -> Total Time: 2.43s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.71\n",
            " -> Total Time: 2.42s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.71\n",
            " -> Total Time: 2.48s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.70\n",
            " -> Total Time: 2.46s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.23s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.47s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.70\n",
            " -> Total Time: 2.41s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.68\n",
            " -> Total Time: 2.43s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.67\n",
            " -> Total Time: 2.47s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hV6_TllxLG_",
        "outputId": "3af4b89f-28aa-42b1-a25c-8ea8cac410fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1) = plt.subplots(1,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"ldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor,'', label=\"ai-dl\") # plot BER vs SNR\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "#ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "#ax2.set_xlabel('$E_b/$N_0$')\n",
        "#ax2.set_ylabel('Decoding Time [s]')\n",
        "#ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "#            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "#            xy=(1, 0.35), xycoords='axes fraction',\n",
        "#            xytext=(-20, 20), textcoords='offset pixels',\n",
        "#            horizontalalignment='right',\n",
        "#            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsQhRxkXxML5"
      },
      "source": [
        ""
      ],
      "execution_count": 40,
      "outputs": []
    }
  ]
}