{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MainModelOneHotModCoding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamviji/project/blob/master/MainModelOneHotModCoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDSPPMfZ9czi",
        "outputId": "b7aa4c7f-b78b-4c1f-e881-f5e7a623b62c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "!rm -rf project\n",
        "!git clone https://github.com/iamviji/project.git\n",
        "!ls\n",
        "!ls project\n",
        "!pip install pyldpc\n",
        "!pip install scikit-commpy\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'project'...\n",
            "remote: Enumerating objects: 208, done.\u001b[K\n",
            "remote: Total 208 (delta 0), reused 0 (delta 0), pack-reused 208\u001b[K\n",
            "Receiving objects: 100% (208/208), 21.58 MiB | 10.64 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n",
            "project  sample_data\n",
            "EncoderModulatorSplitArch.ipynb  MainModelKerasOneHot.ipynb\n",
            "EncoderOutputExperiment.ipynb\t MainModelModCoding.ipynb\n",
            "End2End8PSK.ipynb\t\t MainModelOneHotMethod.ipynb\n",
            "End2EndOneHotQPSK.ipynb\t\t MainModelOneHotMethodSoftMax.ipynb\n",
            "End2EndQPSK.ipynb\t\t MainModelWithSingleBERTraining.ipynb\n",
            "End2EndQPSKRegularized.ipynb\t README.md\n",
            "MainModel.ipynb\t\t\t util.py\n",
            "MainModelKeras.ipynb\n",
            "Collecting pyldpc\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/aa/fd5495869c7106a638ae71aa497d7d266cae7f2a343d1f6a9d0e3a986e1e/pyldpc-0.7.9.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyldpc) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyldpc) (1.4.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from pyldpc) (0.48.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->pyldpc) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->pyldpc) (50.3.0)\n",
            "Building wheels for collected packages: pyldpc\n",
            "  Building wheel for pyldpc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyldpc: filename=pyldpc-0.7.9-cp36-none-any.whl size=14306 sha256=6e3af73af2a8d5a45e2449c04ec3d6f326d260188665f7bed84879dd192585d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/7a/10/e94058ba8b0b6d98bf2719226d18d3dd6056525ad7b984c068\n",
            "Successfully built pyldpc\n",
            "Installing collected packages: pyldpc\n",
            "Successfully installed pyldpc-0.7.9\n",
            "Collecting scikit-commpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/b4/f7fa5bc8864e0ddbd3e7a2290b624b92690f53523474024915c33321802d/scikit_commpy-0.5.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (1.18.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->scikit-commpy) (1.15.0)\n",
            "Installing collected packages: scikit-commpy\n",
            "Successfully installed scikit-commpy-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QOuLqpdDgx2",
        "outputId": "5cdc517c-de33-4e59-b92c-6e552b1f6852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "import pyldpc\n",
        "import commpy\n",
        "import numpy \n",
        "import time\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior ()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YClXJbbr0lc7"
      },
      "source": [
        "SNR_BEGIN = 0\n",
        "SNR_END = 10\n",
        "SNR_STEP_SIZE = 0.5\n",
        "CHANEL_SIZE = 18\n",
        "NUM_OF_INPUT_MESSAGE = 1000\n",
        "LDPC_MAX_ITER = 100\n",
        "num_parity_check = 3\n",
        "num_bits_in_parity_check = 6 \n",
        "input_message_length =  0 # Caculated by channel encoder and initialized later"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvUzIMsB43i0"
      },
      "source": [
        "def timer_update(i,current,time_tot,tic_incr=500):\n",
        "    last = current\n",
        "    current = time.time()\n",
        "    t_diff = current-last\n",
        "    print('SNR: {:04.3f} - Iter: {} - Last {} iterations took {:03.2f}s'.format(snr,i+1,tic_incr,t_diff))\n",
        "    return time_tot + t_diff\n",
        "\n",
        "def Snr2Sigma(snr):\n",
        "  sigma = 10 ** (- snr / 20)\n",
        "  return sigma\n",
        "\n",
        "def pyldpc_encode (CodingMatrix, message):\n",
        "  rng = pyldpc.utils.check_random_state(seed=None)\n",
        "  d = pyldpc.utils.binaryproduct(CodingMatrix, message)\n",
        "  encoded_message = (-1) ** d\n",
        "  return encoded_message\n",
        "\n",
        "def pyldpc_decode (ParityCheckMatrix, CodingMatrix, message, snr, maxiter):\n",
        "  decoded_msg = pyldpc.decode(ParityCheckMatrix, message, snr, maxiter)\n",
        "  out_message = pyldpc.get_message(CodingMatrix, decoded_msg)\n",
        "  return out_message\n",
        "\n",
        "awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [CHANEL_SIZE])\n",
        "awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)\n",
        "\n",
        "init = tf.global_variables_initializer ()\n",
        "sess = tf.Session ()\n",
        "sess.run(init)\n",
        "\n",
        "def AWGNChannelOutput (xx, snr , s):\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  awgn_channel_output_message = s.run ([awgn_channel_output], feed_dict={noise_std_dev:sigma, channel_input:xx})\n",
        "  return awgn_channel_output_message"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jMQG-MZ_pXu",
        "outputId": "76f8e9d8-b36d-4b55-db42-de9870d52881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "\n",
        "ParityCheckMatrix, CodingMatrix = pyldpc.make_ldpc(CHANEL_SIZE, num_parity_check, num_bits_in_parity_check, systematic=True, sparse=True)\n",
        "input_message_length = CodingMatrix.shape[1]\n",
        "print (\"input_message_size=\", input_message_length, \"channel_size=\",CHANEL_SIZE)\n",
        "print (\"input_message_size=\", CodingMatrix.shape[1], \"channel_size=\",CodingMatrix.shape[0])\n",
        "input_message = numpy.random.randint(2, size=(NUM_OF_INPUT_MESSAGE,input_message_length))\n",
        "print (input_message)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_message_size= 11 channel_size= 18\n",
            "input_message_size= 11 channel_size= 18\n",
            "[[1 0 0 ... 0 0 0]\n",
            " [1 0 1 ... 1 1 1]\n",
            " [1 0 0 ... 0 1 1]\n",
            " ...\n",
            " [0 1 1 ... 1 0 1]\n",
            " [0 0 1 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WKg2HU2adgZ"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fL8ptL4aeOY"
      },
      "source": [
        "This section tries to compare BER and Time performance of PYLDPC in following 3 cases\n",
        "1. SNR Noise function provided in encoder function of pyldpc library (pyldpc.encode)\n",
        "2. SNR Noise function provided by commpy library (commpy.channels.awgn) \n",
        "3. SNR Noise function implemented using tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma5zUqFv0TH2",
        "outputId": "2b7da1a7-0e54-4ebf-9b5a-24837090e682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using tensor flow based AWGN, to make sure that effect of it is same as AWGN\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_tensor  = numpy.array(())\n",
        "times_per_iter_tensor = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc_encode (CodingMatrix, input_message[i])\n",
        "    sigma = Snr2Sigma (snr)\n",
        "    awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message})[0]\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      #print (\"count=\",abs(decoded_message-input_message[i]).sum())\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_tensor=numpy.append(ber_per_iter_tensor ,ber)\n",
        "  times_per_iter_tensor=numpy.append(times_per_iter_tensor, total_time)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.75s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 3.24s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 4.69s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 6.22s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.62\n",
            " -> Total Time: 15.90s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.32s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.69s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 4.00s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 5.32s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.56\n",
            " -> Total Time: 13.33s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 1.01s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 2.28s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 3.60s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 4.69s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.50\n",
            " -> Total Time: 11.59s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.95s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.90s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.85s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 3.79s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.43\n",
            " -> Total Time: 9.49s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.66s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.39s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 2.08s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.83s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.33\n",
            " -> Total Time: 6.95s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.67s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.18s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.98s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.54s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.29\n",
            " -> Total Time: 6.37s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.55s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 1.08s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.60s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 2.04s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.22\n",
            " -> Total Time: 5.26s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.45s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.89s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.32s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.77s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.19\n",
            " -> Total Time: 4.43s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.41s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.76s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 1.12s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.48s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.13\n",
            " -> Total Time: 3.78s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.37s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.67s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 1.01s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.34s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.10\n",
            " -> Total Time: 3.39s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.34s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.69s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.98s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 1.28s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.06\n",
            " -> Total Time: 3.29s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.30s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.60s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.91s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 1.20s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 3.01s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.55s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.83s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 1.10s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 2.75s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.79s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 1.09s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 2.67s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.54s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.80s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 1.06s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.67s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.80s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 1.06s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.65s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.77s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 1.03s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.57s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.80s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 1.06s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.67s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.77s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 1.03s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.58s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.78s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 1.04s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.61s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8dIFLg76c7O",
        "outputId": "7aa86e79-a7e8-4457-90cf-d3c11ee9fa08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using commpy based AWGN \n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_awgn  = numpy.array(())\n",
        "times_per_iter_awgn = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc_encode (CodingMatrix, input_message[i])\n",
        "    awgn_channel_output_message = commpy.channels.awgn(encoded_message, snr)\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_awgn=numpy.append(ber_per_iter_awgn ,ber)\n",
        "  times_per_iter_awgn=numpy.append(times_per_iter_awgn, total_time)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.42s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 2.75s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 4.06s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 5.52s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.61\n",
            " -> Total Time: 13.75s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.21s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.41s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 3.49s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 4.62s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.55\n",
            " -> Total Time: 11.73s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 1.09s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 2.03s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 3.00s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 3.92s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.48\n",
            " -> Total Time: 10.04s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.77s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.54s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.31s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 3.22s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.40\n",
            " -> Total Time: 7.84s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.79s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.37s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 2.05s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.77s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.35\n",
            " -> Total Time: 6.97s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.46s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 0.94s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.30s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 1.79s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.26\n",
            " -> Total Time: 4.49s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.42s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.82s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.25s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.70s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.22\n",
            " -> Total Time: 4.19s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.28s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.63s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 0.94s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.23s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.17\n",
            " -> Total Time: 3.07s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.28s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.13\n",
            " -> Total Time: 2.53s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.21s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.43s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.63s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 0.83s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.09\n",
            " -> Total Time: 2.11s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.22s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.39s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.58s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.77s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.07\n",
            " -> Total Time: 1.96s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.38s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.57s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.74s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 1.88s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.21s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.39s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.56s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.75s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 1.91s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.33s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.50s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.66s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 1.64s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.47s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.63s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.56s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.48s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.63s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.58s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.47s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.62s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.56s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.47s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.62s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.57s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.47s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.62s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.55s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.30s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.46s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.61s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ihPKJJk7Jj9",
        "outputId": "1a28340b-ebf6-4846-f382-2dba48b74244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using tensor flow based AWGN, to make sure that effect of it is same as AWGN\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_pyldpc  = numpy.array(())\n",
        "times_per_iter_pyldpc = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc.encode (CodingMatrix, input_message[i], snr)\n",
        "    awgn_channel_output_message = encoded_message\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_pyldpc=numpy.append(ber_per_iter_pyldpc ,ber)\n",
        "  times_per_iter_pyldpc=numpy.append(times_per_iter_pyldpc, total_time)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.47s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 2.70s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 4.04s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 5.55s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.60\n",
            " -> Total Time: 13.77s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.28s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.42s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 3.44s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 4.42s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.57\n",
            " -> Total Time: 11.56s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.98s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 1.97s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 2.99s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 3.90s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.48\n",
            " -> Total Time: 9.84s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.93s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.72s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.47s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 3.15s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.43\n",
            " -> Total Time: 8.26s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.63s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.33s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 1.98s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.46s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.34\n",
            " -> Total Time: 6.40s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.48s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.02s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.60s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.06s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.28\n",
            " -> Total Time: 5.15s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.40s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.77s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.12s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.43s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.20\n",
            " -> Total Time: 3.72s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.59s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 0.90s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.20s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.15\n",
            " -> Total Time: 2.94s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.22s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.43s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.68s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 0.93s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.11\n",
            " -> Total Time: 2.26s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.45s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.67s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 0.92s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.08\n",
            " -> Total Time: 2.30s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.37s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.58s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.75s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.07\n",
            " -> Total Time: 1.90s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.37s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.58s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.78s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 1.91s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.33s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.49s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.67s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 1.67s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.49s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.64s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 1.61s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.30s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.47s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.63s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.55s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.18s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.33s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.49s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.64s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.64s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.48s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.65s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.61s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.30s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.46s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.60s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.52s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.30s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.46s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.61s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.51s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.46s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.61s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR4-FOJ-BkAG",
        "outputId": "f1b20ce7-f3d0-44bd-d0e1-08889f74d906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "# Compare 3 AWGN(Tensorflow, CommPy, PYLDPC) Simulation on LDPC\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_pyldpc,'', label=\"pyldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"tensor\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_awgn,'', label=\"commpy-awgn\") # plot BER vs SNR\n",
        "\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "ax2.set_xlabel('$E_b/$N_0$')\n",
        "ax2.set_ylabel('Decoding Time [s]')\n",
        "ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "            xy=(1, 0.35), xycoords='axes fraction',\n",
        "            xytext=(-20, 20), textcoords='offset pixels',\n",
        "            horizontalalignment='right',\n",
        "            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggY5VwudaRwR"
      },
      "source": [
        "<B>Conclussion:</B>\n",
        "      It proved that tensorflow behaves similar to AWGN noise channel provided by pyldpc, commpy. But tensor flow based one takes adds little more time delay. This need to be offseted if we are comparing performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5wyTuef3s7u"
      },
      "source": [
        "class GetOutOfLoop( Exception ):\n",
        "    pass"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOeuNfeLCgfb",
        "outputId": "4b8d4bf3-1d69-4517-eb5f-dde71eec7e6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Define Model \n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "# input_message_length is initialized by ldpc encoder\n",
        "CHANEL_SIZE = 2\n",
        "num_hidden_1 = CHANEL_SIZE\n",
        "input_message_length = 4\n",
        "print (\"input_message_length=\", input_message_length)\n",
        "\n",
        "lr_x = tf.placeholder(dtype=tf.float32,shape=[])\n",
        "#batch_size_x = tf.placeholder(tf.int32,shape=[])\n",
        "input_message_x_label = tf.placeholder(\"int32\", [None], name=\"input_message_x_label\")\n",
        "input_message_x = tf.placeholder(\"float32\", [None, 2**input_message_length], name=\"input_message_x\")\n",
        "awgn_noise_std_dev_x = tf.placeholder(\"float32\", name =\"awgn_noise_std_dev\")\n",
        "input_channel_x = tf.placeholder(\"float32\", [None, CHANEL_SIZE], name=\"input_channel_x\")\n",
        "\n",
        "weights = {\n",
        "  \"encoder_l1\" : tf.Variable (tf.random_uniform([2**input_message_length, num_hidden_1], -1, 1), name=\"encoder_l1_weights\"),\n",
        "  \"decoder_l1\" : tf.Variable (tf.random_uniform([num_hidden_1, 2**input_message_length], -1, 1), name=\"decoder_l1_weights\"),\n",
        "  \"decoder_l2\" : tf.Variable (tf.random_uniform([2**input_message_length, 2**input_message_length], -1, 1), name=\"decoder_l2_weights\"),\n",
        "}\n",
        "\n",
        "biases = {\n",
        "  \"encoder_l1\" : tf.Variable (tf.random_uniform([num_hidden_1], -1,1), name=\"encoder_l1_bias\"),\n",
        "  \"decoder_l1\" : tf.Variable (tf.random_uniform([2**input_message_length], -1,1), name=\"decoder_l1_bias\"),\n",
        "  \"decoder_l2\" : tf.Variable (tf.random_uniform([2**input_message_length], -1,1), name=\"decoder_l2_bias\"),\n",
        "}\n",
        "\n",
        "def dl_encoder (x):\n",
        "  layer_1 = tf.nn.tanh (tf.matmul(x, weights['encoder_l1']) + biases['encoder_l1'])\n",
        "  #layer_2 = tf.round(layer_1)\n",
        "  #layer_1 = BatchNormalization ()(layer_1)\n",
        "  layer_2 =  layer_1 / tf.sqrt(tf.reduce_mean(tf.square(layer_1)))\n",
        "  #layer_2 =  tf.nn.relu(layer_1)\n",
        "  return layer_2\n",
        "\n",
        "def dl_decoder (x):\n",
        "  layer_1 = tf.nn.tanh (tf.matmul(x, weights['decoder_l1']) + biases['decoder_l1'])\n",
        "  layer_2 = (tf.matmul(layer_1, weights['decoder_l2']) + biases['decoder_l2'])\n",
        "  return layer_2\n",
        "\n",
        "def awgn_layer(x):\n",
        "  awgn_noise = tf.random.normal(tf.shape(x), stddev=awgn_noise_std_dev_x,  name=\"awgn_noise\")\n",
        "  awgn_channel_output = tf.add(x, awgn_noise, name =\"x_and_noise\")\n",
        "  return awgn_channel_output\n",
        "\n",
        "\n",
        "dl_encoder_output = dl_encoder(input_message_x)\n",
        "dl_decoder_input = awgn_layer(dl_encoder_output)\n",
        "#awgn_noise = tf.random.normal(tf.shape(dl_encoder_output), stddev=awgn_noise_std_dev,  name=\"awgn_noise\")\n",
        "#dl_decoder_input = tf.add(dl_encoder_output, awgn_noise, name =\"x_and_noise\")\n",
        "dl_decoder_output = dl_decoder (dl_decoder_input)\n",
        "dl_decoder_only_output = dl_decoder(input_channel_x)\n",
        "\n",
        "\n",
        "#loss1 = tf.reduce_mean (-1 * (input_message_x*tf.log(dl_decoder_output) + (1 - input_message_x)*tf.log(1 - dl_decoder_output) ))\n",
        "loss = tf.losses.sparse_softmax_cross_entropy(labels=input_message_x_label,logits=dl_decoder_output)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr_x).minimize (loss)\n",
        "\n",
        "awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [CHANEL_SIZE])\n",
        "awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_message_length= 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkW8oloyodIF",
        "outputId": "983be0e0-dc67-467c-981c-448498c41692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "import numpy\n",
        "training_input_message = numpy.random.randint(2**input_message_length, size=(1,NUM_OF_INPUT_MESSAGE*10))\n",
        "training_input_message_one_hot = numpy.zeros((training_input_message.size, 2**input_message_length))\n",
        "training_input_message_one_hot[numpy.arange(training_input_message.size),training_input_message] = 1\n",
        "print(training_input_message_one_hot)\n",
        "print (training_input_message_one_hot.shape)\n",
        "print (training_input_message.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]]\n",
            "(10000, 16)\n",
            "(1, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxkLQFuqBT-g",
        "outputId": "87c60e27-7450-4e34-b3fc-31103a516de2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 30\n",
        "\n",
        "# Training\n",
        "train_init = tf.global_variables_initializer ()\n",
        "train_sess = tf.Session ()\n",
        "\n",
        "epochs = 10\n",
        "outer_ephocs = 1\n",
        "display_step = 2\n",
        "num_of_batches = training_input_message.shape[1] / batch_size\n",
        "print (\"batch_size:\", batch_size, \"num_of_batcches:\", num_of_batches)\n",
        "train_sess.run(train_init)\n",
        "l = 0\n",
        "lrate = 0.1\n",
        "i = 0\n",
        "snr_min = 9.5\n",
        "snr_max = 10.5\n",
        "snr_step_size = 0.5\n",
        "max_iteration = epochs * num_of_batches * (snr_max - snr_min) / snr_step_size\n",
        "print (\"max iteration :\",max_iteration,\"num_of_batches:\", num_of_batches)\n",
        "try:\n",
        "  for oe in range(outer_ephocs):\n",
        "    for snr in (numpy.arange (0, 10, SNR_STEP_SIZE)):\n",
        "    #for snr in (numpy.arange (snr_min, snr_max, SNR_STEP_SIZE)):\n",
        "      sigma = 1.0*Snr2Sigma (snr)\n",
        "      print (\"Training for SNR=\", snr, \" sigma=\", sigma, \"iteratin:\", oe) \n",
        "      for e in range(epochs):\n",
        "        for j in range (int(num_of_batches)):\n",
        "          i = i + 1\n",
        "          x_train_batch_one_hot = training_input_message_one_hot [j*batch_size:(j+1)*batch_size]\n",
        "          x_train_batch_one_hot = x_train_batch_one_hot.astype(\"float32\")\n",
        "          x_train_batch_label = training_input_message.reshape(training_input_message.shape[1]) [j*batch_size:(j+1)*batch_size]        \n",
        "          if (i < 100): \n",
        "            lr = 0.0001\n",
        "          elif(i < 200):\n",
        "            lr = 0.00001\n",
        "          else:\n",
        "            lr = 0.000001 \n",
        "          lr = 0.000001\n",
        "          _, l = train_sess.run ([optimizer, loss], feed_dict={input_message_x:x_train_batch_one_hot, awgn_noise_std_dev_x:sigma, lr_x:lr, input_message_x_label:x_train_batch_label.astype(\"int32\")})\n",
        "          if i % display_step == 0:          \n",
        "            print('Step %i: Minibatch Loss: %f' % (i, l ))\n",
        "          if (l < 0.05 and snr >= 9): \n",
        "            print (\"Loss=\", l)\n",
        "            raise GetOutOfLoop\n",
        "except GetOutOfLoop:\n",
        "  print(\"Early Stop\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Step 56610: Minibatch Loss: 2.821163\n",
            "Training for SNR= 8.5  sigma= 0.3758374042884442 iteratin: 0\n",
            "Step 56612: Minibatch Loss: 2.813434\n",
            "Step 56614: Minibatch Loss: 3.180883\n",
            "Step 56616: Minibatch Loss: 2.969514\n",
            "Step 56618: Minibatch Loss: 3.036118\n",
            "Step 56620: Minibatch Loss: 3.302690\n",
            "Step 56622: Minibatch Loss: 2.796726\n",
            "Step 56624: Minibatch Loss: 2.854745\n",
            "Step 56626: Minibatch Loss: 2.963466\n",
            "Step 56628: Minibatch Loss: 2.976900\n",
            "Step 56630: Minibatch Loss: 2.939718\n",
            "Step 56632: Minibatch Loss: 3.020951\n",
            "Step 56634: Minibatch Loss: 3.221586\n",
            "Step 56636: Minibatch Loss: 2.645772\n",
            "Step 56638: Minibatch Loss: 3.289618\n",
            "Step 56640: Minibatch Loss: 2.962748\n",
            "Step 56642: Minibatch Loss: 2.843004\n",
            "Step 56644: Minibatch Loss: 3.034117\n",
            "Step 56646: Minibatch Loss: 3.061188\n",
            "Step 56648: Minibatch Loss: 2.769520\n",
            "Step 56650: Minibatch Loss: 2.910504\n",
            "Step 56652: Minibatch Loss: 3.468576\n",
            "Step 56654: Minibatch Loss: 3.413764\n",
            "Step 56656: Minibatch Loss: 3.061441\n",
            "Step 56658: Minibatch Loss: 3.113899\n",
            "Step 56660: Minibatch Loss: 3.003578\n",
            "Step 56662: Minibatch Loss: 3.152889\n",
            "Step 56664: Minibatch Loss: 2.951700\n",
            "Step 56666: Minibatch Loss: 2.814130\n",
            "Step 56668: Minibatch Loss: 2.994581\n",
            "Step 56670: Minibatch Loss: 3.203800\n",
            "Step 56672: Minibatch Loss: 2.710811\n",
            "Step 56674: Minibatch Loss: 3.194007\n",
            "Step 56676: Minibatch Loss: 3.153211\n",
            "Step 56678: Minibatch Loss: 2.890679\n",
            "Step 56680: Minibatch Loss: 3.045563\n",
            "Step 56682: Minibatch Loss: 3.054217\n",
            "Step 56684: Minibatch Loss: 2.762716\n",
            "Step 56686: Minibatch Loss: 2.973317\n",
            "Step 56688: Minibatch Loss: 3.045006\n",
            "Step 56690: Minibatch Loss: 3.142489\n",
            "Step 56692: Minibatch Loss: 3.011283\n",
            "Step 56694: Minibatch Loss: 3.172692\n",
            "Step 56696: Minibatch Loss: 2.987401\n",
            "Step 56698: Minibatch Loss: 2.978533\n",
            "Step 56700: Minibatch Loss: 2.841664\n",
            "Step 56702: Minibatch Loss: 3.171805\n",
            "Step 56704: Minibatch Loss: 2.736111\n",
            "Step 56706: Minibatch Loss: 3.178077\n",
            "Step 56708: Minibatch Loss: 2.929152\n",
            "Step 56710: Minibatch Loss: 2.850804\n",
            "Step 56712: Minibatch Loss: 3.004765\n",
            "Step 56714: Minibatch Loss: 3.055665\n",
            "Step 56716: Minibatch Loss: 3.102818\n",
            "Step 56718: Minibatch Loss: 3.078647\n",
            "Step 56720: Minibatch Loss: 3.203061\n",
            "Step 56722: Minibatch Loss: 2.697952\n",
            "Step 56724: Minibatch Loss: 2.643633\n",
            "Step 56726: Minibatch Loss: 3.057907\n",
            "Step 56728: Minibatch Loss: 3.085668\n",
            "Step 56730: Minibatch Loss: 2.782101\n",
            "Step 56732: Minibatch Loss: 2.760354\n",
            "Step 56734: Minibatch Loss: 3.020198\n",
            "Step 56736: Minibatch Loss: 3.215301\n",
            "Step 56738: Minibatch Loss: 3.017650\n",
            "Step 56740: Minibatch Loss: 3.076965\n",
            "Step 56742: Minibatch Loss: 3.037509\n",
            "Step 56744: Minibatch Loss: 2.999717\n",
            "Step 56746: Minibatch Loss: 2.784117\n",
            "Step 56748: Minibatch Loss: 2.940667\n",
            "Step 56750: Minibatch Loss: 3.046099\n",
            "Step 56752: Minibatch Loss: 2.982168\n",
            "Step 56754: Minibatch Loss: 2.921102\n",
            "Step 56756: Minibatch Loss: 2.876101\n",
            "Step 56758: Minibatch Loss: 3.069628\n",
            "Step 56760: Minibatch Loss: 2.792819\n",
            "Step 56762: Minibatch Loss: 2.885086\n",
            "Step 56764: Minibatch Loss: 3.243806\n",
            "Step 56766: Minibatch Loss: 2.983834\n",
            "Step 56768: Minibatch Loss: 3.196308\n",
            "Step 56770: Minibatch Loss: 2.965714\n",
            "Step 56772: Minibatch Loss: 2.708964\n",
            "Step 56774: Minibatch Loss: 2.977484\n",
            "Step 56776: Minibatch Loss: 2.766938\n",
            "Step 56778: Minibatch Loss: 2.798457\n",
            "Step 56780: Minibatch Loss: 2.925557\n",
            "Step 56782: Minibatch Loss: 3.117993\n",
            "Step 56784: Minibatch Loss: 3.288472\n",
            "Step 56786: Minibatch Loss: 3.112617\n",
            "Step 56788: Minibatch Loss: 2.814785\n",
            "Step 56790: Minibatch Loss: 2.887493\n",
            "Step 56792: Minibatch Loss: 2.546850\n",
            "Step 56794: Minibatch Loss: 2.872417\n",
            "Step 56796: Minibatch Loss: 2.811039\n",
            "Step 56798: Minibatch Loss: 2.772247\n",
            "Step 56800: Minibatch Loss: 3.194997\n",
            "Step 56802: Minibatch Loss: 3.177832\n",
            "Step 56804: Minibatch Loss: 2.793746\n",
            "Step 56806: Minibatch Loss: 3.168690\n",
            "Step 56808: Minibatch Loss: 3.024802\n",
            "Step 56810: Minibatch Loss: 3.028950\n",
            "Step 56812: Minibatch Loss: 2.862868\n",
            "Step 56814: Minibatch Loss: 3.161748\n",
            "Step 56816: Minibatch Loss: 3.259387\n",
            "Step 56818: Minibatch Loss: 3.129226\n",
            "Step 56820: Minibatch Loss: 2.992529\n",
            "Step 56822: Minibatch Loss: 3.036331\n",
            "Step 56824: Minibatch Loss: 2.956756\n",
            "Step 56826: Minibatch Loss: 3.216792\n",
            "Step 56828: Minibatch Loss: 3.131651\n",
            "Step 56830: Minibatch Loss: 3.149128\n",
            "Step 56832: Minibatch Loss: 3.045357\n",
            "Step 56834: Minibatch Loss: 2.921417\n",
            "Step 56836: Minibatch Loss: 2.629412\n",
            "Step 56838: Minibatch Loss: 2.777928\n",
            "Step 56840: Minibatch Loss: 3.092813\n",
            "Step 56842: Minibatch Loss: 2.765256\n",
            "Step 56844: Minibatch Loss: 3.272946\n",
            "Step 56846: Minibatch Loss: 2.919818\n",
            "Step 56848: Minibatch Loss: 2.965014\n",
            "Step 56850: Minibatch Loss: 3.089104\n",
            "Step 56852: Minibatch Loss: 3.059205\n",
            "Step 56854: Minibatch Loss: 2.874933\n",
            "Step 56856: Minibatch Loss: 2.960101\n",
            "Step 56858: Minibatch Loss: 2.820639\n",
            "Step 56860: Minibatch Loss: 3.115438\n",
            "Step 56862: Minibatch Loss: 2.886864\n",
            "Step 56864: Minibatch Loss: 2.803102\n",
            "Step 56866: Minibatch Loss: 2.993581\n",
            "Step 56868: Minibatch Loss: 3.017465\n",
            "Step 56870: Minibatch Loss: 2.873253\n",
            "Step 56872: Minibatch Loss: 3.090046\n",
            "Step 56874: Minibatch Loss: 3.067054\n",
            "Step 56876: Minibatch Loss: 3.187079\n",
            "Step 56878: Minibatch Loss: 2.960518\n",
            "Step 56880: Minibatch Loss: 2.874924\n",
            "Step 56882: Minibatch Loss: 2.993131\n",
            "Step 56884: Minibatch Loss: 2.758313\n",
            "Step 56886: Minibatch Loss: 3.047474\n",
            "Step 56888: Minibatch Loss: 2.993318\n",
            "Step 56890: Minibatch Loss: 3.251068\n",
            "Step 56892: Minibatch Loss: 2.973330\n",
            "Step 56894: Minibatch Loss: 3.197870\n",
            "Step 56896: Minibatch Loss: 3.294581\n",
            "Step 56898: Minibatch Loss: 3.261264\n",
            "Step 56900: Minibatch Loss: 2.616570\n",
            "Step 56902: Minibatch Loss: 3.026221\n",
            "Step 56904: Minibatch Loss: 2.839909\n",
            "Step 56906: Minibatch Loss: 3.008662\n",
            "Step 56908: Minibatch Loss: 3.156930\n",
            "Step 56910: Minibatch Loss: 2.798013\n",
            "Step 56912: Minibatch Loss: 2.858927\n",
            "Step 56914: Minibatch Loss: 2.845018\n",
            "Step 56916: Minibatch Loss: 2.910374\n",
            "Step 56918: Minibatch Loss: 2.838081\n",
            "Step 56920: Minibatch Loss: 2.943657\n",
            "Step 56922: Minibatch Loss: 2.868334\n",
            "Step 56924: Minibatch Loss: 3.244925\n",
            "Step 56926: Minibatch Loss: 3.043849\n",
            "Step 56928: Minibatch Loss: 3.046560\n",
            "Step 56930: Minibatch Loss: 2.926067\n",
            "Step 56932: Minibatch Loss: 2.851462\n",
            "Step 56934: Minibatch Loss: 2.945624\n",
            "Step 56936: Minibatch Loss: 3.074115\n",
            "Step 56938: Minibatch Loss: 3.288431\n",
            "Step 56940: Minibatch Loss: 3.084391\n",
            "Step 56942: Minibatch Loss: 2.968269\n",
            "Step 56944: Minibatch Loss: 3.031377\n",
            "Step 56946: Minibatch Loss: 2.875167\n",
            "Step 56948: Minibatch Loss: 3.069755\n",
            "Step 56950: Minibatch Loss: 2.773697\n",
            "Step 56952: Minibatch Loss: 3.140523\n",
            "Step 56954: Minibatch Loss: 3.371241\n",
            "Step 56956: Minibatch Loss: 2.961745\n",
            "Step 56958: Minibatch Loss: 3.145273\n",
            "Step 56960: Minibatch Loss: 3.205719\n",
            "Step 56962: Minibatch Loss: 2.899040\n",
            "Step 56964: Minibatch Loss: 2.792792\n",
            "Step 56966: Minibatch Loss: 2.969514\n",
            "Step 56968: Minibatch Loss: 2.943278\n",
            "Step 56970: Minibatch Loss: 3.103759\n",
            "Step 56972: Minibatch Loss: 2.611775\n",
            "Step 56974: Minibatch Loss: 3.002390\n",
            "Step 56976: Minibatch Loss: 3.166408\n",
            "Step 56978: Minibatch Loss: 3.117339\n",
            "Step 56980: Minibatch Loss: 3.073450\n",
            "Step 56982: Minibatch Loss: 2.902153\n",
            "Step 56984: Minibatch Loss: 3.057351\n",
            "Step 56986: Minibatch Loss: 2.874630\n",
            "Step 56988: Minibatch Loss: 2.992831\n",
            "Step 56990: Minibatch Loss: 2.941408\n",
            "Step 56992: Minibatch Loss: 2.978248\n",
            "Step 56994: Minibatch Loss: 3.067618\n",
            "Step 56996: Minibatch Loss: 2.701110\n",
            "Step 56998: Minibatch Loss: 3.231940\n",
            "Step 57000: Minibatch Loss: 2.925156\n",
            "Step 57002: Minibatch Loss: 3.009686\n",
            "Step 57004: Minibatch Loss: 2.890541\n",
            "Step 57006: Minibatch Loss: 2.691131\n",
            "Step 57008: Minibatch Loss: 3.227484\n",
            "Step 57010: Minibatch Loss: 3.269687\n",
            "Step 57012: Minibatch Loss: 2.933453\n",
            "Step 57014: Minibatch Loss: 3.075823\n",
            "Step 57016: Minibatch Loss: 2.857052\n",
            "Step 57018: Minibatch Loss: 2.937673\n",
            "Step 57020: Minibatch Loss: 2.914390\n",
            "Step 57022: Minibatch Loss: 2.683359\n",
            "Step 57024: Minibatch Loss: 2.739242\n",
            "Step 57026: Minibatch Loss: 2.976091\n",
            "Step 57028: Minibatch Loss: 2.829503\n",
            "Step 57030: Minibatch Loss: 3.214010\n",
            "Step 57032: Minibatch Loss: 2.981452\n",
            "Step 57034: Minibatch Loss: 2.871205\n",
            "Step 57036: Minibatch Loss: 3.137129\n",
            "Step 57038: Minibatch Loss: 3.426009\n",
            "Step 57040: Minibatch Loss: 3.014043\n",
            "Step 57042: Minibatch Loss: 3.346427\n",
            "Step 57044: Minibatch Loss: 2.870045\n",
            "Step 57046: Minibatch Loss: 3.201577\n",
            "Step 57048: Minibatch Loss: 3.101566\n",
            "Step 57050: Minibatch Loss: 2.581114\n",
            "Step 57052: Minibatch Loss: 3.339645\n",
            "Step 57054: Minibatch Loss: 2.917323\n",
            "Step 57056: Minibatch Loss: 3.152159\n",
            "Step 57058: Minibatch Loss: 3.096042\n",
            "Step 57060: Minibatch Loss: 2.877098\n",
            "Step 57062: Minibatch Loss: 2.870845\n",
            "Step 57064: Minibatch Loss: 3.118838\n",
            "Step 57066: Minibatch Loss: 3.071930\n",
            "Step 57068: Minibatch Loss: 2.855720\n",
            "Step 57070: Minibatch Loss: 2.902600\n",
            "Step 57072: Minibatch Loss: 2.630278\n",
            "Step 57074: Minibatch Loss: 3.009944\n",
            "Step 57076: Minibatch Loss: 2.847631\n",
            "Step 57078: Minibatch Loss: 3.065912\n",
            "Step 57080: Minibatch Loss: 3.092358\n",
            "Step 57082: Minibatch Loss: 2.788550\n",
            "Step 57084: Minibatch Loss: 3.133941\n",
            "Step 57086: Minibatch Loss: 3.080071\n",
            "Step 57088: Minibatch Loss: 3.208177\n",
            "Step 57090: Minibatch Loss: 3.242026\n",
            "Step 57092: Minibatch Loss: 2.968150\n",
            "Step 57094: Minibatch Loss: 2.980563\n",
            "Step 57096: Minibatch Loss: 2.950560\n",
            "Step 57098: Minibatch Loss: 3.096435\n",
            "Step 57100: Minibatch Loss: 3.053087\n",
            "Step 57102: Minibatch Loss: 3.112556\n",
            "Step 57104: Minibatch Loss: 2.908972\n",
            "Step 57106: Minibatch Loss: 3.025662\n",
            "Step 57108: Minibatch Loss: 3.072956\n",
            "Step 57110: Minibatch Loss: 2.992036\n",
            "Step 57112: Minibatch Loss: 3.087707\n",
            "Step 57114: Minibatch Loss: 3.166823\n",
            "Step 57116: Minibatch Loss: 3.278758\n",
            "Step 57118: Minibatch Loss: 2.842387\n",
            "Step 57120: Minibatch Loss: 3.399262\n",
            "Step 57122: Minibatch Loss: 3.299759\n",
            "Step 57124: Minibatch Loss: 2.849944\n",
            "Step 57126: Minibatch Loss: 2.916218\n",
            "Step 57128: Minibatch Loss: 3.110288\n",
            "Step 57130: Minibatch Loss: 3.038027\n",
            "Step 57132: Minibatch Loss: 2.898214\n",
            "Step 57134: Minibatch Loss: 3.033387\n",
            "Step 57136: Minibatch Loss: 3.227618\n",
            "Step 57138: Minibatch Loss: 3.053223\n",
            "Step 57140: Minibatch Loss: 3.171301\n",
            "Step 57142: Minibatch Loss: 2.975871\n",
            "Step 57144: Minibatch Loss: 3.109328\n",
            "Step 57146: Minibatch Loss: 3.395356\n",
            "Step 57148: Minibatch Loss: 2.907293\n",
            "Step 57150: Minibatch Loss: 3.209089\n",
            "Step 57152: Minibatch Loss: 3.009187\n",
            "Step 57154: Minibatch Loss: 3.233892\n",
            "Step 57156: Minibatch Loss: 3.179444\n",
            "Step 57158: Minibatch Loss: 3.120437\n",
            "Step 57160: Minibatch Loss: 3.245057\n",
            "Step 57162: Minibatch Loss: 3.392109\n",
            "Step 57164: Minibatch Loss: 2.985710\n",
            "Step 57166: Minibatch Loss: 2.939633\n",
            "Step 57168: Minibatch Loss: 2.689950\n",
            "Step 57170: Minibatch Loss: 3.263965\n",
            "Step 57172: Minibatch Loss: 3.192755\n",
            "Step 57174: Minibatch Loss: 3.240347\n",
            "Step 57176: Minibatch Loss: 2.917481\n",
            "Step 57178: Minibatch Loss: 3.075131\n",
            "Step 57180: Minibatch Loss: 3.139516\n",
            "Step 57182: Minibatch Loss: 3.146568\n",
            "Step 57184: Minibatch Loss: 3.012234\n",
            "Step 57186: Minibatch Loss: 3.031706\n",
            "Step 57188: Minibatch Loss: 3.005615\n",
            "Step 57190: Minibatch Loss: 2.893440\n",
            "Step 57192: Minibatch Loss: 2.968467\n",
            "Step 57194: Minibatch Loss: 3.162803\n",
            "Step 57196: Minibatch Loss: 2.990128\n",
            "Step 57198: Minibatch Loss: 2.861432\n",
            "Step 57200: Minibatch Loss: 3.045642\n",
            "Step 57202: Minibatch Loss: 2.938016\n",
            "Step 57204: Minibatch Loss: 3.075335\n",
            "Step 57206: Minibatch Loss: 3.246224\n",
            "Step 57208: Minibatch Loss: 2.960807\n",
            "Step 57210: Minibatch Loss: 2.878062\n",
            "Step 57212: Minibatch Loss: 3.136525\n",
            "Step 57214: Minibatch Loss: 3.519421\n",
            "Step 57216: Minibatch Loss: 2.665948\n",
            "Step 57218: Minibatch Loss: 3.220046\n",
            "Step 57220: Minibatch Loss: 2.883874\n",
            "Step 57222: Minibatch Loss: 3.018052\n",
            "Step 57224: Minibatch Loss: 3.223932\n",
            "Step 57226: Minibatch Loss: 3.067172\n",
            "Step 57228: Minibatch Loss: 2.845361\n",
            "Step 57230: Minibatch Loss: 2.956922\n",
            "Step 57232: Minibatch Loss: 3.154480\n",
            "Step 57234: Minibatch Loss: 2.894691\n",
            "Step 57236: Minibatch Loss: 2.984196\n",
            "Step 57238: Minibatch Loss: 3.036089\n",
            "Step 57240: Minibatch Loss: 2.827708\n",
            "Step 57242: Minibatch Loss: 2.970329\n",
            "Step 57244: Minibatch Loss: 3.308677\n",
            "Step 57246: Minibatch Loss: 2.761271\n",
            "Step 57248: Minibatch Loss: 2.959869\n",
            "Step 57250: Minibatch Loss: 3.063426\n",
            "Step 57252: Minibatch Loss: 2.899466\n",
            "Step 57254: Minibatch Loss: 2.906517\n",
            "Step 57256: Minibatch Loss: 2.908404\n",
            "Step 57258: Minibatch Loss: 2.984078\n",
            "Step 57260: Minibatch Loss: 3.310595\n",
            "Step 57262: Minibatch Loss: 3.148001\n",
            "Step 57264: Minibatch Loss: 2.906780\n",
            "Step 57266: Minibatch Loss: 3.097590\n",
            "Step 57268: Minibatch Loss: 3.253658\n",
            "Step 57270: Minibatch Loss: 3.345527\n",
            "Step 57272: Minibatch Loss: 3.351409\n",
            "Step 57274: Minibatch Loss: 3.080239\n",
            "Step 57276: Minibatch Loss: 2.939317\n",
            "Step 57278: Minibatch Loss: 2.774871\n",
            "Step 57280: Minibatch Loss: 3.160534\n",
            "Step 57282: Minibatch Loss: 3.104202\n",
            "Step 57284: Minibatch Loss: 2.977474\n",
            "Step 57286: Minibatch Loss: 3.246960\n",
            "Step 57288: Minibatch Loss: 2.826596\n",
            "Step 57290: Minibatch Loss: 2.794858\n",
            "Step 57292: Minibatch Loss: 2.997100\n",
            "Step 57294: Minibatch Loss: 2.948470\n",
            "Step 57296: Minibatch Loss: 2.954357\n",
            "Step 57298: Minibatch Loss: 3.013260\n",
            "Step 57300: Minibatch Loss: 3.150161\n",
            "Step 57302: Minibatch Loss: 2.586520\n",
            "Step 57304: Minibatch Loss: 3.292078\n",
            "Step 57306: Minibatch Loss: 2.964326\n",
            "Step 57308: Minibatch Loss: 2.873256\n",
            "Step 57310: Minibatch Loss: 3.101220\n",
            "Step 57312: Minibatch Loss: 3.083621\n",
            "Step 57314: Minibatch Loss: 2.837738\n",
            "Step 57316: Minibatch Loss: 2.880384\n",
            "Step 57318: Minibatch Loss: 3.338650\n",
            "Step 57320: Minibatch Loss: 3.565860\n",
            "Step 57322: Minibatch Loss: 2.984374\n",
            "Step 57324: Minibatch Loss: 2.990058\n",
            "Step 57326: Minibatch Loss: 3.101661\n",
            "Step 57328: Minibatch Loss: 3.038522\n",
            "Step 57330: Minibatch Loss: 2.949609\n",
            "Step 57332: Minibatch Loss: 2.797549\n",
            "Step 57334: Minibatch Loss: 2.964799\n",
            "Step 57336: Minibatch Loss: 3.165396\n",
            "Step 57338: Minibatch Loss: 2.589014\n",
            "Step 57340: Minibatch Loss: 3.115564\n",
            "Step 57342: Minibatch Loss: 3.066848\n",
            "Step 57344: Minibatch Loss: 2.810735\n",
            "Step 57346: Minibatch Loss: 2.962054\n",
            "Step 57348: Minibatch Loss: 2.966168\n",
            "Step 57350: Minibatch Loss: 2.786209\n",
            "Step 57352: Minibatch Loss: 3.004889\n",
            "Step 57354: Minibatch Loss: 2.948380\n",
            "Step 57356: Minibatch Loss: 3.170862\n",
            "Step 57358: Minibatch Loss: 2.921341\n",
            "Step 57360: Minibatch Loss: 3.236016\n",
            "Step 57362: Minibatch Loss: 2.946140\n",
            "Step 57364: Minibatch Loss: 3.000700\n",
            "Step 57366: Minibatch Loss: 2.713699\n",
            "Step 57368: Minibatch Loss: 3.098359\n",
            "Step 57370: Minibatch Loss: 2.717160\n",
            "Step 57372: Minibatch Loss: 3.145609\n",
            "Step 57374: Minibatch Loss: 2.857696\n",
            "Step 57376: Minibatch Loss: 2.851135\n",
            "Step 57378: Minibatch Loss: 3.027930\n",
            "Step 57380: Minibatch Loss: 3.053720\n",
            "Step 57382: Minibatch Loss: 3.099652\n",
            "Step 57384: Minibatch Loss: 2.976089\n",
            "Step 57386: Minibatch Loss: 3.110211\n",
            "Step 57388: Minibatch Loss: 2.844326\n",
            "Step 57390: Minibatch Loss: 2.547732\n",
            "Step 57392: Minibatch Loss: 2.961461\n",
            "Step 57394: Minibatch Loss: 3.074512\n",
            "Step 57396: Minibatch Loss: 2.739200\n",
            "Step 57398: Minibatch Loss: 2.764200\n",
            "Step 57400: Minibatch Loss: 3.059442\n",
            "Step 57402: Minibatch Loss: 3.114042\n",
            "Step 57404: Minibatch Loss: 2.908869\n",
            "Step 57406: Minibatch Loss: 3.038864\n",
            "Step 57408: Minibatch Loss: 3.024644\n",
            "Step 57410: Minibatch Loss: 3.099381\n",
            "Step 57412: Minibatch Loss: 2.833836\n",
            "Step 57414: Minibatch Loss: 2.859807\n",
            "Step 57416: Minibatch Loss: 3.020726\n",
            "Step 57418: Minibatch Loss: 2.914345\n",
            "Step 57420: Minibatch Loss: 2.987560\n",
            "Step 57422: Minibatch Loss: 2.828338\n",
            "Step 57424: Minibatch Loss: 3.163731\n",
            "Step 57426: Minibatch Loss: 2.817676\n",
            "Step 57428: Minibatch Loss: 2.969917\n",
            "Step 57430: Minibatch Loss: 3.268946\n",
            "Step 57432: Minibatch Loss: 2.910457\n",
            "Step 57434: Minibatch Loss: 3.133287\n",
            "Step 57436: Minibatch Loss: 3.032686\n",
            "Step 57438: Minibatch Loss: 2.583571\n",
            "Step 57440: Minibatch Loss: 2.972934\n",
            "Step 57442: Minibatch Loss: 2.768829\n",
            "Step 57444: Minibatch Loss: 2.723663\n",
            "Step 57446: Minibatch Loss: 2.968512\n",
            "Step 57448: Minibatch Loss: 3.150363\n",
            "Step 57450: Minibatch Loss: 3.396753\n",
            "Step 57452: Minibatch Loss: 3.136779\n",
            "Step 57454: Minibatch Loss: 2.835708\n",
            "Step 57456: Minibatch Loss: 2.816187\n",
            "Step 57458: Minibatch Loss: 2.661477\n",
            "Step 57460: Minibatch Loss: 2.918145\n",
            "Step 57462: Minibatch Loss: 2.894190\n",
            "Step 57464: Minibatch Loss: 2.689244\n",
            "Step 57466: Minibatch Loss: 3.251173\n",
            "Step 57468: Minibatch Loss: 3.021755\n",
            "Step 57470: Minibatch Loss: 2.828411\n",
            "Step 57472: Minibatch Loss: 3.107665\n",
            "Step 57474: Minibatch Loss: 3.027559\n",
            "Step 57476: Minibatch Loss: 3.009935\n",
            "Step 57478: Minibatch Loss: 2.803840\n",
            "Step 57480: Minibatch Loss: 3.092926\n",
            "Step 57482: Minibatch Loss: 3.269337\n",
            "Step 57484: Minibatch Loss: 3.047891\n",
            "Step 57486: Minibatch Loss: 3.085318\n",
            "Step 57488: Minibatch Loss: 2.993508\n",
            "Step 57490: Minibatch Loss: 3.013283\n",
            "Step 57492: Minibatch Loss: 3.239864\n",
            "Step 57494: Minibatch Loss: 3.134076\n",
            "Step 57496: Minibatch Loss: 3.184615\n",
            "Step 57498: Minibatch Loss: 2.963632\n",
            "Step 57500: Minibatch Loss: 2.977762\n",
            "Step 57502: Minibatch Loss: 2.600022\n",
            "Step 57504: Minibatch Loss: 2.830822\n",
            "Step 57506: Minibatch Loss: 3.089406\n",
            "Step 57508: Minibatch Loss: 2.826790\n",
            "Step 57510: Minibatch Loss: 3.218229\n",
            "Step 57512: Minibatch Loss: 2.860744\n",
            "Step 57514: Minibatch Loss: 3.070821\n",
            "Step 57516: Minibatch Loss: 3.140873\n",
            "Step 57518: Minibatch Loss: 3.077852\n",
            "Step 57520: Minibatch Loss: 2.889693\n",
            "Step 57522: Minibatch Loss: 2.992197\n",
            "Step 57524: Minibatch Loss: 2.775864\n",
            "Step 57526: Minibatch Loss: 3.100771\n",
            "Step 57528: Minibatch Loss: 2.816739\n",
            "Step 57530: Minibatch Loss: 2.856134\n",
            "Step 57532: Minibatch Loss: 3.071341\n",
            "Step 57534: Minibatch Loss: 2.858348\n",
            "Step 57536: Minibatch Loss: 2.914119\n",
            "Step 57538: Minibatch Loss: 3.046489\n",
            "Step 57540: Minibatch Loss: 3.049016\n",
            "Step 57542: Minibatch Loss: 3.184251\n",
            "Step 57544: Minibatch Loss: 3.041182\n",
            "Step 57546: Minibatch Loss: 2.891907\n",
            "Step 57548: Minibatch Loss: 2.984603\n",
            "Step 57550: Minibatch Loss: 2.766787\n",
            "Step 57552: Minibatch Loss: 3.014962\n",
            "Step 57554: Minibatch Loss: 2.973258\n",
            "Step 57556: Minibatch Loss: 3.204826\n",
            "Step 57558: Minibatch Loss: 2.970180\n",
            "Step 57560: Minibatch Loss: 3.137399\n",
            "Step 57562: Minibatch Loss: 3.289546\n",
            "Step 57564: Minibatch Loss: 3.250596\n",
            "Step 57566: Minibatch Loss: 2.498948\n",
            "Step 57568: Minibatch Loss: 2.985167\n",
            "Step 57570: Minibatch Loss: 2.973002\n",
            "Step 57572: Minibatch Loss: 2.977387\n",
            "Step 57574: Minibatch Loss: 3.037891\n",
            "Step 57576: Minibatch Loss: 2.800943\n",
            "Step 57578: Minibatch Loss: 2.875575\n",
            "Step 57580: Minibatch Loss: 2.724898\n",
            "Step 57582: Minibatch Loss: 2.978409\n",
            "Step 57584: Minibatch Loss: 2.746342\n",
            "Step 57586: Minibatch Loss: 3.056436\n",
            "Step 57588: Minibatch Loss: 2.978688\n",
            "Step 57590: Minibatch Loss: 3.168200\n",
            "Step 57592: Minibatch Loss: 2.994721\n",
            "Step 57594: Minibatch Loss: 3.088234\n",
            "Step 57596: Minibatch Loss: 2.964467\n",
            "Step 57598: Minibatch Loss: 2.943528\n",
            "Step 57600: Minibatch Loss: 2.915031\n",
            "Step 57602: Minibatch Loss: 3.130877\n",
            "Step 57604: Minibatch Loss: 3.336770\n",
            "Step 57606: Minibatch Loss: 3.239005\n",
            "Step 57608: Minibatch Loss: 2.916244\n",
            "Step 57610: Minibatch Loss: 3.075227\n",
            "Step 57612: Minibatch Loss: 2.924551\n",
            "Step 57614: Minibatch Loss: 3.095099\n",
            "Step 57616: Minibatch Loss: 2.805219\n",
            "Step 57618: Minibatch Loss: 3.126402\n",
            "Step 57620: Minibatch Loss: 3.477730\n",
            "Step 57622: Minibatch Loss: 2.925293\n",
            "Step 57624: Minibatch Loss: 3.177030\n",
            "Step 57626: Minibatch Loss: 3.130476\n",
            "Step 57628: Minibatch Loss: 2.835647\n",
            "Step 57630: Minibatch Loss: 2.843962\n",
            "Step 57632: Minibatch Loss: 2.851148\n",
            "Step 57634: Minibatch Loss: 2.935999\n",
            "Step 57636: Minibatch Loss: 3.105828\n",
            "Step 57638: Minibatch Loss: 2.639561\n",
            "Step 57640: Minibatch Loss: 3.069781\n",
            "Step 57642: Minibatch Loss: 3.209032\n",
            "Step 57644: Minibatch Loss: 3.201967\n",
            "Step 57646: Minibatch Loss: 3.054120\n",
            "Step 57648: Minibatch Loss: 2.864419\n",
            "Step 57650: Minibatch Loss: 3.076534\n",
            "Step 57652: Minibatch Loss: 2.812933\n",
            "Step 57654: Minibatch Loss: 3.037845\n",
            "Step 57656: Minibatch Loss: 2.985480\n",
            "Step 57658: Minibatch Loss: 2.958766\n",
            "Step 57660: Minibatch Loss: 2.994852\n",
            "Step 57662: Minibatch Loss: 2.759218\n",
            "Step 57664: Minibatch Loss: 3.008476\n",
            "Step 57666: Minibatch Loss: 2.937488\n",
            "Step 57668: Minibatch Loss: 2.971534\n",
            "Step 57670: Minibatch Loss: 2.788078\n",
            "Step 57672: Minibatch Loss: 2.652804\n",
            "Step 57674: Minibatch Loss: 3.176816\n",
            "Step 57676: Minibatch Loss: 3.168601\n",
            "Step 57678: Minibatch Loss: 3.086610\n",
            "Step 57680: Minibatch Loss: 3.073472\n",
            "Step 57682: Minibatch Loss: 2.789954\n",
            "Step 57684: Minibatch Loss: 2.872980\n",
            "Step 57686: Minibatch Loss: 2.934161\n",
            "Step 57688: Minibatch Loss: 2.710515\n",
            "Step 57690: Minibatch Loss: 2.785955\n",
            "Step 57692: Minibatch Loss: 3.009930\n",
            "Step 57694: Minibatch Loss: 2.938815\n",
            "Step 57696: Minibatch Loss: 3.153379\n",
            "Step 57698: Minibatch Loss: 3.005329\n",
            "Step 57700: Minibatch Loss: 2.735554\n",
            "Step 57702: Minibatch Loss: 3.127137\n",
            "Step 57704: Minibatch Loss: 3.404080\n",
            "Step 57706: Minibatch Loss: 3.020625\n",
            "Step 57708: Minibatch Loss: 3.468333\n",
            "Step 57710: Minibatch Loss: 2.901147\n",
            "Step 57712: Minibatch Loss: 3.204826\n",
            "Step 57714: Minibatch Loss: 3.022534\n",
            "Step 57716: Minibatch Loss: 2.588008\n",
            "Step 57718: Minibatch Loss: 3.372651\n",
            "Step 57720: Minibatch Loss: 3.002479\n",
            "Step 57722: Minibatch Loss: 3.273786\n",
            "Step 57724: Minibatch Loss: 3.033454\n",
            "Step 57726: Minibatch Loss: 2.855592\n",
            "Step 57728: Minibatch Loss: 2.969656\n",
            "Step 57730: Minibatch Loss: 3.174117\n",
            "Step 57732: Minibatch Loss: 3.139785\n",
            "Step 57734: Minibatch Loss: 2.897859\n",
            "Step 57736: Minibatch Loss: 2.763142\n",
            "Step 57738: Minibatch Loss: 2.560642\n",
            "Step 57740: Minibatch Loss: 3.002876\n",
            "Step 57742: Minibatch Loss: 2.891066\n",
            "Step 57744: Minibatch Loss: 3.001070\n",
            "Step 57746: Minibatch Loss: 3.058940\n",
            "Step 57748: Minibatch Loss: 2.823725\n",
            "Step 57750: Minibatch Loss: 3.092229\n",
            "Step 57752: Minibatch Loss: 3.118155\n",
            "Step 57754: Minibatch Loss: 3.009156\n",
            "Step 57756: Minibatch Loss: 3.182178\n",
            "Step 57758: Minibatch Loss: 3.040406\n",
            "Step 57760: Minibatch Loss: 3.025836\n",
            "Step 57762: Minibatch Loss: 2.918550\n",
            "Step 57764: Minibatch Loss: 3.058472\n",
            "Step 57766: Minibatch Loss: 3.115852\n",
            "Step 57768: Minibatch Loss: 3.185765\n",
            "Step 57770: Minibatch Loss: 2.803327\n",
            "Step 57772: Minibatch Loss: 2.903173\n",
            "Step 57774: Minibatch Loss: 3.025112\n",
            "Step 57776: Minibatch Loss: 3.072736\n",
            "Step 57778: Minibatch Loss: 3.012087\n",
            "Step 57780: Minibatch Loss: 3.009277\n",
            "Step 57782: Minibatch Loss: 3.198187\n",
            "Step 57784: Minibatch Loss: 3.002441\n",
            "Step 57786: Minibatch Loss: 3.342299\n",
            "Step 57788: Minibatch Loss: 3.190375\n",
            "Step 57790: Minibatch Loss: 2.856622\n",
            "Step 57792: Minibatch Loss: 2.963559\n",
            "Step 57794: Minibatch Loss: 3.124207\n",
            "Step 57796: Minibatch Loss: 2.968038\n",
            "Step 57798: Minibatch Loss: 2.911585\n",
            "Step 57800: Minibatch Loss: 2.993657\n",
            "Step 57802: Minibatch Loss: 3.112548\n",
            "Step 57804: Minibatch Loss: 3.171697\n",
            "Step 57806: Minibatch Loss: 3.103124\n",
            "Step 57808: Minibatch Loss: 2.961102\n",
            "Step 57810: Minibatch Loss: 3.125429\n",
            "Step 57812: Minibatch Loss: 3.443685\n",
            "Step 57814: Minibatch Loss: 2.748264\n",
            "Step 57816: Minibatch Loss: 3.192975\n",
            "Step 57818: Minibatch Loss: 3.025272\n",
            "Step 57820: Minibatch Loss: 3.240173\n",
            "Step 57822: Minibatch Loss: 3.216862\n",
            "Step 57824: Minibatch Loss: 3.056746\n",
            "Step 57826: Minibatch Loss: 3.194789\n",
            "Step 57828: Minibatch Loss: 3.397168\n",
            "Step 57830: Minibatch Loss: 3.110992\n",
            "Step 57832: Minibatch Loss: 2.829195\n",
            "Step 57834: Minibatch Loss: 2.641737\n",
            "Step 57836: Minibatch Loss: 3.285266\n",
            "Step 57838: Minibatch Loss: 3.132118\n",
            "Step 57840: Minibatch Loss: 3.367019\n",
            "Step 57842: Minibatch Loss: 2.980094\n",
            "Step 57844: Minibatch Loss: 3.100040\n",
            "Step 57846: Minibatch Loss: 3.242676\n",
            "Step 57848: Minibatch Loss: 3.222653\n",
            "Step 57850: Minibatch Loss: 3.044982\n",
            "Step 57852: Minibatch Loss: 3.146683\n",
            "Step 57854: Minibatch Loss: 3.013840\n",
            "Step 57856: Minibatch Loss: 2.997212\n",
            "Step 57858: Minibatch Loss: 3.076913\n",
            "Step 57860: Minibatch Loss: 3.182057\n",
            "Step 57862: Minibatch Loss: 2.930034\n",
            "Step 57864: Minibatch Loss: 2.896325\n",
            "Step 57866: Minibatch Loss: 3.000593\n",
            "Step 57868: Minibatch Loss: 2.916014\n",
            "Step 57870: Minibatch Loss: 3.139384\n",
            "Step 57872: Minibatch Loss: 3.176587\n",
            "Step 57874: Minibatch Loss: 2.905171\n",
            "Step 57876: Minibatch Loss: 2.812010\n",
            "Step 57878: Minibatch Loss: 3.078854\n",
            "Step 57880: Minibatch Loss: 3.490949\n",
            "Step 57882: Minibatch Loss: 2.599647\n",
            "Step 57884: Minibatch Loss: 3.162979\n",
            "Step 57886: Minibatch Loss: 2.810010\n",
            "Step 57888: Minibatch Loss: 3.039316\n",
            "Step 57890: Minibatch Loss: 3.205940\n",
            "Step 57892: Minibatch Loss: 3.169122\n",
            "Step 57894: Minibatch Loss: 2.839852\n",
            "Step 57896: Minibatch Loss: 2.922074\n",
            "Step 57898: Minibatch Loss: 3.170474\n",
            "Step 57900: Minibatch Loss: 2.931579\n",
            "Step 57902: Minibatch Loss: 2.977553\n",
            "Step 57904: Minibatch Loss: 2.898028\n",
            "Step 57906: Minibatch Loss: 2.962039\n",
            "Step 57908: Minibatch Loss: 2.993538\n",
            "Step 57910: Minibatch Loss: 3.205686\n",
            "Step 57912: Minibatch Loss: 2.824606\n",
            "Step 57914: Minibatch Loss: 3.039725\n",
            "Step 57916: Minibatch Loss: 2.996795\n",
            "Step 57918: Minibatch Loss: 2.908280\n",
            "Step 57920: Minibatch Loss: 3.023779\n",
            "Step 57922: Minibatch Loss: 3.109048\n",
            "Step 57924: Minibatch Loss: 2.931927\n",
            "Step 57926: Minibatch Loss: 3.245386\n",
            "Step 57928: Minibatch Loss: 3.275160\n",
            "Step 57930: Minibatch Loss: 2.931297\n",
            "Step 57932: Minibatch Loss: 2.991624\n",
            "Step 57934: Minibatch Loss: 3.088085\n",
            "Step 57936: Minibatch Loss: 3.339975\n",
            "Step 57938: Minibatch Loss: 3.264946\n",
            "Step 57940: Minibatch Loss: 3.121012\n",
            "Step 57942: Minibatch Loss: 2.874812\n",
            "Step 57944: Minibatch Loss: 2.839262\n",
            "Step 57946: Minibatch Loss: 3.234772\n",
            "Step 57948: Minibatch Loss: 3.023122\n",
            "Step 57950: Minibatch Loss: 2.992350\n",
            "Step 57952: Minibatch Loss: 3.225390\n",
            "Step 57954: Minibatch Loss: 2.906514\n",
            "Step 57956: Minibatch Loss: 2.810795\n",
            "Step 57958: Minibatch Loss: 3.116584\n",
            "Step 57960: Minibatch Loss: 2.955962\n",
            "Step 57962: Minibatch Loss: 2.963780\n",
            "Step 57964: Minibatch Loss: 2.921978\n",
            "Step 57966: Minibatch Loss: 3.104743\n",
            "Step 57968: Minibatch Loss: 2.495959\n",
            "Step 57970: Minibatch Loss: 3.280908\n",
            "Step 57972: Minibatch Loss: 2.955154\n",
            "Step 57974: Minibatch Loss: 2.779931\n",
            "Step 57976: Minibatch Loss: 3.073617\n",
            "Step 57978: Minibatch Loss: 3.086271\n",
            "Step 57980: Minibatch Loss: 2.697871\n",
            "Step 57982: Minibatch Loss: 2.893466\n",
            "Step 57984: Minibatch Loss: 3.355798\n",
            "Step 57986: Minibatch Loss: 3.476142\n",
            "Step 57988: Minibatch Loss: 3.038236\n",
            "Step 57990: Minibatch Loss: 2.992415\n",
            "Step 57992: Minibatch Loss: 3.022881\n",
            "Step 57994: Minibatch Loss: 3.020034\n",
            "Step 57996: Minibatch Loss: 2.918974\n",
            "Step 57998: Minibatch Loss: 2.715690\n",
            "Step 58000: Minibatch Loss: 3.012135\n",
            "Step 58002: Minibatch Loss: 3.102799\n",
            "Step 58004: Minibatch Loss: 2.716877\n",
            "Step 58006: Minibatch Loss: 3.251386\n",
            "Step 58008: Minibatch Loss: 3.162254\n",
            "Step 58010: Minibatch Loss: 2.803395\n",
            "Step 58012: Minibatch Loss: 3.041304\n",
            "Step 58014: Minibatch Loss: 3.006729\n",
            "Step 58016: Minibatch Loss: 2.774295\n",
            "Step 58018: Minibatch Loss: 3.045353\n",
            "Step 58020: Minibatch Loss: 2.996552\n",
            "Step 58022: Minibatch Loss: 3.145241\n",
            "Step 58024: Minibatch Loss: 2.991409\n",
            "Step 58026: Minibatch Loss: 3.171224\n",
            "Step 58028: Minibatch Loss: 3.019769\n",
            "Step 58030: Minibatch Loss: 2.845551\n",
            "Step 58032: Minibatch Loss: 2.863451\n",
            "Step 58034: Minibatch Loss: 3.124399\n",
            "Step 58036: Minibatch Loss: 2.773915\n",
            "Step 58038: Minibatch Loss: 3.084049\n",
            "Step 58040: Minibatch Loss: 2.921781\n",
            "Step 58042: Minibatch Loss: 2.835243\n",
            "Step 58044: Minibatch Loss: 3.069373\n",
            "Step 58046: Minibatch Loss: 3.096445\n",
            "Step 58048: Minibatch Loss: 3.105382\n",
            "Step 58050: Minibatch Loss: 3.045303\n",
            "Step 58052: Minibatch Loss: 3.211539\n",
            "Step 58054: Minibatch Loss: 2.733229\n",
            "Step 58056: Minibatch Loss: 2.618906\n",
            "Step 58058: Minibatch Loss: 2.998978\n",
            "Step 58060: Minibatch Loss: 3.064971\n",
            "Step 58062: Minibatch Loss: 2.773416\n",
            "Step 58064: Minibatch Loss: 2.758137\n",
            "Step 58066: Minibatch Loss: 2.927144\n",
            "Step 58068: Minibatch Loss: 3.149304\n",
            "Step 58070: Minibatch Loss: 3.005332\n",
            "Step 58072: Minibatch Loss: 3.172557\n",
            "Step 58074: Minibatch Loss: 3.064991\n",
            "Step 58076: Minibatch Loss: 2.856421\n",
            "Step 58078: Minibatch Loss: 2.855442\n",
            "Step 58080: Minibatch Loss: 3.011338\n",
            "Step 58082: Minibatch Loss: 3.008105\n",
            "Step 58084: Minibatch Loss: 2.901025\n",
            "Step 58086: Minibatch Loss: 2.907127\n",
            "Step 58088: Minibatch Loss: 2.707697\n",
            "Step 58090: Minibatch Loss: 3.087801\n",
            "Step 58092: Minibatch Loss: 2.888004\n",
            "Step 58094: Minibatch Loss: 3.018061\n",
            "Step 58096: Minibatch Loss: 3.214681\n",
            "Step 58098: Minibatch Loss: 2.926264\n",
            "Step 58100: Minibatch Loss: 3.201447\n",
            "Step 58102: Minibatch Loss: 2.951351\n",
            "Step 58104: Minibatch Loss: 2.606549\n",
            "Step 58106: Minibatch Loss: 3.035842\n",
            "Step 58108: Minibatch Loss: 2.776658\n",
            "Step 58110: Minibatch Loss: 2.761124\n",
            "Step 58112: Minibatch Loss: 2.997456\n",
            "Step 58114: Minibatch Loss: 3.062600\n",
            "Step 58116: Minibatch Loss: 3.205929\n",
            "Step 58118: Minibatch Loss: 3.110932\n",
            "Step 58120: Minibatch Loss: 2.804972\n",
            "Step 58122: Minibatch Loss: 2.848876\n",
            "Step 58124: Minibatch Loss: 2.751724\n",
            "Step 58126: Minibatch Loss: 3.021943\n",
            "Step 58128: Minibatch Loss: 2.812549\n",
            "Step 58130: Minibatch Loss: 2.747432\n",
            "Step 58132: Minibatch Loss: 3.159322\n",
            "Step 58134: Minibatch Loss: 3.071397\n",
            "Step 58136: Minibatch Loss: 2.717829\n",
            "Step 58138: Minibatch Loss: 3.079264\n",
            "Step 58140: Minibatch Loss: 3.185097\n",
            "Step 58142: Minibatch Loss: 3.041749\n",
            "Step 58144: Minibatch Loss: 2.775732\n",
            "Step 58146: Minibatch Loss: 3.060792\n",
            "Step 58148: Minibatch Loss: 3.271394\n",
            "Step 58150: Minibatch Loss: 3.046775\n",
            "Step 58152: Minibatch Loss: 2.989672\n",
            "Step 58154: Minibatch Loss: 2.993979\n",
            "Step 58156: Minibatch Loss: 2.926388\n",
            "Step 58158: Minibatch Loss: 3.144386\n",
            "Step 58160: Minibatch Loss: 3.039901\n",
            "Step 58162: Minibatch Loss: 3.138237\n",
            "Step 58164: Minibatch Loss: 2.958126\n",
            "Step 58166: Minibatch Loss: 3.105847\n",
            "Step 58168: Minibatch Loss: 2.594748\n",
            "Step 58170: Minibatch Loss: 2.774505\n",
            "Step 58172: Minibatch Loss: 3.035829\n",
            "Step 58174: Minibatch Loss: 2.634005\n",
            "Step 58176: Minibatch Loss: 3.243221\n",
            "Step 58178: Minibatch Loss: 2.811142\n",
            "Step 58180: Minibatch Loss: 3.026269\n",
            "Step 58182: Minibatch Loss: 3.115134\n",
            "Step 58184: Minibatch Loss: 3.066430\n",
            "Step 58186: Minibatch Loss: 2.815063\n",
            "Step 58188: Minibatch Loss: 2.937139\n",
            "Step 58190: Minibatch Loss: 2.755607\n",
            "Step 58192: Minibatch Loss: 3.100831\n",
            "Step 58194: Minibatch Loss: 2.857371\n",
            "Step 58196: Minibatch Loss: 2.805786\n",
            "Step 58198: Minibatch Loss: 2.992833\n",
            "Step 58200: Minibatch Loss: 2.936841\n",
            "Step 58202: Minibatch Loss: 2.886090\n",
            "Step 58204: Minibatch Loss: 3.092544\n",
            "Step 58206: Minibatch Loss: 3.053302\n",
            "Step 58208: Minibatch Loss: 3.268921\n",
            "Step 58210: Minibatch Loss: 2.891480\n",
            "Step 58212: Minibatch Loss: 2.818297\n",
            "Step 58214: Minibatch Loss: 2.916424\n",
            "Step 58216: Minibatch Loss: 2.714714\n",
            "Step 58218: Minibatch Loss: 3.105769\n",
            "Step 58220: Minibatch Loss: 2.923449\n",
            "Step 58222: Minibatch Loss: 3.283925\n",
            "Step 58224: Minibatch Loss: 2.964594\n",
            "Step 58226: Minibatch Loss: 3.212716\n",
            "Step 58228: Minibatch Loss: 3.268666\n",
            "Step 58230: Minibatch Loss: 3.276829\n",
            "Step 58232: Minibatch Loss: 2.620154\n",
            "Step 58234: Minibatch Loss: 2.916248\n",
            "Step 58236: Minibatch Loss: 2.847262\n",
            "Step 58238: Minibatch Loss: 2.911046\n",
            "Step 58240: Minibatch Loss: 2.954209\n",
            "Step 58242: Minibatch Loss: 2.928888\n",
            "Step 58244: Minibatch Loss: 2.870790\n",
            "Step 58246: Minibatch Loss: 2.865369\n",
            "Step 58248: Minibatch Loss: 2.889184\n",
            "Step 58250: Minibatch Loss: 2.749317\n",
            "Step 58252: Minibatch Loss: 3.009262\n",
            "Step 58254: Minibatch Loss: 2.853061\n",
            "Step 58256: Minibatch Loss: 3.247679\n",
            "Step 58258: Minibatch Loss: 2.979781\n",
            "Step 58260: Minibatch Loss: 3.102400\n",
            "Step 58262: Minibatch Loss: 2.944907\n",
            "Step 58264: Minibatch Loss: 2.938758\n",
            "Step 58266: Minibatch Loss: 2.881581\n",
            "Step 58268: Minibatch Loss: 2.981900\n",
            "Step 58270: Minibatch Loss: 3.380631\n",
            "Step 58272: Minibatch Loss: 3.237400\n",
            "Step 58274: Minibatch Loss: 2.856905\n",
            "Step 58276: Minibatch Loss: 3.037277\n",
            "Step 58278: Minibatch Loss: 2.791847\n",
            "Step 58280: Minibatch Loss: 3.041553\n",
            "Step 58282: Minibatch Loss: 2.795157\n",
            "Step 58284: Minibatch Loss: 3.152143\n",
            "Step 58286: Minibatch Loss: 3.377246\n",
            "Step 58288: Minibatch Loss: 2.931434\n",
            "Step 58290: Minibatch Loss: 3.025198\n",
            "Step 58292: Minibatch Loss: 3.130637\n",
            "Step 58294: Minibatch Loss: 2.841920\n",
            "Step 58296: Minibatch Loss: 2.739730\n",
            "Step 58298: Minibatch Loss: 2.806649\n",
            "Step 58300: Minibatch Loss: 2.978588\n",
            "Step 58302: Minibatch Loss: 3.039044\n",
            "Step 58304: Minibatch Loss: 2.732752\n",
            "Step 58306: Minibatch Loss: 3.113612\n",
            "Step 58308: Minibatch Loss: 3.110968\n",
            "Step 58310: Minibatch Loss: 3.117018\n",
            "Step 58312: Minibatch Loss: 3.212669\n",
            "Step 58314: Minibatch Loss: 2.913505\n",
            "Step 58316: Minibatch Loss: 3.068677\n",
            "Step 58318: Minibatch Loss: 2.971764\n",
            "Step 58320: Minibatch Loss: 3.043580\n",
            "Step 58322: Minibatch Loss: 2.994179\n",
            "Step 58324: Minibatch Loss: 2.915608\n",
            "Step 58326: Minibatch Loss: 3.131819\n",
            "Step 58328: Minibatch Loss: 2.705449\n",
            "Step 58330: Minibatch Loss: 3.058335\n",
            "Step 58332: Minibatch Loss: 2.955140\n",
            "Step 58334: Minibatch Loss: 2.921324\n",
            "Step 58336: Minibatch Loss: 2.775800\n",
            "Step 58338: Minibatch Loss: 2.679492\n",
            "Step 58340: Minibatch Loss: 3.237065\n",
            "Step 58342: Minibatch Loss: 3.194503\n",
            "Step 58344: Minibatch Loss: 3.075126\n",
            "Step 58346: Minibatch Loss: 3.044223\n",
            "Step 58348: Minibatch Loss: 2.834568\n",
            "Step 58350: Minibatch Loss: 2.716083\n",
            "Step 58352: Minibatch Loss: 2.986812\n",
            "Step 58354: Minibatch Loss: 2.706990\n",
            "Step 58356: Minibatch Loss: 2.830894\n",
            "Step 58358: Minibatch Loss: 3.009298\n",
            "Step 58360: Minibatch Loss: 2.871527\n",
            "Step 58362: Minibatch Loss: 3.148609\n",
            "Step 58364: Minibatch Loss: 2.973387\n",
            "Step 58366: Minibatch Loss: 2.901244\n",
            "Step 58368: Minibatch Loss: 3.097188\n",
            "Step 58370: Minibatch Loss: 3.481755\n",
            "Step 58372: Minibatch Loss: 3.090940\n",
            "Step 58374: Minibatch Loss: 3.313804\n",
            "Step 58376: Minibatch Loss: 2.868594\n",
            "Step 58378: Minibatch Loss: 3.268572\n",
            "Step 58380: Minibatch Loss: 3.062614\n",
            "Step 58382: Minibatch Loss: 2.591941\n",
            "Step 58384: Minibatch Loss: 3.412202\n",
            "Step 58386: Minibatch Loss: 3.068502\n",
            "Step 58388: Minibatch Loss: 3.306577\n",
            "Step 58390: Minibatch Loss: 3.133774\n",
            "Step 58392: Minibatch Loss: 2.914845\n",
            "Step 58394: Minibatch Loss: 2.892278\n",
            "Step 58396: Minibatch Loss: 3.184611\n",
            "Step 58398: Minibatch Loss: 3.060756\n",
            "Step 58400: Minibatch Loss: 2.927517\n",
            "Step 58402: Minibatch Loss: 2.867595\n",
            "Step 58404: Minibatch Loss: 2.542614\n",
            "Step 58406: Minibatch Loss: 2.974530\n",
            "Step 58408: Minibatch Loss: 2.852268\n",
            "Step 58410: Minibatch Loss: 3.029001\n",
            "Step 58412: Minibatch Loss: 3.151917\n",
            "Step 58414: Minibatch Loss: 2.774663\n",
            "Step 58416: Minibatch Loss: 3.085968\n",
            "Step 58418: Minibatch Loss: 3.097470\n",
            "Step 58420: Minibatch Loss: 3.099666\n",
            "Step 58422: Minibatch Loss: 3.133736\n",
            "Step 58424: Minibatch Loss: 3.002033\n",
            "Step 58426: Minibatch Loss: 3.033338\n",
            "Step 58428: Minibatch Loss: 2.970713\n",
            "Step 58430: Minibatch Loss: 3.143852\n",
            "Step 58432: Minibatch Loss: 3.101947\n",
            "Step 58434: Minibatch Loss: 3.092246\n",
            "Step 58436: Minibatch Loss: 2.911991\n",
            "Step 58438: Minibatch Loss: 2.949722\n",
            "Step 58440: Minibatch Loss: 2.937766\n",
            "Step 58442: Minibatch Loss: 3.043388\n",
            "Step 58444: Minibatch Loss: 3.053485\n",
            "Step 58446: Minibatch Loss: 2.951706\n",
            "Step 58448: Minibatch Loss: 3.169765\n",
            "Step 58450: Minibatch Loss: 2.996189\n",
            "Step 58452: Minibatch Loss: 3.372859\n",
            "Step 58454: Minibatch Loss: 3.138600\n",
            "Step 58456: Minibatch Loss: 2.913228\n",
            "Step 58458: Minibatch Loss: 2.987843\n",
            "Step 58460: Minibatch Loss: 3.110382\n",
            "Step 58462: Minibatch Loss: 2.913603\n",
            "Step 58464: Minibatch Loss: 2.883596\n",
            "Step 58466: Minibatch Loss: 2.978868\n",
            "Step 58468: Minibatch Loss: 3.266351\n",
            "Step 58470: Minibatch Loss: 3.157715\n",
            "Step 58472: Minibatch Loss: 3.063478\n",
            "Step 58474: Minibatch Loss: 2.929950\n",
            "Step 58476: Minibatch Loss: 3.127322\n",
            "Step 58478: Minibatch Loss: 3.471210\n",
            "Step 58480: Minibatch Loss: 2.871623\n",
            "Step 58482: Minibatch Loss: 3.279406\n",
            "Step 58484: Minibatch Loss: 3.113790\n",
            "Step 58486: Minibatch Loss: 3.152843\n",
            "Step 58488: Minibatch Loss: 3.150050\n",
            "Step 58490: Minibatch Loss: 3.164353\n",
            "Step 58492: Minibatch Loss: 3.248592\n",
            "Step 58494: Minibatch Loss: 3.281575\n",
            "Step 58496: Minibatch Loss: 2.941440\n",
            "Step 58498: Minibatch Loss: 2.912923\n",
            "Step 58500: Minibatch Loss: 2.606128\n",
            "Step 58502: Minibatch Loss: 3.142813\n",
            "Step 58504: Minibatch Loss: 3.106683\n",
            "Step 58506: Minibatch Loss: 3.309322\n",
            "Step 58508: Minibatch Loss: 2.913048\n",
            "Step 58510: Minibatch Loss: 3.047983\n",
            "Step 58512: Minibatch Loss: 3.100501\n",
            "Step 58514: Minibatch Loss: 3.212168\n",
            "Step 58516: Minibatch Loss: 3.090954\n",
            "Step 58518: Minibatch Loss: 2.996130\n",
            "Step 58520: Minibatch Loss: 2.977899\n",
            "Step 58522: Minibatch Loss: 2.885380\n",
            "Step 58524: Minibatch Loss: 3.067736\n",
            "Step 58526: Minibatch Loss: 3.235155\n",
            "Step 58528: Minibatch Loss: 2.990901\n",
            "Step 58530: Minibatch Loss: 2.878835\n",
            "Step 58532: Minibatch Loss: 3.048943\n",
            "Step 58534: Minibatch Loss: 3.014773\n",
            "Step 58536: Minibatch Loss: 3.152653\n",
            "Step 58538: Minibatch Loss: 3.160173\n",
            "Step 58540: Minibatch Loss: 2.989784\n",
            "Step 58542: Minibatch Loss: 2.847478\n",
            "Step 58544: Minibatch Loss: 3.017123\n",
            "Step 58546: Minibatch Loss: 3.475572\n",
            "Step 58548: Minibatch Loss: 2.651852\n",
            "Step 58550: Minibatch Loss: 3.234084\n",
            "Step 58552: Minibatch Loss: 2.845295\n",
            "Step 58554: Minibatch Loss: 3.130769\n",
            "Step 58556: Minibatch Loss: 3.328490\n",
            "Step 58558: Minibatch Loss: 3.156704\n",
            "Step 58560: Minibatch Loss: 2.833123\n",
            "Step 58562: Minibatch Loss: 2.919461\n",
            "Step 58564: Minibatch Loss: 3.218380\n",
            "Step 58566: Minibatch Loss: 2.868440\n",
            "Step 58568: Minibatch Loss: 2.988273\n",
            "Step 58570: Minibatch Loss: 3.041634\n",
            "Step 58572: Minibatch Loss: 2.868805\n",
            "Step 58574: Minibatch Loss: 2.976430\n",
            "Step 58576: Minibatch Loss: 3.197963\n",
            "Step 58578: Minibatch Loss: 2.711867\n",
            "Step 58580: Minibatch Loss: 3.113422\n",
            "Step 58582: Minibatch Loss: 3.143702\n",
            "Step 58584: Minibatch Loss: 2.980346\n",
            "Step 58586: Minibatch Loss: 2.849419\n",
            "Step 58588: Minibatch Loss: 2.949194\n",
            "Step 58590: Minibatch Loss: 2.949751\n",
            "Step 58592: Minibatch Loss: 3.169506\n",
            "Step 58594: Minibatch Loss: 3.025866\n",
            "Step 58596: Minibatch Loss: 2.908654\n",
            "Step 58598: Minibatch Loss: 3.049175\n",
            "Step 58600: Minibatch Loss: 3.298340\n",
            "Step 58602: Minibatch Loss: 3.277339\n",
            "Step 58604: Minibatch Loss: 3.167987\n",
            "Step 58606: Minibatch Loss: 3.079346\n",
            "Step 58608: Minibatch Loss: 2.841858\n",
            "Step 58610: Minibatch Loss: 2.848669\n",
            "Step 58612: Minibatch Loss: 3.071340\n",
            "Step 58614: Minibatch Loss: 3.013310\n",
            "Step 58616: Minibatch Loss: 2.987350\n",
            "Step 58618: Minibatch Loss: 3.119170\n",
            "Step 58620: Minibatch Loss: 2.801903\n",
            "Step 58622: Minibatch Loss: 2.801585\n",
            "Step 58624: Minibatch Loss: 2.962253\n",
            "Step 58626: Minibatch Loss: 3.034181\n",
            "Step 58628: Minibatch Loss: 2.968169\n",
            "Step 58630: Minibatch Loss: 2.867362\n",
            "Step 58632: Minibatch Loss: 3.111671\n",
            "Step 58634: Minibatch Loss: 2.565217\n",
            "Step 58636: Minibatch Loss: 3.129879\n",
            "Step 58638: Minibatch Loss: 2.929649\n",
            "Step 58640: Minibatch Loss: 2.770440\n",
            "Step 58642: Minibatch Loss: 3.092966\n",
            "Step 58644: Minibatch Loss: 3.060349\n",
            "Step 58646: Minibatch Loss: 2.820074\n",
            "Step 58648: Minibatch Loss: 2.936831\n",
            "Step 58650: Minibatch Loss: 3.380045\n",
            "Step 58652: Minibatch Loss: 3.504086\n",
            "Step 58654: Minibatch Loss: 2.921486\n",
            "Step 58656: Minibatch Loss: 2.974550\n",
            "Step 58658: Minibatch Loss: 2.960859\n",
            "Step 58660: Minibatch Loss: 3.045517\n",
            "Step 58662: Minibatch Loss: 2.879104\n",
            "Step 58664: Minibatch Loss: 2.740955\n",
            "Step 58666: Minibatch Loss: 2.973474\n",
            "Step 58668: Minibatch Loss: 3.145862\n",
            "Step 58670: Minibatch Loss: 2.612919\n",
            "Step 58672: Minibatch Loss: 3.293789\n",
            "Step 58674: Minibatch Loss: 3.275500\n",
            "Step 58676: Minibatch Loss: 2.754168\n",
            "Step 58678: Minibatch Loss: 3.003770\n",
            "Step 58680: Minibatch Loss: 2.972003\n",
            "Step 58682: Minibatch Loss: 2.850315\n",
            "Step 58684: Minibatch Loss: 2.964837\n",
            "Step 58686: Minibatch Loss: 2.895772\n",
            "Step 58688: Minibatch Loss: 3.163681\n",
            "Step 58690: Minibatch Loss: 2.893104\n",
            "Step 58692: Minibatch Loss: 3.202455\n",
            "Step 58694: Minibatch Loss: 2.964664\n",
            "Step 58696: Minibatch Loss: 2.981871\n",
            "Step 58698: Minibatch Loss: 2.803226\n",
            "Step 58700: Minibatch Loss: 3.097675\n",
            "Step 58702: Minibatch Loss: 2.635269\n",
            "Step 58704: Minibatch Loss: 3.097094\n",
            "Step 58706: Minibatch Loss: 2.919151\n",
            "Step 58708: Minibatch Loss: 2.933799\n",
            "Step 58710: Minibatch Loss: 2.976847\n",
            "Step 58712: Minibatch Loss: 3.019771\n",
            "Step 58714: Minibatch Loss: 3.055811\n",
            "Step 58716: Minibatch Loss: 3.141176\n",
            "Step 58718: Minibatch Loss: 3.128104\n",
            "Step 58720: Minibatch Loss: 2.707302\n",
            "Step 58722: Minibatch Loss: 2.563214\n",
            "Step 58724: Minibatch Loss: 3.060334\n",
            "Step 58726: Minibatch Loss: 2.948375\n",
            "Step 58728: Minibatch Loss: 2.783358\n",
            "Step 58730: Minibatch Loss: 2.730722\n",
            "Step 58732: Minibatch Loss: 2.997371\n",
            "Step 58734: Minibatch Loss: 3.068580\n",
            "Step 58736: Minibatch Loss: 2.925961\n",
            "Step 58738: Minibatch Loss: 3.120662\n",
            "Step 58740: Minibatch Loss: 2.971347\n",
            "Step 58742: Minibatch Loss: 3.092018\n",
            "Step 58744: Minibatch Loss: 2.868623\n",
            "Step 58746: Minibatch Loss: 2.926413\n",
            "Step 58748: Minibatch Loss: 3.037376\n",
            "Step 58750: Minibatch Loss: 2.820047\n",
            "Step 58752: Minibatch Loss: 2.947372\n",
            "Step 58754: Minibatch Loss: 2.776225\n",
            "Step 58756: Minibatch Loss: 3.011865\n",
            "Step 58758: Minibatch Loss: 2.737775\n",
            "Step 58760: Minibatch Loss: 2.934785\n",
            "Step 58762: Minibatch Loss: 3.194931\n",
            "Step 58764: Minibatch Loss: 2.972167\n",
            "Step 58766: Minibatch Loss: 3.197825\n",
            "Step 58768: Minibatch Loss: 2.883085\n",
            "Step 58770: Minibatch Loss: 2.601705\n",
            "Step 58772: Minibatch Loss: 3.044202\n",
            "Step 58774: Minibatch Loss: 2.660684\n",
            "Step 58776: Minibatch Loss: 2.721824\n",
            "Step 58778: Minibatch Loss: 3.101404\n",
            "Step 58780: Minibatch Loss: 3.046076\n",
            "Step 58782: Minibatch Loss: 3.153065\n",
            "Step 58784: Minibatch Loss: 3.118332\n",
            "Step 58786: Minibatch Loss: 2.802953\n",
            "Step 58788: Minibatch Loss: 2.872096\n",
            "Step 58790: Minibatch Loss: 2.629767\n",
            "Step 58792: Minibatch Loss: 2.913862\n",
            "Step 58794: Minibatch Loss: 2.811738\n",
            "Step 58796: Minibatch Loss: 2.768974\n",
            "Step 58798: Minibatch Loss: 3.162908\n",
            "Step 58800: Minibatch Loss: 3.037833\n",
            "Step 58802: Minibatch Loss: 2.685484\n",
            "Step 58804: Minibatch Loss: 3.040364\n",
            "Step 58806: Minibatch Loss: 3.031924\n",
            "Step 58808: Minibatch Loss: 2.945970\n",
            "Step 58810: Minibatch Loss: 2.914901\n",
            "Step 58812: Minibatch Loss: 3.060903\n",
            "Step 58814: Minibatch Loss: 3.273402\n",
            "Step 58816: Minibatch Loss: 2.988010\n",
            "Step 58818: Minibatch Loss: 3.008515\n",
            "Step 58820: Minibatch Loss: 3.162046\n",
            "Step 58822: Minibatch Loss: 3.054046\n",
            "Step 58824: Minibatch Loss: 3.145768\n",
            "Step 58826: Minibatch Loss: 3.082147\n",
            "Step 58828: Minibatch Loss: 3.209854\n",
            "Step 58830: Minibatch Loss: 2.950393\n",
            "Step 58832: Minibatch Loss: 2.975171\n",
            "Step 58834: Minibatch Loss: 2.662309\n",
            "Step 58836: Minibatch Loss: 2.749282\n",
            "Step 58838: Minibatch Loss: 3.119449\n",
            "Step 58840: Minibatch Loss: 2.807736\n",
            "Step 58842: Minibatch Loss: 3.298285\n",
            "Step 58844: Minibatch Loss: 2.951278\n",
            "Step 58846: Minibatch Loss: 3.069538\n",
            "Step 58848: Minibatch Loss: 3.226550\n",
            "Step 58850: Minibatch Loss: 3.106364\n",
            "Step 58852: Minibatch Loss: 2.702488\n",
            "Step 58854: Minibatch Loss: 2.867804\n",
            "Step 58856: Minibatch Loss: 2.793460\n",
            "Step 58858: Minibatch Loss: 3.207670\n",
            "Step 58860: Minibatch Loss: 2.805885\n",
            "Step 58862: Minibatch Loss: 2.803352\n",
            "Step 58864: Minibatch Loss: 3.017719\n",
            "Step 58866: Minibatch Loss: 2.903086\n",
            "Step 58868: Minibatch Loss: 2.842550\n",
            "Step 58870: Minibatch Loss: 3.055995\n",
            "Step 58872: Minibatch Loss: 3.001238\n",
            "Step 58874: Minibatch Loss: 3.047762\n",
            "Step 58876: Minibatch Loss: 2.968946\n",
            "Step 58878: Minibatch Loss: 2.869631\n",
            "Step 58880: Minibatch Loss: 3.035102\n",
            "Step 58882: Minibatch Loss: 2.764495\n",
            "Step 58884: Minibatch Loss: 2.898415\n",
            "Step 58886: Minibatch Loss: 2.963382\n",
            "Step 58888: Minibatch Loss: 3.292346\n",
            "Step 58890: Minibatch Loss: 2.906168\n",
            "Step 58892: Minibatch Loss: 3.123743\n",
            "Step 58894: Minibatch Loss: 3.271459\n",
            "Step 58896: Minibatch Loss: 3.181782\n",
            "Step 58898: Minibatch Loss: 2.522332\n",
            "Step 58900: Minibatch Loss: 3.054398\n",
            "Step 58902: Minibatch Loss: 2.839154\n",
            "Step 58904: Minibatch Loss: 2.966362\n",
            "Step 58906: Minibatch Loss: 3.166394\n",
            "Step 58908: Minibatch Loss: 2.766124\n",
            "Step 58910: Minibatch Loss: 2.817200\n",
            "Step 58912: Minibatch Loss: 2.797358\n",
            "Step 58914: Minibatch Loss: 2.903573\n",
            "Step 58916: Minibatch Loss: 2.681936\n",
            "Step 58918: Minibatch Loss: 3.062689\n",
            "Step 58920: Minibatch Loss: 2.801659\n",
            "Step 58922: Minibatch Loss: 3.177763\n",
            "Step 58924: Minibatch Loss: 3.141317\n",
            "Step 58926: Minibatch Loss: 3.057778\n",
            "Step 58928: Minibatch Loss: 2.935466\n",
            "Step 58930: Minibatch Loss: 2.947760\n",
            "Step 58932: Minibatch Loss: 2.897729\n",
            "Step 58934: Minibatch Loss: 3.062112\n",
            "Step 58936: Minibatch Loss: 3.359932\n",
            "Step 58938: Minibatch Loss: 3.156256\n",
            "Step 58940: Minibatch Loss: 2.925595\n",
            "Step 58942: Minibatch Loss: 3.108592\n",
            "Step 58944: Minibatch Loss: 2.746885\n",
            "Step 58946: Minibatch Loss: 3.047247\n",
            "Step 58948: Minibatch Loss: 2.768598\n",
            "Step 58950: Minibatch Loss: 3.060325\n",
            "Step 58952: Minibatch Loss: 3.431426\n",
            "Step 58954: Minibatch Loss: 2.968248\n",
            "Step 58956: Minibatch Loss: 3.149198\n",
            "Step 58958: Minibatch Loss: 3.102741\n",
            "Step 58960: Minibatch Loss: 2.902031\n",
            "Step 58962: Minibatch Loss: 2.736550\n",
            "Step 58964: Minibatch Loss: 2.935731\n",
            "Step 58966: Minibatch Loss: 2.988863\n",
            "Step 58968: Minibatch Loss: 3.151285\n",
            "Step 58970: Minibatch Loss: 2.657729\n",
            "Step 58972: Minibatch Loss: 3.074067\n",
            "Step 58974: Minibatch Loss: 3.163879\n",
            "Step 58976: Minibatch Loss: 3.116955\n",
            "Step 58978: Minibatch Loss: 3.030704\n",
            "Step 58980: Minibatch Loss: 2.934029\n",
            "Step 58982: Minibatch Loss: 2.991364\n",
            "Step 58984: Minibatch Loss: 2.883423\n",
            "Step 58986: Minibatch Loss: 3.043278\n",
            "Step 58988: Minibatch Loss: 2.994475\n",
            "Step 58990: Minibatch Loss: 2.945855\n",
            "Step 58992: Minibatch Loss: 3.119720\n",
            "Step 58994: Minibatch Loss: 2.742380\n",
            "Step 58996: Minibatch Loss: 3.101193\n",
            "Step 58998: Minibatch Loss: 2.906530\n",
            "Step 59000: Minibatch Loss: 2.831837\n",
            "Step 59002: Minibatch Loss: 2.822028\n",
            "Step 59004: Minibatch Loss: 2.694664\n",
            "Step 59006: Minibatch Loss: 3.312802\n",
            "Step 59008: Minibatch Loss: 3.256467\n",
            "Step 59010: Minibatch Loss: 2.989557\n",
            "Step 59012: Minibatch Loss: 3.001304\n",
            "Step 59014: Minibatch Loss: 2.807864\n",
            "Step 59016: Minibatch Loss: 2.970463\n",
            "Step 59018: Minibatch Loss: 2.865172\n",
            "Step 59020: Minibatch Loss: 2.649408\n",
            "Step 59022: Minibatch Loss: 2.728436\n",
            "Step 59024: Minibatch Loss: 3.050922\n",
            "Step 59026: Minibatch Loss: 2.847092\n",
            "Step 59028: Minibatch Loss: 3.119419\n",
            "Step 59030: Minibatch Loss: 2.880462\n",
            "Step 59032: Minibatch Loss: 2.734149\n",
            "Step 59034: Minibatch Loss: 3.090366\n",
            "Step 59036: Minibatch Loss: 3.396260\n",
            "Step 59038: Minibatch Loss: 2.954717\n",
            "Step 59040: Minibatch Loss: 3.415251\n",
            "Step 59042: Minibatch Loss: 2.764027\n",
            "Step 59044: Minibatch Loss: 3.305702\n",
            "Step 59046: Minibatch Loss: 3.012765\n",
            "Step 59048: Minibatch Loss: 2.653066\n",
            "Step 59050: Minibatch Loss: 3.307530\n",
            "Step 59052: Minibatch Loss: 2.970233\n",
            "Step 59054: Minibatch Loss: 3.276896\n",
            "Step 59056: Minibatch Loss: 3.048177\n",
            "Step 59058: Minibatch Loss: 2.815063\n",
            "Step 59060: Minibatch Loss: 2.960659\n",
            "Step 59062: Minibatch Loss: 3.289165\n",
            "Step 59064: Minibatch Loss: 3.153469\n",
            "Step 59066: Minibatch Loss: 2.949686\n",
            "Step 59068: Minibatch Loss: 2.831768\n",
            "Step 59070: Minibatch Loss: 2.539150\n",
            "Step 59072: Minibatch Loss: 2.978229\n",
            "Step 59074: Minibatch Loss: 2.822527\n",
            "Step 59076: Minibatch Loss: 3.073646\n",
            "Step 59078: Minibatch Loss: 3.057284\n",
            "Step 59080: Minibatch Loss: 2.744677\n",
            "Step 59082: Minibatch Loss: 3.118683\n",
            "Step 59084: Minibatch Loss: 3.076342\n",
            "Step 59086: Minibatch Loss: 3.070587\n",
            "Step 59088: Minibatch Loss: 3.141609\n",
            "Step 59090: Minibatch Loss: 3.000851\n",
            "Step 59092: Minibatch Loss: 3.099197\n",
            "Step 59094: Minibatch Loss: 2.963183\n",
            "Step 59096: Minibatch Loss: 3.158370\n",
            "Step 59098: Minibatch Loss: 3.034667\n",
            "Step 59100: Minibatch Loss: 3.031103\n",
            "Step 59102: Minibatch Loss: 2.784303\n",
            "Step 59104: Minibatch Loss: 2.879617\n",
            "Step 59106: Minibatch Loss: 3.017235\n",
            "Step 59108: Minibatch Loss: 3.063126\n",
            "Step 59110: Minibatch Loss: 3.156699\n",
            "Step 59112: Minibatch Loss: 3.079494\n",
            "Step 59114: Minibatch Loss: 3.221824\n",
            "Step 59116: Minibatch Loss: 2.955403\n",
            "Step 59118: Minibatch Loss: 3.297471\n",
            "Step 59120: Minibatch Loss: 3.140362\n",
            "Step 59122: Minibatch Loss: 2.841250\n",
            "Step 59124: Minibatch Loss: 2.835393\n",
            "Step 59126: Minibatch Loss: 3.199430\n",
            "Step 59128: Minibatch Loss: 2.981426\n",
            "Step 59130: Minibatch Loss: 2.843916\n",
            "Step 59132: Minibatch Loss: 2.982024\n",
            "Step 59134: Minibatch Loss: 3.348080\n",
            "Step 59136: Minibatch Loss: 3.206346\n",
            "Step 59138: Minibatch Loss: 3.221621\n",
            "Step 59140: Minibatch Loss: 2.975161\n",
            "Step 59142: Minibatch Loss: 3.062444\n",
            "Step 59144: Minibatch Loss: 3.381364\n",
            "Step 59146: Minibatch Loss: 2.736958\n",
            "Step 59148: Minibatch Loss: 3.212935\n",
            "Step 59150: Minibatch Loss: 3.037998\n",
            "Step 59152: Minibatch Loss: 3.207401\n",
            "Step 59154: Minibatch Loss: 3.258142\n",
            "Step 59156: Minibatch Loss: 3.191209\n",
            "Step 59158: Minibatch Loss: 3.360620\n",
            "Step 59160: Minibatch Loss: 3.269980\n",
            "Step 59162: Minibatch Loss: 2.896038\n",
            "Step 59164: Minibatch Loss: 3.008175\n",
            "Step 59166: Minibatch Loss: 2.664670\n",
            "Step 59168: Minibatch Loss: 3.173837\n",
            "Step 59170: Minibatch Loss: 3.055577\n",
            "Step 59172: Minibatch Loss: 3.366569\n",
            "Step 59174: Minibatch Loss: 2.945666\n",
            "Step 59176: Minibatch Loss: 3.025513\n",
            "Step 59178: Minibatch Loss: 3.037253\n",
            "Step 59180: Minibatch Loss: 3.267706\n",
            "Step 59182: Minibatch Loss: 2.955147\n",
            "Step 59184: Minibatch Loss: 3.045772\n",
            "Step 59186: Minibatch Loss: 2.946423\n",
            "Step 59188: Minibatch Loss: 2.928605\n",
            "Step 59190: Minibatch Loss: 2.900323\n",
            "Step 59192: Minibatch Loss: 3.182177\n",
            "Step 59194: Minibatch Loss: 3.068919\n",
            "Step 59196: Minibatch Loss: 2.935491\n",
            "Step 59198: Minibatch Loss: 3.077783\n",
            "Step 59200: Minibatch Loss: 2.855357\n",
            "Step 59202: Minibatch Loss: 3.037531\n",
            "Step 59204: Minibatch Loss: 3.142792\n",
            "Step 59206: Minibatch Loss: 2.910411\n",
            "Step 59208: Minibatch Loss: 2.894173\n",
            "Step 59210: Minibatch Loss: 2.999939\n",
            "Step 59212: Minibatch Loss: 3.575192\n",
            "Step 59214: Minibatch Loss: 2.651730\n",
            "Step 59216: Minibatch Loss: 3.067847\n",
            "Step 59218: Minibatch Loss: 2.856927\n",
            "Step 59220: Minibatch Loss: 3.079136\n",
            "Step 59222: Minibatch Loss: 3.268132\n",
            "Step 59224: Minibatch Loss: 3.017694\n",
            "Step 59226: Minibatch Loss: 2.960592\n",
            "Step 59228: Minibatch Loss: 2.958055\n",
            "Step 59230: Minibatch Loss: 3.093305\n",
            "Step 59232: Minibatch Loss: 2.910665\n",
            "Step 59234: Minibatch Loss: 2.924746\n",
            "Step 59236: Minibatch Loss: 3.094986\n",
            "Step 59238: Minibatch Loss: 2.796911\n",
            "Step 59240: Minibatch Loss: 3.002135\n",
            "Step 59242: Minibatch Loss: 3.197108\n",
            "Step 59244: Minibatch Loss: 2.787850\n",
            "Step 59246: Minibatch Loss: 2.901707\n",
            "Step 59248: Minibatch Loss: 3.009873\n",
            "Step 59250: Minibatch Loss: 2.983533\n",
            "Step 59252: Minibatch Loss: 2.891459\n",
            "Step 59254: Minibatch Loss: 2.955555\n",
            "Step 59256: Minibatch Loss: 2.944138\n",
            "Step 59258: Minibatch Loss: 3.230266\n",
            "Step 59260: Minibatch Loss: 3.126767\n",
            "Step 59262: Minibatch Loss: 2.835530\n",
            "Step 59264: Minibatch Loss: 3.141150\n",
            "Step 59266: Minibatch Loss: 3.171435\n",
            "Step 59268: Minibatch Loss: 3.257691\n",
            "Step 59270: Minibatch Loss: 3.228745\n",
            "Step 59272: Minibatch Loss: 3.168679\n",
            "Step 59274: Minibatch Loss: 2.932193\n",
            "Step 59276: Minibatch Loss: 2.819177\n",
            "Step 59278: Minibatch Loss: 3.129000\n",
            "Step 59280: Minibatch Loss: 3.033090\n",
            "Step 59282: Minibatch Loss: 2.962890\n",
            "Step 59284: Minibatch Loss: 3.123366\n",
            "Step 59286: Minibatch Loss: 2.860390\n",
            "Step 59288: Minibatch Loss: 2.783501\n",
            "Step 59290: Minibatch Loss: 2.998543\n",
            "Step 59292: Minibatch Loss: 2.872249\n",
            "Step 59294: Minibatch Loss: 2.936984\n",
            "Step 59296: Minibatch Loss: 2.852994\n",
            "Step 59298: Minibatch Loss: 3.102515\n",
            "Step 59300: Minibatch Loss: 2.587157\n",
            "Step 59302: Minibatch Loss: 3.019561\n",
            "Step 59304: Minibatch Loss: 2.877293\n",
            "Step 59306: Minibatch Loss: 2.820300\n",
            "Step 59308: Minibatch Loss: 3.067113\n",
            "Step 59310: Minibatch Loss: 3.055085\n",
            "Step 59312: Minibatch Loss: 2.830069\n",
            "Step 59314: Minibatch Loss: 2.866841\n",
            "Step 59316: Minibatch Loss: 3.309699\n",
            "Step 59318: Minibatch Loss: 3.493382\n",
            "Step 59320: Minibatch Loss: 3.022989\n",
            "Step 59322: Minibatch Loss: 2.942403\n",
            "Step 59324: Minibatch Loss: 3.050010\n",
            "Step 59326: Minibatch Loss: 3.031869\n",
            "Step 59328: Minibatch Loss: 2.984404\n",
            "Step 59330: Minibatch Loss: 2.777683\n",
            "Step 59332: Minibatch Loss: 3.086995\n",
            "Step 59334: Minibatch Loss: 3.103374\n",
            "Step 59336: Minibatch Loss: 2.645821\n",
            "Step 59338: Minibatch Loss: 3.313335\n",
            "Step 59340: Minibatch Loss: 3.175637\n",
            "Step 59342: Minibatch Loss: 2.894425\n",
            "Step 59344: Minibatch Loss: 3.006070\n",
            "Step 59346: Minibatch Loss: 2.978783\n",
            "Step 59348: Minibatch Loss: 2.809276\n",
            "Step 59350: Minibatch Loss: 3.034628\n",
            "Step 59352: Minibatch Loss: 2.968530\n",
            "Step 59354: Minibatch Loss: 3.235065\n",
            "Step 59356: Minibatch Loss: 3.001941\n",
            "Step 59358: Minibatch Loss: 3.142891\n",
            "Step 59360: Minibatch Loss: 2.912343\n",
            "Step 59362: Minibatch Loss: 2.925432\n",
            "Step 59364: Minibatch Loss: 2.754216\n",
            "Step 59366: Minibatch Loss: 3.148461\n",
            "Step 59368: Minibatch Loss: 2.635251\n",
            "Step 59370: Minibatch Loss: 3.145964\n",
            "Step 59372: Minibatch Loss: 2.911237\n",
            "Step 59374: Minibatch Loss: 2.922352\n",
            "Step 59376: Minibatch Loss: 2.973215\n",
            "Step 59378: Minibatch Loss: 3.023079\n",
            "Step 59380: Minibatch Loss: 3.037652\n",
            "Step 59382: Minibatch Loss: 2.991722\n",
            "Step 59384: Minibatch Loss: 3.083314\n",
            "Step 59386: Minibatch Loss: 2.778672\n",
            "Step 59388: Minibatch Loss: 2.562816\n",
            "Step 59390: Minibatch Loss: 3.034383\n",
            "Step 59392: Minibatch Loss: 3.038804\n",
            "Step 59394: Minibatch Loss: 2.706590\n",
            "Step 59396: Minibatch Loss: 2.758075\n",
            "Step 59398: Minibatch Loss: 3.044389\n",
            "Step 59400: Minibatch Loss: 3.168822\n",
            "Step 59402: Minibatch Loss: 2.952708\n",
            "Step 59404: Minibatch Loss: 3.152445\n",
            "Step 59406: Minibatch Loss: 3.013586\n",
            "Step 59408: Minibatch Loss: 3.014850\n",
            "Step 59410: Minibatch Loss: 2.754562\n",
            "Step 59412: Minibatch Loss: 2.868857\n",
            "Step 59414: Minibatch Loss: 3.153069\n",
            "Step 59416: Minibatch Loss: 2.861390\n",
            "Step 59418: Minibatch Loss: 2.872247\n",
            "Step 59420: Minibatch Loss: 2.816394\n",
            "Step 59422: Minibatch Loss: 3.052265\n",
            "Step 59424: Minibatch Loss: 2.896763\n",
            "Step 59426: Minibatch Loss: 2.904294\n",
            "Step 59428: Minibatch Loss: 3.235039\n",
            "Step 59430: Minibatch Loss: 3.084203\n",
            "Step 59432: Minibatch Loss: 3.268103\n",
            "Step 59434: Minibatch Loss: 2.939490\n",
            "Step 59436: Minibatch Loss: 2.698978\n",
            "Step 59438: Minibatch Loss: 2.988351\n",
            "Step 59440: Minibatch Loss: 2.759675\n",
            "Step 59442: Minibatch Loss: 2.782199\n",
            "Step 59444: Minibatch Loss: 2.994540\n",
            "Step 59446: Minibatch Loss: 3.170205\n",
            "Step 59448: Minibatch Loss: 3.231447\n",
            "Step 59450: Minibatch Loss: 3.128896\n",
            "Step 59452: Minibatch Loss: 2.805850\n",
            "Step 59454: Minibatch Loss: 2.803133\n",
            "Step 59456: Minibatch Loss: 2.672470\n",
            "Step 59458: Minibatch Loss: 2.844380\n",
            "Step 59460: Minibatch Loss: 2.801018\n",
            "Step 59462: Minibatch Loss: 2.766662\n",
            "Step 59464: Minibatch Loss: 3.220689\n",
            "Step 59466: Minibatch Loss: 3.086891\n",
            "Step 59468: Minibatch Loss: 2.765584\n",
            "Step 59470: Minibatch Loss: 3.112581\n",
            "Step 59472: Minibatch Loss: 3.046478\n",
            "Step 59474: Minibatch Loss: 2.996026\n",
            "Step 59476: Minibatch Loss: 2.853662\n",
            "Step 59478: Minibatch Loss: 3.077466\n",
            "Step 59480: Minibatch Loss: 3.265203\n",
            "Step 59482: Minibatch Loss: 3.117169\n",
            "Step 59484: Minibatch Loss: 3.062722\n",
            "Step 59486: Minibatch Loss: 3.070419\n",
            "Step 59488: Minibatch Loss: 3.034070\n",
            "Step 59490: Minibatch Loss: 3.208104\n",
            "Step 59492: Minibatch Loss: 3.022672\n",
            "Step 59494: Minibatch Loss: 3.201640\n",
            "Step 59496: Minibatch Loss: 2.947795\n",
            "Step 59498: Minibatch Loss: 2.940560\n",
            "Step 59500: Minibatch Loss: 2.621503\n",
            "Step 59502: Minibatch Loss: 2.741488\n",
            "Step 59504: Minibatch Loss: 2.990350\n",
            "Step 59506: Minibatch Loss: 2.759357\n",
            "Step 59508: Minibatch Loss: 3.224879\n",
            "Step 59510: Minibatch Loss: 2.900991\n",
            "Step 59512: Minibatch Loss: 2.995451\n",
            "Step 59514: Minibatch Loss: 3.085473\n",
            "Step 59516: Minibatch Loss: 3.129271\n",
            "Step 59518: Minibatch Loss: 2.762153\n",
            "Step 59520: Minibatch Loss: 2.901013\n",
            "Step 59522: Minibatch Loss: 2.736076\n",
            "Step 59524: Minibatch Loss: 3.126256\n",
            "Step 59526: Minibatch Loss: 2.829813\n",
            "Step 59528: Minibatch Loss: 2.805400\n",
            "Step 59530: Minibatch Loss: 2.927381\n",
            "Step 59532: Minibatch Loss: 2.889867\n",
            "Step 59534: Minibatch Loss: 2.858755\n",
            "Step 59536: Minibatch Loss: 3.129911\n",
            "Step 59538: Minibatch Loss: 3.055466\n",
            "Step 59540: Minibatch Loss: 3.122605\n",
            "Step 59542: Minibatch Loss: 2.962010\n",
            "Step 59544: Minibatch Loss: 2.906113\n",
            "Step 59546: Minibatch Loss: 2.948482\n",
            "Step 59548: Minibatch Loss: 2.737670\n",
            "Step 59550: Minibatch Loss: 3.026716\n",
            "Step 59552: Minibatch Loss: 2.966937\n",
            "Step 59554: Minibatch Loss: 3.269009\n",
            "Step 59556: Minibatch Loss: 2.855755\n",
            "Step 59558: Minibatch Loss: 3.170467\n",
            "Step 59560: Minibatch Loss: 3.264013\n",
            "Step 59562: Minibatch Loss: 3.123197\n",
            "Step 59564: Minibatch Loss: 2.671902\n",
            "Step 59566: Minibatch Loss: 3.074760\n",
            "Step 59568: Minibatch Loss: 2.787604\n",
            "Step 59570: Minibatch Loss: 3.037270\n",
            "Step 59572: Minibatch Loss: 2.985305\n",
            "Step 59574: Minibatch Loss: 2.806605\n",
            "Step 59576: Minibatch Loss: 2.890926\n",
            "Step 59578: Minibatch Loss: 2.798797\n",
            "Step 59580: Minibatch Loss: 2.904708\n",
            "Step 59582: Minibatch Loss: 2.820051\n",
            "Step 59584: Minibatch Loss: 3.064342\n",
            "Step 59586: Minibatch Loss: 2.725984\n",
            "Step 59588: Minibatch Loss: 3.224870\n",
            "Step 59590: Minibatch Loss: 3.063466\n",
            "Step 59592: Minibatch Loss: 3.077512\n",
            "Step 59594: Minibatch Loss: 2.993756\n",
            "Step 59596: Minibatch Loss: 2.903440\n",
            "Step 59598: Minibatch Loss: 2.939592\n",
            "Step 59600: Minibatch Loss: 3.199913\n",
            "Step 59602: Minibatch Loss: 3.161058\n",
            "Step 59604: Minibatch Loss: 3.216306\n",
            "Step 59606: Minibatch Loss: 2.825068\n",
            "Step 59608: Minibatch Loss: 2.993891\n",
            "Step 59610: Minibatch Loss: 2.753233\n",
            "Step 59612: Minibatch Loss: 2.980174\n",
            "Step 59614: Minibatch Loss: 2.737067\n",
            "Step 59616: Minibatch Loss: 3.113316\n",
            "Step 59618: Minibatch Loss: 3.413639\n",
            "Step 59620: Minibatch Loss: 2.960592\n",
            "Step 59622: Minibatch Loss: 2.988858\n",
            "Step 59624: Minibatch Loss: 3.163466\n",
            "Step 59626: Minibatch Loss: 2.931734\n",
            "Step 59628: Minibatch Loss: 2.763974\n",
            "Step 59630: Minibatch Loss: 2.820321\n",
            "Step 59632: Minibatch Loss: 2.966094\n",
            "Step 59634: Minibatch Loss: 3.088242\n",
            "Step 59636: Minibatch Loss: 2.661256\n",
            "Step 59638: Minibatch Loss: 3.107376\n",
            "Step 59640: Minibatch Loss: 3.127091\n",
            "Step 59642: Minibatch Loss: 3.120283\n",
            "Step 59644: Minibatch Loss: 3.068112\n",
            "Step 59646: Minibatch Loss: 2.943733\n",
            "Step 59648: Minibatch Loss: 2.977143\n",
            "Step 59650: Minibatch Loss: 2.862328\n",
            "Step 59652: Minibatch Loss: 3.067754\n",
            "Step 59654: Minibatch Loss: 2.940993\n",
            "Step 59656: Minibatch Loss: 2.897161\n",
            "Step 59658: Minibatch Loss: 3.004752\n",
            "Step 59660: Minibatch Loss: 2.780730\n",
            "Step 59662: Minibatch Loss: 3.049555\n",
            "Step 59664: Minibatch Loss: 3.019406\n",
            "Step 59666: Minibatch Loss: 2.805562\n",
            "Step 59668: Minibatch Loss: 2.784531\n",
            "Step 59670: Minibatch Loss: 2.750852\n",
            "Step 59672: Minibatch Loss: 3.287275\n",
            "Step 59674: Minibatch Loss: 3.203011\n",
            "Step 59676: Minibatch Loss: 2.937132\n",
            "Step 59678: Minibatch Loss: 3.076618\n",
            "Step 59680: Minibatch Loss: 2.771904\n",
            "Step 59682: Minibatch Loss: 2.888353\n",
            "Step 59684: Minibatch Loss: 2.850983\n",
            "Step 59686: Minibatch Loss: 2.678142\n",
            "Step 59688: Minibatch Loss: 2.773025\n",
            "Step 59690: Minibatch Loss: 2.849725\n",
            "Step 59692: Minibatch Loss: 2.937533\n",
            "Step 59694: Minibatch Loss: 3.137037\n",
            "Step 59696: Minibatch Loss: 2.918461\n",
            "Step 59698: Minibatch Loss: 2.681517\n",
            "Step 59700: Minibatch Loss: 2.953072\n",
            "Step 59702: Minibatch Loss: 3.373206\n",
            "Step 59704: Minibatch Loss: 3.002871\n",
            "Step 59706: Minibatch Loss: 3.381714\n",
            "Step 59708: Minibatch Loss: 2.833768\n",
            "Step 59710: Minibatch Loss: 3.251278\n",
            "Step 59712: Minibatch Loss: 3.061188\n",
            "Step 59714: Minibatch Loss: 2.629007\n",
            "Step 59716: Minibatch Loss: 3.282789\n",
            "Step 59718: Minibatch Loss: 3.070757\n",
            "Step 59720: Minibatch Loss: 3.279446\n",
            "Step 59722: Minibatch Loss: 3.129102\n",
            "Step 59724: Minibatch Loss: 2.904029\n",
            "Step 59726: Minibatch Loss: 3.026526\n",
            "Step 59728: Minibatch Loss: 3.240565\n",
            "Step 59730: Minibatch Loss: 3.027072\n",
            "Step 59732: Minibatch Loss: 2.820651\n",
            "Step 59734: Minibatch Loss: 2.771271\n",
            "Step 59736: Minibatch Loss: 2.546427\n",
            "Step 59738: Minibatch Loss: 2.974354\n",
            "Step 59740: Minibatch Loss: 2.751866\n",
            "Step 59742: Minibatch Loss: 3.052512\n",
            "Step 59744: Minibatch Loss: 3.014990\n",
            "Step 59746: Minibatch Loss: 2.781606\n",
            "Step 59748: Minibatch Loss: 3.153773\n",
            "Step 59750: Minibatch Loss: 3.121852\n",
            "Step 59752: Minibatch Loss: 3.161255\n",
            "Step 59754: Minibatch Loss: 3.162567\n",
            "Step 59756: Minibatch Loss: 3.081129\n",
            "Step 59758: Minibatch Loss: 3.037138\n",
            "Step 59760: Minibatch Loss: 2.980494\n",
            "Step 59762: Minibatch Loss: 3.141022\n",
            "Step 59764: Minibatch Loss: 3.039024\n",
            "Step 59766: Minibatch Loss: 3.202117\n",
            "Step 59768: Minibatch Loss: 2.868107\n",
            "Step 59770: Minibatch Loss: 2.880976\n",
            "Step 59772: Minibatch Loss: 2.979308\n",
            "Step 59774: Minibatch Loss: 3.011654\n",
            "Step 59776: Minibatch Loss: 3.012627\n",
            "Step 59778: Minibatch Loss: 3.023993\n",
            "Step 59780: Minibatch Loss: 3.076650\n",
            "Step 59782: Minibatch Loss: 2.958091\n",
            "Step 59784: Minibatch Loss: 3.338625\n",
            "Step 59786: Minibatch Loss: 3.256371\n",
            "Step 59788: Minibatch Loss: 2.858611\n",
            "Step 59790: Minibatch Loss: 2.992131\n",
            "Step 59792: Minibatch Loss: 3.092552\n",
            "Step 59794: Minibatch Loss: 2.981688\n",
            "Step 59796: Minibatch Loss: 2.825544\n",
            "Step 59798: Minibatch Loss: 2.975748\n",
            "Step 59800: Minibatch Loss: 3.282470\n",
            "Step 59802: Minibatch Loss: 3.086174\n",
            "Step 59804: Minibatch Loss: 3.014697\n",
            "Step 59806: Minibatch Loss: 2.922478\n",
            "Step 59808: Minibatch Loss: 3.039897\n",
            "Step 59810: Minibatch Loss: 3.378094\n",
            "Step 59812: Minibatch Loss: 2.774469\n",
            "Step 59814: Minibatch Loss: 3.173627\n",
            "Step 59816: Minibatch Loss: 3.068545\n",
            "Step 59818: Minibatch Loss: 3.156075\n",
            "Step 59820: Minibatch Loss: 3.248281\n",
            "Step 59822: Minibatch Loss: 3.106133\n",
            "Step 59824: Minibatch Loss: 3.149320\n",
            "Step 59826: Minibatch Loss: 3.268070\n",
            "Step 59828: Minibatch Loss: 3.024686\n",
            "Step 59830: Minibatch Loss: 2.968214\n",
            "Step 59832: Minibatch Loss: 2.615227\n",
            "Step 59834: Minibatch Loss: 3.143537\n",
            "Step 59836: Minibatch Loss: 3.145244\n",
            "Step 59838: Minibatch Loss: 3.295547\n",
            "Step 59840: Minibatch Loss: 2.953996\n",
            "Step 59842: Minibatch Loss: 3.110569\n",
            "Step 59844: Minibatch Loss: 3.059442\n",
            "Step 59846: Minibatch Loss: 3.156235\n",
            "Step 59848: Minibatch Loss: 2.995190\n",
            "Step 59850: Minibatch Loss: 3.089737\n",
            "Step 59852: Minibatch Loss: 3.057357\n",
            "Step 59854: Minibatch Loss: 2.829801\n",
            "Step 59856: Minibatch Loss: 3.040673\n",
            "Step 59858: Minibatch Loss: 3.172338\n",
            "Step 59860: Minibatch Loss: 2.880256\n",
            "Step 59862: Minibatch Loss: 2.765031\n",
            "Step 59864: Minibatch Loss: 2.967834\n",
            "Step 59866: Minibatch Loss: 2.931776\n",
            "Step 59868: Minibatch Loss: 3.164635\n",
            "Step 59870: Minibatch Loss: 3.086906\n",
            "Step 59872: Minibatch Loss: 2.958791\n",
            "Step 59874: Minibatch Loss: 2.875159\n",
            "Step 59876: Minibatch Loss: 3.072049\n",
            "Step 59878: Minibatch Loss: 3.453773\n",
            "Step 59880: Minibatch Loss: 2.644081\n",
            "Step 59882: Minibatch Loss: 3.118985\n",
            "Step 59884: Minibatch Loss: 2.829901\n",
            "Step 59886: Minibatch Loss: 2.999736\n",
            "Step 59888: Minibatch Loss: 3.210519\n",
            "Step 59890: Minibatch Loss: 3.110233\n",
            "Step 59892: Minibatch Loss: 2.942929\n",
            "Step 59894: Minibatch Loss: 2.898213\n",
            "Step 59896: Minibatch Loss: 3.039060\n",
            "Step 59898: Minibatch Loss: 2.828813\n",
            "Step 59900: Minibatch Loss: 2.968024\n",
            "Step 59902: Minibatch Loss: 2.976414\n",
            "Step 59904: Minibatch Loss: 2.861261\n",
            "Step 59906: Minibatch Loss: 2.993846\n",
            "Step 59908: Minibatch Loss: 3.245807\n",
            "Step 59910: Minibatch Loss: 2.654790\n",
            "Step 59912: Minibatch Loss: 3.048937\n",
            "Step 59914: Minibatch Loss: 3.004793\n",
            "Step 59916: Minibatch Loss: 2.867576\n",
            "Step 59918: Minibatch Loss: 2.851974\n",
            "Step 59920: Minibatch Loss: 2.963868\n",
            "Step 59922: Minibatch Loss: 2.917376\n",
            "Step 59924: Minibatch Loss: 3.235235\n",
            "Step 59926: Minibatch Loss: 3.073479\n",
            "Step 59928: Minibatch Loss: 2.747184\n",
            "Step 59930: Minibatch Loss: 3.049571\n",
            "Step 59932: Minibatch Loss: 3.170340\n",
            "Step 59934: Minibatch Loss: 3.248749\n",
            "Step 59936: Minibatch Loss: 3.302631\n",
            "Step 59938: Minibatch Loss: 3.099098\n",
            "Step 59940: Minibatch Loss: 2.968028\n",
            "Training for SNR= 9.0  sigma= 0.35481338923357547 iteratin: 0\n",
            "Step 59942: Minibatch Loss: 2.775795\n",
            "Step 59944: Minibatch Loss: 3.233006\n",
            "Step 59946: Minibatch Loss: 3.084589\n",
            "Step 59948: Minibatch Loss: 2.937814\n",
            "Step 59950: Minibatch Loss: 3.173642\n",
            "Step 59952: Minibatch Loss: 2.764469\n",
            "Step 59954: Minibatch Loss: 2.732983\n",
            "Step 59956: Minibatch Loss: 2.990955\n",
            "Step 59958: Minibatch Loss: 2.850676\n",
            "Step 59960: Minibatch Loss: 2.977979\n",
            "Step 59962: Minibatch Loss: 2.974239\n",
            "Step 59964: Minibatch Loss: 3.078899\n",
            "Step 59966: Minibatch Loss: 2.633286\n",
            "Step 59968: Minibatch Loss: 3.260175\n",
            "Step 59970: Minibatch Loss: 2.898860\n",
            "Step 59972: Minibatch Loss: 2.795146\n",
            "Step 59974: Minibatch Loss: 3.071293\n",
            "Step 59976: Minibatch Loss: 2.949912\n",
            "Step 59978: Minibatch Loss: 2.872431\n",
            "Step 59980: Minibatch Loss: 2.859092\n",
            "Step 59982: Minibatch Loss: 3.367762\n",
            "Step 59984: Minibatch Loss: 3.489575\n",
            "Step 59986: Minibatch Loss: 3.092709\n",
            "Step 59988: Minibatch Loss: 3.037127\n",
            "Step 59990: Minibatch Loss: 3.039995\n",
            "Step 59992: Minibatch Loss: 3.043146\n",
            "Step 59994: Minibatch Loss: 2.941474\n",
            "Step 59996: Minibatch Loss: 2.691390\n",
            "Step 59998: Minibatch Loss: 2.883388\n",
            "Step 60000: Minibatch Loss: 3.078359\n",
            "Step 60002: Minibatch Loss: 2.666681\n",
            "Step 60004: Minibatch Loss: 3.228679\n",
            "Step 60006: Minibatch Loss: 3.110275\n",
            "Step 60008: Minibatch Loss: 2.777522\n",
            "Step 60010: Minibatch Loss: 3.025256\n",
            "Step 60012: Minibatch Loss: 2.940225\n",
            "Step 60014: Minibatch Loss: 2.786070\n",
            "Step 60016: Minibatch Loss: 2.970791\n",
            "Step 60018: Minibatch Loss: 3.044936\n",
            "Step 60020: Minibatch Loss: 3.112260\n",
            "Step 60022: Minibatch Loss: 3.001127\n",
            "Step 60024: Minibatch Loss: 3.087069\n",
            "Step 60026: Minibatch Loss: 3.018222\n",
            "Step 60028: Minibatch Loss: 2.900394\n",
            "Step 60030: Minibatch Loss: 2.830462\n",
            "Step 60032: Minibatch Loss: 3.037038\n",
            "Step 60034: Minibatch Loss: 2.674729\n",
            "Step 60036: Minibatch Loss: 3.185673\n",
            "Step 60038: Minibatch Loss: 2.854474\n",
            "Step 60040: Minibatch Loss: 2.867949\n",
            "Step 60042: Minibatch Loss: 2.923959\n",
            "Step 60044: Minibatch Loss: 3.034186\n",
            "Step 60046: Minibatch Loss: 3.013398\n",
            "Step 60048: Minibatch Loss: 3.068717\n",
            "Step 60050: Minibatch Loss: 3.134281\n",
            "Step 60052: Minibatch Loss: 2.774075\n",
            "Step 60054: Minibatch Loss: 2.536211\n",
            "Step 60056: Minibatch Loss: 3.039285\n",
            "Step 60058: Minibatch Loss: 3.024490\n",
            "Step 60060: Minibatch Loss: 2.757284\n",
            "Step 60062: Minibatch Loss: 2.678159\n",
            "Step 60064: Minibatch Loss: 2.972072\n",
            "Step 60066: Minibatch Loss: 3.209862\n",
            "Step 60068: Minibatch Loss: 2.909075\n",
            "Step 60070: Minibatch Loss: 3.169876\n",
            "Step 60072: Minibatch Loss: 2.907628\n",
            "Step 60074: Minibatch Loss: 3.082249\n",
            "Step 60076: Minibatch Loss: 2.875934\n",
            "Step 60078: Minibatch Loss: 2.946040\n",
            "Step 60080: Minibatch Loss: 3.027113\n",
            "Step 60082: Minibatch Loss: 2.859064\n",
            "Step 60084: Minibatch Loss: 2.864911\n",
            "Step 60086: Minibatch Loss: 2.760555\n",
            "Step 60088: Minibatch Loss: 3.113015\n",
            "Step 60090: Minibatch Loss: 2.807641\n",
            "Step 60092: Minibatch Loss: 2.883714\n",
            "Step 60094: Minibatch Loss: 3.163396\n",
            "Step 60096: Minibatch Loss: 2.900528\n",
            "Step 60098: Minibatch Loss: 3.221831\n",
            "Step 60100: Minibatch Loss: 2.983686\n",
            "Step 60102: Minibatch Loss: 2.628884\n",
            "Step 60104: Minibatch Loss: 3.068141\n",
            "Step 60106: Minibatch Loss: 2.817296\n",
            "Step 60108: Minibatch Loss: 2.725831\n",
            "Step 60110: Minibatch Loss: 2.980773\n",
            "Step 60112: Minibatch Loss: 2.985512\n",
            "Step 60114: Minibatch Loss: 3.264005\n",
            "Step 60116: Minibatch Loss: 3.179310\n",
            "Step 60118: Minibatch Loss: 2.727833\n",
            "Step 60120: Minibatch Loss: 2.733260\n",
            "Step 60122: Minibatch Loss: 2.618966\n",
            "Step 60124: Minibatch Loss: 2.857929\n",
            "Step 60126: Minibatch Loss: 2.752277\n",
            "Step 60128: Minibatch Loss: 2.744702\n",
            "Step 60130: Minibatch Loss: 3.271698\n",
            "Step 60132: Minibatch Loss: 3.023102\n",
            "Step 60134: Minibatch Loss: 2.842494\n",
            "Step 60136: Minibatch Loss: 3.068236\n",
            "Step 60138: Minibatch Loss: 2.979100\n",
            "Step 60140: Minibatch Loss: 3.027357\n",
            "Step 60142: Minibatch Loss: 2.818308\n",
            "Step 60144: Minibatch Loss: 3.034102\n",
            "Step 60146: Minibatch Loss: 3.271744\n",
            "Step 60148: Minibatch Loss: 3.038180\n",
            "Step 60150: Minibatch Loss: 3.009547\n",
            "Step 60152: Minibatch Loss: 3.050281\n",
            "Step 60154: Minibatch Loss: 2.955198\n",
            "Step 60156: Minibatch Loss: 3.249867\n",
            "Step 60158: Minibatch Loss: 3.072686\n",
            "Step 60160: Minibatch Loss: 3.073684\n",
            "Step 60162: Minibatch Loss: 2.917741\n",
            "Step 60164: Minibatch Loss: 2.930259\n",
            "Step 60166: Minibatch Loss: 2.677123\n",
            "Step 60168: Minibatch Loss: 2.798099\n",
            "Step 60170: Minibatch Loss: 3.072817\n",
            "Step 60172: Minibatch Loss: 2.824032\n",
            "Step 60174: Minibatch Loss: 3.218747\n",
            "Step 60176: Minibatch Loss: 2.871983\n",
            "Step 60178: Minibatch Loss: 3.066195\n",
            "Step 60180: Minibatch Loss: 3.060074\n",
            "Step 60182: Minibatch Loss: 3.038435\n",
            "Step 60184: Minibatch Loss: 2.852382\n",
            "Step 60186: Minibatch Loss: 2.905891\n",
            "Step 60188: Minibatch Loss: 2.715157\n",
            "Step 60190: Minibatch Loss: 3.173719\n",
            "Step 60192: Minibatch Loss: 2.788956\n",
            "Step 60194: Minibatch Loss: 2.802952\n",
            "Step 60196: Minibatch Loss: 2.884441\n",
            "Step 60198: Minibatch Loss: 2.920876\n",
            "Step 60200: Minibatch Loss: 2.850443\n",
            "Step 60202: Minibatch Loss: 3.109113\n",
            "Step 60204: Minibatch Loss: 3.084372\n",
            "Step 60206: Minibatch Loss: 3.125089\n",
            "Step 60208: Minibatch Loss: 2.940784\n",
            "Step 60210: Minibatch Loss: 2.785389\n",
            "Step 60212: Minibatch Loss: 3.015809\n",
            "Step 60214: Minibatch Loss: 2.720453\n",
            "Step 60216: Minibatch Loss: 2.949017\n",
            "Step 60218: Minibatch Loss: 2.848129\n",
            "Step 60220: Minibatch Loss: 3.209511\n",
            "Step 60222: Minibatch Loss: 2.878698\n",
            "Step 60224: Minibatch Loss: 3.155358\n",
            "Step 60226: Minibatch Loss: 3.333869\n",
            "Step 60228: Minibatch Loss: 3.167592\n",
            "Step 60230: Minibatch Loss: 2.586646\n",
            "Step 60232: Minibatch Loss: 2.949830\n",
            "Step 60234: Minibatch Loss: 2.860503\n",
            "Step 60236: Minibatch Loss: 2.949754\n",
            "Step 60238: Minibatch Loss: 3.074790\n",
            "Step 60240: Minibatch Loss: 2.782771\n",
            "Step 60242: Minibatch Loss: 2.796975\n",
            "Step 60244: Minibatch Loss: 2.769130\n",
            "Step 60246: Minibatch Loss: 2.876540\n",
            "Step 60248: Minibatch Loss: 2.752902\n",
            "Step 60250: Minibatch Loss: 3.008577\n",
            "Step 60252: Minibatch Loss: 2.838555\n",
            "Step 60254: Minibatch Loss: 3.165965\n",
            "Step 60256: Minibatch Loss: 3.009848\n",
            "Step 60258: Minibatch Loss: 3.039853\n",
            "Step 60260: Minibatch Loss: 2.910446\n",
            "Step 60262: Minibatch Loss: 2.970320\n",
            "Step 60264: Minibatch Loss: 2.939641\n",
            "Step 60266: Minibatch Loss: 3.004567\n",
            "Step 60268: Minibatch Loss: 3.335664\n",
            "Step 60270: Minibatch Loss: 3.204801\n",
            "Step 60272: Minibatch Loss: 2.933577\n",
            "Step 60274: Minibatch Loss: 3.029472\n",
            "Step 60276: Minibatch Loss: 2.689780\n",
            "Step 60278: Minibatch Loss: 2.961937\n",
            "Step 60280: Minibatch Loss: 2.720901\n",
            "Step 60282: Minibatch Loss: 3.093716\n",
            "Step 60284: Minibatch Loss: 3.362793\n",
            "Step 60286: Minibatch Loss: 2.872640\n",
            "Step 60288: Minibatch Loss: 3.102971\n",
            "Step 60290: Minibatch Loss: 3.045799\n",
            "Step 60292: Minibatch Loss: 2.861973\n",
            "Step 60294: Minibatch Loss: 2.821713\n",
            "Step 60296: Minibatch Loss: 2.813956\n",
            "Step 60298: Minibatch Loss: 3.083169\n",
            "Step 60300: Minibatch Loss: 3.061543\n",
            "Step 60302: Minibatch Loss: 2.585360\n",
            "Step 60304: Minibatch Loss: 2.947706\n",
            "Step 60306: Minibatch Loss: 3.184578\n",
            "Step 60308: Minibatch Loss: 3.197349\n",
            "Step 60310: Minibatch Loss: 3.011472\n",
            "Step 60312: Minibatch Loss: 2.941929\n",
            "Step 60314: Minibatch Loss: 3.017298\n",
            "Step 60316: Minibatch Loss: 2.814407\n",
            "Step 60318: Minibatch Loss: 3.111058\n",
            "Step 60320: Minibatch Loss: 3.026277\n",
            "Step 60322: Minibatch Loss: 2.887779\n",
            "Step 60324: Minibatch Loss: 3.017628\n",
            "Step 60326: Minibatch Loss: 2.721087\n",
            "Step 60328: Minibatch Loss: 3.011996\n",
            "Step 60330: Minibatch Loss: 3.011538\n",
            "Step 60332: Minibatch Loss: 2.923491\n",
            "Step 60334: Minibatch Loss: 2.766732\n",
            "Step 60336: Minibatch Loss: 2.695625\n",
            "Step 60338: Minibatch Loss: 3.254414\n",
            "Step 60340: Minibatch Loss: 3.278164\n",
            "Step 60342: Minibatch Loss: 2.988910\n",
            "Step 60344: Minibatch Loss: 3.023006\n",
            "Step 60346: Minibatch Loss: 2.803481\n",
            "Step 60348: Minibatch Loss: 2.869959\n",
            "Step 60350: Minibatch Loss: 2.977630\n",
            "Step 60352: Minibatch Loss: 2.753328\n",
            "Step 60354: Minibatch Loss: 2.676733\n",
            "Step 60356: Minibatch Loss: 2.911358\n",
            "Step 60358: Minibatch Loss: 2.790881\n",
            "Step 60360: Minibatch Loss: 3.127067\n",
            "Step 60362: Minibatch Loss: 2.833881\n",
            "Step 60364: Minibatch Loss: 2.548017\n",
            "Step 60366: Minibatch Loss: 3.078939\n",
            "Step 60368: Minibatch Loss: 3.368506\n",
            "Step 60370: Minibatch Loss: 2.991714\n",
            "Step 60372: Minibatch Loss: 3.419369\n",
            "Step 60374: Minibatch Loss: 2.848294\n",
            "Step 60376: Minibatch Loss: 3.286501\n",
            "Step 60378: Minibatch Loss: 3.129368\n",
            "Step 60380: Minibatch Loss: 2.645218\n",
            "Step 60382: Minibatch Loss: 3.323548\n",
            "Step 60384: Minibatch Loss: 3.023601\n",
            "Step 60386: Minibatch Loss: 3.240000\n",
            "Step 60388: Minibatch Loss: 2.972330\n",
            "Step 60390: Minibatch Loss: 2.886531\n",
            "Step 60392: Minibatch Loss: 2.916978\n",
            "Step 60394: Minibatch Loss: 3.198621\n",
            "Step 60396: Minibatch Loss: 3.114506\n",
            "Step 60398: Minibatch Loss: 2.930925\n",
            "Step 60400: Minibatch Loss: 2.842078\n",
            "Step 60402: Minibatch Loss: 2.489324\n",
            "Step 60404: Minibatch Loss: 2.908859\n",
            "Step 60406: Minibatch Loss: 2.701026\n",
            "Step 60408: Minibatch Loss: 2.908410\n",
            "Step 60410: Minibatch Loss: 3.126358\n",
            "Step 60412: Minibatch Loss: 2.795297\n",
            "Step 60414: Minibatch Loss: 3.109925\n",
            "Step 60416: Minibatch Loss: 3.138550\n",
            "Step 60418: Minibatch Loss: 3.093919\n",
            "Step 60420: Minibatch Loss: 3.139583\n",
            "Step 60422: Minibatch Loss: 3.037216\n",
            "Step 60424: Minibatch Loss: 3.013818\n",
            "Step 60426: Minibatch Loss: 2.948083\n",
            "Step 60428: Minibatch Loss: 3.197685\n",
            "Step 60430: Minibatch Loss: 3.110248\n",
            "Step 60432: Minibatch Loss: 3.165671\n",
            "Step 60434: Minibatch Loss: 2.807012\n",
            "Step 60436: Minibatch Loss: 2.858445\n",
            "Step 60438: Minibatch Loss: 3.021643\n",
            "Step 60440: Minibatch Loss: 3.061784\n",
            "Step 60442: Minibatch Loss: 3.042590\n",
            "Step 60444: Minibatch Loss: 3.051003\n",
            "Step 60446: Minibatch Loss: 3.183734\n",
            "Step 60448: Minibatch Loss: 2.920937\n",
            "Step 60450: Minibatch Loss: 3.331552\n",
            "Step 60452: Minibatch Loss: 3.248097\n",
            "Step 60454: Minibatch Loss: 2.713221\n",
            "Step 60456: Minibatch Loss: 2.967894\n",
            "Step 60458: Minibatch Loss: 3.163378\n",
            "Step 60460: Minibatch Loss: 2.793348\n",
            "Step 60462: Minibatch Loss: 2.836766\n",
            "Step 60464: Minibatch Loss: 2.969230\n",
            "Step 60466: Minibatch Loss: 3.101411\n",
            "Step 60468: Minibatch Loss: 3.134993\n",
            "Step 60470: Minibatch Loss: 3.034967\n",
            "Step 60472: Minibatch Loss: 2.959854\n",
            "Step 60474: Minibatch Loss: 3.150699\n",
            "Step 60476: Minibatch Loss: 3.457744\n",
            "Step 60478: Minibatch Loss: 2.739612\n",
            "Step 60480: Minibatch Loss: 3.218800\n",
            "Step 60482: Minibatch Loss: 3.016366\n",
            "Step 60484: Minibatch Loss: 3.283635\n",
            "Step 60486: Minibatch Loss: 3.181446\n",
            "Step 60488: Minibatch Loss: 3.123039\n",
            "Step 60490: Minibatch Loss: 3.287386\n",
            "Step 60492: Minibatch Loss: 3.373727\n",
            "Step 60494: Minibatch Loss: 3.000636\n",
            "Step 60496: Minibatch Loss: 2.901925\n",
            "Step 60498: Minibatch Loss: 2.631914\n",
            "Step 60500: Minibatch Loss: 3.090917\n",
            "Step 60502: Minibatch Loss: 3.044325\n",
            "Step 60504: Minibatch Loss: 3.316439\n",
            "Step 60506: Minibatch Loss: 2.949754\n",
            "Step 60508: Minibatch Loss: 3.000339\n",
            "Step 60510: Minibatch Loss: 3.059855\n",
            "Step 60512: Minibatch Loss: 3.267121\n",
            "Step 60514: Minibatch Loss: 3.087738\n",
            "Step 60516: Minibatch Loss: 3.082596\n",
            "Step 60518: Minibatch Loss: 2.957720\n",
            "Step 60520: Minibatch Loss: 2.854055\n",
            "Step 60522: Minibatch Loss: 2.884094\n",
            "Step 60524: Minibatch Loss: 3.129294\n",
            "Step 60526: Minibatch Loss: 2.861645\n",
            "Step 60528: Minibatch Loss: 2.979892\n",
            "Step 60530: Minibatch Loss: 2.927470\n",
            "Step 60532: Minibatch Loss: 3.007777\n",
            "Step 60534: Minibatch Loss: 3.002949\n",
            "Step 60536: Minibatch Loss: 3.098614\n",
            "Step 60538: Minibatch Loss: 2.961051\n",
            "Step 60540: Minibatch Loss: 2.851258\n",
            "Step 60542: Minibatch Loss: 3.075493\n",
            "Step 60544: Minibatch Loss: 3.443264\n",
            "Step 60546: Minibatch Loss: 2.624702\n",
            "Step 60548: Minibatch Loss: 3.037038\n",
            "Step 60550: Minibatch Loss: 2.816063\n",
            "Step 60552: Minibatch Loss: 3.032009\n",
            "Step 60554: Minibatch Loss: 3.163373\n",
            "Step 60556: Minibatch Loss: 3.008208\n",
            "Step 60558: Minibatch Loss: 2.836090\n",
            "Step 60560: Minibatch Loss: 2.893082\n",
            "Step 60562: Minibatch Loss: 3.153011\n",
            "Step 60564: Minibatch Loss: 2.907060\n",
            "Step 60566: Minibatch Loss: 2.940414\n",
            "Step 60568: Minibatch Loss: 3.046757\n",
            "Step 60570: Minibatch Loss: 2.867449\n",
            "Step 60572: Minibatch Loss: 2.991776\n",
            "Step 60574: Minibatch Loss: 3.196570\n",
            "Step 60576: Minibatch Loss: 2.700532\n",
            "Step 60578: Minibatch Loss: 2.968611\n",
            "Step 60580: Minibatch Loss: 2.920313\n",
            "Step 60582: Minibatch Loss: 2.802456\n",
            "Step 60584: Minibatch Loss: 2.841366\n",
            "Step 60586: Minibatch Loss: 2.884088\n",
            "Step 60588: Minibatch Loss: 2.898056\n",
            "Step 60590: Minibatch Loss: 3.242122\n",
            "Step 60592: Minibatch Loss: 3.134127\n",
            "Step 60594: Minibatch Loss: 2.805313\n",
            "Step 60596: Minibatch Loss: 3.103098\n",
            "Step 60598: Minibatch Loss: 3.191443\n",
            "Step 60600: Minibatch Loss: 3.278609\n",
            "Step 60602: Minibatch Loss: 3.252381\n",
            "Step 60604: Minibatch Loss: 3.056467\n",
            "Step 60606: Minibatch Loss: 2.910356\n",
            "Step 60608: Minibatch Loss: 2.776556\n",
            "Step 60610: Minibatch Loss: 3.090581\n",
            "Step 60612: Minibatch Loss: 2.941935\n",
            "Step 60614: Minibatch Loss: 2.939078\n",
            "Step 60616: Minibatch Loss: 3.255505\n",
            "Step 60618: Minibatch Loss: 2.776603\n",
            "Step 60620: Minibatch Loss: 2.868810\n",
            "Step 60622: Minibatch Loss: 3.070480\n",
            "Step 60624: Minibatch Loss: 3.011541\n",
            "Step 60626: Minibatch Loss: 2.935741\n",
            "Step 60628: Minibatch Loss: 2.951901\n",
            "Step 60630: Minibatch Loss: 3.058746\n",
            "Step 60632: Minibatch Loss: 2.449855\n",
            "Step 60634: Minibatch Loss: 3.243634\n",
            "Step 60636: Minibatch Loss: 2.962951\n",
            "Step 60638: Minibatch Loss: 2.783453\n",
            "Step 60640: Minibatch Loss: 2.959019\n",
            "Step 60642: Minibatch Loss: 3.054568\n",
            "Step 60644: Minibatch Loss: 2.860015\n",
            "Step 60646: Minibatch Loss: 2.957497\n",
            "Step 60648: Minibatch Loss: 3.301025\n",
            "Step 60650: Minibatch Loss: 3.381819\n",
            "Step 60652: Minibatch Loss: 2.993856\n",
            "Step 60654: Minibatch Loss: 2.982103\n",
            "Step 60656: Minibatch Loss: 2.935843\n",
            "Step 60658: Minibatch Loss: 3.047119\n",
            "Step 60660: Minibatch Loss: 2.939128\n",
            "Step 60662: Minibatch Loss: 2.691700\n",
            "Step 60664: Minibatch Loss: 3.018160\n",
            "Step 60666: Minibatch Loss: 3.178548\n",
            "Step 60668: Minibatch Loss: 2.701084\n",
            "Step 60670: Minibatch Loss: 3.183449\n",
            "Step 60672: Minibatch Loss: 3.226671\n",
            "Step 60674: Minibatch Loss: 2.802074\n",
            "Step 60676: Minibatch Loss: 2.960896\n",
            "Step 60678: Minibatch Loss: 3.028884\n",
            "Step 60680: Minibatch Loss: 2.804815\n",
            "Step 60682: Minibatch Loss: 2.891783\n",
            "Step 60684: Minibatch Loss: 2.947418\n",
            "Step 60686: Minibatch Loss: 3.058658\n",
            "Step 60688: Minibatch Loss: 2.932832\n",
            "Step 60690: Minibatch Loss: 3.191606\n",
            "Step 60692: Minibatch Loss: 2.971499\n",
            "Step 60694: Minibatch Loss: 2.895702\n",
            "Step 60696: Minibatch Loss: 2.752513\n",
            "Step 60698: Minibatch Loss: 3.188274\n",
            "Step 60700: Minibatch Loss: 2.690045\n",
            "Step 60702: Minibatch Loss: 3.130893\n",
            "Step 60704: Minibatch Loss: 2.938563\n",
            "Step 60706: Minibatch Loss: 2.885744\n",
            "Step 60708: Minibatch Loss: 3.025818\n",
            "Step 60710: Minibatch Loss: 3.014545\n",
            "Step 60712: Minibatch Loss: 3.073016\n",
            "Step 60714: Minibatch Loss: 3.066438\n",
            "Step 60716: Minibatch Loss: 3.079654\n",
            "Step 60718: Minibatch Loss: 2.800484\n",
            "Step 60720: Minibatch Loss: 2.549558\n",
            "Step 60722: Minibatch Loss: 2.953141\n",
            "Step 60724: Minibatch Loss: 3.055333\n",
            "Step 60726: Minibatch Loss: 2.811274\n",
            "Step 60728: Minibatch Loss: 2.760337\n",
            "Step 60730: Minibatch Loss: 2.963496\n",
            "Step 60732: Minibatch Loss: 3.123875\n",
            "Step 60734: Minibatch Loss: 2.988367\n",
            "Step 60736: Minibatch Loss: 3.040653\n",
            "Step 60738: Minibatch Loss: 2.948847\n",
            "Step 60740: Minibatch Loss: 2.903699\n",
            "Step 60742: Minibatch Loss: 2.796886\n",
            "Step 60744: Minibatch Loss: 2.903745\n",
            "Step 60746: Minibatch Loss: 3.015880\n",
            "Step 60748: Minibatch Loss: 2.782706\n",
            "Step 60750: Minibatch Loss: 2.780156\n",
            "Step 60752: Minibatch Loss: 2.718653\n",
            "Step 60754: Minibatch Loss: 3.032495\n",
            "Step 60756: Minibatch Loss: 2.779547\n",
            "Step 60758: Minibatch Loss: 2.891465\n",
            "Step 60760: Minibatch Loss: 3.184793\n",
            "Step 60762: Minibatch Loss: 2.862525\n",
            "Step 60764: Minibatch Loss: 3.239849\n",
            "Step 60766: Minibatch Loss: 2.961740\n",
            "Step 60768: Minibatch Loss: 2.571035\n",
            "Step 60770: Minibatch Loss: 2.973232\n",
            "Step 60772: Minibatch Loss: 2.686012\n",
            "Step 60774: Minibatch Loss: 2.703319\n",
            "Step 60776: Minibatch Loss: 2.974530\n",
            "Step 60778: Minibatch Loss: 3.070320\n",
            "Step 60780: Minibatch Loss: 3.290930\n",
            "Step 60782: Minibatch Loss: 3.078341\n",
            "Step 60784: Minibatch Loss: 2.755157\n",
            "Step 60786: Minibatch Loss: 2.869130\n",
            "Step 60788: Minibatch Loss: 2.600319\n",
            "Step 60790: Minibatch Loss: 2.910018\n",
            "Step 60792: Minibatch Loss: 2.779451\n",
            "Step 60794: Minibatch Loss: 2.669156\n",
            "Step 60796: Minibatch Loss: 3.211341\n",
            "Step 60798: Minibatch Loss: 3.032835\n",
            "Step 60800: Minibatch Loss: 2.743981\n",
            "Step 60802: Minibatch Loss: 3.055002\n",
            "Step 60804: Minibatch Loss: 2.983611\n",
            "Step 60806: Minibatch Loss: 2.937839\n",
            "Step 60808: Minibatch Loss: 2.880439\n",
            "Step 60810: Minibatch Loss: 3.108948\n",
            "Step 60812: Minibatch Loss: 3.179167\n",
            "Step 60814: Minibatch Loss: 3.103605\n",
            "Step 60816: Minibatch Loss: 3.040252\n",
            "Step 60818: Minibatch Loss: 3.084165\n",
            "Step 60820: Minibatch Loss: 2.960646\n",
            "Step 60822: Minibatch Loss: 3.207003\n",
            "Step 60824: Minibatch Loss: 3.077943\n",
            "Step 60826: Minibatch Loss: 3.051419\n",
            "Step 60828: Minibatch Loss: 2.972230\n",
            "Step 60830: Minibatch Loss: 2.976187\n",
            "Step 60832: Minibatch Loss: 2.689989\n",
            "Step 60834: Minibatch Loss: 2.783530\n",
            "Step 60836: Minibatch Loss: 3.037766\n",
            "Step 60838: Minibatch Loss: 2.727875\n",
            "Step 60840: Minibatch Loss: 3.116572\n",
            "Step 60842: Minibatch Loss: 2.877672\n",
            "Step 60844: Minibatch Loss: 2.991845\n",
            "Step 60846: Minibatch Loss: 2.976299\n",
            "Step 60848: Minibatch Loss: 2.987154\n",
            "Step 60850: Minibatch Loss: 2.784737\n",
            "Step 60852: Minibatch Loss: 2.896478\n",
            "Step 60854: Minibatch Loss: 2.728384\n",
            "Step 60856: Minibatch Loss: 3.145314\n",
            "Step 60858: Minibatch Loss: 2.844422\n",
            "Step 60860: Minibatch Loss: 2.788244\n",
            "Step 60862: Minibatch Loss: 3.056695\n",
            "Step 60864: Minibatch Loss: 2.875949\n",
            "Step 60866: Minibatch Loss: 2.916919\n",
            "Step 60868: Minibatch Loss: 3.150243\n",
            "Step 60870: Minibatch Loss: 3.076276\n",
            "Step 60872: Minibatch Loss: 3.168875\n",
            "Step 60874: Minibatch Loss: 2.850636\n",
            "Step 60876: Minibatch Loss: 2.790013\n",
            "Step 60878: Minibatch Loss: 2.962353\n",
            "Step 60880: Minibatch Loss: 2.672241\n",
            "Step 60882: Minibatch Loss: 2.911281\n",
            "Step 60884: Minibatch Loss: 2.907542\n",
            "Step 60886: Minibatch Loss: 3.151250\n",
            "Step 60888: Minibatch Loss: 2.775923\n",
            "Step 60890: Minibatch Loss: 3.100945\n",
            "Step 60892: Minibatch Loss: 3.281648\n",
            "Step 60894: Minibatch Loss: 3.224683\n",
            "Step 60896: Minibatch Loss: 2.580128\n",
            "Step 60898: Minibatch Loss: 3.053545\n",
            "Step 60900: Minibatch Loss: 2.881932\n",
            "Step 60902: Minibatch Loss: 3.016396\n",
            "Step 60904: Minibatch Loss: 2.919121\n",
            "Step 60906: Minibatch Loss: 2.771649\n",
            "Step 60908: Minibatch Loss: 2.927222\n",
            "Step 60910: Minibatch Loss: 2.692284\n",
            "Step 60912: Minibatch Loss: 2.927979\n",
            "Step 60914: Minibatch Loss: 2.786892\n",
            "Step 60916: Minibatch Loss: 2.945395\n",
            "Step 60918: Minibatch Loss: 2.861602\n",
            "Step 60920: Minibatch Loss: 3.176043\n",
            "Step 60922: Minibatch Loss: 3.097577\n",
            "Step 60924: Minibatch Loss: 3.124277\n",
            "Step 60926: Minibatch Loss: 2.947655\n",
            "Step 60928: Minibatch Loss: 2.978607\n",
            "Step 60930: Minibatch Loss: 2.838502\n",
            "Step 60932: Minibatch Loss: 2.984104\n",
            "Step 60934: Minibatch Loss: 3.208491\n",
            "Step 60936: Minibatch Loss: 3.217357\n",
            "Step 60938: Minibatch Loss: 2.911704\n",
            "Step 60940: Minibatch Loss: 3.112908\n",
            "Step 60942: Minibatch Loss: 2.699784\n",
            "Step 60944: Minibatch Loss: 3.035603\n",
            "Step 60946: Minibatch Loss: 2.765069\n",
            "Step 60948: Minibatch Loss: 3.065273\n",
            "Step 60950: Minibatch Loss: 3.354098\n",
            "Step 60952: Minibatch Loss: 2.889270\n",
            "Step 60954: Minibatch Loss: 3.012636\n",
            "Step 60956: Minibatch Loss: 3.087523\n",
            "Step 60958: Minibatch Loss: 2.828545\n",
            "Step 60960: Minibatch Loss: 2.759236\n",
            "Step 60962: Minibatch Loss: 2.810928\n",
            "Step 60964: Minibatch Loss: 2.950719\n",
            "Step 60966: Minibatch Loss: 3.027057\n",
            "Step 60968: Minibatch Loss: 2.446789\n",
            "Step 60970: Minibatch Loss: 3.033363\n",
            "Step 60972: Minibatch Loss: 3.158470\n",
            "Step 60974: Minibatch Loss: 3.082653\n",
            "Step 60976: Minibatch Loss: 2.993564\n",
            "Step 60978: Minibatch Loss: 2.873122\n",
            "Step 60980: Minibatch Loss: 2.984046\n",
            "Step 60982: Minibatch Loss: 2.918987\n",
            "Step 60984: Minibatch Loss: 3.035812\n",
            "Step 60986: Minibatch Loss: 3.005646\n",
            "Step 60988: Minibatch Loss: 2.874654\n",
            "Step 60990: Minibatch Loss: 2.964908\n",
            "Step 60992: Minibatch Loss: 2.784046\n",
            "Step 60994: Minibatch Loss: 2.995523\n",
            "Step 60996: Minibatch Loss: 3.006307\n",
            "Step 60998: Minibatch Loss: 2.920925\n",
            "Step 61000: Minibatch Loss: 2.788541\n",
            "Step 61002: Minibatch Loss: 2.708571\n",
            "Step 61004: Minibatch Loss: 3.199901\n",
            "Step 61006: Minibatch Loss: 3.195999\n",
            "Step 61008: Minibatch Loss: 2.928378\n",
            "Step 61010: Minibatch Loss: 3.030809\n",
            "Step 61012: Minibatch Loss: 2.781337\n",
            "Step 61014: Minibatch Loss: 2.880414\n",
            "Step 61016: Minibatch Loss: 2.875139\n",
            "Step 61018: Minibatch Loss: 2.722984\n",
            "Step 61020: Minibatch Loss: 2.746401\n",
            "Step 61022: Minibatch Loss: 2.946883\n",
            "Step 61024: Minibatch Loss: 2.855687\n",
            "Step 61026: Minibatch Loss: 3.116335\n",
            "Step 61028: Minibatch Loss: 2.861614\n",
            "Step 61030: Minibatch Loss: 2.669636\n",
            "Step 61032: Minibatch Loss: 2.944975\n",
            "Step 61034: Minibatch Loss: 3.392055\n",
            "Step 61036: Minibatch Loss: 2.913263\n",
            "Step 61038: Minibatch Loss: 3.353504\n",
            "Step 61040: Minibatch Loss: 2.773572\n",
            "Step 61042: Minibatch Loss: 3.341609\n",
            "Step 61044: Minibatch Loss: 3.028560\n",
            "Step 61046: Minibatch Loss: 2.629019\n",
            "Step 61048: Minibatch Loss: 3.304908\n",
            "Step 61050: Minibatch Loss: 2.997310\n",
            "Step 61052: Minibatch Loss: 3.277658\n",
            "Step 61054: Minibatch Loss: 3.092805\n",
            "Step 61056: Minibatch Loss: 2.909904\n",
            "Step 61058: Minibatch Loss: 3.006789\n",
            "Step 61060: Minibatch Loss: 3.194119\n",
            "Step 61062: Minibatch Loss: 2.953396\n",
            "Step 61064: Minibatch Loss: 2.901755\n",
            "Step 61066: Minibatch Loss: 2.866566\n",
            "Step 61068: Minibatch Loss: 2.474034\n",
            "Step 61070: Minibatch Loss: 2.994155\n",
            "Step 61072: Minibatch Loss: 2.884713\n",
            "Step 61074: Minibatch Loss: 3.003312\n",
            "Step 61076: Minibatch Loss: 3.048458\n",
            "Step 61078: Minibatch Loss: 2.739423\n",
            "Step 61080: Minibatch Loss: 3.052056\n",
            "Step 61082: Minibatch Loss: 3.039965\n",
            "Step 61084: Minibatch Loss: 3.083941\n",
            "Step 61086: Minibatch Loss: 3.145273\n",
            "Step 61088: Minibatch Loss: 2.964322\n",
            "Step 61090: Minibatch Loss: 3.019926\n",
            "Step 61092: Minibatch Loss: 2.862784\n",
            "Step 61094: Minibatch Loss: 3.045514\n",
            "Step 61096: Minibatch Loss: 2.980650\n",
            "Step 61098: Minibatch Loss: 3.143111\n",
            "Step 61100: Minibatch Loss: 2.801470\n",
            "Step 61102: Minibatch Loss: 2.804785\n",
            "Step 61104: Minibatch Loss: 2.987458\n",
            "Step 61106: Minibatch Loss: 2.980699\n",
            "Step 61108: Minibatch Loss: 3.080755\n",
            "Step 61110: Minibatch Loss: 2.970586\n",
            "Step 61112: Minibatch Loss: 3.197125\n",
            "Step 61114: Minibatch Loss: 2.877257\n",
            "Step 61116: Minibatch Loss: 3.428123\n",
            "Step 61118: Minibatch Loss: 3.208198\n",
            "Step 61120: Minibatch Loss: 2.767909\n",
            "Step 61122: Minibatch Loss: 2.862471\n",
            "Step 61124: Minibatch Loss: 3.132639\n",
            "Step 61126: Minibatch Loss: 2.793695\n",
            "Step 61128: Minibatch Loss: 2.786579\n",
            "Step 61130: Minibatch Loss: 3.074766\n",
            "Step 61132: Minibatch Loss: 3.174509\n",
            "Step 61134: Minibatch Loss: 3.130908\n",
            "Step 61136: Minibatch Loss: 3.113357\n",
            "Step 61138: Minibatch Loss: 3.029346\n",
            "Step 61140: Minibatch Loss: 3.149868\n",
            "Step 61142: Minibatch Loss: 3.373184\n",
            "Step 61144: Minibatch Loss: 2.754904\n",
            "Step 61146: Minibatch Loss: 3.200735\n",
            "Step 61148: Minibatch Loss: 2.976133\n",
            "Step 61150: Minibatch Loss: 3.161299\n",
            "Step 61152: Minibatch Loss: 3.137658\n",
            "Step 61154: Minibatch Loss: 3.104038\n",
            "Step 61156: Minibatch Loss: 3.251005\n",
            "Step 61158: Minibatch Loss: 3.488492\n",
            "Step 61160: Minibatch Loss: 2.976079\n",
            "Step 61162: Minibatch Loss: 2.862651\n",
            "Step 61164: Minibatch Loss: 2.655395\n",
            "Step 61166: Minibatch Loss: 3.165092\n",
            "Step 61168: Minibatch Loss: 3.056038\n",
            "Step 61170: Minibatch Loss: 3.232357\n",
            "Step 61172: Minibatch Loss: 2.926200\n",
            "Step 61174: Minibatch Loss: 3.061257\n",
            "Step 61176: Minibatch Loss: 3.125687\n",
            "Step 61178: Minibatch Loss: 3.138749\n",
            "Step 61180: Minibatch Loss: 3.030148\n",
            "Step 61182: Minibatch Loss: 3.101692\n",
            "Step 61184: Minibatch Loss: 3.038161\n",
            "Step 61186: Minibatch Loss: 2.842549\n",
            "Step 61188: Minibatch Loss: 2.992129\n",
            "Step 61190: Minibatch Loss: 3.149876\n",
            "Step 61192: Minibatch Loss: 2.986934\n",
            "Step 61194: Minibatch Loss: 2.865876\n",
            "Step 61196: Minibatch Loss: 3.039258\n",
            "Step 61198: Minibatch Loss: 3.008427\n",
            "Step 61200: Minibatch Loss: 2.927190\n",
            "Step 61202: Minibatch Loss: 3.082032\n",
            "Step 61204: Minibatch Loss: 2.870995\n",
            "Step 61206: Minibatch Loss: 2.798000\n",
            "Step 61208: Minibatch Loss: 3.002246\n",
            "Step 61210: Minibatch Loss: 3.455656\n",
            "Step 61212: Minibatch Loss: 2.645460\n",
            "Step 61214: Minibatch Loss: 3.147929\n",
            "Step 61216: Minibatch Loss: 2.841349\n",
            "Step 61218: Minibatch Loss: 3.031699\n",
            "Step 61220: Minibatch Loss: 3.252632\n",
            "Step 61222: Minibatch Loss: 3.026780\n",
            "Step 61224: Minibatch Loss: 2.851861\n",
            "Step 61226: Minibatch Loss: 2.969596\n",
            "Step 61228: Minibatch Loss: 3.062316\n",
            "Step 61230: Minibatch Loss: 2.972065\n",
            "Step 61232: Minibatch Loss: 2.988092\n",
            "Step 61234: Minibatch Loss: 2.973516\n",
            "Step 61236: Minibatch Loss: 2.823145\n",
            "Step 61238: Minibatch Loss: 2.927265\n",
            "Step 61240: Minibatch Loss: 3.221532\n",
            "Step 61242: Minibatch Loss: 2.719857\n",
            "Step 61244: Minibatch Loss: 3.007830\n",
            "Step 61246: Minibatch Loss: 3.048639\n",
            "Step 61248: Minibatch Loss: 2.928355\n",
            "Step 61250: Minibatch Loss: 2.845114\n",
            "Step 61252: Minibatch Loss: 3.070522\n",
            "Step 61254: Minibatch Loss: 2.944743\n",
            "Step 61256: Minibatch Loss: 3.218063\n",
            "Step 61258: Minibatch Loss: 3.099900\n",
            "Step 61260: Minibatch Loss: 2.805818\n",
            "Step 61262: Minibatch Loss: 3.035959\n",
            "Step 61264: Minibatch Loss: 3.141947\n",
            "Step 61266: Minibatch Loss: 3.311118\n",
            "Step 61268: Minibatch Loss: 3.145321\n",
            "Step 61270: Minibatch Loss: 3.025532\n",
            "Step 61272: Minibatch Loss: 2.865771\n",
            "Step 61274: Minibatch Loss: 2.795672\n",
            "Step 61276: Minibatch Loss: 3.175314\n",
            "Step 61278: Minibatch Loss: 2.995010\n",
            "Step 61280: Minibatch Loss: 2.994207\n",
            "Step 61282: Minibatch Loss: 3.184363\n",
            "Step 61284: Minibatch Loss: 2.863252\n",
            "Step 61286: Minibatch Loss: 2.790649\n",
            "Step 61288: Minibatch Loss: 3.009865\n",
            "Step 61290: Minibatch Loss: 2.964793\n",
            "Step 61292: Minibatch Loss: 2.919205\n",
            "Step 61294: Minibatch Loss: 2.960460\n",
            "Step 61296: Minibatch Loss: 3.137008\n",
            "Step 61298: Minibatch Loss: 2.515125\n",
            "Step 61300: Minibatch Loss: 3.126553\n",
            "Step 61302: Minibatch Loss: 2.834659\n",
            "Step 61304: Minibatch Loss: 2.897183\n",
            "Step 61306: Minibatch Loss: 3.004132\n",
            "Step 61308: Minibatch Loss: 3.052750\n",
            "Step 61310: Minibatch Loss: 2.866520\n",
            "Step 61312: Minibatch Loss: 2.977915\n",
            "Step 61314: Minibatch Loss: 3.295343\n",
            "Step 61316: Minibatch Loss: 3.513794\n",
            "Step 61318: Minibatch Loss: 2.974976\n",
            "Step 61320: Minibatch Loss: 3.011309\n",
            "Step 61322: Minibatch Loss: 2.919696\n",
            "Step 61324: Minibatch Loss: 3.072169\n",
            "Step 61326: Minibatch Loss: 2.881521\n",
            "Step 61328: Minibatch Loss: 2.711665\n",
            "Step 61330: Minibatch Loss: 2.889695\n",
            "Step 61332: Minibatch Loss: 3.130621\n",
            "Step 61334: Minibatch Loss: 2.593781\n",
            "Step 61336: Minibatch Loss: 3.309203\n",
            "Step 61338: Minibatch Loss: 3.065110\n",
            "Step 61340: Minibatch Loss: 2.779351\n",
            "Step 61342: Minibatch Loss: 2.995861\n",
            "Step 61344: Minibatch Loss: 3.012002\n",
            "Step 61346: Minibatch Loss: 2.734164\n",
            "Step 61348: Minibatch Loss: 2.928853\n",
            "Step 61350: Minibatch Loss: 2.979433\n",
            "Step 61352: Minibatch Loss: 3.127957\n",
            "Step 61354: Minibatch Loss: 2.998558\n",
            "Step 61356: Minibatch Loss: 3.082618\n",
            "Step 61358: Minibatch Loss: 2.888664\n",
            "Step 61360: Minibatch Loss: 2.909048\n",
            "Step 61362: Minibatch Loss: 2.811841\n",
            "Step 61364: Minibatch Loss: 3.083018\n",
            "Step 61366: Minibatch Loss: 2.669608\n",
            "Step 61368: Minibatch Loss: 3.131423\n",
            "Step 61370: Minibatch Loss: 2.918623\n",
            "Step 61372: Minibatch Loss: 2.862599\n",
            "Step 61374: Minibatch Loss: 3.026109\n",
            "Step 61376: Minibatch Loss: 3.065507\n",
            "Step 61378: Minibatch Loss: 3.101609\n",
            "Step 61380: Minibatch Loss: 2.997226\n",
            "Step 61382: Minibatch Loss: 3.160161\n",
            "Step 61384: Minibatch Loss: 2.751682\n",
            "Step 61386: Minibatch Loss: 2.529344\n",
            "Step 61388: Minibatch Loss: 2.999447\n",
            "Step 61390: Minibatch Loss: 3.124598\n",
            "Step 61392: Minibatch Loss: 2.762720\n",
            "Step 61394: Minibatch Loss: 2.646091\n",
            "Step 61396: Minibatch Loss: 2.981609\n",
            "Step 61398: Minibatch Loss: 3.170102\n",
            "Step 61400: Minibatch Loss: 2.875825\n",
            "Step 61402: Minibatch Loss: 3.064678\n",
            "Step 61404: Minibatch Loss: 2.998262\n",
            "Step 61406: Minibatch Loss: 2.949821\n",
            "Step 61408: Minibatch Loss: 2.807175\n",
            "Step 61410: Minibatch Loss: 2.873096\n",
            "Step 61412: Minibatch Loss: 3.215173\n",
            "Step 61414: Minibatch Loss: 2.879093\n",
            "Step 61416: Minibatch Loss: 2.931154\n",
            "Step 61418: Minibatch Loss: 2.746125\n",
            "Step 61420: Minibatch Loss: 3.019582\n",
            "Step 61422: Minibatch Loss: 2.821021\n",
            "Step 61424: Minibatch Loss: 2.921479\n",
            "Step 61426: Minibatch Loss: 3.175431\n",
            "Step 61428: Minibatch Loss: 2.932551\n",
            "Step 61430: Minibatch Loss: 3.209643\n",
            "Step 61432: Minibatch Loss: 2.993713\n",
            "Step 61434: Minibatch Loss: 2.533826\n",
            "Step 61436: Minibatch Loss: 3.009811\n",
            "Step 61438: Minibatch Loss: 2.714490\n",
            "Step 61440: Minibatch Loss: 2.849000\n",
            "Step 61442: Minibatch Loss: 2.984034\n",
            "Step 61444: Minibatch Loss: 3.105380\n",
            "Step 61446: Minibatch Loss: 3.287338\n",
            "Step 61448: Minibatch Loss: 3.064110\n",
            "Step 61450: Minibatch Loss: 2.781702\n",
            "Step 61452: Minibatch Loss: 2.811330\n",
            "Step 61454: Minibatch Loss: 2.558783\n",
            "Step 61456: Minibatch Loss: 2.944482\n",
            "Step 61458: Minibatch Loss: 2.737567\n",
            "Step 61460: Minibatch Loss: 2.880807\n",
            "Step 61462: Minibatch Loss: 3.113283\n",
            "Step 61464: Minibatch Loss: 3.008179\n",
            "Step 61466: Minibatch Loss: 2.776437\n",
            "Step 61468: Minibatch Loss: 3.133406\n",
            "Step 61470: Minibatch Loss: 2.955386\n",
            "Step 61472: Minibatch Loss: 3.007134\n",
            "Step 61474: Minibatch Loss: 2.808902\n",
            "Step 61476: Minibatch Loss: 3.016892\n",
            "Step 61478: Minibatch Loss: 3.260710\n",
            "Step 61480: Minibatch Loss: 3.126270\n",
            "Step 61482: Minibatch Loss: 3.011703\n",
            "Step 61484: Minibatch Loss: 3.036663\n",
            "Step 61486: Minibatch Loss: 2.904292\n",
            "Step 61488: Minibatch Loss: 3.152766\n",
            "Step 61490: Minibatch Loss: 3.009997\n",
            "Step 61492: Minibatch Loss: 3.115685\n",
            "Step 61494: Minibatch Loss: 2.907806\n",
            "Step 61496: Minibatch Loss: 2.925546\n",
            "Step 61498: Minibatch Loss: 2.588837\n",
            "Step 61500: Minibatch Loss: 2.797214\n",
            "Step 61502: Minibatch Loss: 3.005931\n",
            "Step 61504: Minibatch Loss: 2.798766\n",
            "Step 61506: Minibatch Loss: 3.309119\n",
            "Step 61508: Minibatch Loss: 2.791638\n",
            "Step 61510: Minibatch Loss: 2.900627\n",
            "Step 61512: Minibatch Loss: 3.049844\n",
            "Step 61514: Minibatch Loss: 3.073949\n",
            "Step 61516: Minibatch Loss: 2.793969\n",
            "Step 61518: Minibatch Loss: 2.877103\n",
            "Step 61520: Minibatch Loss: 2.782136\n",
            "Step 61522: Minibatch Loss: 3.223500\n",
            "Step 61524: Minibatch Loss: 2.817241\n",
            "Step 61526: Minibatch Loss: 2.787040\n",
            "Step 61528: Minibatch Loss: 3.043556\n",
            "Step 61530: Minibatch Loss: 2.932851\n",
            "Step 61532: Minibatch Loss: 2.821576\n",
            "Step 61534: Minibatch Loss: 3.087580\n",
            "Step 61536: Minibatch Loss: 3.032800\n",
            "Step 61538: Minibatch Loss: 3.141050\n",
            "Step 61540: Minibatch Loss: 2.935841\n",
            "Step 61542: Minibatch Loss: 2.845527\n",
            "Step 61544: Minibatch Loss: 3.015485\n",
            "Step 61546: Minibatch Loss: 2.711354\n",
            "Step 61548: Minibatch Loss: 3.011474\n",
            "Step 61550: Minibatch Loss: 2.957633\n",
            "Step 61552: Minibatch Loss: 3.258710\n",
            "Step 61554: Minibatch Loss: 2.814472\n",
            "Step 61556: Minibatch Loss: 3.154475\n",
            "Step 61558: Minibatch Loss: 3.304081\n",
            "Step 61560: Minibatch Loss: 3.144382\n",
            "Step 61562: Minibatch Loss: 2.556422\n",
            "Step 61564: Minibatch Loss: 3.052462\n",
            "Step 61566: Minibatch Loss: 2.867782\n",
            "Step 61568: Minibatch Loss: 2.966195\n",
            "Step 61570: Minibatch Loss: 2.998580\n",
            "Step 61572: Minibatch Loss: 2.858838\n",
            "Step 61574: Minibatch Loss: 2.840348\n",
            "Step 61576: Minibatch Loss: 2.806105\n",
            "Step 61578: Minibatch Loss: 2.823675\n",
            "Step 61580: Minibatch Loss: 2.710942\n",
            "Step 61582: Minibatch Loss: 3.045913\n",
            "Step 61584: Minibatch Loss: 2.786534\n",
            "Step 61586: Minibatch Loss: 3.109509\n",
            "Step 61588: Minibatch Loss: 2.992573\n",
            "Step 61590: Minibatch Loss: 3.150764\n",
            "Step 61592: Minibatch Loss: 3.043447\n",
            "Step 61594: Minibatch Loss: 2.927220\n",
            "Step 61596: Minibatch Loss: 2.927544\n",
            "Step 61598: Minibatch Loss: 2.981710\n",
            "Step 61600: Minibatch Loss: 3.198050\n",
            "Step 61602: Minibatch Loss: 3.223740\n",
            "Step 61604: Minibatch Loss: 2.902798\n",
            "Step 61606: Minibatch Loss: 3.100439\n",
            "Step 61608: Minibatch Loss: 2.750846\n",
            "Step 61610: Minibatch Loss: 3.022822\n",
            "Step 61612: Minibatch Loss: 2.753154\n",
            "Step 61614: Minibatch Loss: 3.135676\n",
            "Step 61616: Minibatch Loss: 3.282690\n",
            "Step 61618: Minibatch Loss: 2.930829\n",
            "Step 61620: Minibatch Loss: 3.056958\n",
            "Step 61622: Minibatch Loss: 3.111103\n",
            "Step 61624: Minibatch Loss: 2.853591\n",
            "Step 61626: Minibatch Loss: 2.819376\n",
            "Step 61628: Minibatch Loss: 2.877608\n",
            "Step 61630: Minibatch Loss: 2.922707\n",
            "Step 61632: Minibatch Loss: 3.095880\n",
            "Step 61634: Minibatch Loss: 2.499887\n",
            "Step 61636: Minibatch Loss: 2.991372\n",
            "Step 61638: Minibatch Loss: 3.157555\n",
            "Step 61640: Minibatch Loss: 3.142699\n",
            "Step 61642: Minibatch Loss: 3.128057\n",
            "Step 61644: Minibatch Loss: 2.952430\n",
            "Step 61646: Minibatch Loss: 3.052795\n",
            "Step 61648: Minibatch Loss: 2.778370\n",
            "Step 61650: Minibatch Loss: 3.011919\n",
            "Step 61652: Minibatch Loss: 2.951436\n",
            "Step 61654: Minibatch Loss: 2.880797\n",
            "Step 61656: Minibatch Loss: 3.008943\n",
            "Step 61658: Minibatch Loss: 2.705975\n",
            "Step 61660: Minibatch Loss: 2.947776\n",
            "Step 61662: Minibatch Loss: 2.884742\n",
            "Step 61664: Minibatch Loss: 2.849772\n",
            "Step 61666: Minibatch Loss: 2.957160\n",
            "Step 61668: Minibatch Loss: 2.606949\n",
            "Step 61670: Minibatch Loss: 3.230430\n",
            "Step 61672: Minibatch Loss: 3.176785\n",
            "Step 61674: Minibatch Loss: 2.989158\n",
            "Step 61676: Minibatch Loss: 2.985188\n",
            "Step 61678: Minibatch Loss: 2.726276\n",
            "Step 61680: Minibatch Loss: 2.984295\n",
            "Step 61682: Minibatch Loss: 2.857454\n",
            "Step 61684: Minibatch Loss: 2.674190\n",
            "Step 61686: Minibatch Loss: 2.673979\n",
            "Step 61688: Minibatch Loss: 2.981842\n",
            "Step 61690: Minibatch Loss: 2.770233\n",
            "Step 61692: Minibatch Loss: 3.117908\n",
            "Step 61694: Minibatch Loss: 2.894917\n",
            "Step 61696: Minibatch Loss: 2.802666\n",
            "Step 61698: Minibatch Loss: 3.093633\n",
            "Step 61700: Minibatch Loss: 3.375939\n",
            "Step 61702: Minibatch Loss: 2.928364\n",
            "Step 61704: Minibatch Loss: 3.225538\n",
            "Step 61706: Minibatch Loss: 2.869622\n",
            "Step 61708: Minibatch Loss: 3.215977\n",
            "Step 61710: Minibatch Loss: 3.022660\n",
            "Step 61712: Minibatch Loss: 2.500012\n",
            "Step 61714: Minibatch Loss: 3.232616\n",
            "Step 61716: Minibatch Loss: 2.959498\n",
            "Step 61718: Minibatch Loss: 3.192886\n",
            "Step 61720: Minibatch Loss: 3.115182\n",
            "Step 61722: Minibatch Loss: 2.863635\n",
            "Step 61724: Minibatch Loss: 2.970334\n",
            "Step 61726: Minibatch Loss: 3.158131\n",
            "Step 61728: Minibatch Loss: 3.057099\n",
            "Step 61730: Minibatch Loss: 2.922669\n",
            "Step 61732: Minibatch Loss: 2.824663\n",
            "Step 61734: Minibatch Loss: 2.517142\n",
            "Step 61736: Minibatch Loss: 2.940647\n",
            "Step 61738: Minibatch Loss: 2.879185\n",
            "Step 61740: Minibatch Loss: 2.955699\n",
            "Step 61742: Minibatch Loss: 3.174373\n",
            "Step 61744: Minibatch Loss: 2.777919\n",
            "Step 61746: Minibatch Loss: 3.054450\n",
            "Step 61748: Minibatch Loss: 3.102807\n",
            "Step 61750: Minibatch Loss: 3.075406\n",
            "Step 61752: Minibatch Loss: 3.094924\n",
            "Step 61754: Minibatch Loss: 2.978020\n",
            "Step 61756: Minibatch Loss: 3.016560\n",
            "Step 61758: Minibatch Loss: 2.897403\n",
            "Step 61760: Minibatch Loss: 3.022751\n",
            "Step 61762: Minibatch Loss: 3.090014\n",
            "Step 61764: Minibatch Loss: 3.132791\n",
            "Step 61766: Minibatch Loss: 2.859733\n",
            "Step 61768: Minibatch Loss: 2.908597\n",
            "Step 61770: Minibatch Loss: 2.914129\n",
            "Step 61772: Minibatch Loss: 2.957312\n",
            "Step 61774: Minibatch Loss: 3.091259\n",
            "Step 61776: Minibatch Loss: 3.087551\n",
            "Step 61778: Minibatch Loss: 3.207627\n",
            "Step 61780: Minibatch Loss: 2.794335\n",
            "Step 61782: Minibatch Loss: 3.443678\n",
            "Step 61784: Minibatch Loss: 3.332448\n",
            "Step 61786: Minibatch Loss: 2.796505\n",
            "Step 61788: Minibatch Loss: 2.881235\n",
            "Step 61790: Minibatch Loss: 3.228681\n",
            "Step 61792: Minibatch Loss: 2.922328\n",
            "Step 61794: Minibatch Loss: 2.804808\n",
            "Step 61796: Minibatch Loss: 2.955876\n",
            "Step 61798: Minibatch Loss: 3.058404\n",
            "Step 61800: Minibatch Loss: 3.007109\n",
            "Step 61802: Minibatch Loss: 3.086751\n",
            "Step 61804: Minibatch Loss: 2.807456\n",
            "Step 61806: Minibatch Loss: 3.066759\n",
            "Step 61808: Minibatch Loss: 3.411906\n",
            "Step 61810: Minibatch Loss: 2.754986\n",
            "Step 61812: Minibatch Loss: 3.162662\n",
            "Step 61814: Minibatch Loss: 2.986436\n",
            "Step 61816: Minibatch Loss: 3.193927\n",
            "Step 61818: Minibatch Loss: 3.114045\n",
            "Step 61820: Minibatch Loss: 3.114729\n",
            "Step 61822: Minibatch Loss: 3.280926\n",
            "Step 61824: Minibatch Loss: 3.244860\n",
            "Step 61826: Minibatch Loss: 3.005083\n",
            "Step 61828: Minibatch Loss: 3.001126\n",
            "Step 61830: Minibatch Loss: 2.669751\n",
            "Step 61832: Minibatch Loss: 3.120918\n",
            "Step 61834: Minibatch Loss: 3.074412\n",
            "Step 61836: Minibatch Loss: 3.272472\n",
            "Step 61838: Minibatch Loss: 2.897584\n",
            "Step 61840: Minibatch Loss: 3.007981\n",
            "Step 61842: Minibatch Loss: 3.212459\n",
            "Step 61844: Minibatch Loss: 3.220784\n",
            "Step 61846: Minibatch Loss: 3.051117\n",
            "Step 61848: Minibatch Loss: 2.985937\n",
            "Step 61850: Minibatch Loss: 2.985803\n",
            "Step 61852: Minibatch Loss: 2.829906\n",
            "Step 61854: Minibatch Loss: 3.016612\n",
            "Step 61856: Minibatch Loss: 3.161439\n",
            "Step 61858: Minibatch Loss: 2.956187\n",
            "Step 61860: Minibatch Loss: 2.884104\n",
            "Step 61862: Minibatch Loss: 3.022645\n",
            "Step 61864: Minibatch Loss: 2.933325\n",
            "Step 61866: Minibatch Loss: 3.089655\n",
            "Step 61868: Minibatch Loss: 3.164817\n",
            "Step 61870: Minibatch Loss: 2.832953\n",
            "Step 61872: Minibatch Loss: 2.886155\n",
            "Step 61874: Minibatch Loss: 2.941554\n",
            "Step 61876: Minibatch Loss: 3.536948\n",
            "Step 61878: Minibatch Loss: 2.683059\n",
            "Step 61880: Minibatch Loss: 3.157635\n",
            "Step 61882: Minibatch Loss: 2.901632\n",
            "Step 61884: Minibatch Loss: 3.030876\n",
            "Step 61886: Minibatch Loss: 3.171314\n",
            "Step 61888: Minibatch Loss: 3.072492\n",
            "Step 61890: Minibatch Loss: 2.840757\n",
            "Step 61892: Minibatch Loss: 2.753941\n",
            "Step 61894: Minibatch Loss: 3.069527\n",
            "Step 61896: Minibatch Loss: 2.816166\n",
            "Step 61898: Minibatch Loss: 2.959869\n",
            "Step 61900: Minibatch Loss: 3.002591\n",
            "Step 61902: Minibatch Loss: 2.860057\n",
            "Step 61904: Minibatch Loss: 3.001186\n",
            "Step 61906: Minibatch Loss: 3.206797\n",
            "Step 61908: Minibatch Loss: 2.699538\n",
            "Step 61910: Minibatch Loss: 3.041528\n",
            "Step 61912: Minibatch Loss: 3.034520\n",
            "Step 61914: Minibatch Loss: 2.906890\n",
            "Step 61916: Minibatch Loss: 2.907011\n",
            "Step 61918: Minibatch Loss: 2.895505\n",
            "Step 61920: Minibatch Loss: 2.832498\n",
            "Step 61922: Minibatch Loss: 3.224049\n",
            "Step 61924: Minibatch Loss: 3.078673\n",
            "Step 61926: Minibatch Loss: 2.872568\n",
            "Step 61928: Minibatch Loss: 3.072405\n",
            "Step 61930: Minibatch Loss: 3.204417\n",
            "Step 61932: Minibatch Loss: 3.369200\n",
            "Step 61934: Minibatch Loss: 3.183043\n",
            "Step 61936: Minibatch Loss: 3.067473\n",
            "Step 61938: Minibatch Loss: 2.906742\n",
            "Step 61940: Minibatch Loss: 2.713282\n",
            "Step 61942: Minibatch Loss: 3.186230\n",
            "Step 61944: Minibatch Loss: 3.025981\n",
            "Step 61946: Minibatch Loss: 2.923462\n",
            "Step 61948: Minibatch Loss: 3.169415\n",
            "Step 61950: Minibatch Loss: 2.786265\n",
            "Step 61952: Minibatch Loss: 2.767700\n",
            "Step 61954: Minibatch Loss: 2.966408\n",
            "Step 61956: Minibatch Loss: 2.931507\n",
            "Step 61958: Minibatch Loss: 3.024055\n",
            "Step 61960: Minibatch Loss: 2.917516\n",
            "Step 61962: Minibatch Loss: 3.047175\n",
            "Step 61964: Minibatch Loss: 2.452105\n",
            "Step 61966: Minibatch Loss: 3.140353\n",
            "Step 61968: Minibatch Loss: 2.848207\n",
            "Step 61970: Minibatch Loss: 2.774271\n",
            "Step 61972: Minibatch Loss: 3.063493\n",
            "Step 61974: Minibatch Loss: 3.127253\n",
            "Step 61976: Minibatch Loss: 2.857703\n",
            "Step 61978: Minibatch Loss: 2.893795\n",
            "Step 61980: Minibatch Loss: 3.285580\n",
            "Step 61982: Minibatch Loss: 3.335229\n",
            "Step 61984: Minibatch Loss: 2.957786\n",
            "Step 61986: Minibatch Loss: 3.029524\n",
            "Step 61988: Minibatch Loss: 2.876424\n",
            "Step 61990: Minibatch Loss: 3.044544\n",
            "Step 61992: Minibatch Loss: 2.888155\n",
            "Step 61994: Minibatch Loss: 2.728143\n",
            "Step 61996: Minibatch Loss: 2.917590\n",
            "Step 61998: Minibatch Loss: 3.132679\n",
            "Step 62000: Minibatch Loss: 2.554054\n",
            "Step 62002: Minibatch Loss: 3.112588\n",
            "Step 62004: Minibatch Loss: 3.033883\n",
            "Step 62006: Minibatch Loss: 2.768908\n",
            "Step 62008: Minibatch Loss: 2.929881\n",
            "Step 62010: Minibatch Loss: 2.983600\n",
            "Step 62012: Minibatch Loss: 2.810771\n",
            "Step 62014: Minibatch Loss: 3.005183\n",
            "Step 62016: Minibatch Loss: 2.902783\n",
            "Step 62018: Minibatch Loss: 3.149039\n",
            "Step 62020: Minibatch Loss: 2.962474\n",
            "Step 62022: Minibatch Loss: 3.192334\n",
            "Step 62024: Minibatch Loss: 2.941664\n",
            "Step 62026: Minibatch Loss: 2.876178\n",
            "Step 62028: Minibatch Loss: 2.641037\n",
            "Step 62030: Minibatch Loss: 3.092792\n",
            "Step 62032: Minibatch Loss: 2.652788\n",
            "Step 62034: Minibatch Loss: 3.127586\n",
            "Step 62036: Minibatch Loss: 2.864790\n",
            "Step 62038: Minibatch Loss: 2.866259\n",
            "Step 62040: Minibatch Loss: 2.983159\n",
            "Step 62042: Minibatch Loss: 3.040383\n",
            "Step 62044: Minibatch Loss: 3.079593\n",
            "Step 62046: Minibatch Loss: 2.968159\n",
            "Step 62048: Minibatch Loss: 3.193436\n",
            "Step 62050: Minibatch Loss: 2.789701\n",
            "Step 62052: Minibatch Loss: 2.641299\n",
            "Step 62054: Minibatch Loss: 2.870160\n",
            "Step 62056: Minibatch Loss: 3.001530\n",
            "Step 62058: Minibatch Loss: 2.817885\n",
            "Step 62060: Minibatch Loss: 2.733493\n",
            "Step 62062: Minibatch Loss: 2.957673\n",
            "Step 62064: Minibatch Loss: 3.084554\n",
            "Step 62066: Minibatch Loss: 2.913040\n",
            "Step 62068: Minibatch Loss: 3.070183\n",
            "Step 62070: Minibatch Loss: 2.963550\n",
            "Step 62072: Minibatch Loss: 2.958266\n",
            "Step 62074: Minibatch Loss: 2.753033\n",
            "Step 62076: Minibatch Loss: 2.839845\n",
            "Step 62078: Minibatch Loss: 3.011045\n",
            "Step 62080: Minibatch Loss: 2.823166\n",
            "Step 62082: Minibatch Loss: 2.883185\n",
            "Step 62084: Minibatch Loss: 2.743323\n",
            "Step 62086: Minibatch Loss: 3.080387\n",
            "Step 62088: Minibatch Loss: 2.922300\n",
            "Step 62090: Minibatch Loss: 2.799494\n",
            "Step 62092: Minibatch Loss: 3.184605\n",
            "Step 62094: Minibatch Loss: 2.883922\n",
            "Step 62096: Minibatch Loss: 3.201960\n",
            "Step 62098: Minibatch Loss: 2.906696\n",
            "Step 62100: Minibatch Loss: 2.532981\n",
            "Step 62102: Minibatch Loss: 2.983965\n",
            "Step 62104: Minibatch Loss: 2.706861\n",
            "Step 62106: Minibatch Loss: 2.703709\n",
            "Step 62108: Minibatch Loss: 2.897860\n",
            "Step 62110: Minibatch Loss: 3.126743\n",
            "Step 62112: Minibatch Loss: 3.313635\n",
            "Step 62114: Minibatch Loss: 3.063694\n",
            "Step 62116: Minibatch Loss: 2.703090\n",
            "Step 62118: Minibatch Loss: 2.813346\n",
            "Step 62120: Minibatch Loss: 2.616763\n",
            "Step 62122: Minibatch Loss: 2.866668\n",
            "Step 62124: Minibatch Loss: 2.740420\n",
            "Step 62126: Minibatch Loss: 2.787346\n",
            "Step 62128: Minibatch Loss: 3.119848\n",
            "Step 62130: Minibatch Loss: 2.952560\n",
            "Step 62132: Minibatch Loss: 2.761162\n",
            "Step 62134: Minibatch Loss: 3.029556\n",
            "Step 62136: Minibatch Loss: 3.062366\n",
            "Step 62138: Minibatch Loss: 2.889095\n",
            "Step 62140: Minibatch Loss: 2.777833\n",
            "Step 62142: Minibatch Loss: 3.023004\n",
            "Step 62144: Minibatch Loss: 3.273910\n",
            "Step 62146: Minibatch Loss: 3.039602\n",
            "Step 62148: Minibatch Loss: 3.002154\n",
            "Step 62150: Minibatch Loss: 3.106124\n",
            "Step 62152: Minibatch Loss: 2.965794\n",
            "Step 62154: Minibatch Loss: 3.319296\n",
            "Step 62156: Minibatch Loss: 3.111905\n",
            "Step 62158: Minibatch Loss: 2.952446\n",
            "Step 62160: Minibatch Loss: 2.944533\n",
            "Step 62162: Minibatch Loss: 2.915475\n",
            "Step 62164: Minibatch Loss: 2.576234\n",
            "Step 62166: Minibatch Loss: 2.760893\n",
            "Step 62168: Minibatch Loss: 3.097741\n",
            "Step 62170: Minibatch Loss: 2.811270\n",
            "Step 62172: Minibatch Loss: 3.259580\n",
            "Step 62174: Minibatch Loss: 2.860010\n",
            "Step 62176: Minibatch Loss: 2.976584\n",
            "Step 62178: Minibatch Loss: 2.951798\n",
            "Step 62180: Minibatch Loss: 3.017413\n",
            "Step 62182: Minibatch Loss: 2.756249\n",
            "Step 62184: Minibatch Loss: 2.896260\n",
            "Step 62186: Minibatch Loss: 2.663256\n",
            "Step 62188: Minibatch Loss: 3.132096\n",
            "Step 62190: Minibatch Loss: 2.800056\n",
            "Step 62192: Minibatch Loss: 2.874039\n",
            "Step 62194: Minibatch Loss: 3.055155\n",
            "Step 62196: Minibatch Loss: 2.917703\n",
            "Step 62198: Minibatch Loss: 2.983106\n",
            "Step 62200: Minibatch Loss: 3.080401\n",
            "Step 62202: Minibatch Loss: 2.982046\n",
            "Step 62204: Minibatch Loss: 3.221590\n",
            "Step 62206: Minibatch Loss: 2.819146\n",
            "Step 62208: Minibatch Loss: 2.828716\n",
            "Step 62210: Minibatch Loss: 2.962744\n",
            "Step 62212: Minibatch Loss: 2.656427\n",
            "Step 62214: Minibatch Loss: 2.904611\n",
            "Step 62216: Minibatch Loss: 2.908386\n",
            "Step 62218: Minibatch Loss: 3.312514\n",
            "Step 62220: Minibatch Loss: 2.858632\n",
            "Step 62222: Minibatch Loss: 3.179647\n",
            "Step 62224: Minibatch Loss: 3.296860\n",
            "Step 62226: Minibatch Loss: 3.277183\n",
            "Step 62228: Minibatch Loss: 2.532610\n",
            "Step 62230: Minibatch Loss: 2.976720\n",
            "Step 62232: Minibatch Loss: 2.848609\n",
            "Step 62234: Minibatch Loss: 2.871484\n",
            "Step 62236: Minibatch Loss: 3.060974\n",
            "Step 62238: Minibatch Loss: 2.770513\n",
            "Step 62240: Minibatch Loss: 2.745970\n",
            "Step 62242: Minibatch Loss: 2.719749\n",
            "Step 62244: Minibatch Loss: 2.837121\n",
            "Step 62246: Minibatch Loss: 2.773316\n",
            "Step 62248: Minibatch Loss: 2.988419\n",
            "Step 62250: Minibatch Loss: 2.785688\n",
            "Step 62252: Minibatch Loss: 3.131694\n",
            "Step 62254: Minibatch Loss: 3.045375\n",
            "Step 62256: Minibatch Loss: 3.040602\n",
            "Step 62258: Minibatch Loss: 3.068383\n",
            "Step 62260: Minibatch Loss: 2.999180\n",
            "Step 62262: Minibatch Loss: 2.915108\n",
            "Step 62264: Minibatch Loss: 2.975188\n",
            "Step 62266: Minibatch Loss: 3.178418\n",
            "Step 62268: Minibatch Loss: 3.208080\n",
            "Step 62270: Minibatch Loss: 2.873202\n",
            "Step 62272: Minibatch Loss: 3.028265\n",
            "Step 62274: Minibatch Loss: 2.752174\n",
            "Step 62276: Minibatch Loss: 3.000609\n",
            "Step 62278: Minibatch Loss: 2.828412\n",
            "Step 62280: Minibatch Loss: 3.096992\n",
            "Step 62282: Minibatch Loss: 3.279082\n",
            "Step 62284: Minibatch Loss: 2.977274\n",
            "Step 62286: Minibatch Loss: 3.061778\n",
            "Step 62288: Minibatch Loss: 3.009778\n",
            "Step 62290: Minibatch Loss: 2.820844\n",
            "Step 62292: Minibatch Loss: 2.792624\n",
            "Step 62294: Minibatch Loss: 2.759095\n",
            "Step 62296: Minibatch Loss: 3.110604\n",
            "Step 62298: Minibatch Loss: 3.128892\n",
            "Step 62300: Minibatch Loss: 2.508573\n",
            "Step 62302: Minibatch Loss: 3.035780\n",
            "Step 62304: Minibatch Loss: 3.102237\n",
            "Step 62306: Minibatch Loss: 2.989801\n",
            "Step 62308: Minibatch Loss: 3.010492\n",
            "Step 62310: Minibatch Loss: 2.836786\n",
            "Step 62312: Minibatch Loss: 3.010067\n",
            "Step 62314: Minibatch Loss: 2.877061\n",
            "Step 62316: Minibatch Loss: 3.003227\n",
            "Step 62318: Minibatch Loss: 2.914247\n",
            "Step 62320: Minibatch Loss: 2.824089\n",
            "Step 62322: Minibatch Loss: 2.997506\n",
            "Step 62324: Minibatch Loss: 2.732849\n",
            "Step 62326: Minibatch Loss: 3.063359\n",
            "Step 62328: Minibatch Loss: 2.976084\n",
            "Step 62330: Minibatch Loss: 2.944476\n",
            "Step 62332: Minibatch Loss: 2.788255\n",
            "Step 62334: Minibatch Loss: 2.691403\n",
            "Step 62336: Minibatch Loss: 3.244388\n",
            "Step 62338: Minibatch Loss: 3.159940\n",
            "Step 62340: Minibatch Loss: 2.989990\n",
            "Step 62342: Minibatch Loss: 3.048196\n",
            "Step 62344: Minibatch Loss: 2.791835\n",
            "Step 62346: Minibatch Loss: 2.812704\n",
            "Step 62348: Minibatch Loss: 2.904207\n",
            "Step 62350: Minibatch Loss: 2.686469\n",
            "Step 62352: Minibatch Loss: 2.772227\n",
            "Step 62354: Minibatch Loss: 2.902530\n",
            "Step 62356: Minibatch Loss: 2.751204\n",
            "Step 62358: Minibatch Loss: 3.092633\n",
            "Step 62360: Minibatch Loss: 2.869304\n",
            "Step 62362: Minibatch Loss: 2.680238\n",
            "Step 62364: Minibatch Loss: 2.973343\n",
            "Step 62366: Minibatch Loss: 3.426042\n",
            "Step 62368: Minibatch Loss: 2.957091\n",
            "Step 62370: Minibatch Loss: 3.456192\n",
            "Step 62372: Minibatch Loss: 2.826440\n",
            "Step 62374: Minibatch Loss: 3.229465\n",
            "Step 62376: Minibatch Loss: 3.046181\n",
            "Step 62378: Minibatch Loss: 2.588166\n",
            "Step 62380: Minibatch Loss: 3.299062\n",
            "Step 62382: Minibatch Loss: 2.955787\n",
            "Step 62384: Minibatch Loss: 3.186708\n",
            "Step 62386: Minibatch Loss: 3.061343\n",
            "Step 62388: Minibatch Loss: 2.864284\n",
            "Step 62390: Minibatch Loss: 2.880725\n",
            "Step 62392: Minibatch Loss: 3.188177\n",
            "Step 62394: Minibatch Loss: 3.054305\n",
            "Step 62396: Minibatch Loss: 2.922328\n",
            "Step 62398: Minibatch Loss: 2.820172\n",
            "Step 62400: Minibatch Loss: 2.530474\n",
            "Step 62402: Minibatch Loss: 3.000854\n",
            "Step 62404: Minibatch Loss: 2.833572\n",
            "Step 62406: Minibatch Loss: 2.928679\n",
            "Step 62408: Minibatch Loss: 3.133364\n",
            "Step 62410: Minibatch Loss: 2.784199\n",
            "Step 62412: Minibatch Loss: 3.094752\n",
            "Step 62414: Minibatch Loss: 3.076808\n",
            "Step 62416: Minibatch Loss: 3.109495\n",
            "Step 62418: Minibatch Loss: 3.060183\n",
            "Step 62420: Minibatch Loss: 3.004743\n",
            "Step 62422: Minibatch Loss: 2.916782\n",
            "Step 62424: Minibatch Loss: 2.872535\n",
            "Step 62426: Minibatch Loss: 3.057850\n",
            "Step 62428: Minibatch Loss: 2.917950\n",
            "Step 62430: Minibatch Loss: 3.117913\n",
            "Step 62432: Minibatch Loss: 2.790028\n",
            "Step 62434: Minibatch Loss: 2.936754\n",
            "Step 62436: Minibatch Loss: 3.084530\n",
            "Step 62438: Minibatch Loss: 3.048400\n",
            "Step 62440: Minibatch Loss: 3.081267\n",
            "Step 62442: Minibatch Loss: 3.011085\n",
            "Step 62444: Minibatch Loss: 3.149441\n",
            "Step 62446: Minibatch Loss: 2.881959\n",
            "Step 62448: Minibatch Loss: 3.387485\n",
            "Step 62450: Minibatch Loss: 3.178895\n",
            "Step 62452: Minibatch Loss: 2.767507\n",
            "Step 62454: Minibatch Loss: 2.887164\n",
            "Step 62456: Minibatch Loss: 3.167998\n",
            "Step 62458: Minibatch Loss: 2.916120\n",
            "Step 62460: Minibatch Loss: 2.774243\n",
            "Step 62462: Minibatch Loss: 2.792246\n",
            "Step 62464: Minibatch Loss: 3.199037\n",
            "Step 62466: Minibatch Loss: 3.182563\n",
            "Step 62468: Minibatch Loss: 3.142262\n",
            "Step 62470: Minibatch Loss: 3.006346\n",
            "Step 62472: Minibatch Loss: 3.084568\n",
            "Step 62474: Minibatch Loss: 3.397253\n",
            "Step 62476: Minibatch Loss: 2.740155\n",
            "Step 62478: Minibatch Loss: 3.137389\n",
            "Step 62480: Minibatch Loss: 2.999812\n",
            "Step 62482: Minibatch Loss: 3.160550\n",
            "Step 62484: Minibatch Loss: 3.136535\n",
            "Step 62486: Minibatch Loss: 3.066962\n",
            "Step 62488: Minibatch Loss: 3.299918\n",
            "Step 62490: Minibatch Loss: 3.247990\n",
            "Step 62492: Minibatch Loss: 2.949441\n",
            "Step 62494: Minibatch Loss: 2.911714\n",
            "Step 62496: Minibatch Loss: 2.673799\n",
            "Step 62498: Minibatch Loss: 3.115357\n",
            "Step 62500: Minibatch Loss: 3.077366\n",
            "Step 62502: Minibatch Loss: 3.386670\n",
            "Step 62504: Minibatch Loss: 2.924608\n",
            "Step 62506: Minibatch Loss: 2.925285\n",
            "Step 62508: Minibatch Loss: 3.067818\n",
            "Step 62510: Minibatch Loss: 3.211526\n",
            "Step 62512: Minibatch Loss: 3.092642\n",
            "Step 62514: Minibatch Loss: 3.056641\n",
            "Step 62516: Minibatch Loss: 2.979107\n",
            "Step 62518: Minibatch Loss: 2.964454\n",
            "Step 62520: Minibatch Loss: 2.864167\n",
            "Step 62522: Minibatch Loss: 3.153903\n",
            "Step 62524: Minibatch Loss: 2.912861\n",
            "Step 62526: Minibatch Loss: 2.810853\n",
            "Step 62528: Minibatch Loss: 3.013617\n",
            "Step 62530: Minibatch Loss: 2.929816\n",
            "Step 62532: Minibatch Loss: 2.995269\n",
            "Step 62534: Minibatch Loss: 3.105463\n",
            "Step 62536: Minibatch Loss: 2.916684\n",
            "Step 62538: Minibatch Loss: 2.762024\n",
            "Step 62540: Minibatch Loss: 3.037166\n",
            "Step 62542: Minibatch Loss: 3.352215\n",
            "Step 62544: Minibatch Loss: 2.608944\n",
            "Step 62546: Minibatch Loss: 3.223887\n",
            "Step 62548: Minibatch Loss: 2.920403\n",
            "Step 62550: Minibatch Loss: 3.080548\n",
            "Step 62552: Minibatch Loss: 3.061185\n",
            "Step 62554: Minibatch Loss: 2.999942\n",
            "Step 62556: Minibatch Loss: 2.848045\n",
            "Step 62558: Minibatch Loss: 2.966460\n",
            "Step 62560: Minibatch Loss: 3.129725\n",
            "Step 62562: Minibatch Loss: 2.870407\n",
            "Step 62564: Minibatch Loss: 2.937387\n",
            "Step 62566: Minibatch Loss: 2.914585\n",
            "Step 62568: Minibatch Loss: 2.918815\n",
            "Step 62570: Minibatch Loss: 3.057296\n",
            "Step 62572: Minibatch Loss: 3.207696\n",
            "Step 62574: Minibatch Loss: 2.611208\n",
            "Step 62576: Minibatch Loss: 2.957446\n",
            "Step 62578: Minibatch Loss: 2.934620\n",
            "Step 62580: Minibatch Loss: 2.905044\n",
            "Step 62582: Minibatch Loss: 2.833813\n",
            "Step 62584: Minibatch Loss: 2.934110\n",
            "Step 62586: Minibatch Loss: 2.911179\n",
            "Step 62588: Minibatch Loss: 3.246342\n",
            "Step 62590: Minibatch Loss: 3.003843\n",
            "Step 62592: Minibatch Loss: 2.892652\n",
            "Step 62594: Minibatch Loss: 3.048209\n",
            "Step 62596: Minibatch Loss: 3.109848\n",
            "Step 62598: Minibatch Loss: 3.240997\n",
            "Step 62600: Minibatch Loss: 3.166160\n",
            "Step 62602: Minibatch Loss: 2.944836\n",
            "Step 62604: Minibatch Loss: 2.893726\n",
            "Step 62606: Minibatch Loss: 2.777295\n",
            "Step 62608: Minibatch Loss: 3.133871\n",
            "Step 62610: Minibatch Loss: 3.021233\n",
            "Step 62612: Minibatch Loss: 2.950550\n",
            "Step 62614: Minibatch Loss: 3.104400\n",
            "Step 62616: Minibatch Loss: 2.779068\n",
            "Step 62618: Minibatch Loss: 2.733029\n",
            "Step 62620: Minibatch Loss: 2.949441\n",
            "Step 62622: Minibatch Loss: 2.844786\n",
            "Step 62624: Minibatch Loss: 2.939632\n",
            "Step 62626: Minibatch Loss: 2.807587\n",
            "Step 62628: Minibatch Loss: 3.119462\n",
            "Step 62630: Minibatch Loss: 2.550837\n",
            "Step 62632: Minibatch Loss: 3.102346\n",
            "Step 62634: Minibatch Loss: 2.933575\n",
            "Step 62636: Minibatch Loss: 2.857939\n",
            "Step 62638: Minibatch Loss: 3.077159\n",
            "Step 62640: Minibatch Loss: 3.025211\n",
            "Step 62642: Minibatch Loss: 2.740496\n",
            "Step 62644: Minibatch Loss: 2.927419\n",
            "Step 62646: Minibatch Loss: 3.313816\n",
            "Step 62648: Minibatch Loss: 3.541480\n",
            "Step 62650: Minibatch Loss: 2.931613\n",
            "Step 62652: Minibatch Loss: 2.972933\n",
            "Step 62654: Minibatch Loss: 3.014617\n",
            "Step 62656: Minibatch Loss: 3.037716\n",
            "Step 62658: Minibatch Loss: 2.862602\n",
            "Step 62660: Minibatch Loss: 2.648583\n",
            "Step 62662: Minibatch Loss: 2.827554\n",
            "Step 62664: Minibatch Loss: 3.197096\n",
            "Step 62666: Minibatch Loss: 2.720912\n",
            "Step 62668: Minibatch Loss: 3.160934\n",
            "Step 62670: Minibatch Loss: 3.116672\n",
            "Step 62672: Minibatch Loss: 2.785998\n",
            "Step 62674: Minibatch Loss: 2.900967\n",
            "Step 62676: Minibatch Loss: 3.014070\n",
            "Step 62678: Minibatch Loss: 2.727534\n",
            "Step 62680: Minibatch Loss: 2.922789\n",
            "Step 62682: Minibatch Loss: 2.869211\n",
            "Step 62684: Minibatch Loss: 3.103027\n",
            "Step 62686: Minibatch Loss: 2.952789\n",
            "Step 62688: Minibatch Loss: 3.146798\n",
            "Step 62690: Minibatch Loss: 2.879653\n",
            "Step 62692: Minibatch Loss: 2.936659\n",
            "Step 62694: Minibatch Loss: 2.769320\n",
            "Step 62696: Minibatch Loss: 3.110696\n",
            "Step 62698: Minibatch Loss: 2.671841\n",
            "Step 62700: Minibatch Loss: 3.115260\n",
            "Step 62702: Minibatch Loss: 2.817893\n",
            "Step 62704: Minibatch Loss: 2.847063\n",
            "Step 62706: Minibatch Loss: 3.052789\n",
            "Step 62708: Minibatch Loss: 2.981289\n",
            "Step 62710: Minibatch Loss: 3.086724\n",
            "Step 62712: Minibatch Loss: 3.051175\n",
            "Step 62714: Minibatch Loss: 3.002007\n",
            "Step 62716: Minibatch Loss: 2.698901\n",
            "Step 62718: Minibatch Loss: 2.548215\n",
            "Step 62720: Minibatch Loss: 2.885487\n",
            "Step 62722: Minibatch Loss: 3.079401\n",
            "Step 62724: Minibatch Loss: 2.703409\n",
            "Step 62726: Minibatch Loss: 2.664354\n",
            "Step 62728: Minibatch Loss: 2.917386\n",
            "Step 62730: Minibatch Loss: 3.145047\n",
            "Step 62732: Minibatch Loss: 2.874412\n",
            "Step 62734: Minibatch Loss: 3.066792\n",
            "Step 62736: Minibatch Loss: 2.905070\n",
            "Step 62738: Minibatch Loss: 2.958985\n",
            "Step 62740: Minibatch Loss: 2.801132\n",
            "Step 62742: Minibatch Loss: 2.936154\n",
            "Step 62744: Minibatch Loss: 3.025686\n",
            "Step 62746: Minibatch Loss: 2.827110\n",
            "Step 62748: Minibatch Loss: 2.837733\n",
            "Step 62750: Minibatch Loss: 2.769001\n",
            "Step 62752: Minibatch Loss: 3.066615\n",
            "Step 62754: Minibatch Loss: 2.834547\n",
            "Step 62756: Minibatch Loss: 2.993401\n",
            "Step 62758: Minibatch Loss: 3.216923\n",
            "Step 62760: Minibatch Loss: 2.922811\n",
            "Step 62762: Minibatch Loss: 3.098892\n",
            "Step 62764: Minibatch Loss: 2.916147\n",
            "Step 62766: Minibatch Loss: 2.685620\n",
            "Step 62768: Minibatch Loss: 3.088439\n",
            "Step 62770: Minibatch Loss: 2.714350\n",
            "Step 62772: Minibatch Loss: 2.792257\n",
            "Step 62774: Minibatch Loss: 2.931601\n",
            "Step 62776: Minibatch Loss: 3.073619\n",
            "Step 62778: Minibatch Loss: 3.207480\n",
            "Step 62780: Minibatch Loss: 3.061786\n",
            "Step 62782: Minibatch Loss: 2.641705\n",
            "Step 62784: Minibatch Loss: 2.796971\n",
            "Step 62786: Minibatch Loss: 2.578561\n",
            "Step 62788: Minibatch Loss: 2.919404\n",
            "Step 62790: Minibatch Loss: 2.716845\n",
            "Step 62792: Minibatch Loss: 2.680878\n",
            "Step 62794: Minibatch Loss: 3.089533\n",
            "Step 62796: Minibatch Loss: 2.962404\n",
            "Step 62798: Minibatch Loss: 2.718300\n",
            "Step 62800: Minibatch Loss: 2.991156\n",
            "Step 62802: Minibatch Loss: 3.064673\n",
            "Step 62804: Minibatch Loss: 3.066625\n",
            "Step 62806: Minibatch Loss: 2.787802\n",
            "Step 62808: Minibatch Loss: 3.003019\n",
            "Step 62810: Minibatch Loss: 3.203708\n",
            "Step 62812: Minibatch Loss: 2.990601\n",
            "Step 62814: Minibatch Loss: 2.947608\n",
            "Step 62816: Minibatch Loss: 2.993298\n",
            "Step 62818: Minibatch Loss: 2.988576\n",
            "Step 62820: Minibatch Loss: 3.159215\n",
            "Step 62822: Minibatch Loss: 2.997897\n",
            "Step 62824: Minibatch Loss: 3.059593\n",
            "Step 62826: Minibatch Loss: 2.988966\n",
            "Step 62828: Minibatch Loss: 2.910234\n",
            "Step 62830: Minibatch Loss: 2.557662\n",
            "Step 62832: Minibatch Loss: 2.802958\n",
            "Step 62834: Minibatch Loss: 2.972786\n",
            "Step 62836: Minibatch Loss: 2.713717\n",
            "Step 62838: Minibatch Loss: 3.134490\n",
            "Step 62840: Minibatch Loss: 2.823560\n",
            "Step 62842: Minibatch Loss: 2.881760\n",
            "Step 62844: Minibatch Loss: 3.129638\n",
            "Step 62846: Minibatch Loss: 3.082283\n",
            "Step 62848: Minibatch Loss: 2.800944\n",
            "Step 62850: Minibatch Loss: 2.849329\n",
            "Step 62852: Minibatch Loss: 2.722216\n",
            "Step 62854: Minibatch Loss: 3.112681\n",
            "Step 62856: Minibatch Loss: 2.789884\n",
            "Step 62858: Minibatch Loss: 2.807360\n",
            "Step 62860: Minibatch Loss: 2.990157\n",
            "Step 62862: Minibatch Loss: 2.946587\n",
            "Step 62864: Minibatch Loss: 2.840668\n",
            "Step 62866: Minibatch Loss: 3.038338\n",
            "Step 62868: Minibatch Loss: 3.083629\n",
            "Step 62870: Minibatch Loss: 3.149523\n",
            "Step 62872: Minibatch Loss: 2.943278\n",
            "Step 62874: Minibatch Loss: 2.854280\n",
            "Step 62876: Minibatch Loss: 2.950774\n",
            "Step 62878: Minibatch Loss: 2.742408\n",
            "Step 62880: Minibatch Loss: 2.906013\n",
            "Step 62882: Minibatch Loss: 2.906281\n",
            "Step 62884: Minibatch Loss: 3.243923\n",
            "Step 62886: Minibatch Loss: 2.827377\n",
            "Step 62888: Minibatch Loss: 3.064771\n",
            "Step 62890: Minibatch Loss: 3.207142\n",
            "Step 62892: Minibatch Loss: 3.184759\n",
            "Step 62894: Minibatch Loss: 2.562933\n",
            "Step 62896: Minibatch Loss: 2.953175\n",
            "Step 62898: Minibatch Loss: 2.821637\n",
            "Step 62900: Minibatch Loss: 2.959053\n",
            "Step 62902: Minibatch Loss: 2.997257\n",
            "Step 62904: Minibatch Loss: 2.612170\n",
            "Step 62906: Minibatch Loss: 2.779504\n",
            "Step 62908: Minibatch Loss: 2.753608\n",
            "Step 62910: Minibatch Loss: 2.874144\n",
            "Step 62912: Minibatch Loss: 2.728785\n",
            "Step 62914: Minibatch Loss: 3.009603\n",
            "Step 62916: Minibatch Loss: 2.726601\n",
            "Step 62918: Minibatch Loss: 3.175967\n",
            "Step 62920: Minibatch Loss: 3.077254\n",
            "Step 62922: Minibatch Loss: 3.045308\n",
            "Step 62924: Minibatch Loss: 2.925962\n",
            "Step 62926: Minibatch Loss: 2.951965\n",
            "Step 62928: Minibatch Loss: 2.886093\n",
            "Step 62930: Minibatch Loss: 3.000053\n",
            "Step 62932: Minibatch Loss: 3.275506\n",
            "Step 62934: Minibatch Loss: 3.258366\n",
            "Step 62936: Minibatch Loss: 2.955688\n",
            "Step 62938: Minibatch Loss: 3.046512\n",
            "Step 62940: Minibatch Loss: 2.782905\n",
            "Step 62942: Minibatch Loss: 3.011870\n",
            "Step 62944: Minibatch Loss: 2.696870\n",
            "Step 62946: Minibatch Loss: 3.029272\n",
            "Step 62948: Minibatch Loss: 3.355630\n",
            "Step 62950: Minibatch Loss: 2.864288\n",
            "Step 62952: Minibatch Loss: 3.005687\n",
            "Step 62954: Minibatch Loss: 3.051697\n",
            "Step 62956: Minibatch Loss: 2.789980\n",
            "Step 62958: Minibatch Loss: 2.774602\n",
            "Step 62960: Minibatch Loss: 2.929831\n",
            "Step 62962: Minibatch Loss: 2.953891\n",
            "Step 62964: Minibatch Loss: 3.100193\n",
            "Step 62966: Minibatch Loss: 2.630735\n",
            "Step 62968: Minibatch Loss: 2.927333\n",
            "Step 62970: Minibatch Loss: 3.105896\n",
            "Step 62972: Minibatch Loss: 3.021195\n",
            "Step 62974: Minibatch Loss: 2.962656\n",
            "Step 62976: Minibatch Loss: 2.906998\n",
            "Step 62978: Minibatch Loss: 2.918185\n",
            "Step 62980: Minibatch Loss: 2.828977\n",
            "Step 62982: Minibatch Loss: 2.972003\n",
            "Step 62984: Minibatch Loss: 2.984030\n",
            "Step 62986: Minibatch Loss: 2.848445\n",
            "Step 62988: Minibatch Loss: 3.025816\n",
            "Step 62990: Minibatch Loss: 2.758786\n",
            "Step 62992: Minibatch Loss: 3.200481\n",
            "Step 62994: Minibatch Loss: 2.886559\n",
            "Step 62996: Minibatch Loss: 2.810593\n",
            "Step 62998: Minibatch Loss: 2.864639\n",
            "Step 63000: Minibatch Loss: 2.733669\n",
            "Step 63002: Minibatch Loss: 3.235501\n",
            "Step 63004: Minibatch Loss: 3.185237\n",
            "Step 63006: Minibatch Loss: 2.974652\n",
            "Step 63008: Minibatch Loss: 3.011172\n",
            "Step 63010: Minibatch Loss: 2.676678\n",
            "Step 63012: Minibatch Loss: 2.872049\n",
            "Step 63014: Minibatch Loss: 2.882767\n",
            "Step 63016: Minibatch Loss: 2.724855\n",
            "Step 63018: Minibatch Loss: 2.747879\n",
            "Step 63020: Minibatch Loss: 2.881601\n",
            "Step 63022: Minibatch Loss: 2.929172\n",
            "Step 63024: Minibatch Loss: 3.003516\n",
            "Step 63026: Minibatch Loss: 2.857090\n",
            "Step 63028: Minibatch Loss: 2.823617\n",
            "Step 63030: Minibatch Loss: 3.054679\n",
            "Step 63032: Minibatch Loss: 3.412024\n",
            "Step 63034: Minibatch Loss: 2.999750\n",
            "Step 63036: Minibatch Loss: 3.378032\n",
            "Step 63038: Minibatch Loss: 2.790744\n",
            "Step 63040: Minibatch Loss: 3.235938\n",
            "Step 63042: Minibatch Loss: 2.972006\n",
            "Step 63044: Minibatch Loss: 2.678415\n",
            "Step 63046: Minibatch Loss: 3.340562\n",
            "Step 63048: Minibatch Loss: 3.016179\n",
            "Step 63050: Minibatch Loss: 3.281749\n",
            "Step 63052: Minibatch Loss: 2.992484\n",
            "Step 63054: Minibatch Loss: 2.856416\n",
            "Step 63056: Minibatch Loss: 2.897193\n",
            "Step 63058: Minibatch Loss: 3.067338\n",
            "Step 63060: Minibatch Loss: 3.131650\n",
            "Step 63062: Minibatch Loss: 2.852783\n",
            "Step 63064: Minibatch Loss: 2.824674\n",
            "Step 63066: Minibatch Loss: 2.530241\n",
            "Step 63068: Minibatch Loss: 2.917688\n",
            "Step 63070: Minibatch Loss: 2.823630\n",
            "Step 63072: Minibatch Loss: 3.031516\n",
            "Step 63074: Minibatch Loss: 3.104000\n",
            "Step 63076: Minibatch Loss: 2.752326\n",
            "Step 63078: Minibatch Loss: 3.108143\n",
            "Step 63080: Minibatch Loss: 3.085640\n",
            "Step 63082: Minibatch Loss: 3.013598\n",
            "Step 63084: Minibatch Loss: 2.957836\n",
            "Step 63086: Minibatch Loss: 3.012768\n",
            "Step 63088: Minibatch Loss: 2.895181\n",
            "Step 63090: Minibatch Loss: 2.909863\n",
            "Step 63092: Minibatch Loss: 3.115735\n",
            "Step 63094: Minibatch Loss: 2.960671\n",
            "Step 63096: Minibatch Loss: 3.115416\n",
            "Step 63098: Minibatch Loss: 2.814496\n",
            "Step 63100: Minibatch Loss: 2.902936\n",
            "Step 63102: Minibatch Loss: 2.968194\n",
            "Step 63104: Minibatch Loss: 3.001536\n",
            "Step 63106: Minibatch Loss: 3.062435\n",
            "Step 63108: Minibatch Loss: 2.940672\n",
            "Step 63110: Minibatch Loss: 3.119451\n",
            "Step 63112: Minibatch Loss: 2.935235\n",
            "Step 63114: Minibatch Loss: 3.312949\n",
            "Step 63116: Minibatch Loss: 3.201155\n",
            "Step 63118: Minibatch Loss: 2.847760\n",
            "Step 63120: Minibatch Loss: 2.845992\n",
            "Step 63122: Minibatch Loss: 3.126705\n",
            "Step 63124: Minibatch Loss: 2.901679\n",
            "Step 63126: Minibatch Loss: 2.816857\n",
            "Step 63128: Minibatch Loss: 3.018504\n",
            "Step 63130: Minibatch Loss: 3.128633\n",
            "Step 63132: Minibatch Loss: 3.073381\n",
            "Step 63134: Minibatch Loss: 3.099640\n",
            "Step 63136: Minibatch Loss: 2.978143\n",
            "Step 63138: Minibatch Loss: 3.089689\n",
            "Step 63140: Minibatch Loss: 3.360083\n",
            "Step 63142: Minibatch Loss: 2.768334\n",
            "Step 63144: Minibatch Loss: 3.190827\n",
            "Step 63146: Minibatch Loss: 2.937739\n",
            "Step 63148: Minibatch Loss: 3.164668\n",
            "Step 63150: Minibatch Loss: 3.152202\n",
            "Step 63152: Minibatch Loss: 2.993983\n",
            "Step 63154: Minibatch Loss: 3.245843\n",
            "Step 63156: Minibatch Loss: 3.228290\n",
            "Step 63158: Minibatch Loss: 2.998471\n",
            "Step 63160: Minibatch Loss: 2.831999\n",
            "Step 63162: Minibatch Loss: 2.630229\n",
            "Step 63164: Minibatch Loss: 3.255895\n",
            "Step 63166: Minibatch Loss: 3.051781\n",
            "Step 63168: Minibatch Loss: 3.259368\n",
            "Step 63170: Minibatch Loss: 2.881620\n",
            "Step 63172: Minibatch Loss: 3.011282\n",
            "Step 63174: Minibatch Loss: 3.112552\n",
            "Step 63176: Minibatch Loss: 3.117571\n",
            "Step 63178: Minibatch Loss: 2.960858\n",
            "Step 63180: Minibatch Loss: 3.067549\n",
            "Step 63182: Minibatch Loss: 2.941380\n",
            "Step 63184: Minibatch Loss: 2.903002\n",
            "Step 63186: Minibatch Loss: 2.886542\n",
            "Step 63188: Minibatch Loss: 3.100392\n",
            "Step 63190: Minibatch Loss: 2.868680\n",
            "Step 63192: Minibatch Loss: 2.784631\n",
            "Step 63194: Minibatch Loss: 3.023051\n",
            "Step 63196: Minibatch Loss: 2.902594\n",
            "Step 63198: Minibatch Loss: 3.095854\n",
            "Step 63200: Minibatch Loss: 3.153829\n",
            "Step 63202: Minibatch Loss: 2.864260\n",
            "Step 63204: Minibatch Loss: 2.722568\n",
            "Step 63206: Minibatch Loss: 2.980608\n",
            "Step 63208: Minibatch Loss: 3.442477\n",
            "Step 63210: Minibatch Loss: 2.571050\n",
            "Step 63212: Minibatch Loss: 3.180060\n",
            "Step 63214: Minibatch Loss: 2.881108\n",
            "Step 63216: Minibatch Loss: 2.917672\n",
            "Step 63218: Minibatch Loss: 3.033269\n",
            "Step 63220: Minibatch Loss: 3.043103\n",
            "Step 63222: Minibatch Loss: 2.847035\n",
            "Step 63224: Minibatch Loss: 2.906566\n",
            "Step 63226: Minibatch Loss: 3.097350\n",
            "Step 63228: Minibatch Loss: 2.907783\n",
            "Step 63230: Minibatch Loss: 2.850825\n",
            "Step 63232: Minibatch Loss: 3.011837\n",
            "Step 63234: Minibatch Loss: 2.842458\n",
            "Step 63236: Minibatch Loss: 2.958544\n",
            "Step 63238: Minibatch Loss: 3.243095\n",
            "Step 63240: Minibatch Loss: 2.785002\n",
            "Step 63242: Minibatch Loss: 3.099721\n",
            "Step 63244: Minibatch Loss: 3.011977\n",
            "Step 63246: Minibatch Loss: 2.766508\n",
            "Step 63248: Minibatch Loss: 2.849214\n",
            "Step 63250: Minibatch Loss: 2.845572\n",
            "Step 63252: Minibatch Loss: 2.825862\n",
            "Step 63254: Minibatch Loss: 3.244378\n",
            "Step 63256: Minibatch Loss: 3.054211\n",
            "Step 63258: Minibatch Loss: 2.854361\n",
            "Step 63260: Minibatch Loss: 3.079301\n",
            "Step 63262: Minibatch Loss: 3.128593\n",
            "Step 63264: Minibatch Loss: 3.284680\n",
            "Step 63266: Minibatch Loss: 3.247347\n",
            "Step 63268: Minibatch Loss: 3.006419\n",
            "Step 63270: Minibatch Loss: 2.782531\n",
            "Training for SNR= 9.5  sigma= 0.33496543915782767 iteratin: 0\n",
            "Step 63272: Minibatch Loss: 2.889217\n",
            "Step 63274: Minibatch Loss: 3.097159\n",
            "Step 63276: Minibatch Loss: 2.971731\n",
            "Step 63278: Minibatch Loss: 2.991530\n",
            "Step 63280: Minibatch Loss: 3.136081\n",
            "Step 63282: Minibatch Loss: 2.887484\n",
            "Step 63284: Minibatch Loss: 2.841577\n",
            "Step 63286: Minibatch Loss: 2.925803\n",
            "Step 63288: Minibatch Loss: 2.894576\n",
            "Step 63290: Minibatch Loss: 2.940839\n",
            "Step 63292: Minibatch Loss: 2.960802\n",
            "Step 63294: Minibatch Loss: 3.030363\n",
            "Step 63296: Minibatch Loss: 2.569140\n",
            "Step 63298: Minibatch Loss: 3.184043\n",
            "Step 63300: Minibatch Loss: 2.956474\n",
            "Step 63302: Minibatch Loss: 2.846705\n",
            "Step 63304: Minibatch Loss: 3.088983\n",
            "Step 63306: Minibatch Loss: 2.976990\n",
            "Step 63308: Minibatch Loss: 2.825974\n",
            "Step 63310: Minibatch Loss: 2.900756\n",
            "Step 63312: Minibatch Loss: 3.288550\n",
            "Step 63314: Minibatch Loss: 3.323463\n",
            "Step 63316: Minibatch Loss: 2.902816\n",
            "Step 63318: Minibatch Loss: 2.984073\n",
            "Step 63320: Minibatch Loss: 3.016205\n",
            "Step 63322: Minibatch Loss: 3.023350\n",
            "Step 63324: Minibatch Loss: 2.853914\n",
            "Step 63326: Minibatch Loss: 2.711747\n",
            "Step 63328: Minibatch Loss: 2.983618\n",
            "Step 63330: Minibatch Loss: 3.095054\n",
            "Step 63332: Minibatch Loss: 2.676342\n",
            "Step 63334: Minibatch Loss: 3.248001\n",
            "Step 63336: Minibatch Loss: 3.146403\n",
            "Step 63338: Minibatch Loss: 2.764809\n",
            "Step 63340: Minibatch Loss: 3.022813\n",
            "Step 63342: Minibatch Loss: 2.991184\n",
            "Step 63344: Minibatch Loss: 2.724707\n",
            "Step 63346: Minibatch Loss: 2.920537\n",
            "Step 63348: Minibatch Loss: 3.003869\n",
            "Step 63350: Minibatch Loss: 3.116475\n",
            "Step 63352: Minibatch Loss: 2.998174\n",
            "Step 63354: Minibatch Loss: 3.096935\n",
            "Step 63356: Minibatch Loss: 2.947381\n",
            "Step 63358: Minibatch Loss: 2.848848\n",
            "Step 63360: Minibatch Loss: 2.772907\n",
            "Step 63362: Minibatch Loss: 3.091076\n",
            "Step 63364: Minibatch Loss: 2.645896\n",
            "Step 63366: Minibatch Loss: 3.151242\n",
            "Step 63368: Minibatch Loss: 2.760441\n",
            "Step 63370: Minibatch Loss: 2.779848\n",
            "Step 63372: Minibatch Loss: 3.005207\n",
            "Step 63374: Minibatch Loss: 2.972278\n",
            "Step 63376: Minibatch Loss: 3.049861\n",
            "Step 63378: Minibatch Loss: 3.026118\n",
            "Step 63380: Minibatch Loss: 3.090900\n",
            "Step 63382: Minibatch Loss: 2.742433\n",
            "Step 63384: Minibatch Loss: 2.520624\n",
            "Step 63386: Minibatch Loss: 2.903366\n",
            "Step 63388: Minibatch Loss: 3.032400\n",
            "Step 63390: Minibatch Loss: 2.668236\n",
            "Step 63392: Minibatch Loss: 2.635169\n",
            "Step 63394: Minibatch Loss: 2.971713\n",
            "Step 63396: Minibatch Loss: 3.060125\n",
            "Step 63398: Minibatch Loss: 2.814724\n",
            "Step 63400: Minibatch Loss: 3.097615\n",
            "Step 63402: Minibatch Loss: 2.961106\n",
            "Step 63404: Minibatch Loss: 2.941419\n",
            "Step 63406: Minibatch Loss: 2.892234\n",
            "Step 63408: Minibatch Loss: 2.846206\n",
            "Step 63410: Minibatch Loss: 3.057674\n",
            "Step 63412: Minibatch Loss: 2.770319\n",
            "Step 63414: Minibatch Loss: 2.868964\n",
            "Step 63416: Minibatch Loss: 2.750385\n",
            "Step 63418: Minibatch Loss: 2.989926\n",
            "Step 63420: Minibatch Loss: 2.770458\n",
            "Step 63422: Minibatch Loss: 2.788383\n",
            "Step 63424: Minibatch Loss: 3.140790\n",
            "Step 63426: Minibatch Loss: 2.886747\n",
            "Step 63428: Minibatch Loss: 3.158650\n",
            "Step 63430: Minibatch Loss: 2.904468\n",
            "Step 63432: Minibatch Loss: 2.546684\n",
            "Step 63434: Minibatch Loss: 2.910774\n",
            "Step 63436: Minibatch Loss: 2.730369\n",
            "Step 63438: Minibatch Loss: 2.728457\n",
            "Step 63440: Minibatch Loss: 2.917015\n",
            "Step 63442: Minibatch Loss: 3.121693\n",
            "Step 63444: Minibatch Loss: 3.177306\n",
            "Step 63446: Minibatch Loss: 3.081517\n",
            "Step 63448: Minibatch Loss: 2.749214\n",
            "Step 63450: Minibatch Loss: 2.779843\n",
            "Step 63452: Minibatch Loss: 2.476196\n",
            "Step 63454: Minibatch Loss: 2.898786\n",
            "Step 63456: Minibatch Loss: 2.816786\n",
            "Step 63458: Minibatch Loss: 2.721379\n",
            "Step 63460: Minibatch Loss: 3.120802\n",
            "Step 63462: Minibatch Loss: 3.018643\n",
            "Step 63464: Minibatch Loss: 2.707627\n",
            "Step 63466: Minibatch Loss: 3.097260\n",
            "Step 63468: Minibatch Loss: 2.940089\n",
            "Step 63470: Minibatch Loss: 2.896202\n",
            "Step 63472: Minibatch Loss: 2.839933\n",
            "Step 63474: Minibatch Loss: 3.010501\n",
            "Step 63476: Minibatch Loss: 3.107446\n",
            "Step 63478: Minibatch Loss: 2.998719\n",
            "Step 63480: Minibatch Loss: 2.941780\n",
            "Step 63482: Minibatch Loss: 3.014034\n",
            "Step 63484: Minibatch Loss: 2.945525\n",
            "Step 63486: Minibatch Loss: 3.212779\n",
            "Step 63488: Minibatch Loss: 3.011628\n",
            "Step 63490: Minibatch Loss: 3.094032\n",
            "Step 63492: Minibatch Loss: 2.951010\n",
            "Step 63494: Minibatch Loss: 2.932126\n",
            "Step 63496: Minibatch Loss: 2.522847\n",
            "Step 63498: Minibatch Loss: 2.777858\n",
            "Step 63500: Minibatch Loss: 3.070365\n",
            "Step 63502: Minibatch Loss: 2.744268\n",
            "Step 63504: Minibatch Loss: 3.190478\n",
            "Step 63506: Minibatch Loss: 2.855331\n",
            "Step 63508: Minibatch Loss: 2.973427\n",
            "Step 63510: Minibatch Loss: 2.890545\n",
            "Step 63512: Minibatch Loss: 2.960656\n",
            "Step 63514: Minibatch Loss: 2.761103\n",
            "Step 63516: Minibatch Loss: 2.858499\n",
            "Step 63518: Minibatch Loss: 2.735558\n",
            "Step 63520: Minibatch Loss: 3.109047\n",
            "Step 63522: Minibatch Loss: 2.785103\n",
            "Step 63524: Minibatch Loss: 2.825775\n",
            "Step 63526: Minibatch Loss: 2.977944\n",
            "Step 63528: Minibatch Loss: 2.853780\n",
            "Step 63530: Minibatch Loss: 2.854013\n",
            "Step 63532: Minibatch Loss: 3.010648\n",
            "Step 63534: Minibatch Loss: 3.111932\n",
            "Step 63536: Minibatch Loss: 3.141477\n",
            "Step 63538: Minibatch Loss: 2.998490\n",
            "Step 63540: Minibatch Loss: 2.795323\n",
            "Step 63542: Minibatch Loss: 2.949299\n",
            "Step 63544: Minibatch Loss: 2.755120\n",
            "Step 63546: Minibatch Loss: 2.993953\n",
            "Step 63548: Minibatch Loss: 2.880973\n",
            "Step 63550: Minibatch Loss: 3.232404\n",
            "Step 63552: Minibatch Loss: 2.915909\n",
            "Step 63554: Minibatch Loss: 3.154575\n",
            "Step 63556: Minibatch Loss: 3.250455\n",
            "Step 63558: Minibatch Loss: 3.242959\n",
            "Step 63560: Minibatch Loss: 2.543642\n",
            "Step 63562: Minibatch Loss: 3.030459\n",
            "Step 63564: Minibatch Loss: 2.819160\n",
            "Step 63566: Minibatch Loss: 2.883472\n",
            "Step 63568: Minibatch Loss: 2.896302\n",
            "Step 63570: Minibatch Loss: 2.747635\n",
            "Step 63572: Minibatch Loss: 2.798779\n",
            "Step 63574: Minibatch Loss: 2.786631\n",
            "Step 63576: Minibatch Loss: 2.900058\n",
            "Step 63578: Minibatch Loss: 2.783626\n",
            "Step 63580: Minibatch Loss: 2.980357\n",
            "Step 63582: Minibatch Loss: 2.789637\n",
            "Step 63584: Minibatch Loss: 3.213223\n",
            "Step 63586: Minibatch Loss: 3.069724\n",
            "Step 63588: Minibatch Loss: 3.067654\n",
            "Step 63590: Minibatch Loss: 3.023329\n",
            "Step 63592: Minibatch Loss: 2.991415\n",
            "Step 63594: Minibatch Loss: 2.862771\n",
            "Step 63596: Minibatch Loss: 2.948619\n",
            "Step 63598: Minibatch Loss: 3.356124\n",
            "Step 63600: Minibatch Loss: 3.138797\n",
            "Step 63602: Minibatch Loss: 2.883964\n",
            "Step 63604: Minibatch Loss: 2.963021\n",
            "Step 63606: Minibatch Loss: 2.802271\n",
            "Step 63608: Minibatch Loss: 2.965759\n",
            "Step 63610: Minibatch Loss: 2.798229\n",
            "Step 63612: Minibatch Loss: 3.018875\n",
            "Step 63614: Minibatch Loss: 3.337433\n",
            "Step 63616: Minibatch Loss: 2.939631\n",
            "Step 63618: Minibatch Loss: 3.030092\n",
            "Step 63620: Minibatch Loss: 3.064535\n",
            "Step 63622: Minibatch Loss: 2.806911\n",
            "Step 63624: Minibatch Loss: 2.758359\n",
            "Step 63626: Minibatch Loss: 2.796394\n",
            "Step 63628: Minibatch Loss: 3.013148\n",
            "Step 63630: Minibatch Loss: 3.042687\n",
            "Step 63632: Minibatch Loss: 2.586681\n",
            "Step 63634: Minibatch Loss: 2.994642\n",
            "Step 63636: Minibatch Loss: 3.089593\n",
            "Step 63638: Minibatch Loss: 3.086988\n",
            "Step 63640: Minibatch Loss: 3.082134\n",
            "Step 63642: Minibatch Loss: 2.857620\n",
            "Step 63644: Minibatch Loss: 3.032645\n",
            "Step 63646: Minibatch Loss: 2.741432\n",
            "Step 63648: Minibatch Loss: 2.939825\n",
            "Step 63650: Minibatch Loss: 2.882138\n",
            "Step 63652: Minibatch Loss: 2.811883\n",
            "Step 63654: Minibatch Loss: 2.959894\n",
            "Step 63656: Minibatch Loss: 2.693110\n",
            "Step 63658: Minibatch Loss: 3.120046\n",
            "Step 63660: Minibatch Loss: 2.831161\n",
            "Step 63662: Minibatch Loss: 3.020117\n",
            "Step 63664: Minibatch Loss: 2.755691\n",
            "Step 63666: Minibatch Loss: 2.649561\n",
            "Step 63668: Minibatch Loss: 3.167712\n",
            "Step 63670: Minibatch Loss: 3.184448\n",
            "Step 63672: Minibatch Loss: 2.886985\n",
            "Step 63674: Minibatch Loss: 3.095357\n",
            "Step 63676: Minibatch Loss: 2.764210\n",
            "Step 63678: Minibatch Loss: 2.890204\n",
            "Step 63680: Minibatch Loss: 2.918584\n",
            "Step 63682: Minibatch Loss: 2.633797\n",
            "Step 63684: Minibatch Loss: 2.701034\n",
            "Step 63686: Minibatch Loss: 2.812644\n",
            "Step 63688: Minibatch Loss: 2.841430\n",
            "Step 63690: Minibatch Loss: 3.048517\n",
            "Step 63692: Minibatch Loss: 2.883567\n",
            "Step 63694: Minibatch Loss: 2.709328\n",
            "Step 63696: Minibatch Loss: 2.898355\n",
            "Step 63698: Minibatch Loss: 3.332937\n",
            "Step 63700: Minibatch Loss: 2.920348\n",
            "Step 63702: Minibatch Loss: 3.358568\n",
            "Step 63704: Minibatch Loss: 2.735130\n",
            "Step 63706: Minibatch Loss: 3.243757\n",
            "Step 63708: Minibatch Loss: 3.017322\n",
            "Step 63710: Minibatch Loss: 2.606667\n",
            "Step 63712: Minibatch Loss: 3.275876\n",
            "Step 63714: Minibatch Loss: 2.965009\n",
            "Step 63716: Minibatch Loss: 3.168579\n",
            "Step 63718: Minibatch Loss: 2.966412\n",
            "Step 63720: Minibatch Loss: 2.867867\n",
            "Step 63722: Minibatch Loss: 2.902484\n",
            "Step 63724: Minibatch Loss: 3.155577\n",
            "Step 63726: Minibatch Loss: 3.050242\n",
            "Step 63728: Minibatch Loss: 2.860398\n",
            "Step 63730: Minibatch Loss: 2.789379\n",
            "Step 63732: Minibatch Loss: 2.504669\n",
            "Step 63734: Minibatch Loss: 3.005577\n",
            "Step 63736: Minibatch Loss: 2.854093\n",
            "Step 63738: Minibatch Loss: 2.942809\n",
            "Step 63740: Minibatch Loss: 3.030936\n",
            "Step 63742: Minibatch Loss: 2.750058\n",
            "Step 63744: Minibatch Loss: 3.039300\n",
            "Step 63746: Minibatch Loss: 2.999818\n",
            "Step 63748: Minibatch Loss: 2.955020\n",
            "Step 63750: Minibatch Loss: 3.157539\n",
            "Step 63752: Minibatch Loss: 3.015805\n",
            "Step 63754: Minibatch Loss: 2.923305\n",
            "Step 63756: Minibatch Loss: 2.853850\n",
            "Step 63758: Minibatch Loss: 3.073412\n",
            "Step 63760: Minibatch Loss: 2.879516\n",
            "Step 63762: Minibatch Loss: 3.153453\n",
            "Step 63764: Minibatch Loss: 2.858696\n",
            "Step 63766: Minibatch Loss: 2.832501\n",
            "Step 63768: Minibatch Loss: 2.997920\n",
            "Step 63770: Minibatch Loss: 2.885369\n",
            "Step 63772: Minibatch Loss: 2.977949\n",
            "Step 63774: Minibatch Loss: 2.895045\n",
            "Step 63776: Minibatch Loss: 3.115974\n",
            "Step 63778: Minibatch Loss: 2.870155\n",
            "Step 63780: Minibatch Loss: 3.225355\n",
            "Step 63782: Minibatch Loss: 3.363719\n",
            "Step 63784: Minibatch Loss: 2.810785\n",
            "Step 63786: Minibatch Loss: 2.955119\n",
            "Step 63788: Minibatch Loss: 3.093055\n",
            "Step 63790: Minibatch Loss: 3.022774\n",
            "Step 63792: Minibatch Loss: 2.771871\n",
            "Step 63794: Minibatch Loss: 3.015584\n",
            "Step 63796: Minibatch Loss: 3.161920\n",
            "Step 63798: Minibatch Loss: 3.035342\n",
            "Step 63800: Minibatch Loss: 3.060302\n",
            "Step 63802: Minibatch Loss: 3.005421\n",
            "Step 63804: Minibatch Loss: 3.079969\n",
            "Step 63806: Minibatch Loss: 3.372023\n",
            "Step 63808: Minibatch Loss: 2.749742\n",
            "Step 63810: Minibatch Loss: 3.206319\n",
            "Step 63812: Minibatch Loss: 2.974979\n",
            "Step 63814: Minibatch Loss: 3.097507\n",
            "Step 63816: Minibatch Loss: 3.088299\n",
            "Step 63818: Minibatch Loss: 3.044299\n",
            "Step 63820: Minibatch Loss: 3.309746\n",
            "Step 63822: Minibatch Loss: 3.316094\n",
            "Step 63824: Minibatch Loss: 2.975223\n",
            "Step 63826: Minibatch Loss: 2.957784\n",
            "Step 63828: Minibatch Loss: 2.685175\n",
            "Step 63830: Minibatch Loss: 3.176563\n",
            "Step 63832: Minibatch Loss: 3.118242\n",
            "Step 63834: Minibatch Loss: 3.263018\n",
            "Step 63836: Minibatch Loss: 2.780742\n",
            "Step 63838: Minibatch Loss: 3.004447\n",
            "Step 63840: Minibatch Loss: 3.059580\n",
            "Step 63842: Minibatch Loss: 3.160931\n",
            "Step 63844: Minibatch Loss: 3.030293\n",
            "Step 63846: Minibatch Loss: 3.029665\n",
            "Step 63848: Minibatch Loss: 2.863974\n",
            "Step 63850: Minibatch Loss: 2.835265\n",
            "Step 63852: Minibatch Loss: 2.986138\n",
            "Step 63854: Minibatch Loss: 3.123593\n",
            "Step 63856: Minibatch Loss: 2.913214\n",
            "Step 63858: Minibatch Loss: 2.910065\n",
            "Step 63860: Minibatch Loss: 3.087783\n",
            "Step 63862: Minibatch Loss: 2.944605\n",
            "Step 63864: Minibatch Loss: 3.090459\n",
            "Step 63866: Minibatch Loss: 2.994148\n",
            "Step 63868: Minibatch Loss: 2.942061\n",
            "Step 63870: Minibatch Loss: 2.855595\n",
            "Step 63872: Minibatch Loss: 3.011701\n",
            "Step 63874: Minibatch Loss: 3.403476\n",
            "Step 63876: Minibatch Loss: 2.564442\n",
            "Step 63878: Minibatch Loss: 2.979818\n",
            "Step 63880: Minibatch Loss: 2.864604\n",
            "Step 63882: Minibatch Loss: 3.020904\n",
            "Step 63884: Minibatch Loss: 3.281841\n",
            "Step 63886: Minibatch Loss: 3.013987\n",
            "Step 63888: Minibatch Loss: 2.763158\n",
            "Step 63890: Minibatch Loss: 2.818959\n",
            "Step 63892: Minibatch Loss: 3.115518\n",
            "Step 63894: Minibatch Loss: 2.907051\n",
            "Step 63896: Minibatch Loss: 2.860869\n",
            "Step 63898: Minibatch Loss: 3.023830\n",
            "Step 63900: Minibatch Loss: 2.914823\n",
            "Step 63902: Minibatch Loss: 3.085630\n",
            "Step 63904: Minibatch Loss: 3.234487\n",
            "Step 63906: Minibatch Loss: 2.616695\n",
            "Step 63908: Minibatch Loss: 2.921077\n",
            "Step 63910: Minibatch Loss: 3.008363\n",
            "Step 63912: Minibatch Loss: 2.846139\n",
            "Step 63914: Minibatch Loss: 2.848902\n",
            "Step 63916: Minibatch Loss: 3.005427\n",
            "Step 63918: Minibatch Loss: 2.913990\n",
            "Step 63920: Minibatch Loss: 3.165329\n",
            "Step 63922: Minibatch Loss: 3.117391\n",
            "Step 63924: Minibatch Loss: 2.747570\n",
            "Step 63926: Minibatch Loss: 2.972844\n",
            "Step 63928: Minibatch Loss: 3.061775\n",
            "Step 63930: Minibatch Loss: 3.331139\n",
            "Step 63932: Minibatch Loss: 3.215257\n",
            "Step 63934: Minibatch Loss: 2.988285\n",
            "Step 63936: Minibatch Loss: 2.849900\n",
            "Step 63938: Minibatch Loss: 2.730121\n",
            "Step 63940: Minibatch Loss: 3.110209\n",
            "Step 63942: Minibatch Loss: 2.933727\n",
            "Step 63944: Minibatch Loss: 2.918474\n",
            "Step 63946: Minibatch Loss: 3.162038\n",
            "Step 63948: Minibatch Loss: 2.798875\n",
            "Step 63950: Minibatch Loss: 2.756560\n",
            "Step 63952: Minibatch Loss: 2.891588\n",
            "Step 63954: Minibatch Loss: 2.907934\n",
            "Step 63956: Minibatch Loss: 2.966994\n",
            "Step 63958: Minibatch Loss: 2.909670\n",
            "Step 63960: Minibatch Loss: 3.113069\n",
            "Step 63962: Minibatch Loss: 2.541275\n",
            "Step 63964: Minibatch Loss: 3.112365\n",
            "Step 63966: Minibatch Loss: 2.865814\n",
            "Step 63968: Minibatch Loss: 2.754882\n",
            "Step 63970: Minibatch Loss: 3.049156\n",
            "Step 63972: Minibatch Loss: 3.013366\n",
            "Step 63974: Minibatch Loss: 2.851034\n",
            "Step 63976: Minibatch Loss: 2.927476\n",
            "Step 63978: Minibatch Loss: 3.236366\n",
            "Step 63980: Minibatch Loss: 3.428561\n",
            "Step 63982: Minibatch Loss: 2.891970\n",
            "Step 63984: Minibatch Loss: 3.004309\n",
            "Step 63986: Minibatch Loss: 3.062128\n",
            "Step 63988: Minibatch Loss: 2.963231\n",
            "Step 63990: Minibatch Loss: 2.899399\n",
            "Step 63992: Minibatch Loss: 2.722546\n",
            "Step 63994: Minibatch Loss: 2.844381\n",
            "Step 63996: Minibatch Loss: 3.119437\n",
            "Step 63998: Minibatch Loss: 2.637378\n",
            "Step 64000: Minibatch Loss: 3.173270\n",
            "Step 64002: Minibatch Loss: 3.146286\n",
            "Step 64004: Minibatch Loss: 2.733500\n",
            "Step 64006: Minibatch Loss: 2.945697\n",
            "Step 64008: Minibatch Loss: 2.943080\n",
            "Step 64010: Minibatch Loss: 2.820562\n",
            "Step 64012: Minibatch Loss: 2.889004\n",
            "Step 64014: Minibatch Loss: 2.904866\n",
            "Step 64016: Minibatch Loss: 3.115715\n",
            "Step 64018: Minibatch Loss: 2.957768\n",
            "Step 64020: Minibatch Loss: 3.119339\n",
            "Step 64022: Minibatch Loss: 2.844317\n",
            "Step 64024: Minibatch Loss: 2.915697\n",
            "Step 64026: Minibatch Loss: 2.651792\n",
            "Step 64028: Minibatch Loss: 3.001265\n",
            "Step 64030: Minibatch Loss: 2.660083\n",
            "Step 64032: Minibatch Loss: 3.142493\n",
            "Step 64034: Minibatch Loss: 2.833438\n",
            "Step 64036: Minibatch Loss: 2.822491\n",
            "Step 64038: Minibatch Loss: 3.036160\n",
            "Step 64040: Minibatch Loss: 3.001157\n",
            "Step 64042: Minibatch Loss: 3.067051\n",
            "Step 64044: Minibatch Loss: 3.026963\n",
            "Step 64046: Minibatch Loss: 3.006429\n",
            "Step 64048: Minibatch Loss: 2.787405\n",
            "Step 64050: Minibatch Loss: 2.552549\n",
            "Step 64052: Minibatch Loss: 2.880532\n",
            "Step 64054: Minibatch Loss: 3.015035\n",
            "Step 64056: Minibatch Loss: 2.700898\n",
            "Step 64058: Minibatch Loss: 2.704230\n",
            "Step 64060: Minibatch Loss: 3.004123\n",
            "Step 64062: Minibatch Loss: 3.052426\n",
            "Step 64064: Minibatch Loss: 2.926343\n",
            "Step 64066: Minibatch Loss: 3.101449\n",
            "Step 64068: Minibatch Loss: 3.004337\n",
            "Step 64070: Minibatch Loss: 2.918195\n",
            "Step 64072: Minibatch Loss: 2.805505\n",
            "Step 64074: Minibatch Loss: 2.852250\n",
            "Step 64076: Minibatch Loss: 3.025199\n",
            "Step 64078: Minibatch Loss: 2.822069\n",
            "Step 64080: Minibatch Loss: 2.879631\n",
            "Step 64082: Minibatch Loss: 2.677700\n",
            "Step 64084: Minibatch Loss: 3.048081\n",
            "Step 64086: Minibatch Loss: 2.760992\n",
            "Step 64088: Minibatch Loss: 2.858971\n",
            "Step 64090: Minibatch Loss: 3.155295\n",
            "Step 64092: Minibatch Loss: 2.885907\n",
            "Step 64094: Minibatch Loss: 3.081741\n",
            "Step 64096: Minibatch Loss: 2.955371\n",
            "Step 64098: Minibatch Loss: 2.620649\n",
            "Step 64100: Minibatch Loss: 2.940284\n",
            "Step 64102: Minibatch Loss: 2.632845\n",
            "Step 64104: Minibatch Loss: 2.745476\n",
            "Step 64106: Minibatch Loss: 2.896155\n",
            "Step 64108: Minibatch Loss: 2.997588\n",
            "Step 64110: Minibatch Loss: 3.173477\n",
            "Step 64112: Minibatch Loss: 3.033914\n",
            "Step 64114: Minibatch Loss: 2.757191\n",
            "Step 64116: Minibatch Loss: 2.858781\n",
            "Step 64118: Minibatch Loss: 2.557195\n",
            "Step 64120: Minibatch Loss: 2.804127\n",
            "Step 64122: Minibatch Loss: 2.766718\n",
            "Step 64124: Minibatch Loss: 2.699406\n",
            "Step 64126: Minibatch Loss: 3.204773\n",
            "Step 64128: Minibatch Loss: 3.020262\n",
            "Step 64130: Minibatch Loss: 2.756491\n",
            "Step 64132: Minibatch Loss: 3.017298\n",
            "Step 64134: Minibatch Loss: 2.928917\n",
            "Step 64136: Minibatch Loss: 3.021923\n",
            "Step 64138: Minibatch Loss: 2.746441\n",
            "Step 64140: Minibatch Loss: 3.110972\n",
            "Step 64142: Minibatch Loss: 3.226147\n",
            "Step 64144: Minibatch Loss: 2.929354\n",
            "Step 64146: Minibatch Loss: 2.993217\n",
            "Step 64148: Minibatch Loss: 2.959846\n",
            "Step 64150: Minibatch Loss: 3.038699\n",
            "Step 64152: Minibatch Loss: 3.183242\n",
            "Step 64154: Minibatch Loss: 2.986270\n",
            "Step 64156: Minibatch Loss: 3.006152\n",
            "Step 64158: Minibatch Loss: 2.976944\n",
            "Step 64160: Minibatch Loss: 2.950627\n",
            "Step 64162: Minibatch Loss: 2.547957\n",
            "Step 64164: Minibatch Loss: 2.843466\n",
            "Step 64166: Minibatch Loss: 3.002650\n",
            "Step 64168: Minibatch Loss: 2.755306\n",
            "Step 64170: Minibatch Loss: 3.215463\n",
            "Step 64172: Minibatch Loss: 2.890582\n",
            "Step 64174: Minibatch Loss: 2.979920\n",
            "Step 64176: Minibatch Loss: 2.934253\n",
            "Step 64178: Minibatch Loss: 3.023455\n",
            "Step 64180: Minibatch Loss: 2.797374\n",
            "Step 64182: Minibatch Loss: 2.894271\n",
            "Step 64184: Minibatch Loss: 2.752694\n",
            "Step 64186: Minibatch Loss: 3.128242\n",
            "Step 64188: Minibatch Loss: 2.767748\n",
            "Step 64190: Minibatch Loss: 2.765188\n",
            "Step 64192: Minibatch Loss: 2.925445\n",
            "Step 64194: Minibatch Loss: 2.873645\n",
            "Step 64196: Minibatch Loss: 2.782838\n",
            "Step 64198: Minibatch Loss: 3.038838\n",
            "Step 64200: Minibatch Loss: 2.954609\n",
            "Step 64202: Minibatch Loss: 3.184210\n",
            "Step 64204: Minibatch Loss: 2.923996\n",
            "Step 64206: Minibatch Loss: 2.867922\n",
            "Step 64208: Minibatch Loss: 3.006067\n",
            "Step 64210: Minibatch Loss: 2.668303\n",
            "Step 64212: Minibatch Loss: 2.901206\n",
            "Step 64214: Minibatch Loss: 2.876795\n",
            "Step 64216: Minibatch Loss: 3.259540\n",
            "Step 64218: Minibatch Loss: 2.834080\n",
            "Step 64220: Minibatch Loss: 3.070665\n",
            "Step 64222: Minibatch Loss: 3.223020\n",
            "Step 64224: Minibatch Loss: 3.196751\n",
            "Step 64226: Minibatch Loss: 2.434845\n",
            "Step 64228: Minibatch Loss: 2.960154\n",
            "Step 64230: Minibatch Loss: 2.776400\n",
            "Step 64232: Minibatch Loss: 2.944043\n",
            "Step 64234: Minibatch Loss: 2.951510\n",
            "Step 64236: Minibatch Loss: 2.792723\n",
            "Step 64238: Minibatch Loss: 2.728800\n",
            "Step 64240: Minibatch Loss: 2.799631\n",
            "Step 64242: Minibatch Loss: 2.848377\n",
            "Step 64244: Minibatch Loss: 2.756549\n",
            "Step 64246: Minibatch Loss: 3.005337\n",
            "Step 64248: Minibatch Loss: 2.753859\n",
            "Step 64250: Minibatch Loss: 3.147544\n",
            "Step 64252: Minibatch Loss: 2.900532\n",
            "Step 64254: Minibatch Loss: 3.110942\n",
            "Step 64256: Minibatch Loss: 2.916603\n",
            "Step 64258: Minibatch Loss: 2.946379\n",
            "Step 64260: Minibatch Loss: 2.798917\n",
            "Step 64262: Minibatch Loss: 3.083271\n",
            "Step 64264: Minibatch Loss: 3.167307\n",
            "Step 64266: Minibatch Loss: 3.131836\n",
            "Step 64268: Minibatch Loss: 2.842454\n",
            "Step 64270: Minibatch Loss: 2.962205\n",
            "Step 64272: Minibatch Loss: 2.771607\n",
            "Step 64274: Minibatch Loss: 2.917722\n",
            "Step 64276: Minibatch Loss: 2.696103\n",
            "Step 64278: Minibatch Loss: 3.062789\n",
            "Step 64280: Minibatch Loss: 3.264584\n",
            "Step 64282: Minibatch Loss: 2.838659\n",
            "Step 64284: Minibatch Loss: 2.997994\n",
            "Step 64286: Minibatch Loss: 3.043490\n",
            "Step 64288: Minibatch Loss: 2.694014\n",
            "Step 64290: Minibatch Loss: 2.783581\n",
            "Step 64292: Minibatch Loss: 2.821894\n",
            "Step 64294: Minibatch Loss: 3.010190\n",
            "Step 64296: Minibatch Loss: 2.991720\n",
            "Step 64298: Minibatch Loss: 2.542540\n",
            "Step 64300: Minibatch Loss: 2.921293\n",
            "Step 64302: Minibatch Loss: 3.037674\n",
            "Step 64304: Minibatch Loss: 3.092624\n",
            "Step 64306: Minibatch Loss: 2.956758\n",
            "Step 64308: Minibatch Loss: 2.836416\n",
            "Step 64310: Minibatch Loss: 3.026608\n",
            "Step 64312: Minibatch Loss: 2.893475\n",
            "Step 64314: Minibatch Loss: 3.025613\n",
            "Step 64316: Minibatch Loss: 2.977793\n",
            "Step 64318: Minibatch Loss: 2.851609\n",
            "Step 64320: Minibatch Loss: 3.009355\n",
            "Step 64322: Minibatch Loss: 2.676964\n",
            "Step 64324: Minibatch Loss: 3.025202\n",
            "Step 64326: Minibatch Loss: 2.842831\n",
            "Step 64328: Minibatch Loss: 2.858906\n",
            "Step 64330: Minibatch Loss: 2.855145\n",
            "Step 64332: Minibatch Loss: 2.638339\n",
            "Step 64334: Minibatch Loss: 3.173699\n",
            "Step 64336: Minibatch Loss: 3.132004\n",
            "Step 64338: Minibatch Loss: 3.012329\n",
            "Step 64340: Minibatch Loss: 2.961084\n",
            "Step 64342: Minibatch Loss: 2.776562\n",
            "Step 64344: Minibatch Loss: 2.758368\n",
            "Step 64346: Minibatch Loss: 2.825261\n",
            "Step 64348: Minibatch Loss: 2.588915\n",
            "Step 64350: Minibatch Loss: 2.732395\n",
            "Step 64352: Minibatch Loss: 2.966398\n",
            "Step 64354: Minibatch Loss: 2.764253\n",
            "Step 64356: Minibatch Loss: 3.047294\n",
            "Step 64358: Minibatch Loss: 2.919479\n",
            "Step 64360: Minibatch Loss: 2.644022\n",
            "Step 64362: Minibatch Loss: 2.999347\n",
            "Step 64364: Minibatch Loss: 3.355965\n",
            "Step 64366: Minibatch Loss: 2.918952\n",
            "Step 64368: Minibatch Loss: 3.315810\n",
            "Step 64370: Minibatch Loss: 2.763263\n",
            "Step 64372: Minibatch Loss: 3.195741\n",
            "Step 64374: Minibatch Loss: 3.013924\n",
            "Step 64376: Minibatch Loss: 2.533847\n",
            "Step 64378: Minibatch Loss: 3.262335\n",
            "Step 64380: Minibatch Loss: 2.901163\n",
            "Step 64382: Minibatch Loss: 3.237110\n",
            "Step 64384: Minibatch Loss: 3.085019\n",
            "Step 64386: Minibatch Loss: 2.838709\n",
            "Step 64388: Minibatch Loss: 2.972541\n",
            "Step 64390: Minibatch Loss: 3.148068\n",
            "Step 64392: Minibatch Loss: 3.098663\n",
            "Step 64394: Minibatch Loss: 2.854873\n",
            "Step 64396: Minibatch Loss: 2.797526\n",
            "Step 64398: Minibatch Loss: 2.517800\n",
            "Step 64400: Minibatch Loss: 2.946612\n",
            "Step 64402: Minibatch Loss: 2.797020\n",
            "Step 64404: Minibatch Loss: 2.961109\n",
            "Step 64406: Minibatch Loss: 3.122658\n",
            "Step 64408: Minibatch Loss: 2.696148\n",
            "Step 64410: Minibatch Loss: 3.024245\n",
            "Step 64412: Minibatch Loss: 3.018168\n",
            "Step 64414: Minibatch Loss: 3.121871\n",
            "Step 64416: Minibatch Loss: 3.063838\n",
            "Step 64418: Minibatch Loss: 3.027273\n",
            "Step 64420: Minibatch Loss: 2.971394\n",
            "Step 64422: Minibatch Loss: 2.918251\n",
            "Step 64424: Minibatch Loss: 3.073340\n",
            "Step 64426: Minibatch Loss: 3.035168\n",
            "Step 64428: Minibatch Loss: 3.060421\n",
            "Step 64430: Minibatch Loss: 2.843221\n",
            "Step 64432: Minibatch Loss: 2.930187\n",
            "Step 64434: Minibatch Loss: 3.047378\n",
            "Step 64436: Minibatch Loss: 2.954472\n",
            "Step 64438: Minibatch Loss: 3.008507\n",
            "Step 64440: Minibatch Loss: 2.972314\n",
            "Step 64442: Minibatch Loss: 3.145185\n",
            "Step 64444: Minibatch Loss: 2.971566\n",
            "Step 64446: Minibatch Loss: 3.360443\n",
            "Step 64448: Minibatch Loss: 3.049535\n",
            "Step 64450: Minibatch Loss: 2.771013\n",
            "Step 64452: Minibatch Loss: 2.831101\n",
            "Step 64454: Minibatch Loss: 3.131216\n",
            "Step 64456: Minibatch Loss: 2.935726\n",
            "Step 64458: Minibatch Loss: 2.841848\n",
            "Step 64460: Minibatch Loss: 2.953514\n",
            "Step 64462: Minibatch Loss: 3.191581\n",
            "Step 64464: Minibatch Loss: 3.109024\n",
            "Step 64466: Minibatch Loss: 3.092470\n",
            "Step 64468: Minibatch Loss: 2.934374\n",
            "Step 64470: Minibatch Loss: 2.997544\n",
            "Step 64472: Minibatch Loss: 3.331475\n",
            "Step 64474: Minibatch Loss: 2.706092\n",
            "Step 64476: Minibatch Loss: 3.148446\n",
            "Step 64478: Minibatch Loss: 2.985046\n",
            "Step 64480: Minibatch Loss: 3.162530\n",
            "Step 64482: Minibatch Loss: 3.181830\n",
            "Step 64484: Minibatch Loss: 3.058318\n",
            "Step 64486: Minibatch Loss: 3.183287\n",
            "Step 64488: Minibatch Loss: 3.238884\n",
            "Step 64490: Minibatch Loss: 2.979108\n",
            "Step 64492: Minibatch Loss: 2.888036\n",
            "Step 64494: Minibatch Loss: 2.613567\n",
            "Step 64496: Minibatch Loss: 3.131752\n",
            "Step 64498: Minibatch Loss: 3.133048\n",
            "Step 64500: Minibatch Loss: 3.197110\n",
            "Step 64502: Minibatch Loss: 2.972910\n",
            "Step 64504: Minibatch Loss: 3.039887\n",
            "Step 64506: Minibatch Loss: 2.993037\n",
            "Step 64508: Minibatch Loss: 3.193961\n",
            "Step 64510: Minibatch Loss: 2.937531\n",
            "Step 64512: Minibatch Loss: 3.066611\n",
            "Step 64514: Minibatch Loss: 2.981500\n",
            "Step 64516: Minibatch Loss: 2.841851\n",
            "Step 64518: Minibatch Loss: 2.925505\n",
            "Step 64520: Minibatch Loss: 3.137090\n",
            "Step 64522: Minibatch Loss: 2.828613\n",
            "Step 64524: Minibatch Loss: 2.755102\n",
            "Step 64526: Minibatch Loss: 2.975748\n",
            "Step 64528: Minibatch Loss: 2.836850\n",
            "Step 64530: Minibatch Loss: 3.000960\n",
            "Step 64532: Minibatch Loss: 3.110157\n",
            "Step 64534: Minibatch Loss: 2.857618\n",
            "Step 64536: Minibatch Loss: 2.754032\n",
            "Step 64538: Minibatch Loss: 3.028417\n",
            "Step 64540: Minibatch Loss: 3.417333\n",
            "Step 64542: Minibatch Loss: 2.617866\n",
            "Step 64544: Minibatch Loss: 3.046092\n",
            "Step 64546: Minibatch Loss: 2.844465\n",
            "Step 64548: Minibatch Loss: 3.043749\n",
            "Step 64550: Minibatch Loss: 3.113876\n",
            "Step 64552: Minibatch Loss: 3.007914\n",
            "Step 64554: Minibatch Loss: 2.842200\n",
            "Step 64556: Minibatch Loss: 2.898503\n",
            "Step 64558: Minibatch Loss: 3.064053\n",
            "Step 64560: Minibatch Loss: 2.814104\n",
            "Step 64562: Minibatch Loss: 2.959820\n",
            "Step 64564: Minibatch Loss: 2.994413\n",
            "Step 64566: Minibatch Loss: 2.904529\n",
            "Step 64568: Minibatch Loss: 3.002974\n",
            "Step 64570: Minibatch Loss: 3.190110\n",
            "Step 64572: Minibatch Loss: 2.700973\n",
            "Step 64574: Minibatch Loss: 2.954135\n",
            "Step 64576: Minibatch Loss: 3.025233\n",
            "Step 64578: Minibatch Loss: 2.926448\n",
            "Step 64580: Minibatch Loss: 2.752729\n",
            "Step 64582: Minibatch Loss: 2.826011\n",
            "Step 64584: Minibatch Loss: 2.927285\n",
            "Step 64586: Minibatch Loss: 3.154171\n",
            "Step 64588: Minibatch Loss: 3.120791\n",
            "Step 64590: Minibatch Loss: 2.815723\n",
            "Step 64592: Minibatch Loss: 3.119378\n",
            "Step 64594: Minibatch Loss: 3.115226\n",
            "Step 64596: Minibatch Loss: 3.189534\n",
            "Step 64598: Minibatch Loss: 3.126840\n",
            "Step 64600: Minibatch Loss: 3.046290\n",
            "Step 64602: Minibatch Loss: 2.801732\n",
            "Step 64604: Minibatch Loss: 2.815792\n",
            "Step 64606: Minibatch Loss: 3.234798\n",
            "Step 64608: Minibatch Loss: 3.033599\n",
            "Step 64610: Minibatch Loss: 2.890203\n",
            "Step 64612: Minibatch Loss: 3.117627\n",
            "Step 64614: Minibatch Loss: 2.723753\n",
            "Step 64616: Minibatch Loss: 2.790786\n",
            "Step 64618: Minibatch Loss: 2.969833\n",
            "Step 64620: Minibatch Loss: 2.914071\n",
            "Step 64622: Minibatch Loss: 2.994026\n",
            "Step 64624: Minibatch Loss: 2.957247\n",
            "Step 64626: Minibatch Loss: 3.102303\n",
            "Step 64628: Minibatch Loss: 2.469426\n",
            "Step 64630: Minibatch Loss: 3.130005\n",
            "Step 64632: Minibatch Loss: 2.896758\n",
            "Step 64634: Minibatch Loss: 2.722424\n",
            "Step 64636: Minibatch Loss: 3.046222\n",
            "Step 64638: Minibatch Loss: 2.994820\n",
            "Step 64640: Minibatch Loss: 2.755375\n",
            "Step 64642: Minibatch Loss: 2.904313\n",
            "Step 64644: Minibatch Loss: 3.286861\n",
            "Step 64646: Minibatch Loss: 3.355094\n",
            "Step 64648: Minibatch Loss: 2.854229\n",
            "Step 64650: Minibatch Loss: 2.964665\n",
            "Step 64652: Minibatch Loss: 3.032401\n",
            "Step 64654: Minibatch Loss: 3.016398\n",
            "Step 64656: Minibatch Loss: 2.914032\n",
            "Step 64658: Minibatch Loss: 2.712392\n",
            "Step 64660: Minibatch Loss: 2.969377\n",
            "Step 64662: Minibatch Loss: 3.134685\n",
            "Step 64664: Minibatch Loss: 2.732881\n",
            "Step 64666: Minibatch Loss: 3.125578\n",
            "Step 64668: Minibatch Loss: 3.104383\n",
            "Step 64670: Minibatch Loss: 2.827761\n",
            "Step 64672: Minibatch Loss: 2.868680\n",
            "Step 64674: Minibatch Loss: 2.888853\n",
            "Step 64676: Minibatch Loss: 2.798904\n",
            "Step 64678: Minibatch Loss: 2.966518\n",
            "Step 64680: Minibatch Loss: 2.958524\n",
            "Step 64682: Minibatch Loss: 3.111172\n",
            "Step 64684: Minibatch Loss: 2.939881\n",
            "Step 64686: Minibatch Loss: 3.127473\n",
            "Step 64688: Minibatch Loss: 2.852752\n",
            "Step 64690: Minibatch Loss: 2.802835\n",
            "Step 64692: Minibatch Loss: 2.649505\n",
            "Step 64694: Minibatch Loss: 3.119730\n",
            "Step 64696: Minibatch Loss: 2.589465\n",
            "Step 64698: Minibatch Loss: 3.012150\n",
            "Step 64700: Minibatch Loss: 2.887346\n",
            "Step 64702: Minibatch Loss: 2.889744\n",
            "Step 64704: Minibatch Loss: 2.918971\n",
            "Step 64706: Minibatch Loss: 2.990118\n",
            "Step 64708: Minibatch Loss: 3.088689\n",
            "Step 64710: Minibatch Loss: 2.987374\n",
            "Step 64712: Minibatch Loss: 3.113853\n",
            "Step 64714: Minibatch Loss: 2.724002\n",
            "Step 64716: Minibatch Loss: 2.532181\n",
            "Step 64718: Minibatch Loss: 2.938529\n",
            "Step 64720: Minibatch Loss: 3.013816\n",
            "Step 64722: Minibatch Loss: 2.707986\n",
            "Step 64724: Minibatch Loss: 2.642596\n",
            "Step 64726: Minibatch Loss: 2.850336\n",
            "Step 64728: Minibatch Loss: 3.147371\n",
            "Step 64730: Minibatch Loss: 2.888597\n",
            "Step 64732: Minibatch Loss: 3.064105\n",
            "Step 64734: Minibatch Loss: 2.809939\n",
            "Step 64736: Minibatch Loss: 3.068831\n",
            "Step 64738: Minibatch Loss: 2.743795\n",
            "Step 64740: Minibatch Loss: 2.863497\n",
            "Step 64742: Minibatch Loss: 2.973791\n",
            "Step 64744: Minibatch Loss: 2.847530\n",
            "Step 64746: Minibatch Loss: 2.844172\n",
            "Step 64748: Minibatch Loss: 2.706178\n",
            "Step 64750: Minibatch Loss: 2.994802\n",
            "Step 64752: Minibatch Loss: 2.745946\n",
            "Step 64754: Minibatch Loss: 2.799101\n",
            "Step 64756: Minibatch Loss: 3.241485\n",
            "Step 64758: Minibatch Loss: 2.757062\n",
            "Step 64760: Minibatch Loss: 3.072273\n",
            "Step 64762: Minibatch Loss: 2.940065\n",
            "Step 64764: Minibatch Loss: 2.560420\n",
            "Step 64766: Minibatch Loss: 3.033761\n",
            "Step 64768: Minibatch Loss: 2.682520\n",
            "Step 64770: Minibatch Loss: 2.669079\n",
            "Step 64772: Minibatch Loss: 2.941736\n",
            "Step 64774: Minibatch Loss: 2.968406\n",
            "Step 64776: Minibatch Loss: 3.252100\n",
            "Step 64778: Minibatch Loss: 3.053423\n",
            "Step 64780: Minibatch Loss: 2.750691\n",
            "Step 64782: Minibatch Loss: 2.759747\n",
            "Step 64784: Minibatch Loss: 2.518099\n",
            "Step 64786: Minibatch Loss: 2.851859\n",
            "Step 64788: Minibatch Loss: 2.835276\n",
            "Step 64790: Minibatch Loss: 2.711706\n",
            "Step 64792: Minibatch Loss: 3.148079\n",
            "Step 64794: Minibatch Loss: 2.936901\n",
            "Step 64796: Minibatch Loss: 2.754982\n",
            "Step 64798: Minibatch Loss: 2.971369\n",
            "Step 64800: Minibatch Loss: 2.987020\n",
            "Step 64802: Minibatch Loss: 2.901485\n",
            "Step 64804: Minibatch Loss: 2.768504\n",
            "Step 64806: Minibatch Loss: 3.010324\n",
            "Step 64808: Minibatch Loss: 3.231578\n",
            "Step 64810: Minibatch Loss: 2.977790\n",
            "Step 64812: Minibatch Loss: 2.999482\n",
            "Step 64814: Minibatch Loss: 2.973224\n",
            "Step 64816: Minibatch Loss: 3.014146\n",
            "Step 64818: Minibatch Loss: 3.252879\n",
            "Step 64820: Minibatch Loss: 2.999485\n",
            "Step 64822: Minibatch Loss: 3.052079\n",
            "Step 64824: Minibatch Loss: 2.859565\n",
            "Step 64826: Minibatch Loss: 2.924831\n",
            "Step 64828: Minibatch Loss: 2.560967\n",
            "Step 64830: Minibatch Loss: 2.740399\n",
            "Step 64832: Minibatch Loss: 3.045446\n",
            "Step 64834: Minibatch Loss: 2.645365\n",
            "Step 64836: Minibatch Loss: 3.144012\n",
            "Step 64838: Minibatch Loss: 2.835021\n",
            "Step 64840: Minibatch Loss: 2.889640\n",
            "Step 64842: Minibatch Loss: 3.028669\n",
            "Step 64844: Minibatch Loss: 3.070420\n",
            "Step 64846: Minibatch Loss: 2.800676\n",
            "Step 64848: Minibatch Loss: 2.873530\n",
            "Step 64850: Minibatch Loss: 2.674813\n",
            "Step 64852: Minibatch Loss: 3.146975\n",
            "Step 64854: Minibatch Loss: 2.795861\n",
            "Step 64856: Minibatch Loss: 2.786162\n",
            "Step 64858: Minibatch Loss: 2.834853\n",
            "Step 64860: Minibatch Loss: 2.888152\n",
            "Step 64862: Minibatch Loss: 2.858139\n",
            "Step 64864: Minibatch Loss: 3.051206\n",
            "Step 64866: Minibatch Loss: 3.067328\n",
            "Step 64868: Minibatch Loss: 3.108112\n",
            "Step 64870: Minibatch Loss: 2.925703\n",
            "Step 64872: Minibatch Loss: 2.817923\n",
            "Step 64874: Minibatch Loss: 2.944714\n",
            "Step 64876: Minibatch Loss: 2.733375\n",
            "Step 64878: Minibatch Loss: 2.947768\n",
            "Step 64880: Minibatch Loss: 2.966003\n",
            "Step 64882: Minibatch Loss: 3.218520\n",
            "Step 64884: Minibatch Loss: 2.815260\n",
            "Step 64886: Minibatch Loss: 3.090399\n",
            "Step 64888: Minibatch Loss: 3.329993\n",
            "Step 64890: Minibatch Loss: 3.128649\n",
            "Step 64892: Minibatch Loss: 2.498391\n",
            "Step 64894: Minibatch Loss: 2.939613\n",
            "Step 64896: Minibatch Loss: 2.787982\n",
            "Step 64898: Minibatch Loss: 2.880531\n",
            "Step 64900: Minibatch Loss: 2.930449\n",
            "Step 64902: Minibatch Loss: 2.794373\n",
            "Step 64904: Minibatch Loss: 2.786976\n",
            "Step 64906: Minibatch Loss: 2.715393\n",
            "Step 64908: Minibatch Loss: 2.844916\n",
            "Step 64910: Minibatch Loss: 2.688632\n",
            "Step 64912: Minibatch Loss: 3.020710\n",
            "Step 64914: Minibatch Loss: 2.732732\n",
            "Step 64916: Minibatch Loss: 3.139030\n",
            "Step 64918: Minibatch Loss: 3.008478\n",
            "Step 64920: Minibatch Loss: 3.034505\n",
            "Step 64922: Minibatch Loss: 2.933603\n",
            "Step 64924: Minibatch Loss: 2.921937\n",
            "Step 64926: Minibatch Loss: 2.869277\n",
            "Step 64928: Minibatch Loss: 2.990468\n",
            "Step 64930: Minibatch Loss: 3.257610\n",
            "Step 64932: Minibatch Loss: 3.130486\n",
            "Step 64934: Minibatch Loss: 2.878067\n",
            "Step 64936: Minibatch Loss: 3.032240\n",
            "Step 64938: Minibatch Loss: 2.765317\n",
            "Step 64940: Minibatch Loss: 3.026779\n",
            "Step 64942: Minibatch Loss: 2.760568\n",
            "Step 64944: Minibatch Loss: 3.076009\n",
            "Step 64946: Minibatch Loss: 3.274704\n",
            "Step 64948: Minibatch Loss: 2.952894\n",
            "Step 64950: Minibatch Loss: 2.971643\n",
            "Step 64952: Minibatch Loss: 3.022537\n",
            "Step 64954: Minibatch Loss: 2.801526\n",
            "Step 64956: Minibatch Loss: 2.734315\n",
            "Step 64958: Minibatch Loss: 2.758224\n",
            "Step 64960: Minibatch Loss: 3.019926\n",
            "Step 64962: Minibatch Loss: 3.083386\n",
            "Step 64964: Minibatch Loss: 2.490969\n",
            "Step 64966: Minibatch Loss: 2.979107\n",
            "Step 64968: Minibatch Loss: 3.053670\n",
            "Step 64970: Minibatch Loss: 3.109730\n",
            "Step 64972: Minibatch Loss: 2.945484\n",
            "Step 64974: Minibatch Loss: 2.943289\n",
            "Step 64976: Minibatch Loss: 2.925072\n",
            "Step 64978: Minibatch Loss: 2.913890\n",
            "Step 64980: Minibatch Loss: 2.961102\n",
            "Step 64982: Minibatch Loss: 2.822974\n",
            "Step 64984: Minibatch Loss: 2.804006\n",
            "Step 64986: Minibatch Loss: 3.001667\n",
            "Step 64988: Minibatch Loss: 2.725709\n",
            "Step 64990: Minibatch Loss: 3.046715\n",
            "Step 64992: Minibatch Loss: 2.887531\n",
            "Step 64994: Minibatch Loss: 2.818946\n",
            "Step 64996: Minibatch Loss: 2.782237\n",
            "Step 64998: Minibatch Loss: 2.650755\n",
            "Step 65000: Minibatch Loss: 3.186703\n",
            "Step 65002: Minibatch Loss: 3.133248\n",
            "Step 65004: Minibatch Loss: 2.972094\n",
            "Step 65006: Minibatch Loss: 2.881293\n",
            "Step 65008: Minibatch Loss: 2.806646\n",
            "Step 65010: Minibatch Loss: 2.846470\n",
            "Step 65012: Minibatch Loss: 2.840031\n",
            "Step 65014: Minibatch Loss: 2.658534\n",
            "Step 65016: Minibatch Loss: 2.742449\n",
            "Step 65018: Minibatch Loss: 2.897394\n",
            "Step 65020: Minibatch Loss: 2.837188\n",
            "Step 65022: Minibatch Loss: 3.032464\n",
            "Step 65024: Minibatch Loss: 2.797293\n",
            "Step 65026: Minibatch Loss: 2.683950\n",
            "Step 65028: Minibatch Loss: 3.020180\n",
            "Step 65030: Minibatch Loss: 3.338022\n",
            "Step 65032: Minibatch Loss: 2.896214\n",
            "Step 65034: Minibatch Loss: 3.245540\n",
            "Step 65036: Minibatch Loss: 2.804024\n",
            "Step 65038: Minibatch Loss: 3.231813\n",
            "Step 65040: Minibatch Loss: 3.054804\n",
            "Step 65042: Minibatch Loss: 2.553385\n",
            "Step 65044: Minibatch Loss: 3.326526\n",
            "Step 65046: Minibatch Loss: 2.963962\n",
            "Step 65048: Minibatch Loss: 3.090424\n",
            "Step 65050: Minibatch Loss: 3.053137\n",
            "Step 65052: Minibatch Loss: 2.838199\n",
            "Step 65054: Minibatch Loss: 2.911161\n",
            "Step 65056: Minibatch Loss: 3.208922\n",
            "Step 65058: Minibatch Loss: 3.042475\n",
            "Step 65060: Minibatch Loss: 2.905399\n",
            "Step 65062: Minibatch Loss: 2.771466\n",
            "Step 65064: Minibatch Loss: 2.461509\n",
            "Step 65066: Minibatch Loss: 2.937717\n",
            "Step 65068: Minibatch Loss: 2.763511\n",
            "Step 65070: Minibatch Loss: 2.996667\n",
            "Step 65072: Minibatch Loss: 3.013632\n",
            "Step 65074: Minibatch Loss: 2.740322\n",
            "Step 65076: Minibatch Loss: 2.982277\n",
            "Step 65078: Minibatch Loss: 3.034747\n",
            "Step 65080: Minibatch Loss: 3.011945\n",
            "Step 65082: Minibatch Loss: 3.094101\n",
            "Step 65084: Minibatch Loss: 2.968926\n",
            "Step 65086: Minibatch Loss: 2.963615\n",
            "Step 65088: Minibatch Loss: 2.878246\n",
            "Step 65090: Minibatch Loss: 3.024284\n",
            "Step 65092: Minibatch Loss: 2.993646\n",
            "Step 65094: Minibatch Loss: 3.103275\n",
            "Step 65096: Minibatch Loss: 2.800797\n",
            "Step 65098: Minibatch Loss: 2.828577\n",
            "Step 65100: Minibatch Loss: 2.979575\n",
            "Step 65102: Minibatch Loss: 2.978652\n",
            "Step 65104: Minibatch Loss: 3.066075\n",
            "Step 65106: Minibatch Loss: 2.992644\n",
            "Step 65108: Minibatch Loss: 3.213683\n",
            "Step 65110: Minibatch Loss: 2.884765\n",
            "Step 65112: Minibatch Loss: 3.308623\n",
            "Step 65114: Minibatch Loss: 3.088700\n",
            "Step 65116: Minibatch Loss: 2.816792\n",
            "Step 65118: Minibatch Loss: 2.886739\n",
            "Step 65120: Minibatch Loss: 3.148529\n",
            "Step 65122: Minibatch Loss: 2.924634\n",
            "Step 65124: Minibatch Loss: 2.731777\n",
            "Step 65126: Minibatch Loss: 3.022508\n",
            "Step 65128: Minibatch Loss: 3.103234\n",
            "Step 65130: Minibatch Loss: 3.096048\n",
            "Step 65132: Minibatch Loss: 3.095567\n",
            "Step 65134: Minibatch Loss: 2.854787\n",
            "Step 65136: Minibatch Loss: 3.118223\n",
            "Step 65138: Minibatch Loss: 3.393114\n",
            "Step 65140: Minibatch Loss: 2.764508\n",
            "Step 65142: Minibatch Loss: 3.158899\n",
            "Step 65144: Minibatch Loss: 2.971473\n",
            "Step 65146: Minibatch Loss: 3.194080\n",
            "Step 65148: Minibatch Loss: 3.099394\n",
            "Step 65150: Minibatch Loss: 2.999386\n",
            "Step 65152: Minibatch Loss: 3.175745\n",
            "Step 65154: Minibatch Loss: 3.206281\n",
            "Step 65156: Minibatch Loss: 2.877323\n",
            "Step 65158: Minibatch Loss: 2.944150\n",
            "Step 65160: Minibatch Loss: 2.558347\n",
            "Step 65162: Minibatch Loss: 3.104479\n",
            "Step 65164: Minibatch Loss: 3.049293\n",
            "Step 65166: Minibatch Loss: 3.170290\n",
            "Step 65168: Minibatch Loss: 2.848460\n",
            "Step 65170: Minibatch Loss: 3.038941\n",
            "Step 65172: Minibatch Loss: 3.049850\n",
            "Step 65174: Minibatch Loss: 3.149310\n",
            "Step 65176: Minibatch Loss: 2.987645\n",
            "Step 65178: Minibatch Loss: 3.022253\n",
            "Step 65180: Minibatch Loss: 2.949149\n",
            "Step 65182: Minibatch Loss: 2.858312\n",
            "Step 65184: Minibatch Loss: 2.900540\n",
            "Step 65186: Minibatch Loss: 3.158392\n",
            "Step 65188: Minibatch Loss: 2.898340\n",
            "Step 65190: Minibatch Loss: 2.905941\n",
            "Step 65192: Minibatch Loss: 2.878694\n",
            "Step 65194: Minibatch Loss: 2.832458\n",
            "Step 65196: Minibatch Loss: 3.046897\n",
            "Step 65198: Minibatch Loss: 3.179855\n",
            "Step 65200: Minibatch Loss: 2.912222\n",
            "Step 65202: Minibatch Loss: 2.791353\n",
            "Step 65204: Minibatch Loss: 3.000505\n",
            "Step 65206: Minibatch Loss: 3.414641\n",
            "Step 65208: Minibatch Loss: 2.609460\n",
            "Step 65210: Minibatch Loss: 3.065650\n",
            "Step 65212: Minibatch Loss: 2.853827\n",
            "Step 65214: Minibatch Loss: 2.864654\n",
            "Step 65216: Minibatch Loss: 3.146060\n",
            "Step 65218: Minibatch Loss: 3.031652\n",
            "Step 65220: Minibatch Loss: 2.784365\n",
            "Step 65222: Minibatch Loss: 2.804232\n",
            "Step 65224: Minibatch Loss: 3.029054\n",
            "Step 65226: Minibatch Loss: 2.778054\n",
            "Step 65228: Minibatch Loss: 2.911952\n",
            "Step 65230: Minibatch Loss: 2.879990\n",
            "Step 65232: Minibatch Loss: 2.939761\n",
            "Step 65234: Minibatch Loss: 3.017960\n",
            "Step 65236: Minibatch Loss: 3.148313\n",
            "Step 65238: Minibatch Loss: 2.594821\n",
            "Step 65240: Minibatch Loss: 2.931204\n",
            "Step 65242: Minibatch Loss: 2.916862\n",
            "Step 65244: Minibatch Loss: 2.888495\n",
            "Step 65246: Minibatch Loss: 2.812805\n",
            "Step 65248: Minibatch Loss: 2.842247\n",
            "Step 65250: Minibatch Loss: 2.890970\n",
            "Step 65252: Minibatch Loss: 3.162517\n",
            "Step 65254: Minibatch Loss: 3.033007\n",
            "Step 65256: Minibatch Loss: 2.736214\n",
            "Step 65258: Minibatch Loss: 2.950890\n",
            "Step 65260: Minibatch Loss: 3.182732\n",
            "Step 65262: Minibatch Loss: 3.194428\n",
            "Step 65264: Minibatch Loss: 3.162432\n",
            "Step 65266: Minibatch Loss: 2.839634\n",
            "Step 65268: Minibatch Loss: 2.889162\n",
            "Step 65270: Minibatch Loss: 2.770069\n",
            "Step 65272: Minibatch Loss: 3.065314\n",
            "Step 65274: Minibatch Loss: 2.947283\n",
            "Step 65276: Minibatch Loss: 2.838133\n",
            "Step 65278: Minibatch Loss: 3.195152\n",
            "Step 65280: Minibatch Loss: 2.798598\n",
            "Step 65282: Minibatch Loss: 2.756117\n",
            "Step 65284: Minibatch Loss: 2.960410\n",
            "Step 65286: Minibatch Loss: 2.855968\n",
            "Step 65288: Minibatch Loss: 2.825417\n",
            "Step 65290: Minibatch Loss: 2.830630\n",
            "Step 65292: Minibatch Loss: 2.981427\n",
            "Step 65294: Minibatch Loss: 2.552729\n",
            "Step 65296: Minibatch Loss: 3.249081\n",
            "Step 65298: Minibatch Loss: 2.864819\n",
            "Step 65300: Minibatch Loss: 2.792103\n",
            "Step 65302: Minibatch Loss: 3.063059\n",
            "Step 65304: Minibatch Loss: 3.026937\n",
            "Step 65306: Minibatch Loss: 2.912620\n",
            "Step 65308: Minibatch Loss: 2.804666\n",
            "Step 65310: Minibatch Loss: 3.355824\n",
            "Step 65312: Minibatch Loss: 3.393446\n",
            "Step 65314: Minibatch Loss: 2.853779\n",
            "Step 65316: Minibatch Loss: 2.948290\n",
            "Step 65318: Minibatch Loss: 2.963943\n",
            "Step 65320: Minibatch Loss: 2.982257\n",
            "Step 65322: Minibatch Loss: 2.866084\n",
            "Step 65324: Minibatch Loss: 2.737420\n",
            "Step 65326: Minibatch Loss: 2.924299\n",
            "Step 65328: Minibatch Loss: 3.112655\n",
            "Step 65330: Minibatch Loss: 2.556611\n",
            "Step 65332: Minibatch Loss: 3.143797\n",
            "Step 65334: Minibatch Loss: 3.154603\n",
            "Step 65336: Minibatch Loss: 2.780137\n",
            "Step 65338: Minibatch Loss: 2.895342\n",
            "Step 65340: Minibatch Loss: 2.874168\n",
            "Step 65342: Minibatch Loss: 2.764771\n",
            "Step 65344: Minibatch Loss: 3.002541\n",
            "Step 65346: Minibatch Loss: 3.018459\n",
            "Step 65348: Minibatch Loss: 3.075330\n",
            "Step 65350: Minibatch Loss: 2.921102\n",
            "Step 65352: Minibatch Loss: 3.074517\n",
            "Step 65354: Minibatch Loss: 2.975624\n",
            "Step 65356: Minibatch Loss: 2.902470\n",
            "Step 65358: Minibatch Loss: 2.750055\n",
            "Step 65360: Minibatch Loss: 3.047789\n",
            "Step 65362: Minibatch Loss: 2.560471\n",
            "Step 65364: Minibatch Loss: 3.095885\n",
            "Step 65366: Minibatch Loss: 2.990725\n",
            "Step 65368: Minibatch Loss: 2.878885\n",
            "Step 65370: Minibatch Loss: 3.064003\n",
            "Step 65372: Minibatch Loss: 3.035653\n",
            "Step 65374: Minibatch Loss: 3.018554\n",
            "Step 65376: Minibatch Loss: 2.998803\n",
            "Step 65378: Minibatch Loss: 3.079697\n",
            "Step 65380: Minibatch Loss: 2.724936\n",
            "Step 65382: Minibatch Loss: 2.523890\n",
            "Step 65384: Minibatch Loss: 2.964720\n",
            "Step 65386: Minibatch Loss: 3.017870\n",
            "Step 65388: Minibatch Loss: 2.712232\n",
            "Step 65390: Minibatch Loss: 2.673616\n",
            "Step 65392: Minibatch Loss: 2.939747\n",
            "Step 65394: Minibatch Loss: 3.096845\n",
            "Step 65396: Minibatch Loss: 2.880636\n",
            "Step 65398: Minibatch Loss: 3.042429\n",
            "Step 65400: Minibatch Loss: 2.938814\n",
            "Step 65402: Minibatch Loss: 2.977644\n",
            "Step 65404: Minibatch Loss: 2.840137\n",
            "Step 65406: Minibatch Loss: 2.715487\n",
            "Step 65408: Minibatch Loss: 2.948743\n",
            "Step 65410: Minibatch Loss: 2.761607\n",
            "Step 65412: Minibatch Loss: 2.857671\n",
            "Step 65414: Minibatch Loss: 2.705695\n",
            "Step 65416: Minibatch Loss: 3.027185\n",
            "Step 65418: Minibatch Loss: 2.668486\n",
            "Step 65420: Minibatch Loss: 2.973947\n",
            "Step 65422: Minibatch Loss: 3.104655\n",
            "Step 65424: Minibatch Loss: 2.859914\n",
            "Step 65426: Minibatch Loss: 3.217301\n",
            "Step 65428: Minibatch Loss: 2.903305\n",
            "Step 65430: Minibatch Loss: 2.564660\n",
            "Step 65432: Minibatch Loss: 3.013013\n",
            "Step 65434: Minibatch Loss: 2.752489\n",
            "Step 65436: Minibatch Loss: 2.609942\n",
            "Step 65438: Minibatch Loss: 2.955108\n",
            "Step 65440: Minibatch Loss: 2.999664\n",
            "Step 65442: Minibatch Loss: 3.257653\n",
            "Step 65444: Minibatch Loss: 3.103171\n",
            "Step 65446: Minibatch Loss: 2.668361\n",
            "Step 65448: Minibatch Loss: 2.732606\n",
            "Step 65450: Minibatch Loss: 2.504516\n",
            "Step 65452: Minibatch Loss: 2.872156\n",
            "Step 65454: Minibatch Loss: 2.854205\n",
            "Step 65456: Minibatch Loss: 2.695443\n",
            "Step 65458: Minibatch Loss: 3.107847\n",
            "Step 65460: Minibatch Loss: 2.958887\n",
            "Step 65462: Minibatch Loss: 2.663746\n",
            "Step 65464: Minibatch Loss: 3.036699\n",
            "Step 65466: Minibatch Loss: 3.084544\n",
            "Step 65468: Minibatch Loss: 2.996647\n",
            "Step 65470: Minibatch Loss: 2.800379\n",
            "Step 65472: Minibatch Loss: 3.003144\n",
            "Step 65474: Minibatch Loss: 3.152652\n",
            "Step 65476: Minibatch Loss: 3.017275\n",
            "Step 65478: Minibatch Loss: 3.004557\n",
            "Step 65480: Minibatch Loss: 2.981103\n",
            "Step 65482: Minibatch Loss: 2.900771\n",
            "Step 65484: Minibatch Loss: 3.206315\n",
            "Step 65486: Minibatch Loss: 2.856025\n",
            "Step 65488: Minibatch Loss: 3.041882\n",
            "Step 65490: Minibatch Loss: 3.012115\n",
            "Step 65492: Minibatch Loss: 2.897899\n",
            "Step 65494: Minibatch Loss: 2.594789\n",
            "Step 65496: Minibatch Loss: 2.729438\n",
            "Step 65498: Minibatch Loss: 2.965493\n",
            "Step 65500: Minibatch Loss: 2.681118\n",
            "Step 65502: Minibatch Loss: 3.218020\n",
            "Step 65504: Minibatch Loss: 2.895691\n",
            "Step 65506: Minibatch Loss: 2.957940\n",
            "Step 65508: Minibatch Loss: 3.050035\n",
            "Step 65510: Minibatch Loss: 2.979739\n",
            "Step 65512: Minibatch Loss: 2.737244\n",
            "Step 65514: Minibatch Loss: 2.863712\n",
            "Step 65516: Minibatch Loss: 2.734626\n",
            "Step 65518: Minibatch Loss: 3.045222\n",
            "Step 65520: Minibatch Loss: 2.784169\n",
            "Step 65522: Minibatch Loss: 2.838998\n",
            "Step 65524: Minibatch Loss: 3.013196\n",
            "Step 65526: Minibatch Loss: 2.865825\n",
            "Step 65528: Minibatch Loss: 2.746944\n",
            "Step 65530: Minibatch Loss: 3.006444\n",
            "Step 65532: Minibatch Loss: 3.028678\n",
            "Step 65534: Minibatch Loss: 3.115426\n",
            "Step 65536: Minibatch Loss: 2.907442\n",
            "Step 65538: Minibatch Loss: 2.792736\n",
            "Step 65540: Minibatch Loss: 2.877963\n",
            "Step 65542: Minibatch Loss: 2.684301\n",
            "Step 65544: Minibatch Loss: 2.885238\n",
            "Step 65546: Minibatch Loss: 2.938786\n",
            "Step 65548: Minibatch Loss: 3.176910\n",
            "Step 65550: Minibatch Loss: 2.857021\n",
            "Step 65552: Minibatch Loss: 3.072363\n",
            "Step 65554: Minibatch Loss: 3.299960\n",
            "Step 65556: Minibatch Loss: 3.131666\n",
            "Step 65558: Minibatch Loss: 2.501477\n",
            "Step 65560: Minibatch Loss: 2.946251\n",
            "Step 65562: Minibatch Loss: 2.771754\n",
            "Step 65564: Minibatch Loss: 2.894528\n",
            "Step 65566: Minibatch Loss: 2.982300\n",
            "Step 65568: Minibatch Loss: 2.807726\n",
            "Step 65570: Minibatch Loss: 2.787409\n",
            "Step 65572: Minibatch Loss: 2.733980\n",
            "Step 65574: Minibatch Loss: 2.842915\n",
            "Step 65576: Minibatch Loss: 2.739915\n",
            "Step 65578: Minibatch Loss: 2.980181\n",
            "Step 65580: Minibatch Loss: 2.841101\n",
            "Step 65582: Minibatch Loss: 3.152596\n",
            "Step 65584: Minibatch Loss: 2.985482\n",
            "Step 65586: Minibatch Loss: 2.971146\n",
            "Step 65588: Minibatch Loss: 3.011204\n",
            "Step 65590: Minibatch Loss: 2.850560\n",
            "Step 65592: Minibatch Loss: 2.832039\n",
            "Step 65594: Minibatch Loss: 3.081026\n",
            "Step 65596: Minibatch Loss: 3.206260\n",
            "Step 65598: Minibatch Loss: 2.968356\n",
            "Step 65600: Minibatch Loss: 2.835975\n",
            "Step 65602: Minibatch Loss: 3.053546\n",
            "Step 65604: Minibatch Loss: 2.851524\n",
            "Step 65606: Minibatch Loss: 2.954319\n",
            "Step 65608: Minibatch Loss: 2.705320\n",
            "Step 65610: Minibatch Loss: 3.054697\n",
            "Step 65612: Minibatch Loss: 3.300610\n",
            "Step 65614: Minibatch Loss: 2.865399\n",
            "Step 65616: Minibatch Loss: 2.985667\n",
            "Step 65618: Minibatch Loss: 3.097210\n",
            "Step 65620: Minibatch Loss: 2.869349\n",
            "Step 65622: Minibatch Loss: 2.738328\n",
            "Step 65624: Minibatch Loss: 2.727414\n",
            "Step 65626: Minibatch Loss: 2.953052\n",
            "Step 65628: Minibatch Loss: 3.027950\n",
            "Step 65630: Minibatch Loss: 2.528816\n",
            "Step 65632: Minibatch Loss: 3.012310\n",
            "Step 65634: Minibatch Loss: 3.124463\n",
            "Step 65636: Minibatch Loss: 3.036351\n",
            "Step 65638: Minibatch Loss: 3.070574\n",
            "Step 65640: Minibatch Loss: 2.884730\n",
            "Step 65642: Minibatch Loss: 3.003699\n",
            "Step 65644: Minibatch Loss: 2.856287\n",
            "Step 65646: Minibatch Loss: 2.939752\n",
            "Step 65648: Minibatch Loss: 2.910700\n",
            "Step 65650: Minibatch Loss: 2.922769\n",
            "Step 65652: Minibatch Loss: 3.011534\n",
            "Step 65654: Minibatch Loss: 2.724757\n",
            "Step 65656: Minibatch Loss: 2.993876\n",
            "Step 65658: Minibatch Loss: 2.856091\n",
            "Step 65660: Minibatch Loss: 2.874949\n",
            "Step 65662: Minibatch Loss: 2.707661\n",
            "Step 65664: Minibatch Loss: 2.640470\n",
            "Step 65666: Minibatch Loss: 3.185629\n",
            "Step 65668: Minibatch Loss: 3.221619\n",
            "Step 65670: Minibatch Loss: 2.919213\n",
            "Step 65672: Minibatch Loss: 2.951195\n",
            "Step 65674: Minibatch Loss: 2.661778\n",
            "Step 65676: Minibatch Loss: 2.810206\n",
            "Step 65678: Minibatch Loss: 2.813539\n",
            "Step 65680: Minibatch Loss: 2.648273\n",
            "Step 65682: Minibatch Loss: 2.721321\n",
            "Step 65684: Minibatch Loss: 2.894305\n",
            "Step 65686: Minibatch Loss: 2.811763\n",
            "Step 65688: Minibatch Loss: 3.040925\n",
            "Step 65690: Minibatch Loss: 2.868425\n",
            "Step 65692: Minibatch Loss: 2.674125\n",
            "Step 65694: Minibatch Loss: 2.976705\n",
            "Step 65696: Minibatch Loss: 3.289678\n",
            "Step 65698: Minibatch Loss: 2.948831\n",
            "Step 65700: Minibatch Loss: 3.372133\n",
            "Step 65702: Minibatch Loss: 2.804417\n",
            "Step 65704: Minibatch Loss: 3.236302\n",
            "Step 65706: Minibatch Loss: 2.965351\n",
            "Step 65708: Minibatch Loss: 2.573070\n",
            "Step 65710: Minibatch Loss: 3.261903\n",
            "Step 65712: Minibatch Loss: 2.949379\n",
            "Step 65714: Minibatch Loss: 3.066638\n",
            "Step 65716: Minibatch Loss: 2.998310\n",
            "Step 65718: Minibatch Loss: 2.784893\n",
            "Step 65720: Minibatch Loss: 2.857132\n",
            "Step 65722: Minibatch Loss: 3.135597\n",
            "Step 65724: Minibatch Loss: 3.111221\n",
            "Step 65726: Minibatch Loss: 2.834123\n",
            "Step 65728: Minibatch Loss: 2.795805\n",
            "Step 65730: Minibatch Loss: 2.481976\n",
            "Step 65732: Minibatch Loss: 2.918114\n",
            "Step 65734: Minibatch Loss: 2.736042\n",
            "Step 65736: Minibatch Loss: 2.968118\n",
            "Step 65738: Minibatch Loss: 3.102500\n",
            "Step 65740: Minibatch Loss: 2.746201\n",
            "Step 65742: Minibatch Loss: 3.099148\n",
            "Step 65744: Minibatch Loss: 2.982105\n",
            "Step 65746: Minibatch Loss: 3.018472\n",
            "Step 65748: Minibatch Loss: 3.157133\n",
            "Step 65750: Minibatch Loss: 3.041687\n",
            "Step 65752: Minibatch Loss: 3.034286\n",
            "Step 65754: Minibatch Loss: 2.853708\n",
            "Step 65756: Minibatch Loss: 3.071610\n",
            "Step 65758: Minibatch Loss: 2.987802\n",
            "Step 65760: Minibatch Loss: 3.157487\n",
            "Step 65762: Minibatch Loss: 2.841905\n",
            "Step 65764: Minibatch Loss: 2.892845\n",
            "Step 65766: Minibatch Loss: 2.959667\n",
            "Step 65768: Minibatch Loss: 3.017373\n",
            "Step 65770: Minibatch Loss: 3.035239\n",
            "Step 65772: Minibatch Loss: 3.008212\n",
            "Step 65774: Minibatch Loss: 3.152001\n",
            "Step 65776: Minibatch Loss: 2.821512\n",
            "Step 65778: Minibatch Loss: 3.256401\n",
            "Step 65780: Minibatch Loss: 3.121886\n",
            "Step 65782: Minibatch Loss: 2.734669\n",
            "Step 65784: Minibatch Loss: 2.866143\n",
            "Step 65786: Minibatch Loss: 3.039625\n",
            "Step 65788: Minibatch Loss: 2.935246\n",
            "Step 65790: Minibatch Loss: 2.784056\n",
            "Step 65792: Minibatch Loss: 2.949627\n",
            "Step 65794: Minibatch Loss: 3.102819\n",
            "Step 65796: Minibatch Loss: 3.067753\n",
            "Step 65798: Minibatch Loss: 3.076450\n",
            "Step 65800: Minibatch Loss: 2.848824\n",
            "Step 65802: Minibatch Loss: 3.050941\n",
            "Step 65804: Minibatch Loss: 3.413611\n",
            "Step 65806: Minibatch Loss: 2.638429\n",
            "Step 65808: Minibatch Loss: 3.055491\n",
            "Step 65810: Minibatch Loss: 2.992337\n",
            "Step 65812: Minibatch Loss: 3.126323\n",
            "Step 65814: Minibatch Loss: 3.124672\n",
            "Step 65816: Minibatch Loss: 2.969977\n",
            "Step 65818: Minibatch Loss: 3.248825\n",
            "Step 65820: Minibatch Loss: 3.265798\n",
            "Step 65822: Minibatch Loss: 2.968266\n",
            "Step 65824: Minibatch Loss: 2.728826\n",
            "Step 65826: Minibatch Loss: 2.610879\n",
            "Step 65828: Minibatch Loss: 3.186231\n",
            "Step 65830: Minibatch Loss: 3.046305\n",
            "Step 65832: Minibatch Loss: 3.183441\n",
            "Step 65834: Minibatch Loss: 2.842545\n",
            "Step 65836: Minibatch Loss: 3.024848\n",
            "Step 65838: Minibatch Loss: 3.108634\n",
            "Step 65840: Minibatch Loss: 3.113884\n",
            "Step 65842: Minibatch Loss: 3.045290\n",
            "Step 65844: Minibatch Loss: 3.062674\n",
            "Step 65846: Minibatch Loss: 2.978192\n",
            "Step 65848: Minibatch Loss: 2.855696\n",
            "Step 65850: Minibatch Loss: 2.914807\n",
            "Step 65852: Minibatch Loss: 3.099239\n",
            "Step 65854: Minibatch Loss: 2.862431\n",
            "Step 65856: Minibatch Loss: 2.854271\n",
            "Step 65858: Minibatch Loss: 2.986215\n",
            "Step 65860: Minibatch Loss: 2.807368\n",
            "Step 65862: Minibatch Loss: 3.008443\n",
            "Step 65864: Minibatch Loss: 3.061689\n",
            "Step 65866: Minibatch Loss: 2.858087\n",
            "Step 65868: Minibatch Loss: 2.803544\n",
            "Step 65870: Minibatch Loss: 2.989699\n",
            "Step 65872: Minibatch Loss: 3.392133\n",
            "Step 65874: Minibatch Loss: 2.608602\n",
            "Step 65876: Minibatch Loss: 3.099694\n",
            "Step 65878: Minibatch Loss: 2.878616\n",
            "Step 65880: Minibatch Loss: 2.895974\n",
            "Step 65882: Minibatch Loss: 3.084573\n",
            "Step 65884: Minibatch Loss: 2.995540\n",
            "Step 65886: Minibatch Loss: 2.859552\n",
            "Step 65888: Minibatch Loss: 2.859657\n",
            "Step 65890: Minibatch Loss: 3.006518\n",
            "Step 65892: Minibatch Loss: 2.739834\n",
            "Step 65894: Minibatch Loss: 2.909681\n",
            "Step 65896: Minibatch Loss: 3.028782\n",
            "Step 65898: Minibatch Loss: 2.861349\n",
            "Step 65900: Minibatch Loss: 2.913781\n",
            "Step 65902: Minibatch Loss: 3.133204\n",
            "Step 65904: Minibatch Loss: 2.651065\n",
            "Step 65906: Minibatch Loss: 2.998160\n",
            "Step 65908: Minibatch Loss: 3.054363\n",
            "Step 65910: Minibatch Loss: 2.845928\n",
            "Step 65912: Minibatch Loss: 2.887619\n",
            "Step 65914: Minibatch Loss: 2.918685\n",
            "Step 65916: Minibatch Loss: 2.895622\n",
            "Step 65918: Minibatch Loss: 3.199851\n",
            "Step 65920: Minibatch Loss: 3.101012\n",
            "Step 65922: Minibatch Loss: 2.772186\n",
            "Step 65924: Minibatch Loss: 2.961646\n",
            "Step 65926: Minibatch Loss: 3.093027\n",
            "Step 65928: Minibatch Loss: 3.144362\n",
            "Step 65930: Minibatch Loss: 3.132472\n",
            "Step 65932: Minibatch Loss: 3.045773\n",
            "Step 65934: Minibatch Loss: 2.821070\n",
            "Step 65936: Minibatch Loss: 2.734496\n",
            "Step 65938: Minibatch Loss: 3.024128\n",
            "Step 65940: Minibatch Loss: 2.898218\n",
            "Step 65942: Minibatch Loss: 2.905586\n",
            "Step 65944: Minibatch Loss: 3.102915\n",
            "Step 65946: Minibatch Loss: 2.668766\n",
            "Step 65948: Minibatch Loss: 2.780341\n",
            "Step 65950: Minibatch Loss: 2.931786\n",
            "Step 65952: Minibatch Loss: 2.914374\n",
            "Step 65954: Minibatch Loss: 2.950824\n",
            "Step 65956: Minibatch Loss: 2.875448\n",
            "Step 65958: Minibatch Loss: 3.066204\n",
            "Step 65960: Minibatch Loss: 2.483709\n",
            "Step 65962: Minibatch Loss: 3.142206\n",
            "Step 65964: Minibatch Loss: 2.900637\n",
            "Step 65966: Minibatch Loss: 2.777291\n",
            "Step 65968: Minibatch Loss: 2.961014\n",
            "Step 65970: Minibatch Loss: 3.043497\n",
            "Step 65972: Minibatch Loss: 2.707183\n",
            "Step 65974: Minibatch Loss: 2.919879\n",
            "Step 65976: Minibatch Loss: 3.202506\n",
            "Step 65978: Minibatch Loss: 3.398948\n",
            "Step 65980: Minibatch Loss: 2.934834\n",
            "Step 65982: Minibatch Loss: 2.960587\n",
            "Step 65984: Minibatch Loss: 2.934041\n",
            "Step 65986: Minibatch Loss: 3.003247\n",
            "Step 65988: Minibatch Loss: 2.866397\n",
            "Step 65990: Minibatch Loss: 2.688074\n",
            "Step 65992: Minibatch Loss: 2.994679\n",
            "Step 65994: Minibatch Loss: 3.073695\n",
            "Step 65996: Minibatch Loss: 2.657142\n",
            "Step 65998: Minibatch Loss: 3.194698\n",
            "Step 66000: Minibatch Loss: 3.050469\n",
            "Step 66002: Minibatch Loss: 2.697764\n",
            "Step 66004: Minibatch Loss: 2.991252\n",
            "Step 66006: Minibatch Loss: 2.951506\n",
            "Step 66008: Minibatch Loss: 2.738361\n",
            "Step 66010: Minibatch Loss: 2.866292\n",
            "Step 66012: Minibatch Loss: 2.955952\n",
            "Step 66014: Minibatch Loss: 3.019969\n",
            "Step 66016: Minibatch Loss: 2.842220\n",
            "Step 66018: Minibatch Loss: 3.066127\n",
            "Step 66020: Minibatch Loss: 2.854838\n",
            "Step 66022: Minibatch Loss: 2.896510\n",
            "Step 66024: Minibatch Loss: 2.752168\n",
            "Step 66026: Minibatch Loss: 3.016900\n",
            "Step 66028: Minibatch Loss: 2.596310\n",
            "Step 66030: Minibatch Loss: 3.044963\n",
            "Step 66032: Minibatch Loss: 2.816795\n",
            "Step 66034: Minibatch Loss: 2.814479\n",
            "Step 66036: Minibatch Loss: 2.948814\n",
            "Step 66038: Minibatch Loss: 2.970798\n",
            "Step 66040: Minibatch Loss: 2.980292\n",
            "Step 66042: Minibatch Loss: 2.956269\n",
            "Step 66044: Minibatch Loss: 2.995550\n",
            "Step 66046: Minibatch Loss: 2.718383\n",
            "Step 66048: Minibatch Loss: 2.548760\n",
            "Step 66050: Minibatch Loss: 2.885871\n",
            "Step 66052: Minibatch Loss: 2.932579\n",
            "Step 66054: Minibatch Loss: 2.774795\n",
            "Step 66056: Minibatch Loss: 2.656394\n",
            "Step 66058: Minibatch Loss: 2.922276\n",
            "Step 66060: Minibatch Loss: 3.039826\n",
            "Step 66062: Minibatch Loss: 2.903204\n",
            "Step 66064: Minibatch Loss: 3.084880\n",
            "Step 66066: Minibatch Loss: 2.918514\n",
            "Step 66068: Minibatch Loss: 2.982275\n",
            "Step 66070: Minibatch Loss: 2.778578\n",
            "Step 66072: Minibatch Loss: 2.785902\n",
            "Step 66074: Minibatch Loss: 3.028764\n",
            "Step 66076: Minibatch Loss: 2.812852\n",
            "Step 66078: Minibatch Loss: 2.882012\n",
            "Step 66080: Minibatch Loss: 2.764575\n",
            "Step 66082: Minibatch Loss: 3.028109\n",
            "Step 66084: Minibatch Loss: 2.735632\n",
            "Step 66086: Minibatch Loss: 2.835867\n",
            "Step 66088: Minibatch Loss: 3.106619\n",
            "Step 66090: Minibatch Loss: 2.975839\n",
            "Step 66092: Minibatch Loss: 3.068675\n",
            "Step 66094: Minibatch Loss: 2.946388\n",
            "Step 66096: Minibatch Loss: 2.535998\n",
            "Step 66098: Minibatch Loss: 2.968859\n",
            "Step 66100: Minibatch Loss: 2.729012\n",
            "Step 66102: Minibatch Loss: 2.670929\n",
            "Step 66104: Minibatch Loss: 2.825535\n",
            "Step 66106: Minibatch Loss: 3.062526\n",
            "Step 66108: Minibatch Loss: 3.147167\n",
            "Step 66110: Minibatch Loss: 3.014368\n",
            "Step 66112: Minibatch Loss: 2.742945\n",
            "Step 66114: Minibatch Loss: 2.744906\n",
            "Step 66116: Minibatch Loss: 2.558298\n",
            "Step 66118: Minibatch Loss: 2.875754\n",
            "Step 66120: Minibatch Loss: 2.728561\n",
            "Step 66122: Minibatch Loss: 2.681572\n",
            "Step 66124: Minibatch Loss: 3.164930\n",
            "Step 66126: Minibatch Loss: 2.977088\n",
            "Step 66128: Minibatch Loss: 2.690348\n",
            "Step 66130: Minibatch Loss: 3.007839\n",
            "Step 66132: Minibatch Loss: 3.032676\n",
            "Step 66134: Minibatch Loss: 2.878111\n",
            "Step 66136: Minibatch Loss: 2.836014\n",
            "Step 66138: Minibatch Loss: 2.988572\n",
            "Step 66140: Minibatch Loss: 3.253131\n",
            "Step 66142: Minibatch Loss: 3.014096\n",
            "Step 66144: Minibatch Loss: 2.944697\n",
            "Step 66146: Minibatch Loss: 3.078839\n",
            "Step 66148: Minibatch Loss: 2.899078\n",
            "Step 66150: Minibatch Loss: 3.098197\n",
            "Step 66152: Minibatch Loss: 2.892873\n",
            "Step 66154: Minibatch Loss: 3.083891\n",
            "Step 66156: Minibatch Loss: 2.969310\n",
            "Step 66158: Minibatch Loss: 2.959891\n",
            "Step 66160: Minibatch Loss: 2.526609\n",
            "Step 66162: Minibatch Loss: 2.715440\n",
            "Step 66164: Minibatch Loss: 2.965011\n",
            "Step 66166: Minibatch Loss: 2.726644\n",
            "Step 66168: Minibatch Loss: 3.154799\n",
            "Step 66170: Minibatch Loss: 2.760118\n",
            "Step 66172: Minibatch Loss: 2.936058\n",
            "Step 66174: Minibatch Loss: 2.993678\n",
            "Step 66176: Minibatch Loss: 2.921502\n",
            "Step 66178: Minibatch Loss: 2.766809\n",
            "Step 66180: Minibatch Loss: 2.789813\n",
            "Step 66182: Minibatch Loss: 2.630103\n",
            "Step 66184: Minibatch Loss: 3.127104\n",
            "Step 66186: Minibatch Loss: 2.684176\n",
            "Step 66188: Minibatch Loss: 2.779698\n",
            "Step 66190: Minibatch Loss: 2.914311\n",
            "Step 66192: Minibatch Loss: 2.821541\n",
            "Step 66194: Minibatch Loss: 2.781468\n",
            "Step 66196: Minibatch Loss: 3.041335\n",
            "Step 66198: Minibatch Loss: 2.993096\n",
            "Step 66200: Minibatch Loss: 3.195054\n",
            "Step 66202: Minibatch Loss: 2.890726\n",
            "Step 66204: Minibatch Loss: 2.720948\n",
            "Step 66206: Minibatch Loss: 2.868500\n",
            "Step 66208: Minibatch Loss: 2.693491\n",
            "Step 66210: Minibatch Loss: 2.990912\n",
            "Step 66212: Minibatch Loss: 2.839486\n",
            "Step 66214: Minibatch Loss: 3.192090\n",
            "Step 66216: Minibatch Loss: 2.707119\n",
            "Step 66218: Minibatch Loss: 3.129152\n",
            "Step 66220: Minibatch Loss: 3.142553\n",
            "Step 66222: Minibatch Loss: 3.137595\n",
            "Step 66224: Minibatch Loss: 2.568671\n",
            "Step 66226: Minibatch Loss: 2.961473\n",
            "Step 66228: Minibatch Loss: 2.834333\n",
            "Step 66230: Minibatch Loss: 2.939430\n",
            "Step 66232: Minibatch Loss: 2.891780\n",
            "Step 66234: Minibatch Loss: 2.755939\n",
            "Step 66236: Minibatch Loss: 2.780286\n",
            "Step 66238: Minibatch Loss: 2.776083\n",
            "Step 66240: Minibatch Loss: 2.823024\n",
            "Step 66242: Minibatch Loss: 2.765895\n",
            "Step 66244: Minibatch Loss: 2.977094\n",
            "Step 66246: Minibatch Loss: 2.834442\n",
            "Step 66248: Minibatch Loss: 3.134757\n",
            "Step 66250: Minibatch Loss: 3.083470\n",
            "Step 66252: Minibatch Loss: 3.002343\n",
            "Step 66254: Minibatch Loss: 2.858990\n",
            "Step 66256: Minibatch Loss: 2.909280\n",
            "Step 66258: Minibatch Loss: 2.839014\n",
            "Step 66260: Minibatch Loss: 2.997285\n",
            "Step 66262: Minibatch Loss: 3.237538\n",
            "Step 66264: Minibatch Loss: 3.107911\n",
            "Step 66266: Minibatch Loss: 2.813529\n",
            "Step 66268: Minibatch Loss: 2.991765\n",
            "Step 66270: Minibatch Loss: 2.629998\n",
            "Step 66272: Minibatch Loss: 3.000725\n",
            "Step 66274: Minibatch Loss: 2.734672\n",
            "Step 66276: Minibatch Loss: 3.088470\n",
            "Step 66278: Minibatch Loss: 3.237096\n",
            "Step 66280: Minibatch Loss: 2.802158\n",
            "Step 66282: Minibatch Loss: 3.060055\n",
            "Step 66284: Minibatch Loss: 3.074273\n",
            "Step 66286: Minibatch Loss: 2.844737\n",
            "Step 66288: Minibatch Loss: 2.683181\n",
            "Step 66290: Minibatch Loss: 2.732408\n",
            "Step 66292: Minibatch Loss: 2.936338\n",
            "Step 66294: Minibatch Loss: 3.141040\n",
            "Step 66296: Minibatch Loss: 2.611685\n",
            "Step 66298: Minibatch Loss: 3.033176\n",
            "Step 66300: Minibatch Loss: 3.094579\n",
            "Step 66302: Minibatch Loss: 3.113682\n",
            "Step 66304: Minibatch Loss: 2.950832\n",
            "Step 66306: Minibatch Loss: 2.866655\n",
            "Step 66308: Minibatch Loss: 2.930994\n",
            "Step 66310: Minibatch Loss: 2.800208\n",
            "Step 66312: Minibatch Loss: 2.959613\n",
            "Step 66314: Minibatch Loss: 2.872982\n",
            "Step 66316: Minibatch Loss: 2.900551\n",
            "Step 66318: Minibatch Loss: 3.003768\n",
            "Step 66320: Minibatch Loss: 2.794521\n",
            "Step 66322: Minibatch Loss: 3.077055\n",
            "Step 66324: Minibatch Loss: 2.840206\n",
            "Step 66326: Minibatch Loss: 2.863688\n",
            "Step 66328: Minibatch Loss: 2.814541\n",
            "Step 66330: Minibatch Loss: 2.654223\n",
            "Step 66332: Minibatch Loss: 3.160332\n",
            "Step 66334: Minibatch Loss: 3.106037\n",
            "Step 66336: Minibatch Loss: 2.960126\n",
            "Step 66338: Minibatch Loss: 2.965014\n",
            "Step 66340: Minibatch Loss: 2.730806\n",
            "Step 66342: Minibatch Loss: 2.815701\n",
            "Step 66344: Minibatch Loss: 2.848956\n",
            "Step 66346: Minibatch Loss: 2.648297\n",
            "Step 66348: Minibatch Loss: 2.685486\n",
            "Step 66350: Minibatch Loss: 2.931278\n",
            "Step 66352: Minibatch Loss: 2.827736\n",
            "Step 66354: Minibatch Loss: 3.081889\n",
            "Step 66356: Minibatch Loss: 2.912086\n",
            "Step 66358: Minibatch Loss: 2.703655\n",
            "Step 66360: Minibatch Loss: 3.095243\n",
            "Step 66362: Minibatch Loss: 3.340625\n",
            "Step 66364: Minibatch Loss: 2.949018\n",
            "Step 66366: Minibatch Loss: 3.311291\n",
            "Step 66368: Minibatch Loss: 2.825085\n",
            "Step 66370: Minibatch Loss: 3.187302\n",
            "Step 66372: Minibatch Loss: 3.030275\n",
            "Step 66374: Minibatch Loss: 2.550812\n",
            "Step 66376: Minibatch Loss: 3.267636\n",
            "Step 66378: Minibatch Loss: 2.939271\n",
            "Step 66380: Minibatch Loss: 3.270150\n",
            "Step 66382: Minibatch Loss: 2.983364\n",
            "Step 66384: Minibatch Loss: 2.747691\n",
            "Step 66386: Minibatch Loss: 2.962856\n",
            "Step 66388: Minibatch Loss: 3.062826\n",
            "Step 66390: Minibatch Loss: 2.979087\n",
            "Step 66392: Minibatch Loss: 2.858598\n",
            "Step 66394: Minibatch Loss: 2.740206\n",
            "Step 66396: Minibatch Loss: 2.421378\n",
            "Step 66398: Minibatch Loss: 2.925198\n",
            "Step 66400: Minibatch Loss: 2.827456\n",
            "Step 66402: Minibatch Loss: 2.951725\n",
            "Step 66404: Minibatch Loss: 3.023140\n",
            "Step 66406: Minibatch Loss: 2.697719\n",
            "Step 66408: Minibatch Loss: 3.086119\n",
            "Step 66410: Minibatch Loss: 3.038208\n",
            "Step 66412: Minibatch Loss: 3.025041\n",
            "Step 66414: Minibatch Loss: 2.994402\n",
            "Step 66416: Minibatch Loss: 2.998981\n",
            "Step 66418: Minibatch Loss: 2.995143\n",
            "Step 66420: Minibatch Loss: 2.803346\n",
            "Step 66422: Minibatch Loss: 3.044372\n",
            "Step 66424: Minibatch Loss: 2.968049\n",
            "Step 66426: Minibatch Loss: 3.032883\n",
            "Step 66428: Minibatch Loss: 2.760183\n",
            "Step 66430: Minibatch Loss: 2.826760\n",
            "Step 66432: Minibatch Loss: 2.980799\n",
            "Step 66434: Minibatch Loss: 2.964614\n",
            "Step 66436: Minibatch Loss: 3.039569\n",
            "Step 66438: Minibatch Loss: 2.940118\n",
            "Step 66440: Minibatch Loss: 3.106557\n",
            "Step 66442: Minibatch Loss: 2.828368\n",
            "Step 66444: Minibatch Loss: 3.284564\n",
            "Step 66446: Minibatch Loss: 3.121903\n",
            "Step 66448: Minibatch Loss: 2.732494\n",
            "Step 66450: Minibatch Loss: 2.894774\n",
            "Step 66452: Minibatch Loss: 3.127500\n",
            "Step 66454: Minibatch Loss: 2.905307\n",
            "Step 66456: Minibatch Loss: 2.720845\n",
            "Step 66458: Minibatch Loss: 2.938346\n",
            "Step 66460: Minibatch Loss: 3.143875\n",
            "Step 66462: Minibatch Loss: 3.025355\n",
            "Step 66464: Minibatch Loss: 3.031678\n",
            "Step 66466: Minibatch Loss: 3.007961\n",
            "Step 66468: Minibatch Loss: 3.039084\n",
            "Step 66470: Minibatch Loss: 3.372218\n",
            "Step 66472: Minibatch Loss: 2.678924\n",
            "Step 66474: Minibatch Loss: 3.100215\n",
            "Step 66476: Minibatch Loss: 2.875750\n",
            "Step 66478: Minibatch Loss: 3.111413\n",
            "Step 66480: Minibatch Loss: 3.033582\n",
            "Step 66482: Minibatch Loss: 3.044711\n",
            "Step 66484: Minibatch Loss: 3.333893\n",
            "Step 66486: Minibatch Loss: 3.227272\n",
            "Step 66488: Minibatch Loss: 2.975547\n",
            "Step 66490: Minibatch Loss: 2.859790\n",
            "Step 66492: Minibatch Loss: 2.656034\n",
            "Step 66494: Minibatch Loss: 3.087688\n",
            "Step 66496: Minibatch Loss: 2.947701\n",
            "Step 66498: Minibatch Loss: 3.230940\n",
            "Step 66500: Minibatch Loss: 2.888572\n",
            "Step 66502: Minibatch Loss: 3.017706\n",
            "Step 66504: Minibatch Loss: 3.042350\n",
            "Step 66506: Minibatch Loss: 3.204726\n",
            "Step 66508: Minibatch Loss: 2.988724\n",
            "Step 66510: Minibatch Loss: 2.965628\n",
            "Step 66512: Minibatch Loss: 2.880618\n",
            "Step 66514: Minibatch Loss: 2.851517\n",
            "Step 66516: Minibatch Loss: 2.925679\n",
            "Step 66518: Minibatch Loss: 3.059431\n",
            "Step 66520: Minibatch Loss: 2.955324\n",
            "Step 66522: Minibatch Loss: 2.807729\n",
            "Step 66524: Minibatch Loss: 3.056410\n",
            "Step 66526: Minibatch Loss: 2.861545\n",
            "Step 66528: Minibatch Loss: 3.019775\n",
            "Step 66530: Minibatch Loss: 3.113005\n",
            "Step 66532: Minibatch Loss: 2.782382\n",
            "Step 66534: Minibatch Loss: 2.730905\n",
            "Step 66536: Minibatch Loss: 3.040682\n",
            "Step 66538: Minibatch Loss: 3.413156\n",
            "Step 66540: Minibatch Loss: 2.595606\n",
            "Step 66542: Minibatch Loss: 3.010569\n",
            "Step 66544: Minibatch Loss: 2.746330\n",
            "Step 66546: Minibatch Loss: 2.886399\n",
            "Step 66548: Minibatch Loss: 3.067790\n",
            "Step 66550: Minibatch Loss: 3.034055\n",
            "Step 66552: Minibatch Loss: 2.780808\n",
            "Step 66554: Minibatch Loss: 2.905906\n",
            "Step 66556: Minibatch Loss: 3.090030\n",
            "Step 66558: Minibatch Loss: 2.891739\n",
            "Step 66560: Minibatch Loss: 2.834702\n",
            "Step 66562: Minibatch Loss: 2.931564\n",
            "Step 66564: Minibatch Loss: 2.727134\n",
            "Step 66566: Minibatch Loss: 2.903713\n",
            "Step 66568: Minibatch Loss: 3.268080\n",
            "Step 66570: Minibatch Loss: 2.648571\n",
            "Step 66572: Minibatch Loss: 2.987845\n",
            "Step 66574: Minibatch Loss: 2.984825\n",
            "Step 66576: Minibatch Loss: 2.843242\n",
            "Step 66578: Minibatch Loss: 2.738950\n",
            "Step 66580: Minibatch Loss: 2.822144\n",
            "Step 66582: Minibatch Loss: 2.830277\n",
            "Step 66584: Minibatch Loss: 3.249592\n",
            "Step 66586: Minibatch Loss: 3.065870\n",
            "Step 66588: Minibatch Loss: 2.848014\n",
            "Step 66590: Minibatch Loss: 2.948704\n",
            "Step 66592: Minibatch Loss: 3.079296\n",
            "Step 66594: Minibatch Loss: 3.265617\n",
            "Step 66596: Minibatch Loss: 3.205291\n",
            "Step 66598: Minibatch Loss: 2.960984\n",
            "Step 66600: Minibatch Loss: 2.809082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHByzQbTUqbv",
        "outputId": "8458c41e-cceb-490d-98f0-c94c2d07a47b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using trained model\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_dl_tensor  = numpy.array(())\n",
        "times_per_iter_dl_tensor = numpy.array(())\n",
        "\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  lrate = 0.001\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    input_message_xx = training_input_message_one_hot [i:i+1]\n",
        "    input_message_xx = input_message_xx.astype(\"float32\")\n",
        "    #,input_message_x_label:training_input_message [i]\n",
        "    encoded_message = train_sess.run ([dl_encoder_output], feed_dict={input_message_x:input_message_xx })\n",
        "    encoded_message = encoded_message[0][0]\n",
        "    #encoded_message = numpy.around(encoded_message[0][0]> 0).astype(int)\n",
        "    #print (encoded_message[0][0])\n",
        "    awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message})\n",
        "    #print (awgn_channel_output_message)\n",
        "    decoded_message = train_sess.run ([dl_decoder_only_output], feed_dict={input_channel_x:awgn_channel_output_message})\n",
        "    #print (\"input\", input_message[i])\n",
        "    #decoded_message = numpy.around(decoded_message[0][0]> 0).astype(int)\n",
        "    #rint (\"output\", decoded_message)\n",
        "    #print (\"output\", numpy.argmax(training_input_message_one_hot[i]), numpy.argmax(decoded_message[0][0]))\n",
        "    if (numpy.argmax(training_input_message_one_hot[i]) != numpy.argmax(decoded_message[0][0])):\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_dl_tensor=numpy.append(ber_per_iter_dl_tensor ,ber)\n",
        "  times_per_iter_dl_tensor=numpy.append(times_per_iter_dl_tensor, total_time)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.91\n",
            " -> Total Time: 2.46s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.90\n",
            " -> Total Time: 2.45s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.90\n",
            " -> Total Time: 2.45s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.89\n",
            " -> Total Time: 2.43s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.90\n",
            " -> Total Time: 2.43s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.90\n",
            " -> Total Time: 2.50s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.91\n",
            " -> Total Time: 2.46s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.90\n",
            " -> Total Time: 2.43s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.91\n",
            " -> Total Time: 2.50s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.90\n",
            " -> Total Time: 2.43s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.90\n",
            " -> Total Time: 2.44s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.90\n",
            " -> Total Time: 2.42s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.91\n",
            " -> Total Time: 2.44s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.90\n",
            " -> Total Time: 2.43s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.90\n",
            " -> Total Time: 2.48s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.89\n",
            " -> Total Time: 2.46s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.91\n",
            " -> Total Time: 2.44s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.90\n",
            " -> Total Time: 2.41s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.23s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.47s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.90\n",
            " -> Total Time: 2.39s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.23s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.47s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.89\n",
            " -> Total Time: 2.39s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syUQij3fuxRm",
        "outputId": "b2954b24-f1fc-47c1-b7a8-fecdee342ac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1) = plt.subplots(1,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"ldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor,'', label=\"ai-dl\") # plot BER vs SNR\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "#ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "#ax2.set_xlabel('$E_b/$N_0$')\n",
        "#ax2.set_ylabel('Decoding Time [s]')\n",
        "#ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "#            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "#            xy=(1, 0.35), xycoords='axes fraction',\n",
        "#            xytext=(-20, 20), textcoords='offset pixels',\n",
        "#            horizontalalignment='right',\n",
        "#            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpIbD_FUw-iL",
        "outputId": "4fffbd5f-74e4-466c-e111-f3949b63152d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "import numpy\n",
        "training_input_message = numpy.random.randint(2**input_message_length, size=(1,NUM_OF_INPUT_MESSAGE))\n",
        "training_input_message_one_hot = numpy.zeros((training_input_message.size, 2**input_message_length))\n",
        "training_input_message_one_hot[numpy.arange(training_input_message.size),training_input_message] = 1\n",
        "print(training_input_message_one_hot)\n",
        "print (training_input_message_one_hot.shape)\n",
        "print (training_input_message.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(1000, 16)\n",
            "(1, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "farYxiF5xJFj",
        "outputId": "0a3de053-e7ab-4931-f107-b4aeea8ba9cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using trained model\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_dl_tensor  = numpy.array(())\n",
        "times_per_iter_dl_tensor = numpy.array(())\n",
        "\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  lrate = 0.001\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    input_message_xx = training_input_message_one_hot [i:i+1]\n",
        "    input_message_xx = input_message_xx.astype(\"float32\")\n",
        "    #,input_message_x_label:training_input_message [i]\n",
        "    encoded_message = train_sess.run ([dl_encoder_output], feed_dict={input_message_x:input_message_xx })\n",
        "    encoded_message = encoded_message[0][0]\n",
        "    #encoded_message = numpy.around(encoded_message[0][0]> 0).astype(int)\n",
        "    #print (encoded_message[0][0])\n",
        "    awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message})\n",
        "    #print (awgn_channel_output_message)\n",
        "    decoded_message = train_sess.run ([dl_decoder_only_output], feed_dict={input_channel_x:awgn_channel_output_message})\n",
        "    #print (\"input\", input_message[i])\n",
        "    #decoded_message = numpy.around(decoded_message[0][0]> 0).astype(int)\n",
        "    #rint (\"output\", decoded_message)\n",
        "    #print (\"output\", numpy.argmax(training_input_message_one_hot[i]), numpy.argmax(decoded_message[0][0]))\n",
        "    if (numpy.argmax(training_input_message_one_hot[i]) != numpy.argmax(decoded_message[0][0])):\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_dl_tensor=numpy.append(ber_per_iter_dl_tensor ,ber)\n",
        "  times_per_iter_dl_tensor=numpy.append(times_per_iter_dl_tensor, total_time)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.92\n",
            " -> Total Time: 2.44s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.92\n",
            " -> Total Time: 2.52s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.90\n",
            " -> Total Time: 2.43s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.91\n",
            " -> Total Time: 2.45s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.92\n",
            " -> Total Time: 2.42s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.91\n",
            " -> Total Time: 2.44s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.91\n",
            " -> Total Time: 2.43s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.90\n",
            " -> Total Time: 2.44s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.90\n",
            " -> Total Time: 2.46s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.91\n",
            " -> Total Time: 2.42s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.91\n",
            " -> Total Time: 2.43s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.91\n",
            " -> Total Time: 2.42s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.91\n",
            " -> Total Time: 2.39s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.89\n",
            " -> Total Time: 2.47s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.91\n",
            " -> Total Time: 2.40s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.92\n",
            " -> Total Time: 2.42s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.90\n",
            " -> Total Time: 2.41s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.23s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.71s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.95s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.92\n",
            " -> Total Time: 2.37s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.23s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.47s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.71s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.91\n",
            " -> Total Time: 2.38s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.90\n",
            " -> Total Time: 2.44s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hV6_TllxLG_",
        "outputId": "dc75d21f-67f8-45db-928a-477b00ddaa92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1) = plt.subplots(1,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"ldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor,'', label=\"ai-dl\") # plot BER vs SNR\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "#ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "#ax2.set_xlabel('$E_b/$N_0$')\n",
        "#ax2.set_ylabel('Decoding Time [s]')\n",
        "#ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "#            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "#            xy=(1, 0.35), xycoords='axes fraction',\n",
        "#            xytext=(-20, 20), textcoords='offset pixels',\n",
        "#            horizontalalignment='right',\n",
        "#            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsQhRxkXxML5"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}