{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MainModelOneHotMethod.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamviji/project/blob/master/MainModelOneHotMethod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDSPPMfZ9czi",
        "outputId": "bdaae849-54a1-49f5-e270-0c40ad4711b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        }
      },
      "source": [
        "!rm -rf project\n",
        "!git clone https://github.com/iamviji/project.git\n",
        "!ls\n",
        "!ls project\n",
        "!pip install pyldpc\n",
        "!pip install scikit-commpy\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'project'...\n",
            "remote: Enumerating objects: 86, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
            "remote: Total 86 (delta 28), reused 8 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (86/86), done.\n",
            "project  sample_data\n",
            "MainModel.ipynb\t\t     MainModelOneHotMethodSoftMax.ipynb    util.py\n",
            "MainModelKeras.ipynb\t     MainModelWithSingleBERTraining.ipynb\n",
            "MainModelOneHotMethod.ipynb  README.md\n",
            "Collecting pyldpc\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/aa/fd5495869c7106a638ae71aa497d7d266cae7f2a343d1f6a9d0e3a986e1e/pyldpc-0.7.9.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyldpc) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyldpc) (1.4.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from pyldpc) (0.48.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->pyldpc) (50.3.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->pyldpc) (0.31.0)\n",
            "Building wheels for collected packages: pyldpc\n",
            "  Building wheel for pyldpc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyldpc: filename=pyldpc-0.7.9-cp36-none-any.whl size=14306 sha256=1c933bf817c02ba7027e623d39a9af49f5aca00fa7dc1abdc2b7ac94f1462f6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/7a/10/e94058ba8b0b6d98bf2719226d18d3dd6056525ad7b984c068\n",
            "Successfully built pyldpc\n",
            "Installing collected packages: pyldpc\n",
            "Successfully installed pyldpc-0.7.9\n",
            "Collecting scikit-commpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/b4/f7fa5bc8864e0ddbd3e7a2290b624b92690f53523474024915c33321802d/scikit_commpy-0.5.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->scikit-commpy) (1.15.0)\n",
            "Installing collected packages: scikit-commpy\n",
            "Successfully installed scikit-commpy-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QOuLqpdDgx2",
        "outputId": "4311209d-eb19-4afd-b92a-cdc21ae9b03a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import pyldpc\n",
        "import commpy\n",
        "import numpy \n",
        "import time\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior ()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YClXJbbr0lc7"
      },
      "source": [
        "SNR_BEGIN = 0\n",
        "SNR_END = 10\n",
        "SNR_STEP_SIZE = 0.5\n",
        "CHANEL_SIZE = 18\n",
        "NUM_OF_INPUT_MESSAGE = 1000\n",
        "LDPC_MAX_ITER = 100\n",
        "num_parity_check = 3\n",
        "num_bits_in_parity_check = 6 \n",
        "input_message_length =  0 # Caculated by channel encoder and initialized later"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvUzIMsB43i0"
      },
      "source": [
        "def timer_update(i,current,time_tot,tic_incr=500):\n",
        "    last = current\n",
        "    current = time.time()\n",
        "    t_diff = current-last\n",
        "    print('SNR: {:04.3f} - Iter: {} - Last {} iterations took {:03.2f}s'.format(snr,i+1,tic_incr,t_diff))\n",
        "    return time_tot + t_diff\n",
        "\n",
        "def Snr2Sigma(snr):\n",
        "  sigma = 10 ** (- snr / 20)\n",
        "  return sigma\n",
        "\n",
        "def pyldpc_encode (CodingMatrix, message):\n",
        "  rng = pyldpc.utils.check_random_state(seed=None)\n",
        "  d = pyldpc.utils.binaryproduct(CodingMatrix, message)\n",
        "  encoded_message = (-1) ** d\n",
        "  return encoded_message\n",
        "\n",
        "def pyldpc_decode (ParityCheckMatrix, CodingMatrix, message, snr, maxiter):\n",
        "  decoded_msg = pyldpc.decode(ParityCheckMatrix, message, snr, maxiter)\n",
        "  out_message = pyldpc.get_message(CodingMatrix, decoded_msg)\n",
        "  return out_message\n",
        "\n",
        "awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [CHANEL_SIZE])\n",
        "awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)\n",
        "\n",
        "init = tf.global_variables_initializer ()\n",
        "sess = tf.Session ()\n",
        "sess.run(init)\n",
        "\n",
        "def AWGNChannelOutput (xx, snr , s):\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  awgn_channel_output_message = s.run ([awgn_channel_output], feed_dict={noise_std_dev:sigma, channel_input:xx})\n",
        "  return awgn_channel_output_message"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jMQG-MZ_pXu",
        "outputId": "a3b7a755-a6ab-48c5-f574-1cb10f14e1bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "\n",
        "ParityCheckMatrix, CodingMatrix = pyldpc.make_ldpc(CHANEL_SIZE, num_parity_check, num_bits_in_parity_check, systematic=True, sparse=True)\n",
        "input_message_length = CodingMatrix.shape[1]\n",
        "print (\"input_message_size=\", input_message_length, \"channel_size=\",CHANEL_SIZE)\n",
        "print (\"input_message_size=\", CodingMatrix.shape[1], \"channel_size=\",CodingMatrix.shape[0])\n",
        "input_message = numpy.random.randint(2, size=(NUM_OF_INPUT_MESSAGE,input_message_length))\n",
        "print (input_message)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_message_size= 11 channel_size= 18\n",
            "input_message_size= 11 channel_size= 18\n",
            "[[1 1 0 ... 0 0 0]\n",
            " [1 0 1 ... 1 0 0]\n",
            " [0 0 1 ... 1 0 0]\n",
            " ...\n",
            " [0 1 0 ... 0 0 1]\n",
            " [1 1 1 ... 0 1 0]\n",
            " [0 0 0 ... 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WKg2HU2adgZ"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fL8ptL4aeOY"
      },
      "source": [
        "This section tries to compare BER and Time performance of PYLDPC in following 3 cases\n",
        "1. SNR Noise function provided in encoder function of pyldpc library (pyldpc.encode)\n",
        "2. SNR Noise function provided by commpy library (commpy.channels.awgn) \n",
        "3. SNR Noise function implemented using tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma5zUqFv0TH2",
        "outputId": "c06d593e-c349-4dcf-8388-016346f13e0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using tensor flow based AWGN, to make sure that effect of it is same as AWGN\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_tensor  = numpy.array(())\n",
        "times_per_iter_tensor = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc_encode (CodingMatrix, input_message[i])\n",
        "    sigma = Snr2Sigma (snr)\n",
        "    awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message})[0]\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      #print (\"count=\",abs(decoded_message-input_message[i]).sum())\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_tensor=numpy.append(ber_per_iter_tensor ,ber)\n",
        "  times_per_iter_tensor=numpy.append(times_per_iter_tensor, total_time)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.55s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 3.03s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 4.50s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 6.14s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.61\n",
            " -> Total Time: 15.22s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.23s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.55s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 4.04s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 5.20s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.55\n",
            " -> Total Time: 13.03s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 1.07s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 2.28s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 3.29s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 4.39s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.46\n",
            " -> Total Time: 11.02s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.93s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.84s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.66s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 3.51s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.35\n",
            " -> Total Time: 8.94s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.68s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.34s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 2.11s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.76s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.31\n",
            " -> Total Time: 6.89s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.59s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.21s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.85s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.45s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.26\n",
            " -> Total Time: 6.10s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.43s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.83s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.28s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.68s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.16\n",
            " -> Total Time: 4.23s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.36s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.68s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.09s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.51s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.12\n",
            " -> Total Time: 3.63s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.31s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.70s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 1.06s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.39s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.11\n",
            " -> Total Time: 3.45s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.31s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.60s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.92s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.23s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.07\n",
            " -> Total Time: 3.06s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.56s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.87s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 1.15s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 2.84s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.58s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.88s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 1.15s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 2.88s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.54s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.81s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 1.08s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 2.70s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.78s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 1.04s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 2.61s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.77s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 1.03s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.57s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.77s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 1.03s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.56s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.77s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 1.02s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.57s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.77s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 1.04s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.60s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.79s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 1.08s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.62s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.78s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 1.05s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.63s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8dIFLg76c7O",
        "outputId": "56af3666-10f3-4175-d291-85bc0198c00d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using commpy based AWGN \n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_awgn  = numpy.array(())\n",
        "times_per_iter_awgn = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc_encode (CodingMatrix, input_message[i])\n",
        "    awgn_channel_output_message = commpy.channels.awgn(encoded_message, snr)\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_awgn=numpy.append(ber_per_iter_awgn ,ber)\n",
        "  times_per_iter_awgn=numpy.append(times_per_iter_awgn, total_time)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.33s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 2.67s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 3.95s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 5.22s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.58\n",
            " -> Total Time: 13.18s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.05s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.23s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 3.26s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 4.36s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.55\n",
            " -> Total Time: 10.89s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.92s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 1.86s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 2.90s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 3.80s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.47\n",
            " -> Total Time: 9.47s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.78s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.51s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.19s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 3.00s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.40\n",
            " -> Total Time: 7.49s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.57s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.05s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 1.60s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.15s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.32\n",
            " -> Total Time: 5.38s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.40s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 0.88s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.35s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 1.80s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.26\n",
            " -> Total Time: 4.42s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.35s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.71s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.06s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.44s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.18\n",
            " -> Total Time: 3.56s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.36s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.60s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 0.87s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.22s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.12\n",
            " -> Total Time: 3.05s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 0.94s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.11\n",
            " -> Total Time: 2.47s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.20s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.42s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.62s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 0.83s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.07\n",
            " -> Total Time: 2.06s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.22s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.40s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.58s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.76s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 1.97s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.33s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.51s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.69s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 1.69s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.49s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.65s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 1.62s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.34s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.50s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.66s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.66s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.48s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.65s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.61s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.46s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.62s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.56s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.18s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.35s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.54s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.69s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.76s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.46s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.61s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.54s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.33s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.48s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.64s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.62s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.50s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.65s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.62s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ihPKJJk7Jj9",
        "outputId": "7d85f523-88bc-4b59-a889-af67cbde32d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using tensor flow based AWGN, to make sure that effect of it is same as AWGN\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_pyldpc  = numpy.array(())\n",
        "times_per_iter_pyldpc = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc.encode (CodingMatrix, input_message[i], snr)\n",
        "    awgn_channel_output_message = encoded_message\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_pyldpc=numpy.append(ber_per_iter_pyldpc ,ber)\n",
        "  times_per_iter_pyldpc=numpy.append(times_per_iter_pyldpc, total_time)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.48s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 2.85s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 4.15s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 5.72s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.64\n",
            " -> Total Time: 14.20s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.06s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.08s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 3.25s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 4.26s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.52\n",
            " -> Total Time: 10.66s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.81s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 1.69s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 2.68s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 3.69s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.47\n",
            " -> Total Time: 8.87s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.76s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.44s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.22s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 2.83s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.38\n",
            " -> Total Time: 7.25s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.53s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.12s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 1.68s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.23s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.31\n",
            " -> Total Time: 5.56s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.49s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 0.98s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.40s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 1.82s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.24\n",
            " -> Total Time: 4.68s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.32s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.66s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.02s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.45s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.18\n",
            " -> Total Time: 3.45s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.55s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 0.91s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.18s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.13\n",
            " -> Total Time: 2.91s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.67s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.11\n",
            " -> Total Time: 2.39s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.42s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.61s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 0.79s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.07\n",
            " -> Total Time: 1.99s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.18s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.38s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.57s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.76s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.06\n",
            " -> Total Time: 1.88s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.33s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.50s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.66s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 1.67s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.48s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.64s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 1.60s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.47s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.63s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.58s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.34s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.51s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.70s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.71s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.34s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.50s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.65s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.66s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.47s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.62s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.56s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.30s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.45s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.61s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.51s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.30s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.44s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.59s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.48s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.30s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.46s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.61s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.51s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR4-FOJ-BkAG",
        "outputId": "cb453fac-33d0-45fa-fee7-2fe364380444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "# Compare 3 AWGN(Tensorflow, CommPy, PYLDPC) Simulation on LDPC\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_pyldpc,'', label=\"pyldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"tensor\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_awgn,'', label=\"commpy-awgn\") # plot BER vs SNR\n",
        "\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "ax2.set_xlabel('$E_b/$N_0$')\n",
        "ax2.set_ylabel('Decoding Time [s]')\n",
        "ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "            xy=(1, 0.35), xycoords='axes fraction',\n",
        "            xytext=(-20, 20), textcoords='offset pixels',\n",
        "            horizontalalignment='right',\n",
        "            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggY5VwudaRwR"
      },
      "source": [
        "<B>Conclussion:</B>\n",
        "      It proved that tensorflow behaves similar to AWGN noise channel provided by pyldpc, commpy. But tensor flow based one takes adds little more time delay. This need to be offseted if we are comparing performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5wyTuef3s7u"
      },
      "source": [
        "class GetOutOfLoop( Exception ):\n",
        "    pass"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOeuNfeLCgfb",
        "outputId": "f54704e0-4f4c-4652-e6ec-791848ba7097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Define Model \n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "# input_message_length is initialized by ldpc encoder\n",
        "num_hidden_1 = CHANEL_SIZE\n",
        "print (\"input_message_length=\", input_message_length)\n",
        "\n",
        "lr_x = tf.placeholder(dtype=tf.float32,shape=[])\n",
        "#batch_size_x = tf.placeholder(tf.int32,shape=[])\n",
        "input_message_x_label = tf.placeholder(\"int32\", [None], name=\"input_message_x_label\")\n",
        "input_message_x = tf.placeholder(\"float32\", [None, 2**input_message_length], name=\"input_message_x\")\n",
        "awgn_noise_std_dev_x = tf.placeholder(\"float32\", name =\"awgn_noise_std_dev\")\n",
        "input_channel_x = tf.placeholder(\"float32\", [None, CHANEL_SIZE], name=\"input_channel_x\")\n",
        "\n",
        "weights = {\n",
        "  \"encoder_l1\" : tf.Variable (tf.random_uniform([2**input_message_length, num_hidden_1], -1, 1), name=\"encoder_l1_weights\"),\n",
        "  \"decoder_l1\" : tf.Variable (tf.random_uniform([num_hidden_1, 2**input_message_length], -1, 1), name=\"decoder_l1_weights\"),\n",
        "  \"decoder_l2\" : tf.Variable (tf.random_uniform([2**input_message_length, 2**input_message_length], -1, 1), name=\"decoder_l2_weights\"),\n",
        "}\n",
        "\n",
        "biases = {\n",
        "  \"encoder_l1\" : tf.Variable (tf.random_uniform([num_hidden_1], -1,1), name=\"encoder_l1_bias\"),\n",
        "  \"decoder_l1\" : tf.Variable (tf.random_uniform([2**input_message_length], -1,1), name=\"decoder_l1_bias\"),\n",
        "  \"decoder_l2\" : tf.Variable (tf.random_uniform([2**input_message_length], -1,1), name=\"decoder_l2_bias\"),\n",
        "}\n",
        "\n",
        "def dl_encoder (x):\n",
        "  layer_1 = tf.nn.tanh (tf.matmul(x, weights['encoder_l1']) + biases['encoder_l1'])\n",
        "  #layer_2 = tf.round(layer_1)\n",
        "  #layer_1 = BatchNormalization ()(layer_1)\n",
        "  layer_2 =  layer_1 / tf.sqrt(tf.reduce_mean(tf.square(layer_1)))\n",
        "  #layer_2 =  tf.nn.relu(layer_1)\n",
        "  return layer_2\n",
        "\n",
        "def dl_decoder (x):\n",
        "  layer_1 = tf.nn.tanh (tf.matmul(x, weights['decoder_l1']) + biases['decoder_l1'])\n",
        "  layer_2 = (tf.matmul(layer_1, weights['decoder_l2']) + biases['decoder_l2'])\n",
        "  return layer_2\n",
        "\n",
        "def awgn_layer(x):\n",
        "  awgn_noise = tf.random.normal(tf.shape(x), stddev=awgn_noise_std_dev_x,  name=\"awgn_noise\")\n",
        "  awgn_channel_output = tf.add(x, awgn_noise, name =\"x_and_noise\")\n",
        "  return awgn_channel_output\n",
        "\n",
        "\n",
        "dl_encoder_output = dl_encoder(input_message_x)\n",
        "dl_decoder_input = awgn_layer(dl_encoder_output)\n",
        "#awgn_noise = tf.random.normal(tf.shape(dl_encoder_output), stddev=awgn_noise_std_dev,  name=\"awgn_noise\")\n",
        "#dl_decoder_input = tf.add(dl_encoder_output, awgn_noise, name =\"x_and_noise\")\n",
        "dl_decoder_output = dl_decoder (dl_decoder_input)\n",
        "dl_decoder_only_output = dl_decoder(input_channel_x)\n",
        "\n",
        "\n",
        "#loss1 = tf.reduce_mean (-1 * (input_message_x*tf.log(dl_decoder_output) + (1 - input_message_x)*tf.log(1 - dl_decoder_output) ))\n",
        "loss = tf.losses.sparse_softmax_cross_entropy(labels=input_message_x_label,logits=dl_decoder_output)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr_x).minimize (loss)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_message_length= 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkW8oloyodIF",
        "outputId": "71c292c5-b4e4-4a4b-8df6-d0da275c86e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "import numpy\n",
        "training_input_message = numpy.random.randint(2**input_message_length, size=(1,NUM_OF_INPUT_MESSAGE*10))\n",
        "training_input_message_one_hot = numpy.zeros((training_input_message.size, 2**input_message_length))\n",
        "training_input_message_one_hot[numpy.arange(training_input_message.size),training_input_message] = 1\n",
        "print(training_input_message_one_hot)\n",
        "print (training_input_message_one_hot.shape)\n",
        "print (training_input_message.shape)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(10000, 2048)\n",
            "(1, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxkLQFuqBT-g",
        "outputId": "1a0f617a-a036-4b8c-f065-6c5d25ddb02a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 500\n",
        "\n",
        "# Training\n",
        "train_init = tf.global_variables_initializer ()\n",
        "train_sess = tf.Session ()\n",
        "\n",
        "epochs = 10\n",
        "outer_ephocs = 1\n",
        "display_step = 2\n",
        "num_of_batches = training_input_message.shape[1] / batch_size\n",
        "print (\"batch_size:\", batch_size, \"num_of_batcches:\", num_of_batches)\n",
        "train_sess.run(train_init)\n",
        "l = 0\n",
        "lrate = 0.1\n",
        "i = 0\n",
        "snr_min = 9.5\n",
        "snr_max = 10.5\n",
        "snr_step_size = 0.5\n",
        "max_iteration = epochs * num_of_batches * (snr_max - snr_min) / snr_step_size\n",
        "print (\"max iteration :\",max_iteration,\"num_of_batches:\", num_of_batches)\n",
        "try:\n",
        "  for oe in range(outer_ephocs):\n",
        "    for snr in (numpy.arange (0, 10, SNR_STEP_SIZE)):\n",
        "    #for snr in (numpy.arange (snr_min, snr_max, SNR_STEP_SIZE)):\n",
        "      sigma = 1.0*Snr2Sigma (snr)\n",
        "      print (\"Training for SNR=\", snr, \" sigma=\", sigma, \"iteratin:\", oe) \n",
        "      for e in range(epochs):\n",
        "        for j in range (int(num_of_batches)):\n",
        "          i = i + 1\n",
        "          x_train_batch_one_hot = training_input_message_one_hot [j*batch_size:(j+1)*batch_size]\n",
        "          x_train_batch_one_hot = x_train_batch_one_hot.astype(\"float32\")\n",
        "          x_train_batch_label = training_input_message.reshape(training_input_message.shape[1]) [j*batch_size:(j+1)*batch_size]        \n",
        "          if (i < 100): \n",
        "            lr = 0.1\n",
        "          elif(i < 200):\n",
        "            lr = 0.01\n",
        "          else:\n",
        "            lr = 0.001 \n",
        "          _, l = train_sess.run ([optimizer, loss], feed_dict={input_message_x:x_train_batch_one_hot, awgn_noise_std_dev_x:sigma, lr_x:lr, input_message_x_label:x_train_batch_label.astype(\"int32\")})\n",
        "          if i % display_step == 0:          \n",
        "            print('Step %i: Minibatch Loss: %f' % (i, l ))\n",
        "          if (l < 0.05 and snr >= 9): \n",
        "            print (\"Loss=\", l)\n",
        "            raise GetOutOfLoop\n",
        "except GetOutOfLoop:\n",
        "  print(\"Early Stop\")"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch_size: 500 num_of_batcches: 20.0\n",
            "max iteration : 400.0 num_of_batches: 20.0\n",
            "Training for SNR= 0.0  sigma= 1.0 iteratin: 0\n",
            "Step 2: Minibatch Loss: 128.252090\n",
            "Step 4: Minibatch Loss: 154.409851\n",
            "Step 6: Minibatch Loss: 149.371445\n",
            "Step 8: Minibatch Loss: 145.603485\n",
            "Step 10: Minibatch Loss: 137.764404\n",
            "Step 12: Minibatch Loss: 132.074158\n",
            "Step 14: Minibatch Loss: 129.249176\n",
            "Step 16: Minibatch Loss: 137.549026\n",
            "Step 18: Minibatch Loss: 132.854538\n",
            "Step 20: Minibatch Loss: 132.333618\n",
            "Step 22: Minibatch Loss: 136.100693\n",
            "Step 24: Minibatch Loss: 131.995453\n",
            "Step 26: Minibatch Loss: 136.370605\n",
            "Step 28: Minibatch Loss: 137.458572\n",
            "Step 30: Minibatch Loss: 128.167480\n",
            "Step 32: Minibatch Loss: 126.725937\n",
            "Step 34: Minibatch Loss: 135.894623\n",
            "Step 36: Minibatch Loss: 136.222351\n",
            "Step 38: Minibatch Loss: 141.393356\n",
            "Step 40: Minibatch Loss: 120.798019\n",
            "Step 42: Minibatch Loss: 113.857719\n",
            "Step 44: Minibatch Loss: 121.094856\n",
            "Step 46: Minibatch Loss: 131.562408\n",
            "Step 48: Minibatch Loss: 121.937874\n",
            "Step 50: Minibatch Loss: 118.482773\n",
            "Step 52: Minibatch Loss: 128.463959\n",
            "Step 54: Minibatch Loss: 125.120720\n",
            "Step 56: Minibatch Loss: 127.699333\n",
            "Step 58: Minibatch Loss: 120.837921\n",
            "Step 60: Minibatch Loss: 115.023079\n",
            "Step 62: Minibatch Loss: 114.308090\n",
            "Step 64: Minibatch Loss: 120.591789\n",
            "Step 66: Minibatch Loss: 117.210152\n",
            "Step 68: Minibatch Loss: 122.211685\n",
            "Step 70: Minibatch Loss: 124.050682\n",
            "Step 72: Minibatch Loss: 103.327507\n",
            "Step 74: Minibatch Loss: 121.563530\n",
            "Step 76: Minibatch Loss: 119.796349\n",
            "Step 78: Minibatch Loss: 123.100548\n",
            "Step 80: Minibatch Loss: 124.885811\n",
            "Step 82: Minibatch Loss: 119.374344\n",
            "Step 84: Minibatch Loss: 116.489365\n",
            "Step 86: Minibatch Loss: 126.305908\n",
            "Step 88: Minibatch Loss: 120.811668\n",
            "Step 90: Minibatch Loss: 118.184425\n",
            "Step 92: Minibatch Loss: 121.772552\n",
            "Step 94: Minibatch Loss: 121.529976\n",
            "Step 96: Minibatch Loss: 124.050591\n",
            "Step 98: Minibatch Loss: 123.973366\n",
            "Step 100: Minibatch Loss: 124.205742\n",
            "Step 102: Minibatch Loss: 118.725296\n",
            "Step 104: Minibatch Loss: 110.657448\n",
            "Step 106: Minibatch Loss: 100.222664\n",
            "Step 108: Minibatch Loss: 100.058311\n",
            "Step 110: Minibatch Loss: 100.304359\n",
            "Step 112: Minibatch Loss: 82.776009\n",
            "Step 114: Minibatch Loss: 90.910690\n",
            "Step 116: Minibatch Loss: 93.197067\n",
            "Step 118: Minibatch Loss: 80.453476\n",
            "Step 120: Minibatch Loss: 92.540741\n",
            "Step 122: Minibatch Loss: 79.352776\n",
            "Step 124: Minibatch Loss: 77.153938\n",
            "Step 126: Minibatch Loss: 78.765411\n",
            "Step 128: Minibatch Loss: 74.590439\n",
            "Step 130: Minibatch Loss: 74.846756\n",
            "Step 132: Minibatch Loss: 68.545792\n",
            "Step 134: Minibatch Loss: 74.407761\n",
            "Step 136: Minibatch Loss: 63.714767\n",
            "Step 138: Minibatch Loss: 62.323826\n",
            "Step 140: Minibatch Loss: 72.495903\n",
            "Step 142: Minibatch Loss: 66.624146\n",
            "Step 144: Minibatch Loss: 63.458527\n",
            "Step 146: Minibatch Loss: 69.956932\n",
            "Step 148: Minibatch Loss: 59.908669\n",
            "Step 150: Minibatch Loss: 64.507729\n",
            "Step 152: Minibatch Loss: 63.293625\n",
            "Step 154: Minibatch Loss: 65.297577\n",
            "Step 156: Minibatch Loss: 62.677265\n",
            "Step 158: Minibatch Loss: 56.224403\n",
            "Step 160: Minibatch Loss: 61.448063\n",
            "Step 162: Minibatch Loss: 57.411747\n",
            "Step 164: Minibatch Loss: 54.254589\n",
            "Step 166: Minibatch Loss: 60.408417\n",
            "Step 168: Minibatch Loss: 54.882591\n",
            "Step 170: Minibatch Loss: 61.500729\n",
            "Step 172: Minibatch Loss: 51.135868\n",
            "Step 174: Minibatch Loss: 54.492306\n",
            "Step 176: Minibatch Loss: 58.115452\n",
            "Step 178: Minibatch Loss: 52.192978\n",
            "Step 180: Minibatch Loss: 48.426907\n",
            "Step 182: Minibatch Loss: 53.184818\n",
            "Step 184: Minibatch Loss: 57.503563\n",
            "Step 186: Minibatch Loss: 53.847687\n",
            "Step 188: Minibatch Loss: 54.450916\n",
            "Step 190: Minibatch Loss: 52.180668\n",
            "Step 192: Minibatch Loss: 53.909477\n",
            "Step 194: Minibatch Loss: 53.130753\n",
            "Step 196: Minibatch Loss: 51.720314\n",
            "Step 198: Minibatch Loss: 50.754250\n",
            "Step 200: Minibatch Loss: 53.194775\n",
            "Training for SNR= 0.5  sigma= 0.9440608762859234 iteratin: 0\n",
            "Step 202: Minibatch Loss: 37.030689\n",
            "Step 204: Minibatch Loss: 41.267262\n",
            "Step 206: Minibatch Loss: 43.483086\n",
            "Step 208: Minibatch Loss: 39.356644\n",
            "Step 210: Minibatch Loss: 40.020370\n",
            "Step 212: Minibatch Loss: 35.578690\n",
            "Step 214: Minibatch Loss: 44.633358\n",
            "Step 216: Minibatch Loss: 39.462181\n",
            "Step 218: Minibatch Loss: 40.497040\n",
            "Step 220: Minibatch Loss: 39.600174\n",
            "Step 222: Minibatch Loss: 44.067017\n",
            "Step 224: Minibatch Loss: 44.299408\n",
            "Step 226: Minibatch Loss: 40.681320\n",
            "Step 228: Minibatch Loss: 40.433475\n",
            "Step 230: Minibatch Loss: 39.513954\n",
            "Step 232: Minibatch Loss: 35.600212\n",
            "Step 234: Minibatch Loss: 44.261017\n",
            "Step 236: Minibatch Loss: 42.713566\n",
            "Step 238: Minibatch Loss: 35.311344\n",
            "Step 240: Minibatch Loss: 41.375237\n",
            "Step 242: Minibatch Loss: 37.221123\n",
            "Step 244: Minibatch Loss: 37.811756\n",
            "Step 246: Minibatch Loss: 46.778336\n",
            "Step 248: Minibatch Loss: 41.324898\n",
            "Step 250: Minibatch Loss: 40.093750\n",
            "Step 252: Minibatch Loss: 36.134903\n",
            "Step 254: Minibatch Loss: 38.665764\n",
            "Step 256: Minibatch Loss: 40.461334\n",
            "Step 258: Minibatch Loss: 43.050262\n",
            "Step 260: Minibatch Loss: 37.373661\n",
            "Step 262: Minibatch Loss: 35.835064\n",
            "Step 264: Minibatch Loss: 41.918449\n",
            "Step 266: Minibatch Loss: 40.252560\n",
            "Step 268: Minibatch Loss: 34.914871\n",
            "Step 270: Minibatch Loss: 42.805241\n",
            "Step 272: Minibatch Loss: 43.403133\n",
            "Step 274: Minibatch Loss: 42.642361\n",
            "Step 276: Minibatch Loss: 39.547619\n",
            "Step 278: Minibatch Loss: 42.028351\n",
            "Step 280: Minibatch Loss: 39.927193\n",
            "Step 282: Minibatch Loss: 36.595016\n",
            "Step 284: Minibatch Loss: 38.045883\n",
            "Step 286: Minibatch Loss: 44.579891\n",
            "Step 288: Minibatch Loss: 38.706291\n",
            "Step 290: Minibatch Loss: 39.393871\n",
            "Step 292: Minibatch Loss: 36.655575\n",
            "Step 294: Minibatch Loss: 40.703465\n",
            "Step 296: Minibatch Loss: 42.704189\n",
            "Step 298: Minibatch Loss: 38.103992\n",
            "Step 300: Minibatch Loss: 40.486214\n",
            "Step 302: Minibatch Loss: 35.440487\n",
            "Step 304: Minibatch Loss: 37.315006\n",
            "Step 306: Minibatch Loss: 39.195160\n",
            "Step 308: Minibatch Loss: 36.305874\n",
            "Step 310: Minibatch Loss: 36.900532\n",
            "Step 312: Minibatch Loss: 35.649609\n",
            "Step 314: Minibatch Loss: 37.622864\n",
            "Step 316: Minibatch Loss: 39.516518\n",
            "Step 318: Minibatch Loss: 37.516411\n",
            "Step 320: Minibatch Loss: 40.245678\n",
            "Step 322: Minibatch Loss: 37.471962\n",
            "Step 324: Minibatch Loss: 37.220318\n",
            "Step 326: Minibatch Loss: 33.254284\n",
            "Step 328: Minibatch Loss: 35.970039\n",
            "Step 330: Minibatch Loss: 35.694675\n",
            "Step 332: Minibatch Loss: 36.522339\n",
            "Step 334: Minibatch Loss: 37.654671\n",
            "Step 336: Minibatch Loss: 37.907249\n",
            "Step 338: Minibatch Loss: 38.861111\n",
            "Step 340: Minibatch Loss: 38.767456\n",
            "Step 342: Minibatch Loss: 33.466049\n",
            "Step 344: Minibatch Loss: 35.481846\n",
            "Step 346: Minibatch Loss: 37.442638\n",
            "Step 348: Minibatch Loss: 37.828663\n",
            "Step 350: Minibatch Loss: 37.704174\n",
            "Step 352: Minibatch Loss: 38.974815\n",
            "Step 354: Minibatch Loss: 34.992050\n",
            "Step 356: Minibatch Loss: 42.074768\n",
            "Step 358: Minibatch Loss: 33.570213\n",
            "Step 360: Minibatch Loss: 34.933720\n",
            "Step 362: Minibatch Loss: 40.260475\n",
            "Step 364: Minibatch Loss: 39.764492\n",
            "Step 366: Minibatch Loss: 39.146999\n",
            "Step 368: Minibatch Loss: 35.518341\n",
            "Step 370: Minibatch Loss: 38.416843\n",
            "Step 372: Minibatch Loss: 35.285774\n",
            "Step 374: Minibatch Loss: 36.700531\n",
            "Step 376: Minibatch Loss: 33.881477\n",
            "Step 378: Minibatch Loss: 40.763824\n",
            "Step 380: Minibatch Loss: 34.016251\n",
            "Step 382: Minibatch Loss: 35.171963\n",
            "Step 384: Minibatch Loss: 37.870766\n",
            "Step 386: Minibatch Loss: 35.484779\n",
            "Step 388: Minibatch Loss: 35.362820\n",
            "Step 390: Minibatch Loss: 36.535957\n",
            "Step 392: Minibatch Loss: 37.827396\n",
            "Step 394: Minibatch Loss: 30.158201\n",
            "Step 396: Minibatch Loss: 39.862030\n",
            "Step 398: Minibatch Loss: 34.113087\n",
            "Step 400: Minibatch Loss: 39.174393\n",
            "Training for SNR= 1.0  sigma= 0.8912509381337456 iteratin: 0\n",
            "Step 402: Minibatch Loss: 27.882067\n",
            "Step 404: Minibatch Loss: 27.080589\n",
            "Step 406: Minibatch Loss: 29.043282\n",
            "Step 408: Minibatch Loss: 24.855999\n",
            "Step 410: Minibatch Loss: 29.266197\n",
            "Step 412: Minibatch Loss: 27.229427\n",
            "Step 414: Minibatch Loss: 25.000187\n",
            "Step 416: Minibatch Loss: 31.778170\n",
            "Step 418: Minibatch Loss: 27.943554\n",
            "Step 420: Minibatch Loss: 27.745464\n",
            "Step 422: Minibatch Loss: 28.749121\n",
            "Step 424: Minibatch Loss: 32.564125\n",
            "Step 426: Minibatch Loss: 32.982067\n",
            "Step 428: Minibatch Loss: 28.627958\n",
            "Step 430: Minibatch Loss: 25.975477\n",
            "Step 432: Minibatch Loss: 27.348776\n",
            "Step 434: Minibatch Loss: 31.058229\n",
            "Step 436: Minibatch Loss: 27.544004\n",
            "Step 438: Minibatch Loss: 30.536804\n",
            "Step 440: Minibatch Loss: 29.820171\n",
            "Step 442: Minibatch Loss: 25.730703\n",
            "Step 444: Minibatch Loss: 29.880527\n",
            "Step 446: Minibatch Loss: 27.641176\n",
            "Step 448: Minibatch Loss: 28.790844\n",
            "Step 450: Minibatch Loss: 27.860851\n",
            "Step 452: Minibatch Loss: 27.692591\n",
            "Step 454: Minibatch Loss: 31.434998\n",
            "Step 456: Minibatch Loss: 33.254253\n",
            "Step 458: Minibatch Loss: 29.662052\n",
            "Step 460: Minibatch Loss: 26.621243\n",
            "Step 462: Minibatch Loss: 26.898922\n",
            "Step 464: Minibatch Loss: 27.218256\n",
            "Step 466: Minibatch Loss: 28.772903\n",
            "Step 468: Minibatch Loss: 28.147402\n",
            "Step 470: Minibatch Loss: 29.961613\n",
            "Step 472: Minibatch Loss: 26.841940\n",
            "Step 474: Minibatch Loss: 27.049623\n",
            "Step 476: Minibatch Loss: 27.592316\n",
            "Step 478: Minibatch Loss: 26.070889\n",
            "Step 480: Minibatch Loss: 29.431025\n",
            "Step 482: Minibatch Loss: 29.180115\n",
            "Step 484: Minibatch Loss: 28.621489\n",
            "Step 486: Minibatch Loss: 30.328352\n",
            "Step 488: Minibatch Loss: 28.016710\n",
            "Step 490: Minibatch Loss: 27.815434\n",
            "Step 492: Minibatch Loss: 28.185249\n",
            "Step 494: Minibatch Loss: 32.678085\n",
            "Step 496: Minibatch Loss: 30.424656\n",
            "Step 498: Minibatch Loss: 25.385626\n",
            "Step 500: Minibatch Loss: 31.060621\n",
            "Step 502: Minibatch Loss: 28.551205\n",
            "Step 504: Minibatch Loss: 27.230644\n",
            "Step 506: Minibatch Loss: 28.573160\n",
            "Step 508: Minibatch Loss: 26.238176\n",
            "Step 510: Minibatch Loss: 26.027645\n",
            "Step 512: Minibatch Loss: 26.561525\n",
            "Step 514: Minibatch Loss: 28.325293\n",
            "Step 516: Minibatch Loss: 26.918907\n",
            "Step 518: Minibatch Loss: 32.214878\n",
            "Step 520: Minibatch Loss: 29.019129\n",
            "Step 522: Minibatch Loss: 31.330950\n",
            "Step 524: Minibatch Loss: 29.272852\n",
            "Step 526: Minibatch Loss: 29.060713\n",
            "Step 528: Minibatch Loss: 27.776501\n",
            "Step 530: Minibatch Loss: 29.238750\n",
            "Step 532: Minibatch Loss: 27.135208\n",
            "Step 534: Minibatch Loss: 27.314432\n",
            "Step 536: Minibatch Loss: 29.963869\n",
            "Step 538: Minibatch Loss: 26.355707\n",
            "Step 540: Minibatch Loss: 29.295895\n",
            "Step 542: Minibatch Loss: 25.652397\n",
            "Step 544: Minibatch Loss: 24.730749\n",
            "Step 546: Minibatch Loss: 28.432125\n",
            "Step 548: Minibatch Loss: 29.437122\n",
            "Step 550: Minibatch Loss: 31.630320\n",
            "Step 552: Minibatch Loss: 24.548008\n",
            "Step 554: Minibatch Loss: 29.773819\n",
            "Step 556: Minibatch Loss: 27.304216\n",
            "Step 558: Minibatch Loss: 26.333677\n",
            "Step 560: Minibatch Loss: 22.391041\n",
            "Step 562: Minibatch Loss: 25.055536\n",
            "Step 564: Minibatch Loss: 27.522047\n",
            "Step 566: Minibatch Loss: 25.902788\n",
            "Step 568: Minibatch Loss: 24.929462\n",
            "Step 570: Minibatch Loss: 23.087147\n",
            "Step 572: Minibatch Loss: 25.906862\n",
            "Step 574: Minibatch Loss: 27.569180\n",
            "Step 576: Minibatch Loss: 27.022497\n",
            "Step 578: Minibatch Loss: 27.175381\n",
            "Step 580: Minibatch Loss: 27.229351\n",
            "Step 582: Minibatch Loss: 27.382198\n",
            "Step 584: Minibatch Loss: 32.287296\n",
            "Step 586: Minibatch Loss: 26.694595\n",
            "Step 588: Minibatch Loss: 24.007479\n",
            "Step 590: Minibatch Loss: 28.194727\n",
            "Step 592: Minibatch Loss: 25.996477\n",
            "Step 594: Minibatch Loss: 25.432846\n",
            "Step 596: Minibatch Loss: 28.019987\n",
            "Step 598: Minibatch Loss: 26.947351\n",
            "Step 600: Minibatch Loss: 28.713028\n",
            "Training for SNR= 1.5  sigma= 0.8413951416451951 iteratin: 0\n",
            "Step 602: Minibatch Loss: 21.919535\n",
            "Step 604: Minibatch Loss: 23.350428\n",
            "Step 606: Minibatch Loss: 22.182680\n",
            "Step 608: Minibatch Loss: 19.885500\n",
            "Step 610: Minibatch Loss: 17.109537\n",
            "Step 612: Minibatch Loss: 19.288916\n",
            "Step 614: Minibatch Loss: 16.227839\n",
            "Step 616: Minibatch Loss: 20.642481\n",
            "Step 618: Minibatch Loss: 21.183605\n",
            "Step 620: Minibatch Loss: 22.690619\n",
            "Step 622: Minibatch Loss: 18.825811\n",
            "Step 624: Minibatch Loss: 21.808304\n",
            "Step 626: Minibatch Loss: 21.922585\n",
            "Step 628: Minibatch Loss: 18.998594\n",
            "Step 630: Minibatch Loss: 19.889992\n",
            "Step 632: Minibatch Loss: 17.465332\n",
            "Step 634: Minibatch Loss: 20.930424\n",
            "Step 636: Minibatch Loss: 20.594036\n",
            "Step 638: Minibatch Loss: 21.431458\n",
            "Step 640: Minibatch Loss: 21.982477\n",
            "Step 642: Minibatch Loss: 18.628525\n",
            "Step 644: Minibatch Loss: 22.089628\n",
            "Step 646: Minibatch Loss: 20.765265\n",
            "Step 648: Minibatch Loss: 21.400700\n",
            "Step 650: Minibatch Loss: 17.083628\n",
            "Step 652: Minibatch Loss: 18.361019\n",
            "Step 654: Minibatch Loss: 22.842409\n",
            "Step 656: Minibatch Loss: 17.266747\n",
            "Step 658: Minibatch Loss: 20.753616\n",
            "Step 660: Minibatch Loss: 20.528479\n",
            "Step 662: Minibatch Loss: 15.182069\n",
            "Step 664: Minibatch Loss: 18.274866\n",
            "Step 666: Minibatch Loss: 19.554260\n",
            "Step 668: Minibatch Loss: 23.454008\n",
            "Step 670: Minibatch Loss: 20.867607\n",
            "Step 672: Minibatch Loss: 19.901402\n",
            "Step 674: Minibatch Loss: 22.243532\n",
            "Step 676: Minibatch Loss: 18.945465\n",
            "Step 678: Minibatch Loss: 19.909611\n",
            "Step 680: Minibatch Loss: 20.292669\n",
            "Step 682: Minibatch Loss: 21.309505\n",
            "Step 684: Minibatch Loss: 19.144251\n",
            "Step 686: Minibatch Loss: 20.882183\n",
            "Step 688: Minibatch Loss: 18.735125\n",
            "Step 690: Minibatch Loss: 16.692013\n",
            "Step 692: Minibatch Loss: 18.927076\n",
            "Step 694: Minibatch Loss: 17.821402\n",
            "Step 696: Minibatch Loss: 20.709221\n",
            "Step 698: Minibatch Loss: 16.922115\n",
            "Step 700: Minibatch Loss: 22.330408\n",
            "Step 702: Minibatch Loss: 17.963602\n",
            "Step 704: Minibatch Loss: 19.870743\n",
            "Step 706: Minibatch Loss: 19.837658\n",
            "Step 708: Minibatch Loss: 21.183498\n",
            "Step 710: Minibatch Loss: 21.231350\n",
            "Step 712: Minibatch Loss: 17.423594\n",
            "Step 714: Minibatch Loss: 17.865406\n",
            "Step 716: Minibatch Loss: 21.555037\n",
            "Step 718: Minibatch Loss: 22.106544\n",
            "Step 720: Minibatch Loss: 22.656631\n",
            "Step 722: Minibatch Loss: 19.079563\n",
            "Step 724: Minibatch Loss: 21.355398\n",
            "Step 726: Minibatch Loss: 19.804777\n",
            "Step 728: Minibatch Loss: 19.753881\n",
            "Step 730: Minibatch Loss: 20.880804\n",
            "Step 732: Minibatch Loss: 19.110077\n",
            "Step 734: Minibatch Loss: 21.603514\n",
            "Step 736: Minibatch Loss: 17.505787\n",
            "Step 738: Minibatch Loss: 17.820553\n",
            "Step 740: Minibatch Loss: 18.241623\n",
            "Step 742: Minibatch Loss: 19.649893\n",
            "Step 744: Minibatch Loss: 19.153181\n",
            "Step 746: Minibatch Loss: 22.668831\n",
            "Step 748: Minibatch Loss: 21.100883\n",
            "Step 750: Minibatch Loss: 17.186438\n",
            "Step 752: Minibatch Loss: 19.836262\n",
            "Step 754: Minibatch Loss: 19.143555\n",
            "Step 756: Minibatch Loss: 20.426626\n",
            "Step 758: Minibatch Loss: 18.652170\n",
            "Step 760: Minibatch Loss: 19.062181\n",
            "Step 762: Minibatch Loss: 20.587992\n",
            "Step 764: Minibatch Loss: 20.168123\n",
            "Step 766: Minibatch Loss: 20.975929\n",
            "Step 768: Minibatch Loss: 17.781694\n",
            "Step 770: Minibatch Loss: 21.582771\n",
            "Step 772: Minibatch Loss: 17.955593\n",
            "Step 774: Minibatch Loss: 23.032150\n",
            "Step 776: Minibatch Loss: 18.268799\n",
            "Step 778: Minibatch Loss: 16.197605\n",
            "Step 780: Minibatch Loss: 20.564238\n",
            "Step 782: Minibatch Loss: 20.285975\n",
            "Step 784: Minibatch Loss: 22.393471\n",
            "Step 786: Minibatch Loss: 16.637493\n",
            "Step 788: Minibatch Loss: 18.715893\n",
            "Step 790: Minibatch Loss: 17.425634\n",
            "Step 792: Minibatch Loss: 19.504374\n",
            "Step 794: Minibatch Loss: 16.792465\n",
            "Step 796: Minibatch Loss: 17.496977\n",
            "Step 798: Minibatch Loss: 19.609182\n",
            "Step 800: Minibatch Loss: 17.517271\n",
            "Training for SNR= 2.0  sigma= 0.7943282347242815 iteratin: 0\n",
            "Step 802: Minibatch Loss: 11.811654\n",
            "Step 804: Minibatch Loss: 16.913712\n",
            "Step 806: Minibatch Loss: 12.337240\n",
            "Step 808: Minibatch Loss: 15.172187\n",
            "Step 810: Minibatch Loss: 11.612597\n",
            "Step 812: Minibatch Loss: 13.407949\n",
            "Step 814: Minibatch Loss: 14.219689\n",
            "Step 816: Minibatch Loss: 13.578317\n",
            "Step 818: Minibatch Loss: 13.530162\n",
            "Step 820: Minibatch Loss: 15.063520\n",
            "Step 822: Minibatch Loss: 12.605035\n",
            "Step 824: Minibatch Loss: 16.347288\n",
            "Step 826: Minibatch Loss: 16.019619\n",
            "Step 828: Minibatch Loss: 10.766273\n",
            "Step 830: Minibatch Loss: 9.668730\n",
            "Step 832: Minibatch Loss: 12.843610\n",
            "Step 834: Minibatch Loss: 12.495905\n",
            "Step 836: Minibatch Loss: 15.107432\n",
            "Step 838: Minibatch Loss: 13.415865\n",
            "Step 840: Minibatch Loss: 14.223529\n",
            "Step 842: Minibatch Loss: 13.704030\n",
            "Step 844: Minibatch Loss: 14.478710\n",
            "Step 846: Minibatch Loss: 18.883608\n",
            "Step 848: Minibatch Loss: 14.124731\n",
            "Step 850: Minibatch Loss: 13.651471\n",
            "Step 852: Minibatch Loss: 11.542793\n",
            "Step 854: Minibatch Loss: 13.545141\n",
            "Step 856: Minibatch Loss: 14.284367\n",
            "Step 858: Minibatch Loss: 13.724470\n",
            "Step 860: Minibatch Loss: 14.340285\n",
            "Step 862: Minibatch Loss: 14.336722\n",
            "Step 864: Minibatch Loss: 12.911972\n",
            "Step 866: Minibatch Loss: 15.785449\n",
            "Step 868: Minibatch Loss: 14.368427\n",
            "Step 870: Minibatch Loss: 12.499771\n",
            "Step 872: Minibatch Loss: 15.017287\n",
            "Step 874: Minibatch Loss: 14.737634\n",
            "Step 876: Minibatch Loss: 15.965048\n",
            "Step 878: Minibatch Loss: 13.409250\n",
            "Step 880: Minibatch Loss: 15.333535\n",
            "Step 882: Minibatch Loss: 12.181036\n",
            "Step 884: Minibatch Loss: 13.191007\n",
            "Step 886: Minibatch Loss: 12.789147\n",
            "Step 888: Minibatch Loss: 14.743945\n",
            "Step 890: Minibatch Loss: 11.808029\n",
            "Step 892: Minibatch Loss: 11.191799\n",
            "Step 894: Minibatch Loss: 12.429613\n",
            "Step 896: Minibatch Loss: 13.874244\n",
            "Step 898: Minibatch Loss: 12.762662\n",
            "Step 900: Minibatch Loss: 13.966845\n",
            "Step 902: Minibatch Loss: 12.775414\n",
            "Step 904: Minibatch Loss: 12.012567\n",
            "Step 906: Minibatch Loss: 13.361450\n",
            "Step 908: Minibatch Loss: 14.656777\n",
            "Step 910: Minibatch Loss: 12.974592\n",
            "Step 912: Minibatch Loss: 13.824982\n",
            "Step 914: Minibatch Loss: 13.722624\n",
            "Step 916: Minibatch Loss: 13.381025\n",
            "Step 918: Minibatch Loss: 13.237653\n",
            "Step 920: Minibatch Loss: 12.691106\n",
            "Step 922: Minibatch Loss: 13.023641\n",
            "Step 924: Minibatch Loss: 11.868424\n",
            "Step 926: Minibatch Loss: 13.174476\n",
            "Step 928: Minibatch Loss: 11.305219\n",
            "Step 930: Minibatch Loss: 12.431835\n",
            "Step 932: Minibatch Loss: 13.665180\n",
            "Step 934: Minibatch Loss: 14.878983\n",
            "Step 936: Minibatch Loss: 13.631327\n",
            "Step 938: Minibatch Loss: 13.438449\n",
            "Step 940: Minibatch Loss: 12.760720\n",
            "Step 942: Minibatch Loss: 12.837604\n",
            "Step 944: Minibatch Loss: 15.611524\n",
            "Step 946: Minibatch Loss: 11.978036\n",
            "Step 948: Minibatch Loss: 12.834886\n",
            "Step 950: Minibatch Loss: 13.684343\n",
            "Step 952: Minibatch Loss: 13.113193\n",
            "Step 954: Minibatch Loss: 13.044205\n",
            "Step 956: Minibatch Loss: 12.386417\n",
            "Step 958: Minibatch Loss: 13.285082\n",
            "Step 960: Minibatch Loss: 13.810680\n",
            "Step 962: Minibatch Loss: 13.251677\n",
            "Step 964: Minibatch Loss: 12.778395\n",
            "Step 966: Minibatch Loss: 9.891321\n",
            "Step 968: Minibatch Loss: 14.661501\n",
            "Step 970: Minibatch Loss: 10.971036\n",
            "Step 972: Minibatch Loss: 12.556261\n",
            "Step 974: Minibatch Loss: 14.139442\n",
            "Step 976: Minibatch Loss: 13.122372\n",
            "Step 978: Minibatch Loss: 12.158173\n",
            "Step 980: Minibatch Loss: 13.674382\n",
            "Step 982: Minibatch Loss: 13.960553\n",
            "Step 984: Minibatch Loss: 11.655639\n",
            "Step 986: Minibatch Loss: 15.526313\n",
            "Step 988: Minibatch Loss: 12.213402\n",
            "Step 990: Minibatch Loss: 9.995480\n",
            "Step 992: Minibatch Loss: 14.339586\n",
            "Step 994: Minibatch Loss: 14.626469\n",
            "Step 996: Minibatch Loss: 13.596179\n",
            "Step 998: Minibatch Loss: 12.828676\n",
            "Step 1000: Minibatch Loss: 12.408483\n",
            "Training for SNR= 2.5  sigma= 0.7498942093324559 iteratin: 0\n",
            "Step 1002: Minibatch Loss: 9.766963\n",
            "Step 1004: Minibatch Loss: 10.378843\n",
            "Step 1006: Minibatch Loss: 8.744571\n",
            "Step 1008: Minibatch Loss: 8.174998\n",
            "Step 1010: Minibatch Loss: 8.025640\n",
            "Step 1012: Minibatch Loss: 9.279832\n",
            "Step 1014: Minibatch Loss: 9.160617\n",
            "Step 1016: Minibatch Loss: 9.793339\n",
            "Step 1018: Minibatch Loss: 9.882504\n",
            "Step 1020: Minibatch Loss: 7.690560\n",
            "Step 1022: Minibatch Loss: 7.785919\n",
            "Step 1024: Minibatch Loss: 9.666116\n",
            "Step 1026: Minibatch Loss: 8.849571\n",
            "Step 1028: Minibatch Loss: 8.921879\n",
            "Step 1030: Minibatch Loss: 9.275712\n",
            "Step 1032: Minibatch Loss: 10.174088\n",
            "Step 1034: Minibatch Loss: 8.758589\n",
            "Step 1036: Minibatch Loss: 10.574318\n",
            "Step 1038: Minibatch Loss: 8.503372\n",
            "Step 1040: Minibatch Loss: 9.410830\n",
            "Step 1042: Minibatch Loss: 9.629402\n",
            "Step 1044: Minibatch Loss: 9.082027\n",
            "Step 1046: Minibatch Loss: 9.410752\n",
            "Step 1048: Minibatch Loss: 10.408269\n",
            "Step 1050: Minibatch Loss: 9.520050\n",
            "Step 1052: Minibatch Loss: 8.414265\n",
            "Step 1054: Minibatch Loss: 9.740369\n",
            "Step 1056: Minibatch Loss: 9.562366\n",
            "Step 1058: Minibatch Loss: 10.893870\n",
            "Step 1060: Minibatch Loss: 9.755041\n",
            "Step 1062: Minibatch Loss: 9.882448\n",
            "Step 1064: Minibatch Loss: 8.078742\n",
            "Step 1066: Minibatch Loss: 10.177913\n",
            "Step 1068: Minibatch Loss: 9.568075\n",
            "Step 1070: Minibatch Loss: 8.747632\n",
            "Step 1072: Minibatch Loss: 9.703332\n",
            "Step 1074: Minibatch Loss: 10.027535\n",
            "Step 1076: Minibatch Loss: 10.826769\n",
            "Step 1078: Minibatch Loss: 8.460145\n",
            "Step 1080: Minibatch Loss: 7.476790\n",
            "Step 1082: Minibatch Loss: 9.433568\n",
            "Step 1084: Minibatch Loss: 7.615544\n",
            "Step 1086: Minibatch Loss: 8.944434\n",
            "Step 1088: Minibatch Loss: 8.987893\n",
            "Step 1090: Minibatch Loss: 7.165794\n",
            "Step 1092: Minibatch Loss: 8.383190\n",
            "Step 1094: Minibatch Loss: 10.037272\n",
            "Step 1096: Minibatch Loss: 10.224683\n",
            "Step 1098: Minibatch Loss: 9.900284\n",
            "Step 1100: Minibatch Loss: 8.243859\n",
            "Step 1102: Minibatch Loss: 10.766476\n",
            "Step 1104: Minibatch Loss: 9.018010\n",
            "Step 1106: Minibatch Loss: 8.916236\n",
            "Step 1108: Minibatch Loss: 9.172166\n",
            "Step 1110: Minibatch Loss: 7.554913\n",
            "Step 1112: Minibatch Loss: 8.129849\n",
            "Step 1114: Minibatch Loss: 9.815629\n",
            "Step 1116: Minibatch Loss: 9.884928\n",
            "Step 1118: Minibatch Loss: 10.901745\n",
            "Step 1120: Minibatch Loss: 10.414528\n",
            "Step 1122: Minibatch Loss: 9.370211\n",
            "Step 1124: Minibatch Loss: 8.704161\n",
            "Step 1126: Minibatch Loss: 8.593279\n",
            "Step 1128: Minibatch Loss: 8.597068\n",
            "Step 1130: Minibatch Loss: 8.415449\n",
            "Step 1132: Minibatch Loss: 9.178531\n",
            "Step 1134: Minibatch Loss: 7.043725\n",
            "Step 1136: Minibatch Loss: 7.538878\n",
            "Step 1138: Minibatch Loss: 8.887351\n",
            "Step 1140: Minibatch Loss: 8.718731\n",
            "Step 1142: Minibatch Loss: 11.378670\n",
            "Step 1144: Minibatch Loss: 11.448661\n",
            "Step 1146: Minibatch Loss: 10.294207\n",
            "Step 1148: Minibatch Loss: 8.099361\n",
            "Step 1150: Minibatch Loss: 8.265325\n",
            "Step 1152: Minibatch Loss: 7.239790\n",
            "Step 1154: Minibatch Loss: 8.163555\n",
            "Step 1156: Minibatch Loss: 10.195872\n",
            "Step 1158: Minibatch Loss: 7.125557\n",
            "Step 1160: Minibatch Loss: 9.715094\n",
            "Step 1162: Minibatch Loss: 10.559852\n",
            "Step 1164: Minibatch Loss: 7.415614\n",
            "Step 1166: Minibatch Loss: 7.307575\n",
            "Step 1168: Minibatch Loss: 7.427750\n",
            "Step 1170: Minibatch Loss: 10.429334\n",
            "Step 1172: Minibatch Loss: 9.034528\n",
            "Step 1174: Minibatch Loss: 8.602909\n",
            "Step 1176: Minibatch Loss: 8.579397\n",
            "Step 1178: Minibatch Loss: 10.077388\n",
            "Step 1180: Minibatch Loss: 8.209181\n",
            "Step 1182: Minibatch Loss: 9.812158\n",
            "Step 1184: Minibatch Loss: 9.147381\n",
            "Step 1186: Minibatch Loss: 7.337481\n",
            "Step 1188: Minibatch Loss: 8.729523\n",
            "Step 1190: Minibatch Loss: 8.612548\n",
            "Step 1192: Minibatch Loss: 9.452718\n",
            "Step 1194: Minibatch Loss: 8.698456\n",
            "Step 1196: Minibatch Loss: 8.528995\n",
            "Step 1198: Minibatch Loss: 7.959944\n",
            "Step 1200: Minibatch Loss: 9.935657\n",
            "Training for SNR= 3.0  sigma= 0.7079457843841379 iteratin: 0\n",
            "Step 1202: Minibatch Loss: 5.450241\n",
            "Step 1204: Minibatch Loss: 5.909347\n",
            "Step 1206: Minibatch Loss: 4.254924\n",
            "Step 1208: Minibatch Loss: 6.145493\n",
            "Step 1210: Minibatch Loss: 5.870574\n",
            "Step 1212: Minibatch Loss: 6.146163\n",
            "Step 1214: Minibatch Loss: 6.068976\n",
            "Step 1216: Minibatch Loss: 6.972834\n",
            "Step 1218: Minibatch Loss: 5.857898\n",
            "Step 1220: Minibatch Loss: 5.426735\n",
            "Step 1222: Minibatch Loss: 4.670724\n",
            "Step 1224: Minibatch Loss: 4.855752\n",
            "Step 1226: Minibatch Loss: 5.269273\n",
            "Step 1228: Minibatch Loss: 5.382008\n",
            "Step 1230: Minibatch Loss: 6.071765\n",
            "Step 1232: Minibatch Loss: 3.605506\n",
            "Step 1234: Minibatch Loss: 8.139674\n",
            "Step 1236: Minibatch Loss: 5.467430\n",
            "Step 1238: Minibatch Loss: 5.226698\n",
            "Step 1240: Minibatch Loss: 7.148465\n",
            "Step 1242: Minibatch Loss: 5.181963\n",
            "Step 1244: Minibatch Loss: 6.460685\n",
            "Step 1246: Minibatch Loss: 6.425796\n",
            "Step 1248: Minibatch Loss: 5.754621\n",
            "Step 1250: Minibatch Loss: 5.954835\n",
            "Step 1252: Minibatch Loss: 6.162399\n",
            "Step 1254: Minibatch Loss: 4.709116\n",
            "Step 1256: Minibatch Loss: 4.948615\n",
            "Step 1258: Minibatch Loss: 6.206715\n",
            "Step 1260: Minibatch Loss: 4.312270\n",
            "Step 1262: Minibatch Loss: 4.645296\n",
            "Step 1264: Minibatch Loss: 5.198447\n",
            "Step 1266: Minibatch Loss: 6.446351\n",
            "Step 1268: Minibatch Loss: 6.073832\n",
            "Step 1270: Minibatch Loss: 5.217285\n",
            "Step 1272: Minibatch Loss: 6.328289\n",
            "Step 1274: Minibatch Loss: 6.892550\n",
            "Step 1276: Minibatch Loss: 7.615604\n",
            "Step 1278: Minibatch Loss: 5.967938\n",
            "Step 1280: Minibatch Loss: 5.528756\n",
            "Step 1282: Minibatch Loss: 5.796671\n",
            "Step 1284: Minibatch Loss: 6.212769\n",
            "Step 1286: Minibatch Loss: 6.908156\n",
            "Step 1288: Minibatch Loss: 5.951231\n",
            "Step 1290: Minibatch Loss: 7.460938\n",
            "Step 1292: Minibatch Loss: 4.869613\n",
            "Step 1294: Minibatch Loss: 5.109651\n",
            "Step 1296: Minibatch Loss: 6.500565\n",
            "Step 1298: Minibatch Loss: 7.311855\n",
            "Step 1300: Minibatch Loss: 4.707847\n",
            "Step 1302: Minibatch Loss: 5.783176\n",
            "Step 1304: Minibatch Loss: 5.617507\n",
            "Step 1306: Minibatch Loss: 8.692623\n",
            "Step 1308: Minibatch Loss: 8.097898\n",
            "Step 1310: Minibatch Loss: 3.703037\n",
            "Step 1312: Minibatch Loss: 5.034449\n",
            "Step 1314: Minibatch Loss: 6.100098\n",
            "Step 1316: Minibatch Loss: 6.536524\n",
            "Step 1318: Minibatch Loss: 4.734040\n",
            "Step 1320: Minibatch Loss: 5.082990\n",
            "Step 1322: Minibatch Loss: 4.956312\n",
            "Step 1324: Minibatch Loss: 5.516616\n",
            "Step 1326: Minibatch Loss: 5.725936\n",
            "Step 1328: Minibatch Loss: 6.434799\n",
            "Step 1330: Minibatch Loss: 5.569388\n",
            "Step 1332: Minibatch Loss: 5.437948\n",
            "Step 1334: Minibatch Loss: 6.469962\n",
            "Step 1336: Minibatch Loss: 7.078126\n",
            "Step 1338: Minibatch Loss: 5.875405\n",
            "Step 1340: Minibatch Loss: 6.158346\n",
            "Step 1342: Minibatch Loss: 5.930424\n",
            "Step 1344: Minibatch Loss: 6.513494\n",
            "Step 1346: Minibatch Loss: 5.791703\n",
            "Step 1348: Minibatch Loss: 6.321410\n",
            "Step 1350: Minibatch Loss: 6.049796\n",
            "Step 1352: Minibatch Loss: 6.663924\n",
            "Step 1354: Minibatch Loss: 6.160047\n",
            "Step 1356: Minibatch Loss: 6.330927\n",
            "Step 1358: Minibatch Loss: 4.778969\n",
            "Step 1360: Minibatch Loss: 6.424111\n",
            "Step 1362: Minibatch Loss: 5.145556\n",
            "Step 1364: Minibatch Loss: 6.202770\n",
            "Step 1366: Minibatch Loss: 5.991941\n",
            "Step 1368: Minibatch Loss: 6.352517\n",
            "Step 1370: Minibatch Loss: 5.632593\n",
            "Step 1372: Minibatch Loss: 4.823962\n",
            "Step 1374: Minibatch Loss: 5.714699\n",
            "Step 1376: Minibatch Loss: 6.651489\n",
            "Step 1378: Minibatch Loss: 5.488107\n",
            "Step 1380: Minibatch Loss: 6.688329\n",
            "Step 1382: Minibatch Loss: 7.076177\n",
            "Step 1384: Minibatch Loss: 6.007663\n",
            "Step 1386: Minibatch Loss: 4.791378\n",
            "Step 1388: Minibatch Loss: 5.027571\n",
            "Step 1390: Minibatch Loss: 6.014021\n",
            "Step 1392: Minibatch Loss: 5.007575\n",
            "Step 1394: Minibatch Loss: 5.661194\n",
            "Step 1396: Minibatch Loss: 4.748905\n",
            "Step 1398: Minibatch Loss: 3.290849\n",
            "Step 1400: Minibatch Loss: 5.519371\n",
            "Training for SNR= 3.5  sigma= 0.6683439175686147 iteratin: 0\n",
            "Step 1402: Minibatch Loss: 4.144168\n",
            "Step 1404: Minibatch Loss: 3.666800\n",
            "Step 1406: Minibatch Loss: 3.264863\n",
            "Step 1408: Minibatch Loss: 5.434621\n",
            "Step 1410: Minibatch Loss: 4.149926\n",
            "Step 1412: Minibatch Loss: 3.279872\n",
            "Step 1414: Minibatch Loss: 3.493312\n",
            "Step 1416: Minibatch Loss: 3.958489\n",
            "Step 1418: Minibatch Loss: 3.662439\n",
            "Step 1420: Minibatch Loss: 4.617814\n",
            "Step 1422: Minibatch Loss: 4.358887\n",
            "Step 1424: Minibatch Loss: 3.262352\n",
            "Step 1426: Minibatch Loss: 3.507677\n",
            "Step 1428: Minibatch Loss: 3.894508\n",
            "Step 1430: Minibatch Loss: 3.262116\n",
            "Step 1432: Minibatch Loss: 3.345935\n",
            "Step 1434: Minibatch Loss: 2.668190\n",
            "Step 1436: Minibatch Loss: 4.378493\n",
            "Step 1438: Minibatch Loss: 3.089140\n",
            "Step 1440: Minibatch Loss: 3.772256\n",
            "Step 1442: Minibatch Loss: 2.884941\n",
            "Step 1444: Minibatch Loss: 3.331920\n",
            "Step 1446: Minibatch Loss: 3.249222\n",
            "Step 1448: Minibatch Loss: 3.736749\n",
            "Step 1450: Minibatch Loss: 4.108197\n",
            "Step 1452: Minibatch Loss: 5.545595\n",
            "Step 1454: Minibatch Loss: 3.499126\n",
            "Step 1456: Minibatch Loss: 3.968404\n",
            "Step 1458: Minibatch Loss: 4.277669\n",
            "Step 1460: Minibatch Loss: 4.078637\n",
            "Step 1462: Minibatch Loss: 4.096181\n",
            "Step 1464: Minibatch Loss: 2.944288\n",
            "Step 1466: Minibatch Loss: 3.847822\n",
            "Step 1468: Minibatch Loss: 2.799087\n",
            "Step 1470: Minibatch Loss: 3.433541\n",
            "Step 1472: Minibatch Loss: 3.864740\n",
            "Step 1474: Minibatch Loss: 4.417610\n",
            "Step 1476: Minibatch Loss: 4.566922\n",
            "Step 1478: Minibatch Loss: 3.721331\n",
            "Step 1480: Minibatch Loss: 4.266151\n",
            "Step 1482: Minibatch Loss: 3.125536\n",
            "Step 1484: Minibatch Loss: 4.560036\n",
            "Step 1486: Minibatch Loss: 2.665968\n",
            "Step 1488: Minibatch Loss: 3.354870\n",
            "Step 1490: Minibatch Loss: 3.846774\n",
            "Step 1492: Minibatch Loss: 4.166695\n",
            "Step 1494: Minibatch Loss: 2.142181\n",
            "Step 1496: Minibatch Loss: 4.489755\n",
            "Step 1498: Minibatch Loss: 2.826801\n",
            "Step 1500: Minibatch Loss: 3.992335\n",
            "Step 1502: Minibatch Loss: 3.914051\n",
            "Step 1504: Minibatch Loss: 3.562500\n",
            "Step 1506: Minibatch Loss: 3.555064\n",
            "Step 1508: Minibatch Loss: 4.210003\n",
            "Step 1510: Minibatch Loss: 3.998270\n",
            "Step 1512: Minibatch Loss: 4.702621\n",
            "Step 1514: Minibatch Loss: 4.191467\n",
            "Step 1516: Minibatch Loss: 4.698290\n",
            "Step 1518: Minibatch Loss: 2.758622\n",
            "Step 1520: Minibatch Loss: 2.927501\n",
            "Step 1522: Minibatch Loss: 3.151847\n",
            "Step 1524: Minibatch Loss: 4.706636\n",
            "Step 1526: Minibatch Loss: 3.182339\n",
            "Step 1528: Minibatch Loss: 2.885441\n",
            "Step 1530: Minibatch Loss: 4.337767\n",
            "Step 1532: Minibatch Loss: 4.035454\n",
            "Step 1534: Minibatch Loss: 3.494005\n",
            "Step 1536: Minibatch Loss: 3.834233\n",
            "Step 1538: Minibatch Loss: 4.074659\n",
            "Step 1540: Minibatch Loss: 3.726490\n",
            "Step 1542: Minibatch Loss: 4.207802\n",
            "Step 1544: Minibatch Loss: 2.769713\n",
            "Step 1546: Minibatch Loss: 3.468929\n",
            "Step 1548: Minibatch Loss: 2.343395\n",
            "Step 1550: Minibatch Loss: 4.508158\n",
            "Step 1552: Minibatch Loss: 3.320952\n",
            "Step 1554: Minibatch Loss: 3.908721\n",
            "Step 1556: Minibatch Loss: 4.131418\n",
            "Step 1558: Minibatch Loss: 3.937267\n",
            "Step 1560: Minibatch Loss: 3.866991\n",
            "Step 1562: Minibatch Loss: 3.984671\n",
            "Step 1564: Minibatch Loss: 3.380573\n",
            "Step 1566: Minibatch Loss: 3.318963\n",
            "Step 1568: Minibatch Loss: 2.986514\n",
            "Step 1570: Minibatch Loss: 3.573044\n",
            "Step 1572: Minibatch Loss: 3.748968\n",
            "Step 1574: Minibatch Loss: 3.646195\n",
            "Step 1576: Minibatch Loss: 4.338561\n",
            "Step 1578: Minibatch Loss: 3.444778\n",
            "Step 1580: Minibatch Loss: 3.649966\n",
            "Step 1582: Minibatch Loss: 3.249416\n",
            "Step 1584: Minibatch Loss: 3.967356\n",
            "Step 1586: Minibatch Loss: 3.618771\n",
            "Step 1588: Minibatch Loss: 2.174320\n",
            "Step 1590: Minibatch Loss: 2.371989\n",
            "Step 1592: Minibatch Loss: 3.255364\n",
            "Step 1594: Minibatch Loss: 3.472128\n",
            "Step 1596: Minibatch Loss: 4.805997\n",
            "Step 1598: Minibatch Loss: 3.425972\n",
            "Step 1600: Minibatch Loss: 2.965352\n",
            "Training for SNR= 4.0  sigma= 0.6309573444801932 iteratin: 0\n",
            "Step 1602: Minibatch Loss: 2.711348\n",
            "Step 1604: Minibatch Loss: 1.990855\n",
            "Step 1606: Minibatch Loss: 2.509445\n",
            "Step 1608: Minibatch Loss: 1.525283\n",
            "Step 1610: Minibatch Loss: 2.572206\n",
            "Step 1612: Minibatch Loss: 1.828356\n",
            "Step 1614: Minibatch Loss: 1.987112\n",
            "Step 1616: Minibatch Loss: 2.907217\n",
            "Step 1618: Minibatch Loss: 2.180061\n",
            "Step 1620: Minibatch Loss: 2.529300\n",
            "Step 1622: Minibatch Loss: 1.545123\n",
            "Step 1624: Minibatch Loss: 1.433531\n",
            "Step 1626: Minibatch Loss: 2.467983\n",
            "Step 1628: Minibatch Loss: 2.230376\n",
            "Step 1630: Minibatch Loss: 2.572591\n",
            "Step 1632: Minibatch Loss: 1.487065\n",
            "Step 1634: Minibatch Loss: 1.713615\n",
            "Step 1636: Minibatch Loss: 3.029840\n",
            "Step 1638: Minibatch Loss: 2.426204\n",
            "Step 1640: Minibatch Loss: 1.922165\n",
            "Step 1642: Minibatch Loss: 2.639203\n",
            "Step 1644: Minibatch Loss: 2.074019\n",
            "Step 1646: Minibatch Loss: 2.170203\n",
            "Step 1648: Minibatch Loss: 2.928355\n",
            "Step 1650: Minibatch Loss: 2.606659\n",
            "Step 1652: Minibatch Loss: 1.870059\n",
            "Step 1654: Minibatch Loss: 2.551341\n",
            "Step 1656: Minibatch Loss: 2.059975\n",
            "Step 1658: Minibatch Loss: 1.609269\n",
            "Step 1660: Minibatch Loss: 1.142930\n",
            "Step 1662: Minibatch Loss: 1.486703\n",
            "Step 1664: Minibatch Loss: 2.427267\n",
            "Step 1666: Minibatch Loss: 1.966083\n",
            "Step 1668: Minibatch Loss: 2.119959\n",
            "Step 1670: Minibatch Loss: 2.529183\n",
            "Step 1672: Minibatch Loss: 1.956232\n",
            "Step 1674: Minibatch Loss: 2.337330\n",
            "Step 1676: Minibatch Loss: 2.258964\n",
            "Step 1678: Minibatch Loss: 1.987040\n",
            "Step 1680: Minibatch Loss: 2.728620\n",
            "Step 1682: Minibatch Loss: 2.382323\n",
            "Step 1684: Minibatch Loss: 2.407838\n",
            "Step 1686: Minibatch Loss: 2.111330\n",
            "Step 1688: Minibatch Loss: 1.579953\n",
            "Step 1690: Minibatch Loss: 1.824121\n",
            "Step 1692: Minibatch Loss: 1.849110\n",
            "Step 1694: Minibatch Loss: 2.379838\n",
            "Step 1696: Minibatch Loss: 1.520430\n",
            "Step 1698: Minibatch Loss: 1.722912\n",
            "Step 1700: Minibatch Loss: 2.116011\n",
            "Step 1702: Minibatch Loss: 2.410140\n",
            "Step 1704: Minibatch Loss: 2.296662\n",
            "Step 1706: Minibatch Loss: 2.550845\n",
            "Step 1708: Minibatch Loss: 1.468668\n",
            "Step 1710: Minibatch Loss: 2.272085\n",
            "Step 1712: Minibatch Loss: 2.027469\n",
            "Step 1714: Minibatch Loss: 2.909991\n",
            "Step 1716: Minibatch Loss: 2.652568\n",
            "Step 1718: Minibatch Loss: 2.762808\n",
            "Step 1720: Minibatch Loss: 1.828709\n",
            "Step 1722: Minibatch Loss: 2.813414\n",
            "Step 1724: Minibatch Loss: 1.854980\n",
            "Step 1726: Minibatch Loss: 2.335364\n",
            "Step 1728: Minibatch Loss: 1.221809\n",
            "Step 1730: Minibatch Loss: 1.713983\n",
            "Step 1732: Minibatch Loss: 2.047791\n",
            "Step 1734: Minibatch Loss: 2.575583\n",
            "Step 1736: Minibatch Loss: 2.140682\n",
            "Step 1738: Minibatch Loss: 2.575060\n",
            "Step 1740: Minibatch Loss: 1.657778\n",
            "Step 1742: Minibatch Loss: 1.793187\n",
            "Step 1744: Minibatch Loss: 2.891117\n",
            "Step 1746: Minibatch Loss: 1.474436\n",
            "Step 1748: Minibatch Loss: 2.297626\n",
            "Step 1750: Minibatch Loss: 1.349615\n",
            "Step 1752: Minibatch Loss: 1.928677\n",
            "Step 1754: Minibatch Loss: 2.092913\n",
            "Step 1756: Minibatch Loss: 1.605355\n",
            "Step 1758: Minibatch Loss: 2.528994\n",
            "Step 1760: Minibatch Loss: 2.095870\n",
            "Step 1762: Minibatch Loss: 1.773479\n",
            "Step 1764: Minibatch Loss: 2.036593\n",
            "Step 1766: Minibatch Loss: 2.165982\n",
            "Step 1768: Minibatch Loss: 2.202470\n",
            "Step 1770: Minibatch Loss: 2.428929\n",
            "Step 1772: Minibatch Loss: 1.727483\n",
            "Step 1774: Minibatch Loss: 1.796108\n",
            "Step 1776: Minibatch Loss: 2.092900\n",
            "Step 1778: Minibatch Loss: 2.692482\n",
            "Step 1780: Minibatch Loss: 1.685851\n",
            "Step 1782: Minibatch Loss: 1.960557\n",
            "Step 1784: Minibatch Loss: 2.280409\n",
            "Step 1786: Minibatch Loss: 1.996074\n",
            "Step 1788: Minibatch Loss: 2.760930\n",
            "Step 1790: Minibatch Loss: 1.993991\n",
            "Step 1792: Minibatch Loss: 1.649036\n",
            "Step 1794: Minibatch Loss: 2.379043\n",
            "Step 1796: Minibatch Loss: 2.159829\n",
            "Step 1798: Minibatch Loss: 1.676626\n",
            "Step 1800: Minibatch Loss: 3.028056\n",
            "Training for SNR= 4.5  sigma= 0.5956621435290105 iteratin: 0\n",
            "Step 1802: Minibatch Loss: 1.257730\n",
            "Step 1804: Minibatch Loss: 1.540901\n",
            "Step 1806: Minibatch Loss: 1.460590\n",
            "Step 1808: Minibatch Loss: 1.145050\n",
            "Step 1810: Minibatch Loss: 1.401725\n",
            "Step 1812: Minibatch Loss: 0.508927\n",
            "Step 1814: Minibatch Loss: 0.739447\n",
            "Step 1816: Minibatch Loss: 1.930964\n",
            "Step 1818: Minibatch Loss: 1.204909\n",
            "Step 1820: Minibatch Loss: 1.334927\n",
            "Step 1822: Minibatch Loss: 0.915407\n",
            "Step 1824: Minibatch Loss: 1.761786\n",
            "Step 1826: Minibatch Loss: 1.040437\n",
            "Step 1828: Minibatch Loss: 1.216682\n",
            "Step 1830: Minibatch Loss: 1.238123\n",
            "Step 1832: Minibatch Loss: 1.644783\n",
            "Step 1834: Minibatch Loss: 1.427487\n",
            "Step 1836: Minibatch Loss: 1.509080\n",
            "Step 1838: Minibatch Loss: 1.141212\n",
            "Step 1840: Minibatch Loss: 1.224469\n",
            "Step 1842: Minibatch Loss: 1.171668\n",
            "Step 1844: Minibatch Loss: 1.219528\n",
            "Step 1846: Minibatch Loss: 1.183192\n",
            "Step 1848: Minibatch Loss: 1.299890\n",
            "Step 1850: Minibatch Loss: 1.436849\n",
            "Step 1852: Minibatch Loss: 1.974869\n",
            "Step 1854: Minibatch Loss: 0.672182\n",
            "Step 1856: Minibatch Loss: 1.538552\n",
            "Step 1858: Minibatch Loss: 0.750959\n",
            "Step 1860: Minibatch Loss: 1.223212\n",
            "Step 1862: Minibatch Loss: 1.444967\n",
            "Step 1864: Minibatch Loss: 1.358023\n",
            "Step 1866: Minibatch Loss: 1.487416\n",
            "Step 1868: Minibatch Loss: 1.229450\n",
            "Step 1870: Minibatch Loss: 2.097846\n",
            "Step 1872: Minibatch Loss: 1.261115\n",
            "Step 1874: Minibatch Loss: 1.653851\n",
            "Step 1876: Minibatch Loss: 1.451162\n",
            "Step 1878: Minibatch Loss: 1.385482\n",
            "Step 1880: Minibatch Loss: 1.384240\n",
            "Step 1882: Minibatch Loss: 1.132784\n",
            "Step 1884: Minibatch Loss: 0.889073\n",
            "Step 1886: Minibatch Loss: 1.282249\n",
            "Step 1888: Minibatch Loss: 1.125356\n",
            "Step 1890: Minibatch Loss: 1.311030\n",
            "Step 1892: Minibatch Loss: 1.534731\n",
            "Step 1894: Minibatch Loss: 1.734460\n",
            "Step 1896: Minibatch Loss: 1.306784\n",
            "Step 1898: Minibatch Loss: 0.532676\n",
            "Step 1900: Minibatch Loss: 1.171667\n",
            "Step 1902: Minibatch Loss: 1.079231\n",
            "Step 1904: Minibatch Loss: 0.634630\n",
            "Step 1906: Minibatch Loss: 1.109075\n",
            "Step 1908: Minibatch Loss: 1.067706\n",
            "Step 1910: Minibatch Loss: 0.947579\n",
            "Step 1912: Minibatch Loss: 1.400166\n",
            "Step 1914: Minibatch Loss: 1.432888\n",
            "Step 1916: Minibatch Loss: 1.097467\n",
            "Step 1918: Minibatch Loss: 1.318695\n",
            "Step 1920: Minibatch Loss: 1.802775\n",
            "Step 1922: Minibatch Loss: 1.045078\n",
            "Step 1924: Minibatch Loss: 1.035859\n",
            "Step 1926: Minibatch Loss: 1.632082\n",
            "Step 1928: Minibatch Loss: 1.030197\n",
            "Step 1930: Minibatch Loss: 1.273546\n",
            "Step 1932: Minibatch Loss: 1.481378\n",
            "Step 1934: Minibatch Loss: 1.509734\n",
            "Step 1936: Minibatch Loss: 1.402509\n",
            "Step 1938: Minibatch Loss: 2.053333\n",
            "Step 1940: Minibatch Loss: 1.254385\n",
            "Step 1942: Minibatch Loss: 1.248570\n",
            "Step 1944: Minibatch Loss: 1.119889\n",
            "Step 1946: Minibatch Loss: 1.535036\n",
            "Step 1948: Minibatch Loss: 0.810100\n",
            "Step 1950: Minibatch Loss: 1.291917\n",
            "Step 1952: Minibatch Loss: 1.164308\n",
            "Step 1954: Minibatch Loss: 1.144906\n",
            "Step 1956: Minibatch Loss: 1.607864\n",
            "Step 1958: Minibatch Loss: 0.653165\n",
            "Step 1960: Minibatch Loss: 1.339536\n",
            "Step 1962: Minibatch Loss: 1.658128\n",
            "Step 1964: Minibatch Loss: 1.850937\n",
            "Step 1966: Minibatch Loss: 1.288046\n",
            "Step 1968: Minibatch Loss: 1.178676\n",
            "Step 1970: Minibatch Loss: 0.980164\n",
            "Step 1972: Minibatch Loss: 1.396562\n",
            "Step 1974: Minibatch Loss: 1.421018\n",
            "Step 1976: Minibatch Loss: 0.998699\n",
            "Step 1978: Minibatch Loss: 1.015815\n",
            "Step 1980: Minibatch Loss: 1.201917\n",
            "Step 1982: Minibatch Loss: 1.238673\n",
            "Step 1984: Minibatch Loss: 1.327760\n",
            "Step 1986: Minibatch Loss: 1.447507\n",
            "Step 1988: Minibatch Loss: 1.551530\n",
            "Step 1990: Minibatch Loss: 1.051659\n",
            "Step 1992: Minibatch Loss: 1.399156\n",
            "Step 1994: Minibatch Loss: 0.960678\n",
            "Step 1996: Minibatch Loss: 1.443004\n",
            "Step 1998: Minibatch Loss: 1.002688\n",
            "Step 2000: Minibatch Loss: 1.013719\n",
            "Training for SNR= 5.0  sigma= 0.5623413251903491 iteratin: 0\n",
            "Step 2002: Minibatch Loss: 0.333554\n",
            "Step 2004: Minibatch Loss: 0.842422\n",
            "Step 2006: Minibatch Loss: 0.309549\n",
            "Step 2008: Minibatch Loss: 0.742676\n",
            "Step 2010: Minibatch Loss: 1.394121\n",
            "Step 2012: Minibatch Loss: 0.379744\n",
            "Step 2014: Minibatch Loss: 0.553758\n",
            "Step 2016: Minibatch Loss: 0.844792\n",
            "Step 2018: Minibatch Loss: 0.356639\n",
            "Step 2020: Minibatch Loss: 0.596738\n",
            "Step 2022: Minibatch Loss: 1.074480\n",
            "Step 2024: Minibatch Loss: 0.798217\n",
            "Step 2026: Minibatch Loss: 0.779766\n",
            "Step 2028: Minibatch Loss: 0.272292\n",
            "Step 2030: Minibatch Loss: 0.844487\n",
            "Step 2032: Minibatch Loss: 0.871251\n",
            "Step 2034: Minibatch Loss: 1.200930\n",
            "Step 2036: Minibatch Loss: 0.537207\n",
            "Step 2038: Minibatch Loss: 0.885789\n",
            "Step 2040: Minibatch Loss: 0.395479\n",
            "Step 2042: Minibatch Loss: 0.740782\n",
            "Step 2044: Minibatch Loss: 0.890384\n",
            "Step 2046: Minibatch Loss: 1.064156\n",
            "Step 2048: Minibatch Loss: 1.095808\n",
            "Step 2050: Minibatch Loss: 0.399677\n",
            "Step 2052: Minibatch Loss: 0.420203\n",
            "Step 2054: Minibatch Loss: 0.466480\n",
            "Step 2056: Minibatch Loss: 0.504189\n",
            "Step 2058: Minibatch Loss: 0.912318\n",
            "Step 2060: Minibatch Loss: 0.932219\n",
            "Step 2062: Minibatch Loss: 0.376156\n",
            "Step 2064: Minibatch Loss: 1.292332\n",
            "Step 2066: Minibatch Loss: 0.594727\n",
            "Step 2068: Minibatch Loss: 0.876010\n",
            "Step 2070: Minibatch Loss: 0.746149\n",
            "Step 2072: Minibatch Loss: 0.644233\n",
            "Step 2074: Minibatch Loss: 0.241572\n",
            "Step 2076: Minibatch Loss: 0.222566\n",
            "Step 2078: Minibatch Loss: 0.438230\n",
            "Step 2080: Minibatch Loss: 0.480652\n",
            "Step 2082: Minibatch Loss: 0.782255\n",
            "Step 2084: Minibatch Loss: 1.071391\n",
            "Step 2086: Minibatch Loss: 0.621403\n",
            "Step 2088: Minibatch Loss: 0.646868\n",
            "Step 2090: Minibatch Loss: 0.635757\n",
            "Step 2092: Minibatch Loss: 0.596840\n",
            "Step 2094: Minibatch Loss: 0.617568\n",
            "Step 2096: Minibatch Loss: 0.893646\n",
            "Step 2098: Minibatch Loss: 0.469580\n",
            "Step 2100: Minibatch Loss: 0.427973\n",
            "Step 2102: Minibatch Loss: 1.026956\n",
            "Step 2104: Minibatch Loss: 0.754542\n",
            "Step 2106: Minibatch Loss: 0.354757\n",
            "Step 2108: Minibatch Loss: 0.580309\n",
            "Step 2110: Minibatch Loss: 1.284174\n",
            "Step 2112: Minibatch Loss: 0.487793\n",
            "Step 2114: Minibatch Loss: 0.674756\n",
            "Step 2116: Minibatch Loss: 0.451238\n",
            "Step 2118: Minibatch Loss: 0.831325\n",
            "Step 2120: Minibatch Loss: 0.393968\n",
            "Step 2122: Minibatch Loss: 0.546715\n",
            "Step 2124: Minibatch Loss: 0.688188\n",
            "Step 2126: Minibatch Loss: 0.739757\n",
            "Step 2128: Minibatch Loss: 0.725778\n",
            "Step 2130: Minibatch Loss: 0.412465\n",
            "Step 2132: Minibatch Loss: 0.274774\n",
            "Step 2134: Minibatch Loss: 0.887516\n",
            "Step 2136: Minibatch Loss: 1.486787\n",
            "Step 2138: Minibatch Loss: 0.755090\n",
            "Step 2140: Minibatch Loss: 0.935377\n",
            "Step 2142: Minibatch Loss: 0.697974\n",
            "Step 2144: Minibatch Loss: 0.602058\n",
            "Step 2146: Minibatch Loss: 0.406418\n",
            "Step 2148: Minibatch Loss: 0.506776\n",
            "Step 2150: Minibatch Loss: 1.090499\n",
            "Step 2152: Minibatch Loss: 0.342311\n",
            "Step 2154: Minibatch Loss: 0.870430\n",
            "Step 2156: Minibatch Loss: 0.434246\n",
            "Step 2158: Minibatch Loss: 0.405150\n",
            "Step 2160: Minibatch Loss: 0.803453\n",
            "Step 2162: Minibatch Loss: 1.286537\n",
            "Step 2164: Minibatch Loss: 0.639933\n",
            "Step 2166: Minibatch Loss: 0.722398\n",
            "Step 2168: Minibatch Loss: 0.665987\n",
            "Step 2170: Minibatch Loss: 0.658574\n",
            "Step 2172: Minibatch Loss: 1.299197\n",
            "Step 2174: Minibatch Loss: 0.325528\n",
            "Step 2176: Minibatch Loss: 1.066295\n",
            "Step 2178: Minibatch Loss: 0.900175\n",
            "Step 2180: Minibatch Loss: 0.847381\n",
            "Step 2182: Minibatch Loss: 0.621776\n",
            "Step 2184: Minibatch Loss: 0.800815\n",
            "Step 2186: Minibatch Loss: 0.290260\n",
            "Step 2188: Minibatch Loss: 0.875704\n",
            "Step 2190: Minibatch Loss: 0.744630\n",
            "Step 2192: Minibatch Loss: 1.104583\n",
            "Step 2194: Minibatch Loss: 0.793616\n",
            "Step 2196: Minibatch Loss: 0.818684\n",
            "Step 2198: Minibatch Loss: 0.659356\n",
            "Step 2200: Minibatch Loss: 0.766112\n",
            "Training for SNR= 5.5  sigma= 0.5308844442309884 iteratin: 0\n",
            "Step 2202: Minibatch Loss: 0.118854\n",
            "Step 2204: Minibatch Loss: 0.379169\n",
            "Step 2206: Minibatch Loss: 0.110403\n",
            "Step 2208: Minibatch Loss: 0.635991\n",
            "Step 2210: Minibatch Loss: 0.213399\n",
            "Step 2212: Minibatch Loss: 0.359292\n",
            "Step 2214: Minibatch Loss: 0.203363\n",
            "Step 2216: Minibatch Loss: 0.309127\n",
            "Step 2218: Minibatch Loss: 0.626483\n",
            "Step 2220: Minibatch Loss: 0.225845\n",
            "Step 2222: Minibatch Loss: 0.190728\n",
            "Step 2224: Minibatch Loss: 0.701598\n",
            "Step 2226: Minibatch Loss: 0.934951\n",
            "Step 2228: Minibatch Loss: 0.690410\n",
            "Step 2230: Minibatch Loss: 0.325117\n",
            "Step 2232: Minibatch Loss: 0.232511\n",
            "Step 2234: Minibatch Loss: 0.397301\n",
            "Step 2236: Minibatch Loss: 0.525029\n",
            "Step 2238: Minibatch Loss: 0.181824\n",
            "Step 2240: Minibatch Loss: 0.327109\n",
            "Step 2242: Minibatch Loss: 0.445608\n",
            "Step 2244: Minibatch Loss: 0.208598\n",
            "Step 2246: Minibatch Loss: 0.268589\n",
            "Step 2248: Minibatch Loss: 0.382056\n",
            "Step 2250: Minibatch Loss: 0.054463\n",
            "Step 2252: Minibatch Loss: 0.464731\n",
            "Step 2254: Minibatch Loss: 0.219009\n",
            "Step 2256: Minibatch Loss: 0.131515\n",
            "Step 2258: Minibatch Loss: 0.292337\n",
            "Step 2260: Minibatch Loss: 0.152129\n",
            "Step 2262: Minibatch Loss: 0.140926\n",
            "Step 2264: Minibatch Loss: 0.604365\n",
            "Step 2266: Minibatch Loss: 0.174859\n",
            "Step 2268: Minibatch Loss: 0.324905\n",
            "Step 2270: Minibatch Loss: 0.380007\n",
            "Step 2272: Minibatch Loss: 0.236626\n",
            "Step 2274: Minibatch Loss: 0.480947\n",
            "Step 2276: Minibatch Loss: 0.315695\n",
            "Step 2278: Minibatch Loss: 0.763619\n",
            "Step 2280: Minibatch Loss: 0.537233\n",
            "Step 2282: Minibatch Loss: 0.385531\n",
            "Step 2284: Minibatch Loss: 0.314923\n",
            "Step 2286: Minibatch Loss: 0.306554\n",
            "Step 2288: Minibatch Loss: 0.122407\n",
            "Step 2290: Minibatch Loss: 0.277689\n",
            "Step 2292: Minibatch Loss: 0.178254\n",
            "Step 2294: Minibatch Loss: 0.457243\n",
            "Step 2296: Minibatch Loss: 0.395549\n",
            "Step 2298: Minibatch Loss: 0.225512\n",
            "Step 2300: Minibatch Loss: 0.576407\n",
            "Step 2302: Minibatch Loss: 0.601008\n",
            "Step 2304: Minibatch Loss: 0.272481\n",
            "Step 2306: Minibatch Loss: 0.173410\n",
            "Step 2308: Minibatch Loss: 0.314280\n",
            "Step 2310: Minibatch Loss: 0.428215\n",
            "Step 2312: Minibatch Loss: 0.316243\n",
            "Step 2314: Minibatch Loss: 0.264049\n",
            "Step 2316: Minibatch Loss: 0.566241\n",
            "Step 2318: Minibatch Loss: 0.403352\n",
            "Step 2320: Minibatch Loss: 0.180921\n",
            "Step 2322: Minibatch Loss: 0.141167\n",
            "Step 2324: Minibatch Loss: 0.435534\n",
            "Step 2326: Minibatch Loss: 0.392654\n",
            "Step 2328: Minibatch Loss: 0.234023\n",
            "Step 2330: Minibatch Loss: 0.181693\n",
            "Step 2332: Minibatch Loss: 0.221362\n",
            "Step 2334: Minibatch Loss: 0.555260\n",
            "Step 2336: Minibatch Loss: 0.585868\n",
            "Step 2338: Minibatch Loss: 0.192347\n",
            "Step 2340: Minibatch Loss: 0.206286\n",
            "Step 2342: Minibatch Loss: 0.232197\n",
            "Step 2344: Minibatch Loss: 0.646423\n",
            "Step 2346: Minibatch Loss: 0.369802\n",
            "Step 2348: Minibatch Loss: 0.397743\n",
            "Step 2350: Minibatch Loss: 0.382979\n",
            "Step 2352: Minibatch Loss: 0.070479\n",
            "Step 2354: Minibatch Loss: 0.436620\n",
            "Step 2356: Minibatch Loss: 0.666069\n",
            "Step 2358: Minibatch Loss: 0.269708\n",
            "Step 2360: Minibatch Loss: 0.322357\n",
            "Step 2362: Minibatch Loss: 0.139529\n",
            "Step 2364: Minibatch Loss: 0.309437\n",
            "Step 2366: Minibatch Loss: 0.569573\n",
            "Step 2368: Minibatch Loss: 0.393903\n",
            "Step 2370: Minibatch Loss: 0.639060\n",
            "Step 2372: Minibatch Loss: 0.511948\n",
            "Step 2374: Minibatch Loss: 0.338939\n",
            "Step 2376: Minibatch Loss: 0.272805\n",
            "Step 2378: Minibatch Loss: 0.107537\n",
            "Step 2380: Minibatch Loss: 0.495444\n",
            "Step 2382: Minibatch Loss: 0.076751\n",
            "Step 2384: Minibatch Loss: 0.250012\n",
            "Step 2386: Minibatch Loss: 0.109552\n",
            "Step 2388: Minibatch Loss: 0.557605\n",
            "Step 2390: Minibatch Loss: 0.272918\n",
            "Step 2392: Minibatch Loss: 0.658486\n",
            "Step 2394: Minibatch Loss: 0.105253\n",
            "Step 2396: Minibatch Loss: 0.485316\n",
            "Step 2398: Minibatch Loss: 0.157373\n",
            "Step 2400: Minibatch Loss: 0.445200\n",
            "Training for SNR= 6.0  sigma= 0.5011872336272722 iteratin: 0\n",
            "Step 2402: Minibatch Loss: 0.231391\n",
            "Step 2404: Minibatch Loss: 0.150589\n",
            "Step 2406: Minibatch Loss: 0.019903\n",
            "Step 2408: Minibatch Loss: 0.138659\n",
            "Step 2410: Minibatch Loss: 0.232829\n",
            "Step 2412: Minibatch Loss: 0.059439\n",
            "Step 2414: Minibatch Loss: 0.122105\n",
            "Step 2416: Minibatch Loss: 0.388522\n",
            "Step 2418: Minibatch Loss: 0.292932\n",
            "Step 2420: Minibatch Loss: 0.132804\n",
            "Step 2422: Minibatch Loss: 0.183277\n",
            "Step 2424: Minibatch Loss: 0.144594\n",
            "Step 2426: Minibatch Loss: 0.342063\n",
            "Step 2428: Minibatch Loss: 0.179664\n",
            "Step 2430: Minibatch Loss: 0.150603\n",
            "Step 2432: Minibatch Loss: 0.428608\n",
            "Step 2434: Minibatch Loss: 0.119441\n",
            "Step 2436: Minibatch Loss: 0.127304\n",
            "Step 2438: Minibatch Loss: 0.291955\n",
            "Step 2440: Minibatch Loss: 0.098358\n",
            "Step 2442: Minibatch Loss: 0.106023\n",
            "Step 2444: Minibatch Loss: 0.130611\n",
            "Step 2446: Minibatch Loss: 0.381488\n",
            "Step 2448: Minibatch Loss: 0.179941\n",
            "Step 2450: Minibatch Loss: 0.138338\n",
            "Step 2452: Minibatch Loss: 0.240103\n",
            "Step 2454: Minibatch Loss: 0.411359\n",
            "Step 2456: Minibatch Loss: 0.237345\n",
            "Step 2458: Minibatch Loss: 0.160385\n",
            "Step 2460: Minibatch Loss: 0.091227\n",
            "Step 2462: Minibatch Loss: 0.212766\n",
            "Step 2464: Minibatch Loss: 0.253328\n",
            "Step 2466: Minibatch Loss: 0.149337\n",
            "Step 2468: Minibatch Loss: 0.080942\n",
            "Step 2470: Minibatch Loss: 0.179831\n",
            "Step 2472: Minibatch Loss: 0.085060\n",
            "Step 2474: Minibatch Loss: 0.257437\n",
            "Step 2476: Minibatch Loss: 0.291988\n",
            "Step 2478: Minibatch Loss: 0.108699\n",
            "Step 2480: Minibatch Loss: 0.323680\n",
            "Step 2482: Minibatch Loss: 0.407108\n",
            "Step 2484: Minibatch Loss: 0.102151\n",
            "Step 2486: Minibatch Loss: 0.572561\n",
            "Step 2488: Minibatch Loss: 0.425616\n",
            "Step 2490: Minibatch Loss: 0.068550\n",
            "Step 2492: Minibatch Loss: 0.275897\n",
            "Step 2494: Minibatch Loss: 0.366775\n",
            "Step 2496: Minibatch Loss: 0.212732\n",
            "Step 2498: Minibatch Loss: 0.285282\n",
            "Step 2500: Minibatch Loss: 0.026678\n",
            "Step 2502: Minibatch Loss: 0.348198\n",
            "Step 2504: Minibatch Loss: 0.234690\n",
            "Step 2506: Minibatch Loss: 0.201018\n",
            "Step 2508: Minibatch Loss: 0.317149\n",
            "Step 2510: Minibatch Loss: 0.143979\n",
            "Step 2512: Minibatch Loss: 0.177675\n",
            "Step 2514: Minibatch Loss: 0.299487\n",
            "Step 2516: Minibatch Loss: 0.321378\n",
            "Step 2518: Minibatch Loss: 0.260353\n",
            "Step 2520: Minibatch Loss: 0.201754\n",
            "Step 2522: Minibatch Loss: 0.138631\n",
            "Step 2524: Minibatch Loss: 0.012824\n",
            "Step 2526: Minibatch Loss: 0.086454\n",
            "Step 2528: Minibatch Loss: 0.343893\n",
            "Step 2530: Minibatch Loss: 0.353509\n",
            "Step 2532: Minibatch Loss: 0.259052\n",
            "Step 2534: Minibatch Loss: 0.597775\n",
            "Step 2536: Minibatch Loss: 0.243878\n",
            "Step 2538: Minibatch Loss: 0.074274\n",
            "Step 2540: Minibatch Loss: 0.162968\n",
            "Step 2542: Minibatch Loss: 0.209214\n",
            "Step 2544: Minibatch Loss: 0.181621\n",
            "Step 2546: Minibatch Loss: 0.232457\n",
            "Step 2548: Minibatch Loss: 0.132005\n",
            "Step 2550: Minibatch Loss: 0.538711\n",
            "Step 2552: Minibatch Loss: 0.311600\n",
            "Step 2554: Minibatch Loss: 0.162041\n",
            "Step 2556: Minibatch Loss: 0.236037\n",
            "Step 2558: Minibatch Loss: 0.212886\n",
            "Step 2560: Minibatch Loss: 0.025056\n",
            "Step 2562: Minibatch Loss: 0.056234\n",
            "Step 2564: Minibatch Loss: 0.105736\n",
            "Step 2566: Minibatch Loss: 0.097505\n",
            "Step 2568: Minibatch Loss: 0.235816\n",
            "Step 2570: Minibatch Loss: 0.256220\n",
            "Step 2572: Minibatch Loss: 0.187135\n",
            "Step 2574: Minibatch Loss: 0.068119\n",
            "Step 2576: Minibatch Loss: 0.060408\n",
            "Step 2578: Minibatch Loss: 0.161133\n",
            "Step 2580: Minibatch Loss: 0.150086\n",
            "Step 2582: Minibatch Loss: 0.007397\n",
            "Step 2584: Minibatch Loss: 0.218052\n",
            "Step 2586: Minibatch Loss: 0.188143\n",
            "Step 2588: Minibatch Loss: 0.280800\n",
            "Step 2590: Minibatch Loss: 0.270594\n",
            "Step 2592: Minibatch Loss: 0.174402\n",
            "Step 2594: Minibatch Loss: 0.040532\n",
            "Step 2596: Minibatch Loss: 0.356086\n",
            "Step 2598: Minibatch Loss: 0.254363\n",
            "Step 2600: Minibatch Loss: 0.118055\n",
            "Training for SNR= 6.5  sigma= 0.47315125896148047 iteratin: 0\n",
            "Step 2602: Minibatch Loss: 0.131700\n",
            "Step 2604: Minibatch Loss: 0.186980\n",
            "Step 2606: Minibatch Loss: 0.023837\n",
            "Step 2608: Minibatch Loss: 0.054241\n",
            "Step 2610: Minibatch Loss: 0.261713\n",
            "Step 2612: Minibatch Loss: 0.038369\n",
            "Step 2614: Minibatch Loss: 0.143465\n",
            "Step 2616: Minibatch Loss: 0.098926\n",
            "Step 2618: Minibatch Loss: 0.170815\n",
            "Step 2620: Minibatch Loss: 0.005372\n",
            "Step 2622: Minibatch Loss: 0.143822\n",
            "Step 2624: Minibatch Loss: 0.045430\n",
            "Step 2626: Minibatch Loss: 0.029177\n",
            "Step 2628: Minibatch Loss: 0.040365\n",
            "Step 2630: Minibatch Loss: 0.031164\n",
            "Step 2632: Minibatch Loss: 0.039499\n",
            "Step 2634: Minibatch Loss: 0.037979\n",
            "Step 2636: Minibatch Loss: 0.000000\n",
            "Step 2638: Minibatch Loss: 0.131499\n",
            "Step 2640: Minibatch Loss: 0.117917\n",
            "Step 2642: Minibatch Loss: 0.046186\n",
            "Step 2644: Minibatch Loss: 0.118530\n",
            "Step 2646: Minibatch Loss: 0.040650\n",
            "Step 2648: Minibatch Loss: 0.025599\n",
            "Step 2650: Minibatch Loss: 0.006643\n",
            "Step 2652: Minibatch Loss: 0.099583\n",
            "Step 2654: Minibatch Loss: 0.085182\n",
            "Step 2656: Minibatch Loss: 0.000010\n",
            "Step 2658: Minibatch Loss: 0.327293\n",
            "Step 2660: Minibatch Loss: 0.054287\n",
            "Step 2662: Minibatch Loss: 0.113972\n",
            "Step 2664: Minibatch Loss: 0.028236\n",
            "Step 2666: Minibatch Loss: 0.064192\n",
            "Step 2668: Minibatch Loss: 0.035280\n",
            "Step 2670: Minibatch Loss: 0.040430\n",
            "Step 2672: Minibatch Loss: 0.025153\n",
            "Step 2674: Minibatch Loss: 0.010047\n",
            "Step 2676: Minibatch Loss: 0.057245\n",
            "Step 2678: Minibatch Loss: 0.064716\n",
            "Step 2680: Minibatch Loss: 0.134700\n",
            "Step 2682: Minibatch Loss: 0.039115\n",
            "Step 2684: Minibatch Loss: 0.104266\n",
            "Step 2686: Minibatch Loss: 0.161809\n",
            "Step 2688: Minibatch Loss: 0.031461\n",
            "Step 2690: Minibatch Loss: 0.013997\n",
            "Step 2692: Minibatch Loss: 0.075134\n",
            "Step 2694: Minibatch Loss: 0.082730\n",
            "Step 2696: Minibatch Loss: 0.001477\n",
            "Step 2698: Minibatch Loss: 0.037686\n",
            "Step 2700: Minibatch Loss: 0.104706\n",
            "Step 2702: Minibatch Loss: 0.023210\n",
            "Step 2704: Minibatch Loss: 0.198876\n",
            "Step 2706: Minibatch Loss: 0.020456\n",
            "Step 2708: Minibatch Loss: 0.000763\n",
            "Step 2710: Minibatch Loss: 0.133659\n",
            "Step 2712: Minibatch Loss: 0.014445\n",
            "Step 2714: Minibatch Loss: 0.010761\n",
            "Step 2716: Minibatch Loss: 0.165156\n",
            "Step 2718: Minibatch Loss: 0.028279\n",
            "Step 2720: Minibatch Loss: 0.259203\n",
            "Step 2722: Minibatch Loss: 0.266194\n",
            "Step 2724: Minibatch Loss: 0.067746\n",
            "Step 2726: Minibatch Loss: 0.115049\n",
            "Step 2728: Minibatch Loss: 0.053038\n",
            "Step 2730: Minibatch Loss: 0.261119\n",
            "Step 2732: Minibatch Loss: 0.112957\n",
            "Step 2734: Minibatch Loss: 0.170729\n",
            "Step 2736: Minibatch Loss: 0.024432\n",
            "Step 2738: Minibatch Loss: 0.000562\n",
            "Step 2740: Minibatch Loss: 0.183446\n",
            "Step 2742: Minibatch Loss: 0.362192\n",
            "Step 2744: Minibatch Loss: 0.086297\n",
            "Step 2746: Minibatch Loss: 0.096622\n",
            "Step 2748: Minibatch Loss: 0.040269\n",
            "Step 2750: Minibatch Loss: 0.218967\n",
            "Step 2752: Minibatch Loss: 0.098357\n",
            "Step 2754: Minibatch Loss: 0.045805\n",
            "Step 2756: Minibatch Loss: 0.057222\n",
            "Step 2758: Minibatch Loss: 0.022114\n",
            "Step 2760: Minibatch Loss: 0.226442\n",
            "Step 2762: Minibatch Loss: 0.008665\n",
            "Step 2764: Minibatch Loss: 0.012101\n",
            "Step 2766: Minibatch Loss: 0.071058\n",
            "Step 2768: Minibatch Loss: 0.116194\n",
            "Step 2770: Minibatch Loss: 0.040719\n",
            "Step 2772: Minibatch Loss: 0.000066\n",
            "Step 2774: Minibatch Loss: 0.148999\n",
            "Step 2776: Minibatch Loss: 0.146195\n",
            "Step 2778: Minibatch Loss: 0.101394\n",
            "Step 2780: Minibatch Loss: 0.320500\n",
            "Step 2782: Minibatch Loss: 0.000004\n",
            "Step 2784: Minibatch Loss: 0.048467\n",
            "Step 2786: Minibatch Loss: 0.057181\n",
            "Step 2788: Minibatch Loss: 0.048131\n",
            "Step 2790: Minibatch Loss: 0.012236\n",
            "Step 2792: Minibatch Loss: 0.103276\n",
            "Step 2794: Minibatch Loss: 0.069447\n",
            "Step 2796: Minibatch Loss: 0.063093\n",
            "Step 2798: Minibatch Loss: 0.001724\n",
            "Step 2800: Minibatch Loss: 0.265828\n",
            "Training for SNR= 7.0  sigma= 0.44668359215096315 iteratin: 0\n",
            "Step 2802: Minibatch Loss: 0.210076\n",
            "Step 2804: Minibatch Loss: 0.049645\n",
            "Step 2806: Minibatch Loss: 0.107056\n",
            "Step 2808: Minibatch Loss: 0.000000\n",
            "Step 2810: Minibatch Loss: 0.000000\n",
            "Step 2812: Minibatch Loss: 0.000000\n",
            "Step 2814: Minibatch Loss: 0.015886\n",
            "Step 2816: Minibatch Loss: 0.017818\n",
            "Step 2818: Minibatch Loss: 0.156322\n",
            "Step 2820: Minibatch Loss: 0.076990\n",
            "Step 2822: Minibatch Loss: 0.047268\n",
            "Step 2824: Minibatch Loss: 0.111399\n",
            "Step 2826: Minibatch Loss: 0.004335\n",
            "Step 2828: Minibatch Loss: 0.031666\n",
            "Step 2830: Minibatch Loss: 0.005404\n",
            "Step 2832: Minibatch Loss: 0.000000\n",
            "Step 2834: Minibatch Loss: 0.066735\n",
            "Step 2836: Minibatch Loss: 0.003511\n",
            "Step 2838: Minibatch Loss: 0.160805\n",
            "Step 2840: Minibatch Loss: 0.000100\n",
            "Step 2842: Minibatch Loss: 0.095332\n",
            "Step 2844: Minibatch Loss: 0.042990\n",
            "Step 2846: Minibatch Loss: 0.085715\n",
            "Step 2848: Minibatch Loss: 0.079532\n",
            "Step 2850: Minibatch Loss: 0.243980\n",
            "Step 2852: Minibatch Loss: 0.058404\n",
            "Step 2854: Minibatch Loss: 0.000000\n",
            "Step 2856: Minibatch Loss: 0.108455\n",
            "Step 2858: Minibatch Loss: 0.091993\n",
            "Step 2860: Minibatch Loss: 0.010005\n",
            "Step 2862: Minibatch Loss: 0.061669\n",
            "Step 2864: Minibatch Loss: 0.002502\n",
            "Step 2866: Minibatch Loss: 0.027312\n",
            "Step 2868: Minibatch Loss: 0.025599\n",
            "Step 2870: Minibatch Loss: 0.038919\n",
            "Step 2872: Minibatch Loss: 0.001334\n",
            "Step 2874: Minibatch Loss: 0.000070\n",
            "Step 2876: Minibatch Loss: 0.090928\n",
            "Step 2878: Minibatch Loss: 0.000000\n",
            "Step 2880: Minibatch Loss: 0.174133\n",
            "Step 2882: Minibatch Loss: 0.039334\n",
            "Step 2884: Minibatch Loss: 0.000003\n",
            "Step 2886: Minibatch Loss: 0.145216\n",
            "Step 2888: Minibatch Loss: 0.004109\n",
            "Step 2890: Minibatch Loss: 0.000001\n",
            "Step 2892: Minibatch Loss: 0.055955\n",
            "Step 2894: Minibatch Loss: 0.000006\n",
            "Step 2896: Minibatch Loss: 0.000842\n",
            "Step 2898: Minibatch Loss: 0.026835\n",
            "Step 2900: Minibatch Loss: 0.000111\n",
            "Step 2902: Minibatch Loss: 0.000000\n",
            "Step 2904: Minibatch Loss: 0.056226\n",
            "Step 2906: Minibatch Loss: 0.000000\n",
            "Step 2908: Minibatch Loss: 0.094649\n",
            "Step 2910: Minibatch Loss: 0.047098\n",
            "Step 2912: Minibatch Loss: 0.000000\n",
            "Step 2914: Minibatch Loss: 0.003028\n",
            "Step 2916: Minibatch Loss: 0.054245\n",
            "Step 2918: Minibatch Loss: 0.000000\n",
            "Step 2920: Minibatch Loss: 0.049448\n",
            "Step 2922: Minibatch Loss: 0.000003\n",
            "Step 2924: Minibatch Loss: 0.026412\n",
            "Step 2926: Minibatch Loss: 0.038448\n",
            "Step 2928: Minibatch Loss: 0.001265\n",
            "Step 2930: Minibatch Loss: 0.047073\n",
            "Step 2932: Minibatch Loss: 0.000001\n",
            "Step 2934: Minibatch Loss: 0.000060\n",
            "Step 2936: Minibatch Loss: 0.036096\n",
            "Step 2938: Minibatch Loss: 0.000374\n",
            "Step 2940: Minibatch Loss: 0.049830\n",
            "Step 2942: Minibatch Loss: 0.114901\n",
            "Step 2944: Minibatch Loss: 0.000002\n",
            "Step 2946: Minibatch Loss: 0.091110\n",
            "Step 2948: Minibatch Loss: 0.059520\n",
            "Step 2950: Minibatch Loss: 0.000089\n",
            "Step 2952: Minibatch Loss: 0.062371\n",
            "Step 2954: Minibatch Loss: 0.053998\n",
            "Step 2956: Minibatch Loss: 0.002964\n",
            "Step 2958: Minibatch Loss: 0.011939\n",
            "Step 2960: Minibatch Loss: 0.050551\n",
            "Step 2962: Minibatch Loss: 0.048962\n",
            "Step 2964: Minibatch Loss: 0.043323\n",
            "Step 2966: Minibatch Loss: 0.046373\n",
            "Step 2968: Minibatch Loss: 0.070843\n",
            "Step 2970: Minibatch Loss: 0.031888\n",
            "Step 2972: Minibatch Loss: 0.068620\n",
            "Step 2974: Minibatch Loss: 0.032069\n",
            "Step 2976: Minibatch Loss: 0.000000\n",
            "Step 2978: Minibatch Loss: 0.000000\n",
            "Step 2980: Minibatch Loss: 0.121198\n",
            "Step 2982: Minibatch Loss: 0.001206\n",
            "Step 2984: Minibatch Loss: 0.241577\n",
            "Step 2986: Minibatch Loss: 0.057760\n",
            "Step 2988: Minibatch Loss: 0.015191\n",
            "Step 2990: Minibatch Loss: 0.004674\n",
            "Step 2992: Minibatch Loss: 0.042085\n",
            "Step 2994: Minibatch Loss: 0.021242\n",
            "Step 2996: Minibatch Loss: 0.020807\n",
            "Step 2998: Minibatch Loss: 0.076519\n",
            "Step 3000: Minibatch Loss: 0.036512\n",
            "Training for SNR= 7.5  sigma= 0.4216965034285822 iteratin: 0\n",
            "Step 3002: Minibatch Loss: 0.069008\n",
            "Step 3004: Minibatch Loss: 0.000000\n",
            "Step 3006: Minibatch Loss: 0.000004\n",
            "Step 3008: Minibatch Loss: 0.000000\n",
            "Step 3010: Minibatch Loss: 0.000000\n",
            "Step 3012: Minibatch Loss: 0.000000\n",
            "Step 3014: Minibatch Loss: 0.046231\n",
            "Step 3016: Minibatch Loss: 0.020902\n",
            "Step 3018: Minibatch Loss: 0.000000\n",
            "Step 3020: Minibatch Loss: 0.022084\n",
            "Step 3022: Minibatch Loss: 0.000000\n",
            "Step 3024: Minibatch Loss: 0.000000\n",
            "Step 3026: Minibatch Loss: 0.040759\n",
            "Step 3028: Minibatch Loss: 0.014734\n",
            "Step 3030: Minibatch Loss: 0.003657\n",
            "Step 3032: Minibatch Loss: 0.066504\n",
            "Step 3034: Minibatch Loss: 0.002001\n",
            "Step 3036: Minibatch Loss: 0.000061\n",
            "Step 3038: Minibatch Loss: 0.000000\n",
            "Step 3040: Minibatch Loss: 0.000000\n",
            "Step 3042: Minibatch Loss: 0.076599\n",
            "Step 3044: Minibatch Loss: 0.000053\n",
            "Step 3046: Minibatch Loss: 0.000000\n",
            "Step 3048: Minibatch Loss: 0.071489\n",
            "Step 3050: Minibatch Loss: 0.020092\n",
            "Step 3052: Minibatch Loss: 0.000000\n",
            "Step 3054: Minibatch Loss: 0.003270\n",
            "Step 3056: Minibatch Loss: 0.000003\n",
            "Step 3058: Minibatch Loss: 0.000001\n",
            "Step 3060: Minibatch Loss: 0.148640\n",
            "Step 3062: Minibatch Loss: 0.103502\n",
            "Step 3064: Minibatch Loss: 0.000000\n",
            "Step 3066: Minibatch Loss: 0.000000\n",
            "Step 3068: Minibatch Loss: 0.017226\n",
            "Step 3070: Minibatch Loss: 0.022314\n",
            "Step 3072: Minibatch Loss: 0.000520\n",
            "Step 3074: Minibatch Loss: 0.075523\n",
            "Step 3076: Minibatch Loss: 0.000000\n",
            "Step 3078: Minibatch Loss: 0.016055\n",
            "Step 3080: Minibatch Loss: 0.000179\n",
            "Step 3082: Minibatch Loss: 0.000000\n",
            "Step 3084: Minibatch Loss: 0.011202\n",
            "Step 3086: Minibatch Loss: 0.001282\n",
            "Step 3088: Minibatch Loss: 0.000698\n",
            "Step 3090: Minibatch Loss: 0.013281\n",
            "Step 3092: Minibatch Loss: 0.000000\n",
            "Step 3094: Minibatch Loss: 0.089631\n",
            "Step 3096: Minibatch Loss: 0.055803\n",
            "Step 3098: Minibatch Loss: 0.073771\n",
            "Step 3100: Minibatch Loss: 0.011670\n",
            "Step 3102: Minibatch Loss: 0.005965\n",
            "Step 3104: Minibatch Loss: 0.000000\n",
            "Step 3106: Minibatch Loss: 0.044768\n",
            "Step 3108: Minibatch Loss: 0.000000\n",
            "Step 3110: Minibatch Loss: 0.001485\n",
            "Step 3112: Minibatch Loss: 0.000000\n",
            "Step 3114: Minibatch Loss: 0.000000\n",
            "Step 3116: Minibatch Loss: 0.053299\n",
            "Step 3118: Minibatch Loss: 0.022996\n",
            "Step 3120: Minibatch Loss: 0.014038\n",
            "Step 3122: Minibatch Loss: 0.037528\n",
            "Step 3124: Minibatch Loss: 0.051697\n",
            "Step 3126: Minibatch Loss: 0.095848\n",
            "Step 3128: Minibatch Loss: 0.017895\n",
            "Step 3130: Minibatch Loss: 0.000020\n",
            "Step 3132: Minibatch Loss: 0.036343\n",
            "Step 3134: Minibatch Loss: 0.073064\n",
            "Step 3136: Minibatch Loss: 0.029058\n",
            "Step 3138: Minibatch Loss: 0.000073\n",
            "Step 3140: Minibatch Loss: 0.000000\n",
            "Step 3142: Minibatch Loss: 0.026654\n",
            "Step 3144: Minibatch Loss: 0.028240\n",
            "Step 3146: Minibatch Loss: 0.043910\n",
            "Step 3148: Minibatch Loss: 0.021689\n",
            "Step 3150: Minibatch Loss: 0.000001\n",
            "Step 3152: Minibatch Loss: 0.014593\n",
            "Step 3154: Minibatch Loss: 0.000000\n",
            "Step 3156: Minibatch Loss: 0.000024\n",
            "Step 3158: Minibatch Loss: 0.000000\n",
            "Step 3160: Minibatch Loss: 0.000009\n",
            "Step 3162: Minibatch Loss: 0.003715\n",
            "Step 3164: Minibatch Loss: 0.005276\n",
            "Step 3166: Minibatch Loss: 0.000000\n",
            "Step 3168: Minibatch Loss: 0.000013\n",
            "Step 3170: Minibatch Loss: 0.000005\n",
            "Step 3172: Minibatch Loss: 0.043328\n",
            "Step 3174: Minibatch Loss: 0.000000\n",
            "Step 3176: Minibatch Loss: 0.018181\n",
            "Step 3178: Minibatch Loss: 0.009610\n",
            "Step 3180: Minibatch Loss: 0.000003\n",
            "Step 3182: Minibatch Loss: 0.000003\n",
            "Step 3184: Minibatch Loss: 0.014189\n",
            "Step 3186: Minibatch Loss: 0.000912\n",
            "Step 3188: Minibatch Loss: 0.000000\n",
            "Step 3190: Minibatch Loss: 0.000000\n",
            "Step 3192: Minibatch Loss: 0.000000\n",
            "Step 3194: Minibatch Loss: 0.000010\n",
            "Step 3196: Minibatch Loss: 0.000007\n",
            "Step 3198: Minibatch Loss: 0.005646\n",
            "Step 3200: Minibatch Loss: 0.013057\n",
            "Training for SNR= 8.0  sigma= 0.3981071705534972 iteratin: 0\n",
            "Step 3202: Minibatch Loss: 0.000019\n",
            "Step 3204: Minibatch Loss: 0.000000\n",
            "Step 3206: Minibatch Loss: 0.000000\n",
            "Step 3208: Minibatch Loss: 0.000000\n",
            "Step 3210: Minibatch Loss: 0.000779\n",
            "Step 3212: Minibatch Loss: 0.000035\n",
            "Step 3214: Minibatch Loss: 0.022465\n",
            "Step 3216: Minibatch Loss: 0.000000\n",
            "Step 3218: Minibatch Loss: 0.033641\n",
            "Step 3220: Minibatch Loss: 0.000000\n",
            "Step 3222: Minibatch Loss: 0.002343\n",
            "Step 3224: Minibatch Loss: 0.000000\n",
            "Step 3226: Minibatch Loss: 0.000000\n",
            "Step 3228: Minibatch Loss: 0.000000\n",
            "Step 3230: Minibatch Loss: 0.066809\n",
            "Step 3232: Minibatch Loss: 0.000000\n",
            "Step 3234: Minibatch Loss: 0.086715\n",
            "Step 3236: Minibatch Loss: 0.000000\n",
            "Step 3238: Minibatch Loss: 0.000000\n",
            "Step 3240: Minibatch Loss: 0.000003\n",
            "Step 3242: Minibatch Loss: 0.016826\n",
            "Step 3244: Minibatch Loss: 0.000000\n",
            "Step 3246: Minibatch Loss: 0.000000\n",
            "Step 3248: Minibatch Loss: 0.000000\n",
            "Step 3250: Minibatch Loss: 0.000000\n",
            "Step 3252: Minibatch Loss: 0.011135\n",
            "Step 3254: Minibatch Loss: 0.000001\n",
            "Step 3256: Minibatch Loss: 0.000000\n",
            "Step 3258: Minibatch Loss: 0.000000\n",
            "Step 3260: Minibatch Loss: 0.000000\n",
            "Step 3262: Minibatch Loss: 0.011167\n",
            "Step 3264: Minibatch Loss: 0.000000\n",
            "Step 3266: Minibatch Loss: 0.000000\n",
            "Step 3268: Minibatch Loss: 0.000000\n",
            "Step 3270: Minibatch Loss: 0.005979\n",
            "Step 3272: Minibatch Loss: 0.000000\n",
            "Step 3274: Minibatch Loss: 0.000000\n",
            "Step 3276: Minibatch Loss: 0.071920\n",
            "Step 3278: Minibatch Loss: 0.000002\n",
            "Step 3280: Minibatch Loss: 0.000000\n",
            "Step 3282: Minibatch Loss: 0.000000\n",
            "Step 3284: Minibatch Loss: 0.000849\n",
            "Step 3286: Minibatch Loss: 0.000000\n",
            "Step 3288: Minibatch Loss: 0.015672\n",
            "Step 3290: Minibatch Loss: 0.000001\n",
            "Step 3292: Minibatch Loss: 0.000000\n",
            "Step 3294: Minibatch Loss: 0.000000\n",
            "Step 3296: Minibatch Loss: 0.026982\n",
            "Step 3298: Minibatch Loss: 0.000000\n",
            "Step 3300: Minibatch Loss: 0.000000\n",
            "Step 3302: Minibatch Loss: 0.000002\n",
            "Step 3304: Minibatch Loss: 0.000001\n",
            "Step 3306: Minibatch Loss: 0.004393\n",
            "Step 3308: Minibatch Loss: 0.000000\n",
            "Step 3310: Minibatch Loss: 0.075866\n",
            "Step 3312: Minibatch Loss: 0.073796\n",
            "Step 3314: Minibatch Loss: 0.000000\n",
            "Step 3316: Minibatch Loss: 0.041655\n",
            "Step 3318: Minibatch Loss: 0.000000\n",
            "Step 3320: Minibatch Loss: 0.000000\n",
            "Step 3322: Minibatch Loss: 0.000000\n",
            "Step 3324: Minibatch Loss: 0.111266\n",
            "Step 3326: Minibatch Loss: 0.000000\n",
            "Step 3328: Minibatch Loss: 0.068653\n",
            "Step 3330: Minibatch Loss: 0.000000\n",
            "Step 3332: Minibatch Loss: 0.000000\n",
            "Step 3334: Minibatch Loss: 0.000000\n",
            "Step 3336: Minibatch Loss: 0.000000\n",
            "Step 3338: Minibatch Loss: 0.000000\n",
            "Step 3340: Minibatch Loss: 0.000000\n",
            "Step 3342: Minibatch Loss: 0.000000\n",
            "Step 3344: Minibatch Loss: 0.000027\n",
            "Step 3346: Minibatch Loss: 0.000000\n",
            "Step 3348: Minibatch Loss: 0.000001\n",
            "Step 3350: Minibatch Loss: 0.000000\n",
            "Step 3352: Minibatch Loss: 0.000099\n",
            "Step 3354: Minibatch Loss: 0.000000\n",
            "Step 3356: Minibatch Loss: 0.000000\n",
            "Step 3358: Minibatch Loss: 0.000000\n",
            "Step 3360: Minibatch Loss: 0.000000\n",
            "Step 3362: Minibatch Loss: 0.028035\n",
            "Step 3364: Minibatch Loss: 0.034718\n",
            "Step 3366: Minibatch Loss: 0.000000\n",
            "Step 3368: Minibatch Loss: 0.005241\n",
            "Step 3370: Minibatch Loss: 0.000106\n",
            "Step 3372: Minibatch Loss: 0.000000\n",
            "Step 3374: Minibatch Loss: 0.000000\n",
            "Step 3376: Minibatch Loss: 0.103588\n",
            "Step 3378: Minibatch Loss: 0.000000\n",
            "Step 3380: Minibatch Loss: 0.000000\n",
            "Step 3382: Minibatch Loss: 0.000003\n",
            "Step 3384: Minibatch Loss: 0.000012\n",
            "Step 3386: Minibatch Loss: 0.000000\n",
            "Step 3388: Minibatch Loss: 0.000000\n",
            "Step 3390: Minibatch Loss: 0.000000\n",
            "Step 3392: Minibatch Loss: 0.009130\n",
            "Step 3394: Minibatch Loss: 0.000000\n",
            "Step 3396: Minibatch Loss: 0.000221\n",
            "Step 3398: Minibatch Loss: 0.000000\n",
            "Step 3400: Minibatch Loss: 0.000000\n",
            "Training for SNR= 8.5  sigma= 0.3758374042884442 iteratin: 0\n",
            "Step 3402: Minibatch Loss: 0.000000\n",
            "Step 3404: Minibatch Loss: 0.034701\n",
            "Step 3406: Minibatch Loss: 0.054038\n",
            "Step 3408: Minibatch Loss: 0.000000\n",
            "Step 3410: Minibatch Loss: 0.000000\n",
            "Step 3412: Minibatch Loss: 0.000003\n",
            "Step 3414: Minibatch Loss: 0.013007\n",
            "Step 3416: Minibatch Loss: 0.023211\n",
            "Step 3418: Minibatch Loss: 0.000000\n",
            "Step 3420: Minibatch Loss: 0.000000\n",
            "Step 3422: Minibatch Loss: 0.000014\n",
            "Step 3424: Minibatch Loss: 0.011872\n",
            "Step 3426: Minibatch Loss: 0.000000\n",
            "Step 3428: Minibatch Loss: 0.000000\n",
            "Step 3430: Minibatch Loss: 0.000000\n",
            "Step 3432: Minibatch Loss: 0.000000\n",
            "Step 3434: Minibatch Loss: 0.000000\n",
            "Step 3436: Minibatch Loss: 0.000000\n",
            "Step 3438: Minibatch Loss: 0.000000\n",
            "Step 3440: Minibatch Loss: 0.000000\n",
            "Step 3442: Minibatch Loss: 0.000000\n",
            "Step 3444: Minibatch Loss: 0.067866\n",
            "Step 3446: Minibatch Loss: 0.000000\n",
            "Step 3448: Minibatch Loss: 0.000000\n",
            "Step 3450: Minibatch Loss: 0.000000\n",
            "Step 3452: Minibatch Loss: 0.000000\n",
            "Step 3454: Minibatch Loss: 0.000000\n",
            "Step 3456: Minibatch Loss: 0.000000\n",
            "Step 3458: Minibatch Loss: 0.000000\n",
            "Step 3460: Minibatch Loss: 0.000000\n",
            "Step 3462: Minibatch Loss: 0.000000\n",
            "Step 3464: Minibatch Loss: 0.010632\n",
            "Step 3466: Minibatch Loss: 0.007015\n",
            "Step 3468: Minibatch Loss: 0.000007\n",
            "Step 3470: Minibatch Loss: 0.000002\n",
            "Step 3472: Minibatch Loss: 0.000000\n",
            "Step 3474: Minibatch Loss: 0.000000\n",
            "Step 3476: Minibatch Loss: 0.000000\n",
            "Step 3478: Minibatch Loss: 0.000000\n",
            "Step 3480: Minibatch Loss: 0.015475\n",
            "Step 3482: Minibatch Loss: 0.000000\n",
            "Step 3484: Minibatch Loss: 0.000000\n",
            "Step 3486: Minibatch Loss: 0.000000\n",
            "Step 3488: Minibatch Loss: 0.000000\n",
            "Step 3490: Minibatch Loss: 0.000000\n",
            "Step 3492: Minibatch Loss: 0.000000\n",
            "Step 3494: Minibatch Loss: 0.000000\n",
            "Step 3496: Minibatch Loss: 0.000010\n",
            "Step 3498: Minibatch Loss: 0.000711\n",
            "Step 3500: Minibatch Loss: 0.000000\n",
            "Step 3502: Minibatch Loss: 0.000000\n",
            "Step 3504: Minibatch Loss: 0.000000\n",
            "Step 3506: Minibatch Loss: 0.000000\n",
            "Step 3508: Minibatch Loss: 0.000000\n",
            "Step 3510: Minibatch Loss: 0.000000\n",
            "Step 3512: Minibatch Loss: 0.000000\n",
            "Step 3514: Minibatch Loss: 0.000000\n",
            "Step 3516: Minibatch Loss: 0.014412\n",
            "Step 3518: Minibatch Loss: 0.000000\n",
            "Step 3520: Minibatch Loss: 0.000000\n",
            "Step 3522: Minibatch Loss: 0.095606\n",
            "Step 3524: Minibatch Loss: 0.000000\n",
            "Step 3526: Minibatch Loss: 0.000004\n",
            "Step 3528: Minibatch Loss: 0.073632\n",
            "Step 3530: Minibatch Loss: 0.000000\n",
            "Step 3532: Minibatch Loss: 0.000000\n",
            "Step 3534: Minibatch Loss: 0.021765\n",
            "Step 3536: Minibatch Loss: 0.006149\n",
            "Step 3538: Minibatch Loss: 0.000000\n",
            "Step 3540: Minibatch Loss: 0.000001\n",
            "Step 3542: Minibatch Loss: 0.000000\n",
            "Step 3544: Minibatch Loss: 0.000000\n",
            "Step 3546: Minibatch Loss: 0.000370\n",
            "Step 3548: Minibatch Loss: 0.000000\n",
            "Step 3550: Minibatch Loss: 0.000000\n",
            "Step 3552: Minibatch Loss: 0.000115\n",
            "Step 3554: Minibatch Loss: 0.000000\n",
            "Step 3556: Minibatch Loss: 0.006195\n",
            "Step 3558: Minibatch Loss: 0.000000\n",
            "Step 3560: Minibatch Loss: 0.000000\n",
            "Step 3562: Minibatch Loss: 0.000001\n",
            "Step 3564: Minibatch Loss: 0.000000\n",
            "Step 3566: Minibatch Loss: 0.000000\n",
            "Step 3568: Minibatch Loss: 0.061416\n",
            "Step 3570: Minibatch Loss: 0.000002\n",
            "Step 3572: Minibatch Loss: 0.000000\n",
            "Step 3574: Minibatch Loss: 0.000000\n",
            "Step 3576: Minibatch Loss: 0.000013\n",
            "Step 3578: Minibatch Loss: 0.000000\n",
            "Step 3580: Minibatch Loss: 0.000000\n",
            "Step 3582: Minibatch Loss: 0.027052\n",
            "Step 3584: Minibatch Loss: 0.000000\n",
            "Step 3586: Minibatch Loss: 0.000000\n",
            "Step 3588: Minibatch Loss: 0.000000\n",
            "Step 3590: Minibatch Loss: 0.000000\n",
            "Step 3592: Minibatch Loss: 0.000000\n",
            "Step 3594: Minibatch Loss: 0.000000\n",
            "Step 3596: Minibatch Loss: 0.000000\n",
            "Step 3598: Minibatch Loss: 0.000000\n",
            "Step 3600: Minibatch Loss: 0.000000\n",
            "Training for SNR= 9.0  sigma= 0.35481338923357547 iteratin: 0\n",
            "Loss= 0.0\n",
            "Early Stop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHByzQbTUqbv",
        "outputId": "ea805693-5cd0-4385-f118-cbe77f5f4a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using trained model\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_dl_tensor  = numpy.array(())\n",
        "times_per_iter_dl_tensor = numpy.array(())\n",
        "\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  lrate = 0.001\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    input_message_xx = training_input_message_one_hot [i:i+1]\n",
        "    input_message_xx = input_message_xx.astype(\"float32\")\n",
        "    #,input_message_x_label:training_input_message [i]\n",
        "    encoded_message = train_sess.run ([dl_encoder_output], feed_dict={input_message_x:input_message_xx })\n",
        "    encoded_message = encoded_message[0][0]\n",
        "    #encoded_message = numpy.around(encoded_message[0][0]> 0).astype(int)\n",
        "    #print (encoded_message[0][0])\n",
        "    awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message})\n",
        "    #print (awgn_channel_output_message)\n",
        "    decoded_message = train_sess.run ([dl_decoder_only_output], feed_dict={input_channel_x:awgn_channel_output_message})\n",
        "    #print (\"input\", input_message[i])\n",
        "    #decoded_message = numpy.around(decoded_message[0][0]> 0).astype(int)\n",
        "    #rint (\"output\", decoded_message)\n",
        "    #print (\"output\", numpy.argmax(training_input_message_one_hot[i]), numpy.argmax(decoded_message[0][0]))\n",
        "    if (numpy.argmax(training_input_message_one_hot[i]) != numpy.argmax(decoded_message[0][0])):\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_dl_tensor=numpy.append(ber_per_iter_dl_tensor ,ber)\n",
        "  times_per_iter_dl_tensor=numpy.append(times_per_iter_dl_tensor, total_time)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 0.76s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 1.31s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 1.84s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 2.39s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.54\n",
            " -> Total Time: 6.30s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 0.51s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 1.02s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 1.54s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 2.05s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.45\n",
            " -> Total Time: 5.12s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.55s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 1.10s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 1.64s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 2.17s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.40\n",
            " -> Total Time: 5.47s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.58s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.10s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 1.65s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 2.17s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.32\n",
            " -> Total Time: 5.50s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.53s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.06s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 1.60s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.14s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.26\n",
            " -> Total Time: 5.34s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.56s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.10s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.65s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.20s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.19\n",
            " -> Total Time: 5.51s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.51s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 1.01s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.52s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 2.02s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.14\n",
            " -> Total Time: 5.06s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.51s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 1.02s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.53s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 2.04s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.10\n",
            " -> Total Time: 5.11s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.51s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 1.02s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 1.56s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 2.13s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.08\n",
            " -> Total Time: 5.22s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.50s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 1.00s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 1.52s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 2.02s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 5.04s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.51s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 1.02s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 1.51s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 2.02s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 5.05s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.51s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 1.04s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 1.57s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 2.10s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 5.22s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.54s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 1.07s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 1.58s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 2.11s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 5.31s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.49s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 1.01s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 1.51s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 2.05s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 5.05s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.51s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 1.03s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 1.55s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 2.10s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 5.19s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.57s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 1.10s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 1.64s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 2.17s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 5.48s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.51s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 1.02s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 1.53s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 2.05s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 5.11s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.50s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 1.04s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 1.58s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 2.10s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 5.23s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.52s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 1.07s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 1.64s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 2.22s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 5.44s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.53s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 1.06s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 1.60s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 2.12s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 5.31s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syUQij3fuxRm",
        "outputId": "ab6a8de3-9c8f-48ef-b916-82b908a00201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1) = plt.subplots(1,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"ldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor,'', label=\"ai-dl\") # plot BER vs SNR\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "#ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "#ax2.set_xlabel('$E_b/$N_0$')\n",
        "#ax2.set_ylabel('Decoding Time [s]')\n",
        "#ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "#            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "#            xy=(1, 0.35), xycoords='axes fraction',\n",
        "#            xytext=(-20, 20), textcoords='offset pixels',\n",
        "#            horizontalalignment='right',\n",
        "#            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpIbD_FUw-iL",
        "outputId": "1c2c54f1-5a29-4e5e-e694-8ad262aaf431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "import numpy\n",
        "training_input_message = numpy.random.randint(2**input_message_length, size=(1,NUM_OF_INPUT_MESSAGE))\n",
        "training_input_message_one_hot = numpy.zeros((training_input_message.size, 2**input_message_length))\n",
        "training_input_message_one_hot[numpy.arange(training_input_message.size),training_input_message] = 1\n",
        "print(training_input_message_one_hot)\n",
        "print (training_input_message_one_hot.shape)\n",
        "print (training_input_message.shape)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(1000, 2048)\n",
            "(1, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "farYxiF5xJFj",
        "outputId": "0c7b8cad-230d-4e46-d0d1-041885166f91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using trained model\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_dl_tensor  = numpy.array(())\n",
        "times_per_iter_dl_tensor = numpy.array(())\n",
        "\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  lrate = 0.001\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    input_message_xx = training_input_message_one_hot [i:i+1]\n",
        "    input_message_xx = input_message_xx.astype(\"float32\")\n",
        "    #,input_message_x_label:training_input_message [i]\n",
        "    encoded_message = train_sess.run ([dl_encoder_output], feed_dict={input_message_x:input_message_xx })\n",
        "    encoded_message = encoded_message[0][0]\n",
        "    #encoded_message = numpy.around(encoded_message[0][0]> 0).astype(int)\n",
        "    #print (encoded_message[0][0])\n",
        "    awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message})\n",
        "    #print (awgn_channel_output_message)\n",
        "    decoded_message = train_sess.run ([dl_decoder_only_output], feed_dict={input_channel_x:awgn_channel_output_message})\n",
        "    #print (\"input\", input_message[i])\n",
        "    #decoded_message = numpy.around(decoded_message[0][0]> 0).astype(int)\n",
        "    #rint (\"output\", decoded_message)\n",
        "    #print (\"output\", numpy.argmax(training_input_message_one_hot[i]), numpy.argmax(decoded_message[0][0]))\n",
        "    if (numpy.argmax(training_input_message_one_hot[i]) != numpy.argmax(decoded_message[0][0])):\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_dl_tensor=numpy.append(ber_per_iter_dl_tensor ,ber)\n",
        "  times_per_iter_dl_tensor=numpy.append(times_per_iter_dl_tensor, total_time)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 0.56s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 1.12s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 1.65s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 2.19s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.53\n",
            " -> Total Time: 5.53s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 0.58s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 1.10s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 1.66s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 2.21s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.47\n",
            " -> Total Time: 5.55s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.57s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 1.09s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 1.63s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 2.19s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.42\n",
            " -> Total Time: 5.48s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.56s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.10s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 1.63s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 2.18s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.31\n",
            " -> Total Time: 5.47s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.53s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.11s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 1.67s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.22s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.29\n",
            " -> Total Time: 5.54s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.51s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.04s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.54s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.10s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.21\n",
            " -> Total Time: 5.20s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.51s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 1.05s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.58s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 2.13s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.18\n",
            " -> Total Time: 5.27s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.56s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 1.14s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.71s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 2.26s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.12\n",
            " -> Total Time: 5.67s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.56s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 1.12s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 1.68s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 2.24s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.08\n",
            " -> Total Time: 5.61s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.55s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 1.11s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 1.67s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 2.21s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.07\n",
            " -> Total Time: 5.54s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.53s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 1.10s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 1.69s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 2.24s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 5.55s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.55s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 1.12s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 1.66s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 2.24s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 5.57s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.55s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 1.09s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 1.64s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 2.19s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 5.46s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.54s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 1.09s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 1.66s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 2.20s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 5.49s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.53s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 1.06s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 1.61s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 2.14s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 5.34s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.57s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 1.15s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 1.68s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 2.23s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 5.63s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.53s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 1.08s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 1.64s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 2.20s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 5.46s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.54s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 1.07s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 1.59s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 2.17s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 5.37s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.57s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 1.10s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 1.63s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 2.14s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 5.43s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.53s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 1.06s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 1.60s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 2.18s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 5.38s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hV6_TllxLG_",
        "outputId": "35e02d4a-60e3-42e0-e971-f2757d3ee048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1) = plt.subplots(1,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"ldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor,'', label=\"ai-dl\") # plot BER vs SNR\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "#ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "#ax2.set_xlabel('$E_b/$N_0$')\n",
        "#ax2.set_ylabel('Decoding Time [s]')\n",
        "#ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "#            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "#            xy=(1, 0.35), xycoords='axes fraction',\n",
        "#            xytext=(-20, 20), textcoords='offset pixels',\n",
        "#            horizontalalignment='right',\n",
        "#            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsQhRxkXxML5"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}