{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MainModelOneHotMethod.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamviji/project/blob/master/MainModelOneHotMethod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDSPPMfZ9czi",
        "outputId": "bdaae849-54a1-49f5-e270-0c40ad4711b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        }
      },
      "source": [
        "!rm -rf project\n",
        "!git clone https://github.com/iamviji/project.git\n",
        "!ls\n",
        "!ls project\n",
        "!pip install pyldpc\n",
        "!pip install scikit-commpy\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'project'...\n",
            "remote: Enumerating objects: 86, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
            "remote: Total 86 (delta 28), reused 8 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (86/86), done.\n",
            "project  sample_data\n",
            "MainModel.ipynb\t\t     MainModelOneHotMethodSoftMax.ipynb    util.py\n",
            "MainModelKeras.ipynb\t     MainModelWithSingleBERTraining.ipynb\n",
            "MainModelOneHotMethod.ipynb  README.md\n",
            "Collecting pyldpc\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/aa/fd5495869c7106a638ae71aa497d7d266cae7f2a343d1f6a9d0e3a986e1e/pyldpc-0.7.9.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyldpc) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyldpc) (1.4.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from pyldpc) (0.48.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->pyldpc) (50.3.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->pyldpc) (0.31.0)\n",
            "Building wheels for collected packages: pyldpc\n",
            "  Building wheel for pyldpc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyldpc: filename=pyldpc-0.7.9-cp36-none-any.whl size=14306 sha256=1c933bf817c02ba7027e623d39a9af49f5aca00fa7dc1abdc2b7ac94f1462f6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/7a/10/e94058ba8b0b6d98bf2719226d18d3dd6056525ad7b984c068\n",
            "Successfully built pyldpc\n",
            "Installing collected packages: pyldpc\n",
            "Successfully installed pyldpc-0.7.9\n",
            "Collecting scikit-commpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/b4/f7fa5bc8864e0ddbd3e7a2290b624b92690f53523474024915c33321802d/scikit_commpy-0.5.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->scikit-commpy) (1.15.0)\n",
            "Installing collected packages: scikit-commpy\n",
            "Successfully installed scikit-commpy-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QOuLqpdDgx2",
        "outputId": "4311209d-eb19-4afd-b92a-cdc21ae9b03a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import pyldpc\n",
        "import commpy\n",
        "import numpy \n",
        "import time\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior ()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YClXJbbr0lc7"
      },
      "source": [
        "SNR_BEGIN = 0\n",
        "SNR_END = 10\n",
        "SNR_STEP_SIZE = 0.5\n",
        "CHANEL_SIZE = 18\n",
        "NUM_OF_INPUT_MESSAGE = 1000\n",
        "LDPC_MAX_ITER = 100\n",
        "num_parity_check = 3\n",
        "num_bits_in_parity_check = 6 \n",
        "input_message_length =  0 # Caculated by channel encoder and initialized later"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvUzIMsB43i0"
      },
      "source": [
        "def timer_update(i,current,time_tot,tic_incr=500):\n",
        "    last = current\n",
        "    current = time.time()\n",
        "    t_diff = current-last\n",
        "    print('SNR: {:04.3f} - Iter: {} - Last {} iterations took {:03.2f}s'.format(snr,i+1,tic_incr,t_diff))\n",
        "    return time_tot + t_diff\n",
        "\n",
        "def Snr2Sigma(snr):\n",
        "  sigma = 10 ** (- snr / 20)\n",
        "  return sigma\n",
        "\n",
        "def pyldpc_encode (CodingMatrix, message):\n",
        "  rng = pyldpc.utils.check_random_state(seed=None)\n",
        "  d = pyldpc.utils.binaryproduct(CodingMatrix, message)\n",
        "  encoded_message = (-1) ** d\n",
        "  return encoded_message\n",
        "\n",
        "def pyldpc_decode (ParityCheckMatrix, CodingMatrix, message, snr, maxiter):\n",
        "  decoded_msg = pyldpc.decode(ParityCheckMatrix, message, snr, maxiter)\n",
        "  out_message = pyldpc.get_message(CodingMatrix, decoded_msg)\n",
        "  return out_message\n",
        "\n",
        "awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [CHANEL_SIZE])\n",
        "awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)\n",
        "\n",
        "init = tf.global_variables_initializer ()\n",
        "sess = tf.Session ()\n",
        "sess.run(init)\n",
        "\n",
        "def AWGNChannelOutput (xx, snr , s):\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  awgn_channel_output_message = s.run ([awgn_channel_output], feed_dict={noise_std_dev:sigma, channel_input:xx})\n",
        "  return awgn_channel_output_message"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jMQG-MZ_pXu",
        "outputId": "a3b7a755-a6ab-48c5-f574-1cb10f14e1bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "\n",
        "ParityCheckMatrix, CodingMatrix = pyldpc.make_ldpc(CHANEL_SIZE, num_parity_check, num_bits_in_parity_check, systematic=True, sparse=True)\n",
        "input_message_length = CodingMatrix.shape[1]\n",
        "print (\"input_message_size=\", input_message_length, \"channel_size=\",CHANEL_SIZE)\n",
        "print (\"input_message_size=\", CodingMatrix.shape[1], \"channel_size=\",CodingMatrix.shape[0])\n",
        "input_message = numpy.random.randint(2, size=(NUM_OF_INPUT_MESSAGE,input_message_length))\n",
        "print (input_message)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_message_size= 11 channel_size= 18\n",
            "input_message_size= 11 channel_size= 18\n",
            "[[1 1 0 ... 0 0 0]\n",
            " [1 0 1 ... 1 0 0]\n",
            " [0 0 1 ... 1 0 0]\n",
            " ...\n",
            " [0 1 0 ... 0 0 1]\n",
            " [1 1 1 ... 0 1 0]\n",
            " [0 0 0 ... 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WKg2HU2adgZ"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fL8ptL4aeOY"
      },
      "source": [
        "This section tries to compare BER and Time performance of PYLDPC in following 3 cases\n",
        "1. SNR Noise function provided in encoder function of pyldpc library (pyldpc.encode)\n",
        "2. SNR Noise function provided by commpy library (commpy.channels.awgn) \n",
        "3. SNR Noise function implemented using tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma5zUqFv0TH2",
        "outputId": "c06d593e-c349-4dcf-8388-016346f13e0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using tensor flow based AWGN, to make sure that effect of it is same as AWGN\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_tensor  = numpy.array(())\n",
        "times_per_iter_tensor = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc_encode (CodingMatrix, input_message[i])\n",
        "    sigma = Snr2Sigma (snr)\n",
        "    awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message})[0]\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      #print (\"count=\",abs(decoded_message-input_message[i]).sum())\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_tensor=numpy.append(ber_per_iter_tensor ,ber)\n",
        "  times_per_iter_tensor=numpy.append(times_per_iter_tensor, total_time)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.55s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 3.03s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 4.50s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 6.14s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.61\n",
            " -> Total Time: 15.22s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.23s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.55s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 4.04s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 5.20s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.55\n",
            " -> Total Time: 13.03s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 1.07s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 2.28s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 3.29s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 4.39s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.46\n",
            " -> Total Time: 11.02s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.93s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.84s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.66s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 3.51s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.35\n",
            " -> Total Time: 8.94s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.68s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.34s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 2.11s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.76s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.31\n",
            " -> Total Time: 6.89s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.59s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.21s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.85s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.45s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.26\n",
            " -> Total Time: 6.10s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.43s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.83s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.28s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.68s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.16\n",
            " -> Total Time: 4.23s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.36s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.68s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.09s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.51s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.12\n",
            " -> Total Time: 3.63s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.31s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.70s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 1.06s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.39s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.11\n",
            " -> Total Time: 3.45s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.31s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.60s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.92s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.23s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.07\n",
            " -> Total Time: 3.06s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.56s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.87s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 1.15s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 2.84s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.58s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.88s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 1.15s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 2.88s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.54s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.81s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 1.08s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 2.70s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.78s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 1.04s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 2.61s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.77s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 1.03s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.57s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.77s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 1.03s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.56s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.77s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 1.02s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.57s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.77s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 1.04s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.60s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.79s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 1.08s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.62s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.78s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 1.05s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.63s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8dIFLg76c7O",
        "outputId": "56af3666-10f3-4175-d291-85bc0198c00d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using commpy based AWGN \n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_awgn  = numpy.array(())\n",
        "times_per_iter_awgn = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc_encode (CodingMatrix, input_message[i])\n",
        "    awgn_channel_output_message = commpy.channels.awgn(encoded_message, snr)\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_awgn=numpy.append(ber_per_iter_awgn ,ber)\n",
        "  times_per_iter_awgn=numpy.append(times_per_iter_awgn, total_time)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.33s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 2.67s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 3.95s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 5.22s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.58\n",
            " -> Total Time: 13.18s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.05s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.23s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 3.26s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 4.36s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.55\n",
            " -> Total Time: 10.89s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.92s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 1.86s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 2.90s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 3.80s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.47\n",
            " -> Total Time: 9.47s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.78s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.51s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.19s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 3.00s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.40\n",
            " -> Total Time: 7.49s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.57s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.05s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 1.60s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.15s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.32\n",
            " -> Total Time: 5.38s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.40s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 0.88s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.35s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 1.80s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.26\n",
            " -> Total Time: 4.42s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.35s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.71s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.06s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.44s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.18\n",
            " -> Total Time: 3.56s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.36s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.60s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 0.87s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.22s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.12\n",
            " -> Total Time: 3.05s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 0.94s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.11\n",
            " -> Total Time: 2.47s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.20s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.42s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.62s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 0.83s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.07\n",
            " -> Total Time: 2.06s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.22s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.40s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.58s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.76s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 1.97s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.33s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.51s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.69s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 1.69s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.49s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.65s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 1.62s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.34s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.50s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.66s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.66s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.48s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.65s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.61s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.46s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.62s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.56s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.18s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.35s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.54s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.69s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.76s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.46s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.61s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.54s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.33s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.48s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.64s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.62s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.50s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.65s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.62s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ihPKJJk7Jj9",
        "outputId": "7d85f523-88bc-4b59-a889-af67cbde32d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using tensor flow based AWGN, to make sure that effect of it is same as AWGN\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_pyldpc  = numpy.array(())\n",
        "times_per_iter_pyldpc = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc.encode (CodingMatrix, input_message[i], snr)\n",
        "    awgn_channel_output_message = encoded_message\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_pyldpc=numpy.append(ber_per_iter_pyldpc ,ber)\n",
        "  times_per_iter_pyldpc=numpy.append(times_per_iter_pyldpc, total_time)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.48s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 2.85s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 4.15s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 5.72s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.64\n",
            " -> Total Time: 14.20s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.06s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.08s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 3.25s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 4.26s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.52\n",
            " -> Total Time: 10.66s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.81s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 1.69s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 2.68s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 3.69s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.47\n",
            " -> Total Time: 8.87s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.76s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.44s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.22s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 2.83s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.38\n",
            " -> Total Time: 7.25s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.53s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.12s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 1.68s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.23s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.31\n",
            " -> Total Time: 5.56s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.49s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 0.98s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.40s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 1.82s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.24\n",
            " -> Total Time: 4.68s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.32s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.66s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.02s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.45s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.18\n",
            " -> Total Time: 3.45s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.55s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 0.91s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.18s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.13\n",
            " -> Total Time: 2.91s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.67s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.11\n",
            " -> Total Time: 2.39s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.42s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.61s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 0.79s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.07\n",
            " -> Total Time: 1.99s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.18s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.38s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.57s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.76s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.06\n",
            " -> Total Time: 1.88s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.33s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.50s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.66s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 1.67s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.48s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.64s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 1.60s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.47s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.63s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.58s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.34s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.51s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.70s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.71s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.34s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.50s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.65s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.66s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.47s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.62s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.56s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.30s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.45s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.61s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.51s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.30s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.44s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.59s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.48s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.30s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.46s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.61s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.51s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR4-FOJ-BkAG",
        "outputId": "cb453fac-33d0-45fa-fee7-2fe364380444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "# Compare 3 AWGN(Tensorflow, CommPy, PYLDPC) Simulation on LDPC\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_pyldpc,'', label=\"pyldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"tensor\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_awgn,'', label=\"commpy-awgn\") # plot BER vs SNR\n",
        "\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "ax2.set_xlabel('$E_b/$N_0$')\n",
        "ax2.set_ylabel('Decoding Time [s]')\n",
        "ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "            xy=(1, 0.35), xycoords='axes fraction',\n",
        "            xytext=(-20, 20), textcoords='offset pixels',\n",
        "            horizontalalignment='right',\n",
        "            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggY5VwudaRwR"
      },
      "source": [
        "<B>Conclussion:</B>\n",
        "      It proved that tensorflow behaves similar to AWGN noise channel provided by pyldpc, commpy. But tensor flow based one takes adds little more time delay. This need to be offseted if we are comparing performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5wyTuef3s7u"
      },
      "source": [
        "class GetOutOfLoop( Exception ):\n",
        "    pass"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOeuNfeLCgfb",
        "outputId": "632f33d2-182a-43a5-98ef-bfdbf9f50dd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Define Model \n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "# input_message_length is initialized by ldpc encoder\n",
        "num_hidden_1 = CHANEL_SIZE\n",
        "print (\"input_message_length=\", input_message_length)\n",
        "\n",
        "lr_x = tf.placeholder(dtype=tf.float32,shape=[])\n",
        "#batch_size_x = tf.placeholder(tf.int32,shape=[])\n",
        "input_message_x_label = tf.placeholder(\"int32\", [None], name=\"input_message_x_label\")\n",
        "input_message_x = tf.placeholder(\"float32\", [None, 2**input_message_length], name=\"input_message_x\")\n",
        "awgn_noise_std_dev_x = tf.placeholder(\"float32\", name =\"awgn_noise_std_dev\")\n",
        "input_channel_x = tf.placeholder(\"float32\", [None, CHANEL_SIZE], name=\"input_channel_x\")\n",
        "\n",
        "weights = {\n",
        "  \"encoder_l1\" : tf.Variable (tf.random_uniform([2**input_message_length, num_hidden_1], -1, 1), name=\"encoder_l1_weights\"),\n",
        "  \"decoder_l1\" : tf.Variable (tf.random_uniform([num_hidden_1, 2**input_message_length], -1, 1), name=\"decoder_l1_weights\"),\n",
        "  \"decoder_l2\" : tf.Variable (tf.random_uniform([2**input_message_length, 2**input_message_length], -1, 1), name=\"decoder_l2_weights\"),\n",
        "}\n",
        "\n",
        "biases = {\n",
        "  \"encoder_l1\" : tf.Variable (tf.random_uniform([num_hidden_1], -1,1), name=\"encoder_l1_bias\"),\n",
        "  \"decoder_l1\" : tf.Variable (tf.random_uniform([2**input_message_length], -1,1), name=\"decoder_l1_bias\"),\n",
        "  \"decoder_l2\" : tf.Variable (tf.random_uniform([2**input_message_length], -1,1), name=\"decoder_l2_bias\"),\n",
        "}\n",
        "\n",
        "def dl_encoder (x):\n",
        "  layer_1 = tf.nn.tanh (tf.matmul(x, weights['encoder_l1']) + biases['encoder_l1'])\n",
        "  #layer_2 = tf.round(layer_1)\n",
        "  #layer_1 = BatchNormalization ()(layer_1)\n",
        "  layer_2 =  layer_1 / tf.sqrt(tf.reduce_mean(tf.square(layer_1)))\n",
        "  #layer_2 =  tf.nn.relu(layer_1)\n",
        "  return layer_2\n",
        "\n",
        "def dl_decoder (x):\n",
        "  layer_1 = tf.nn.tanh (tf.matmul(x, weights['decoder_l1']) + biases['decoder_l1'])\n",
        "  layer_2 = (tf.matmul(layer_1, weights['decoder_l2']) + biases['decoder_l2'])\n",
        "  return layer_2\n",
        "\n",
        "def awgn_layer(x):\n",
        "  awgn_noise = tf.random.normal(tf.shape(x), stddev=awgn_noise_std_dev_x,  name=\"awgn_noise\")\n",
        "  awgn_channel_output = tf.add(x, awgn_noise, name =\"x_and_noise\")\n",
        "  return awgn_channel_output\n",
        "\n",
        "\n",
        "dl_encoder_output = dl_encoder(input_message_x)\n",
        "dl_decoder_input = awgn_layer(dl_encoder_output)\n",
        "#awgn_noise = tf.random.normal(tf.shape(dl_encoder_output), stddev=awgn_noise_std_dev,  name=\"awgn_noise\")\n",
        "#dl_decoder_input = tf.add(dl_encoder_output, awgn_noise, name =\"x_and_noise\")\n",
        "dl_decoder_output = dl_decoder (dl_decoder_input)\n",
        "dl_decoder_only_output = dl_decoder(input_channel_x)\n",
        "\n",
        "\n",
        "#loss1 = tf.reduce_mean (-1 * (input_message_x*tf.log(dl_decoder_output) + (1 - input_message_x)*tf.log(1 - dl_decoder_output) ))\n",
        "loss = tf.losses.sparse_softmax_cross_entropy(labels=input_message_x_label,logits=dl_decoder_output)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr_x).minimize (loss)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_message_length= 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkW8oloyodIF",
        "outputId": "b798d68d-308a-4396-b769-6004fbc09ed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "import numpy\n",
        "training_input_message = numpy.random.randint(2**input_message_length, size=(1,NUM_OF_INPUT_MESSAGE*10))\n",
        "training_input_message_one_hot = numpy.zeros((training_input_message.size, 2**input_message_length))\n",
        "training_input_message_one_hot[numpy.arange(training_input_message.size),training_input_message] = 1\n",
        "print(training_input_message_one_hot)\n",
        "print (training_input_message_one_hot.shape)\n",
        "print (training_input_message.shape)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(10000, 2048)\n",
            "(1, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxkLQFuqBT-g",
        "outputId": "d691ec27-a819-4979-8a13-2c4eb755d059",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 500\n",
        "\n",
        "# Training\n",
        "train_init = tf.global_variables_initializer ()\n",
        "train_sess = tf.Session ()\n",
        "\n",
        "epochs = 10\n",
        "outer_ephocs = 1\n",
        "display_step = 2\n",
        "num_of_batches = training_input_message.shape[1] / batch_size\n",
        "print (\"batch_size:\", batch_size, \"num_of_batcches:\", num_of_batches)\n",
        "train_sess.run(train_init)\n",
        "l = 0\n",
        "lrate = 0.1\n",
        "i = 0\n",
        "snr_min = 9.5\n",
        "snr_max = 10.5\n",
        "snr_step_size = 0.5\n",
        "max_iteration = epochs * num_of_batches * (snr_max - snr_min) / snr_step_size\n",
        "print (\"max iteration :\",max_iteration,\"num_of_batches:\", num_of_batches)\n",
        "try:\n",
        "  for oe in range(outer_ephocs):\n",
        "    for snr in (numpy.arange (0, 10, SNR_STEP_SIZE)):\n",
        "    #for snr in (numpy.arange (snr_min, snr_max, SNR_STEP_SIZE)):\n",
        "      sigma = 1.0*Snr2Sigma (snr)\n",
        "      print (\"Training for SNR=\", snr, \" sigma=\", sigma, \"iteratin:\", oe) \n",
        "      for e in range(epochs):\n",
        "        for j in range (int(num_of_batches)):\n",
        "          i = i + 1\n",
        "          x_train_batch_one_hot = training_input_message_one_hot [j*batch_size:(j+1)*batch_size]\n",
        "          x_train_batch_one_hot = x_train_batch_one_hot.astype(\"float32\")\n",
        "          x_train_batch_label = training_input_message.reshape(training_input_message.shape[1]) [j*batch_size:(j+1)*batch_size]        \n",
        "          if (i < 100): \n",
        "            lr = 0.1\n",
        "          elif(i < 200):\n",
        "            lr = 0.01\n",
        "          else:\n",
        "            lr = 0.001 \n",
        "          _, l = train_sess.run ([optimizer, loss], feed_dict={input_message_x:x_train_batch_one_hot, awgn_noise_std_dev_x:sigma, lr_x:lr, input_message_x_label:x_train_batch_label.astype(\"int32\")})\n",
        "          if i % display_step == 0:          \n",
        "            print('Step %i: Minibatch Loss: %f' % (i, l ))\n",
        "          if (l < 0.05): \n",
        "            print (\"Loss=\", l)\n",
        "            raise GetOutOfLoop\n",
        "except GetOutOfLoop:\n",
        "  print(\"Early Stop\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch_size: 500 num_of_batcches: 20.0\n",
            "max iteration : 400.0 num_of_batches: 20.0\n",
            "Training for SNR= 0.0  sigma= 1.0 iteratin: 0\n",
            "Step 2: Minibatch Loss: 127.803581\n",
            "Step 4: Minibatch Loss: 152.912109\n",
            "Step 6: Minibatch Loss: 150.874451\n",
            "Step 8: Minibatch Loss: 145.215759\n",
            "Step 10: Minibatch Loss: 134.836075\n",
            "Step 12: Minibatch Loss: 134.805283\n",
            "Step 14: Minibatch Loss: 141.114059\n",
            "Step 16: Minibatch Loss: 139.244415\n",
            "Step 18: Minibatch Loss: 141.300827\n",
            "Step 20: Minibatch Loss: 131.932053\n",
            "Step 22: Minibatch Loss: 144.825821\n",
            "Step 24: Minibatch Loss: 134.288757\n",
            "Step 26: Minibatch Loss: 133.254807\n",
            "Step 28: Minibatch Loss: 134.119293\n",
            "Step 30: Minibatch Loss: 135.544174\n",
            "Step 32: Minibatch Loss: 133.270615\n",
            "Step 34: Minibatch Loss: 139.800034\n",
            "Step 36: Minibatch Loss: 128.173111\n",
            "Step 38: Minibatch Loss: 130.969528\n",
            "Step 40: Minibatch Loss: 121.429329\n",
            "Step 42: Minibatch Loss: 111.852631\n",
            "Step 44: Minibatch Loss: 120.702156\n",
            "Step 46: Minibatch Loss: 121.426712\n",
            "Step 48: Minibatch Loss: 122.743896\n",
            "Step 50: Minibatch Loss: 123.714973\n",
            "Step 52: Minibatch Loss: 126.206421\n",
            "Step 54: Minibatch Loss: 128.882217\n",
            "Step 56: Minibatch Loss: 122.917946\n",
            "Step 58: Minibatch Loss: 127.992111\n",
            "Step 60: Minibatch Loss: 111.094917\n",
            "Step 62: Minibatch Loss: 119.819611\n",
            "Step 64: Minibatch Loss: 116.460190\n",
            "Step 66: Minibatch Loss: 113.146278\n",
            "Step 68: Minibatch Loss: 113.573799\n",
            "Step 70: Minibatch Loss: 118.854576\n",
            "Step 72: Minibatch Loss: 119.019615\n",
            "Step 74: Minibatch Loss: 112.712219\n",
            "Step 76: Minibatch Loss: 112.052460\n",
            "Step 78: Minibatch Loss: 122.353775\n",
            "Step 80: Minibatch Loss: 116.815804\n",
            "Step 82: Minibatch Loss: 117.270576\n",
            "Step 84: Minibatch Loss: 131.268539\n",
            "Step 86: Minibatch Loss: 118.921959\n",
            "Step 88: Minibatch Loss: 109.304176\n",
            "Step 90: Minibatch Loss: 128.545853\n",
            "Step 92: Minibatch Loss: 134.505753\n",
            "Step 94: Minibatch Loss: 115.300049\n",
            "Step 96: Minibatch Loss: 120.431961\n",
            "Step 98: Minibatch Loss: 120.326462\n",
            "Step 100: Minibatch Loss: 111.386177\n",
            "Step 102: Minibatch Loss: 110.605659\n",
            "Step 104: Minibatch Loss: 104.967697\n",
            "Step 106: Minibatch Loss: 109.944107\n",
            "Step 108: Minibatch Loss: 98.075287\n",
            "Step 110: Minibatch Loss: 93.582504\n",
            "Step 112: Minibatch Loss: 85.856308\n",
            "Step 114: Minibatch Loss: 85.102280\n",
            "Step 116: Minibatch Loss: 81.918266\n",
            "Step 118: Minibatch Loss: 81.455864\n",
            "Step 120: Minibatch Loss: 85.923195\n",
            "Step 122: Minibatch Loss: 84.871452\n",
            "Step 124: Minibatch Loss: 77.444527\n",
            "Step 126: Minibatch Loss: 80.195847\n",
            "Step 128: Minibatch Loss: 75.299622\n",
            "Step 130: Minibatch Loss: 77.210335\n",
            "Step 132: Minibatch Loss: 74.455437\n",
            "Step 134: Minibatch Loss: 67.534370\n",
            "Step 136: Minibatch Loss: 59.623417\n",
            "Step 138: Minibatch Loss: 68.020332\n",
            "Step 140: Minibatch Loss: 71.901955\n",
            "Step 142: Minibatch Loss: 70.102783\n",
            "Step 144: Minibatch Loss: 66.985817\n",
            "Step 146: Minibatch Loss: 67.517273\n",
            "Step 148: Minibatch Loss: 62.414085\n",
            "Step 150: Minibatch Loss: 69.071289\n",
            "Step 152: Minibatch Loss: 60.485821\n",
            "Step 154: Minibatch Loss: 57.997635\n",
            "Step 156: Minibatch Loss: 57.325020\n",
            "Step 158: Minibatch Loss: 63.396927\n",
            "Step 160: Minibatch Loss: 61.283680\n",
            "Step 162: Minibatch Loss: 59.662022\n",
            "Step 164: Minibatch Loss: 53.632500\n",
            "Step 166: Minibatch Loss: 56.845539\n",
            "Step 168: Minibatch Loss: 56.647827\n",
            "Step 170: Minibatch Loss: 52.215294\n",
            "Step 172: Minibatch Loss: 52.940605\n",
            "Step 174: Minibatch Loss: 57.077530\n",
            "Step 176: Minibatch Loss: 56.208199\n",
            "Step 178: Minibatch Loss: 59.310287\n",
            "Step 180: Minibatch Loss: 57.589821\n",
            "Step 182: Minibatch Loss: 57.451363\n",
            "Step 184: Minibatch Loss: 53.032249\n",
            "Step 186: Minibatch Loss: 53.285240\n",
            "Step 188: Minibatch Loss: 53.368656\n",
            "Step 190: Minibatch Loss: 50.896656\n",
            "Step 192: Minibatch Loss: 52.260136\n",
            "Step 194: Minibatch Loss: 53.694672\n",
            "Step 196: Minibatch Loss: 56.545296\n",
            "Step 198: Minibatch Loss: 52.699883\n",
            "Step 200: Minibatch Loss: 55.329937\n",
            "Training for SNR= 0.5  sigma= 0.9440608762859234 iteratin: 0\n",
            "Step 202: Minibatch Loss: 44.376362\n",
            "Step 204: Minibatch Loss: 40.724350\n",
            "Step 206: Minibatch Loss: 41.511967\n",
            "Step 208: Minibatch Loss: 45.447029\n",
            "Step 210: Minibatch Loss: 33.686001\n",
            "Step 212: Minibatch Loss: 35.590446\n",
            "Step 214: Minibatch Loss: 36.986641\n",
            "Step 216: Minibatch Loss: 41.742828\n",
            "Step 218: Minibatch Loss: 39.832577\n",
            "Step 220: Minibatch Loss: 42.387142\n",
            "Step 222: Minibatch Loss: 41.182167\n",
            "Step 224: Minibatch Loss: 38.671017\n",
            "Step 226: Minibatch Loss: 38.454460\n",
            "Step 228: Minibatch Loss: 34.450188\n",
            "Step 230: Minibatch Loss: 38.396694\n",
            "Step 232: Minibatch Loss: 42.435543\n",
            "Step 234: Minibatch Loss: 39.408516\n",
            "Step 236: Minibatch Loss: 41.176163\n",
            "Step 238: Minibatch Loss: 36.141029\n",
            "Step 240: Minibatch Loss: 41.934254\n",
            "Step 242: Minibatch Loss: 37.997684\n",
            "Step 244: Minibatch Loss: 45.143204\n",
            "Step 246: Minibatch Loss: 36.275227\n",
            "Step 248: Minibatch Loss: 37.543434\n",
            "Step 250: Minibatch Loss: 36.068184\n",
            "Step 252: Minibatch Loss: 39.287430\n",
            "Step 254: Minibatch Loss: 43.492584\n",
            "Step 256: Minibatch Loss: 40.373817\n",
            "Step 258: Minibatch Loss: 38.201218\n",
            "Step 260: Minibatch Loss: 39.925034\n",
            "Step 262: Minibatch Loss: 35.949406\n",
            "Step 264: Minibatch Loss: 35.365883\n",
            "Step 266: Minibatch Loss: 45.549294\n",
            "Step 268: Minibatch Loss: 40.762596\n",
            "Step 270: Minibatch Loss: 36.223038\n",
            "Step 272: Minibatch Loss: 40.485218\n",
            "Step 274: Minibatch Loss: 38.020851\n",
            "Step 276: Minibatch Loss: 39.836292\n",
            "Step 278: Minibatch Loss: 38.571915\n",
            "Step 280: Minibatch Loss: 41.769768\n",
            "Step 282: Minibatch Loss: 35.584507\n",
            "Step 284: Minibatch Loss: 36.684761\n",
            "Step 286: Minibatch Loss: 43.418716\n",
            "Step 288: Minibatch Loss: 37.834591\n",
            "Step 290: Minibatch Loss: 39.967976\n",
            "Step 292: Minibatch Loss: 38.224045\n",
            "Step 294: Minibatch Loss: 37.548306\n",
            "Step 296: Minibatch Loss: 38.607918\n",
            "Step 298: Minibatch Loss: 41.110161\n",
            "Step 300: Minibatch Loss: 36.832565\n",
            "Step 302: Minibatch Loss: 38.307957\n",
            "Step 304: Minibatch Loss: 40.541504\n",
            "Step 306: Minibatch Loss: 35.907188\n",
            "Step 308: Minibatch Loss: 39.530548\n",
            "Step 310: Minibatch Loss: 36.042980\n",
            "Step 312: Minibatch Loss: 40.940754\n",
            "Step 314: Minibatch Loss: 41.134731\n",
            "Step 316: Minibatch Loss: 35.106319\n",
            "Step 318: Minibatch Loss: 33.160278\n",
            "Step 320: Minibatch Loss: 36.763641\n",
            "Step 322: Minibatch Loss: 38.858398\n",
            "Step 324: Minibatch Loss: 34.545963\n",
            "Step 326: Minibatch Loss: 32.953583\n",
            "Step 328: Minibatch Loss: 36.002968\n",
            "Step 330: Minibatch Loss: 38.713745\n",
            "Step 332: Minibatch Loss: 35.767048\n",
            "Step 334: Minibatch Loss: 40.523083\n",
            "Step 336: Minibatch Loss: 34.303692\n",
            "Step 338: Minibatch Loss: 37.939964\n",
            "Step 340: Minibatch Loss: 37.336864\n",
            "Step 342: Minibatch Loss: 39.962765\n",
            "Step 344: Minibatch Loss: 37.141479\n",
            "Step 346: Minibatch Loss: 34.990517\n",
            "Step 348: Minibatch Loss: 38.842499\n",
            "Step 350: Minibatch Loss: 37.153366\n",
            "Step 352: Minibatch Loss: 43.399227\n",
            "Step 354: Minibatch Loss: 34.412289\n",
            "Step 356: Minibatch Loss: 41.846100\n",
            "Step 358: Minibatch Loss: 36.136131\n",
            "Step 360: Minibatch Loss: 35.032604\n",
            "Step 362: Minibatch Loss: 32.594959\n",
            "Step 364: Minibatch Loss: 38.734955\n",
            "Step 366: Minibatch Loss: 39.380215\n",
            "Step 368: Minibatch Loss: 34.091709\n",
            "Step 370: Minibatch Loss: 34.483574\n",
            "Step 372: Minibatch Loss: 37.074242\n",
            "Step 374: Minibatch Loss: 40.460361\n",
            "Step 376: Minibatch Loss: 36.815735\n",
            "Step 378: Minibatch Loss: 31.881685\n",
            "Step 380: Minibatch Loss: 34.082630\n",
            "Step 382: Minibatch Loss: 40.117695\n",
            "Step 384: Minibatch Loss: 35.943218\n",
            "Step 386: Minibatch Loss: 34.899597\n",
            "Step 388: Minibatch Loss: 38.255054\n",
            "Step 390: Minibatch Loss: 35.897854\n",
            "Step 392: Minibatch Loss: 37.841297\n",
            "Step 394: Minibatch Loss: 31.909800\n",
            "Step 396: Minibatch Loss: 32.965130\n",
            "Step 398: Minibatch Loss: 38.404785\n",
            "Step 400: Minibatch Loss: 39.415146\n",
            "Training for SNR= 1.0  sigma= 0.8912509381337456 iteratin: 0\n",
            "Step 402: Minibatch Loss: 28.509983\n",
            "Step 404: Minibatch Loss: 28.319576\n",
            "Step 406: Minibatch Loss: 28.485390\n",
            "Step 408: Minibatch Loss: 27.417469\n",
            "Step 410: Minibatch Loss: 28.484699\n",
            "Step 412: Minibatch Loss: 30.090126\n",
            "Step 414: Minibatch Loss: 29.042250\n",
            "Step 416: Minibatch Loss: 29.933281\n",
            "Step 418: Minibatch Loss: 28.068476\n",
            "Step 420: Minibatch Loss: 30.682138\n",
            "Step 422: Minibatch Loss: 29.858583\n",
            "Step 424: Minibatch Loss: 28.903435\n",
            "Step 426: Minibatch Loss: 30.514198\n",
            "Step 428: Minibatch Loss: 27.047695\n",
            "Step 430: Minibatch Loss: 24.622057\n",
            "Step 432: Minibatch Loss: 27.472000\n",
            "Step 434: Minibatch Loss: 27.758108\n",
            "Step 436: Minibatch Loss: 27.397261\n",
            "Step 438: Minibatch Loss: 30.854034\n",
            "Step 440: Minibatch Loss: 28.871452\n",
            "Step 442: Minibatch Loss: 31.549103\n",
            "Step 444: Minibatch Loss: 30.338621\n",
            "Step 446: Minibatch Loss: 22.733135\n",
            "Step 448: Minibatch Loss: 27.679993\n",
            "Step 450: Minibatch Loss: 29.441353\n",
            "Step 452: Minibatch Loss: 30.483015\n",
            "Step 454: Minibatch Loss: 26.536375\n",
            "Step 456: Minibatch Loss: 25.124451\n",
            "Step 458: Minibatch Loss: 29.883526\n",
            "Step 460: Minibatch Loss: 26.167917\n",
            "Step 462: Minibatch Loss: 30.211884\n",
            "Step 464: Minibatch Loss: 28.311659\n",
            "Step 466: Minibatch Loss: 28.045662\n",
            "Step 468: Minibatch Loss: 29.214123\n",
            "Step 470: Minibatch Loss: 29.772261\n",
            "Step 472: Minibatch Loss: 29.854475\n",
            "Step 474: Minibatch Loss: 32.445004\n",
            "Step 476: Minibatch Loss: 26.569115\n",
            "Step 478: Minibatch Loss: 27.494408\n",
            "Step 480: Minibatch Loss: 30.199379\n",
            "Step 482: Minibatch Loss: 25.518908\n",
            "Step 484: Minibatch Loss: 25.557661\n",
            "Step 486: Minibatch Loss: 28.650681\n",
            "Step 488: Minibatch Loss: 25.634720\n",
            "Step 490: Minibatch Loss: 28.245235\n",
            "Step 492: Minibatch Loss: 31.825470\n",
            "Step 494: Minibatch Loss: 26.362415\n",
            "Step 496: Minibatch Loss: 27.212275\n",
            "Step 498: Minibatch Loss: 29.484209\n",
            "Step 500: Minibatch Loss: 30.349892\n",
            "Step 502: Minibatch Loss: 25.165936\n",
            "Step 504: Minibatch Loss: 27.577229\n",
            "Step 506: Minibatch Loss: 25.864056\n",
            "Step 508: Minibatch Loss: 28.038355\n",
            "Step 510: Minibatch Loss: 28.082504\n",
            "Step 512: Minibatch Loss: 29.030207\n",
            "Step 514: Minibatch Loss: 26.674730\n",
            "Step 516: Minibatch Loss: 24.726948\n",
            "Step 518: Minibatch Loss: 26.640127\n",
            "Step 520: Minibatch Loss: 28.044731\n",
            "Step 522: Minibatch Loss: 25.440676\n",
            "Step 524: Minibatch Loss: 28.452494\n",
            "Step 526: Minibatch Loss: 28.636339\n",
            "Step 528: Minibatch Loss: 26.265808\n",
            "Step 530: Minibatch Loss: 27.642317\n",
            "Step 532: Minibatch Loss: 27.050266\n",
            "Step 534: Minibatch Loss: 27.758072\n",
            "Step 536: Minibatch Loss: 24.014723\n",
            "Step 538: Minibatch Loss: 24.398325\n",
            "Step 540: Minibatch Loss: 30.298258\n",
            "Step 542: Minibatch Loss: 26.263422\n",
            "Step 544: Minibatch Loss: 29.806553\n",
            "Step 546: Minibatch Loss: 26.615707\n",
            "Step 548: Minibatch Loss: 26.445229\n",
            "Step 550: Minibatch Loss: 29.515524\n",
            "Step 552: Minibatch Loss: 29.885838\n",
            "Step 554: Minibatch Loss: 29.344490\n",
            "Step 556: Minibatch Loss: 27.657185\n",
            "Step 558: Minibatch Loss: 27.658308\n",
            "Step 560: Minibatch Loss: 25.634277\n",
            "Step 562: Minibatch Loss: 26.886600\n",
            "Step 564: Minibatch Loss: 24.905535\n",
            "Step 566: Minibatch Loss: 24.817745\n",
            "Step 568: Minibatch Loss: 28.475315\n",
            "Step 570: Minibatch Loss: 27.756788\n",
            "Step 572: Minibatch Loss: 26.980402\n",
            "Step 574: Minibatch Loss: 25.406136\n",
            "Step 576: Minibatch Loss: 24.547394\n",
            "Step 578: Minibatch Loss: 24.541878\n",
            "Step 580: Minibatch Loss: 25.441402\n",
            "Step 582: Minibatch Loss: 29.019310\n",
            "Step 584: Minibatch Loss: 29.828907\n",
            "Step 586: Minibatch Loss: 28.337976\n",
            "Step 588: Minibatch Loss: 27.077887\n",
            "Step 590: Minibatch Loss: 27.151125\n",
            "Step 592: Minibatch Loss: 27.925606\n",
            "Step 594: Minibatch Loss: 27.296640\n",
            "Step 596: Minibatch Loss: 22.950609\n",
            "Step 598: Minibatch Loss: 24.945683\n",
            "Step 600: Minibatch Loss: 23.957375\n",
            "Training for SNR= 1.5  sigma= 0.8413951416451951 iteratin: 0\n",
            "Step 602: Minibatch Loss: 19.956892\n",
            "Step 604: Minibatch Loss: 20.948078\n",
            "Step 606: Minibatch Loss: 18.732897\n",
            "Step 608: Minibatch Loss: 18.625614\n",
            "Step 610: Minibatch Loss: 21.511425\n",
            "Step 612: Minibatch Loss: 22.084145\n",
            "Step 614: Minibatch Loss: 20.614557\n",
            "Step 616: Minibatch Loss: 23.521372\n",
            "Step 618: Minibatch Loss: 23.228865\n",
            "Step 620: Minibatch Loss: 19.213818\n",
            "Step 622: Minibatch Loss: 19.539886\n",
            "Step 624: Minibatch Loss: 18.952866\n",
            "Step 626: Minibatch Loss: 18.347212\n",
            "Step 628: Minibatch Loss: 21.559153\n",
            "Step 630: Minibatch Loss: 21.420570\n",
            "Step 632: Minibatch Loss: 19.502800\n",
            "Step 634: Minibatch Loss: 23.020714\n",
            "Step 636: Minibatch Loss: 18.266182\n",
            "Step 638: Minibatch Loss: 21.729605\n",
            "Step 640: Minibatch Loss: 20.834097\n",
            "Step 642: Minibatch Loss: 18.849110\n",
            "Step 644: Minibatch Loss: 23.051260\n",
            "Step 646: Minibatch Loss: 19.086845\n",
            "Step 648: Minibatch Loss: 17.836908\n",
            "Step 650: Minibatch Loss: 19.386835\n",
            "Step 652: Minibatch Loss: 15.845496\n",
            "Step 654: Minibatch Loss: 20.664690\n",
            "Step 656: Minibatch Loss: 20.571283\n",
            "Step 658: Minibatch Loss: 20.843512\n",
            "Step 660: Minibatch Loss: 20.444216\n",
            "Step 662: Minibatch Loss: 16.341578\n",
            "Step 664: Minibatch Loss: 18.676907\n",
            "Step 666: Minibatch Loss: 19.752243\n",
            "Step 668: Minibatch Loss: 17.762209\n",
            "Step 670: Minibatch Loss: 20.988136\n",
            "Step 672: Minibatch Loss: 18.955061\n",
            "Step 674: Minibatch Loss: 20.710800\n",
            "Step 676: Minibatch Loss: 18.854216\n",
            "Step 678: Minibatch Loss: 20.631121\n",
            "Step 680: Minibatch Loss: 21.306877\n",
            "Step 682: Minibatch Loss: 15.742764\n",
            "Step 684: Minibatch Loss: 20.224882\n",
            "Step 686: Minibatch Loss: 21.705536\n",
            "Step 688: Minibatch Loss: 19.977514\n",
            "Step 690: Minibatch Loss: 20.154396\n",
            "Step 692: Minibatch Loss: 21.771385\n",
            "Step 694: Minibatch Loss: 19.988716\n",
            "Step 696: Minibatch Loss: 18.984409\n",
            "Step 698: Minibatch Loss: 17.279106\n",
            "Step 700: Minibatch Loss: 19.185982\n",
            "Step 702: Minibatch Loss: 19.361328\n",
            "Step 704: Minibatch Loss: 17.940020\n",
            "Step 706: Minibatch Loss: 18.484079\n",
            "Step 708: Minibatch Loss: 18.899773\n",
            "Step 710: Minibatch Loss: 20.594427\n",
            "Step 712: Minibatch Loss: 20.644341\n",
            "Step 714: Minibatch Loss: 17.369659\n",
            "Step 716: Minibatch Loss: 18.484758\n",
            "Step 718: Minibatch Loss: 22.102530\n",
            "Step 720: Minibatch Loss: 20.120153\n",
            "Step 722: Minibatch Loss: 19.494972\n",
            "Step 724: Minibatch Loss: 19.265648\n",
            "Step 726: Minibatch Loss: 22.718700\n",
            "Step 728: Minibatch Loss: 18.718008\n",
            "Step 730: Minibatch Loss: 15.563560\n",
            "Step 732: Minibatch Loss: 20.796844\n",
            "Step 734: Minibatch Loss: 20.835007\n",
            "Step 736: Minibatch Loss: 18.535507\n",
            "Step 738: Minibatch Loss: 19.207941\n",
            "Step 740: Minibatch Loss: 19.421906\n",
            "Step 742: Minibatch Loss: 20.183542\n",
            "Step 744: Minibatch Loss: 20.554831\n",
            "Step 746: Minibatch Loss: 21.691706\n",
            "Step 748: Minibatch Loss: 17.609585\n",
            "Step 750: Minibatch Loss: 16.259584\n",
            "Step 752: Minibatch Loss: 19.547596\n",
            "Step 754: Minibatch Loss: 17.707253\n",
            "Step 756: Minibatch Loss: 19.475534\n",
            "Step 758: Minibatch Loss: 20.433420\n",
            "Step 760: Minibatch Loss: 16.066412\n",
            "Step 762: Minibatch Loss: 16.738945\n",
            "Step 764: Minibatch Loss: 16.221550\n",
            "Step 766: Minibatch Loss: 19.466864\n",
            "Step 768: Minibatch Loss: 22.696512\n",
            "Step 770: Minibatch Loss: 19.031977\n",
            "Step 772: Minibatch Loss: 17.602364\n",
            "Step 774: Minibatch Loss: 21.601206\n",
            "Step 776: Minibatch Loss: 18.562555\n",
            "Step 778: Minibatch Loss: 23.474535\n",
            "Step 780: Minibatch Loss: 17.790552\n",
            "Step 782: Minibatch Loss: 19.536703\n",
            "Step 784: Minibatch Loss: 17.871634\n",
            "Step 786: Minibatch Loss: 16.146233\n",
            "Step 788: Minibatch Loss: 15.411615\n",
            "Step 790: Minibatch Loss: 18.278685\n",
            "Step 792: Minibatch Loss: 19.384460\n",
            "Step 794: Minibatch Loss: 21.178097\n",
            "Step 796: Minibatch Loss: 19.939360\n",
            "Step 798: Minibatch Loss: 16.651283\n",
            "Step 800: Minibatch Loss: 20.410576\n",
            "Training for SNR= 2.0  sigma= 0.7943282347242815 iteratin: 0\n",
            "Step 802: Minibatch Loss: 16.033669\n",
            "Step 804: Minibatch Loss: 15.440797\n",
            "Step 806: Minibatch Loss: 14.664072\n",
            "Step 808: Minibatch Loss: 12.574460\n",
            "Step 810: Minibatch Loss: 15.252677\n",
            "Step 812: Minibatch Loss: 16.735577\n",
            "Step 814: Minibatch Loss: 12.818278\n",
            "Step 816: Minibatch Loss: 15.061657\n",
            "Step 818: Minibatch Loss: 15.094305\n",
            "Step 820: Minibatch Loss: 13.470240\n",
            "Step 822: Minibatch Loss: 13.656723\n",
            "Step 824: Minibatch Loss: 13.806772\n",
            "Step 826: Minibatch Loss: 14.031531\n",
            "Step 828: Minibatch Loss: 14.134514\n",
            "Step 830: Minibatch Loss: 12.808427\n",
            "Step 832: Minibatch Loss: 15.330827\n",
            "Step 834: Minibatch Loss: 12.685910\n",
            "Step 836: Minibatch Loss: 11.443621\n",
            "Step 838: Minibatch Loss: 11.221274\n",
            "Step 840: Minibatch Loss: 12.529115\n",
            "Step 842: Minibatch Loss: 14.467874\n",
            "Step 844: Minibatch Loss: 14.406713\n",
            "Step 846: Minibatch Loss: 13.624137\n",
            "Step 848: Minibatch Loss: 13.068595\n",
            "Step 850: Minibatch Loss: 13.983320\n",
            "Step 852: Minibatch Loss: 12.927193\n",
            "Step 854: Minibatch Loss: 12.417113\n",
            "Step 856: Minibatch Loss: 11.194253\n",
            "Step 858: Minibatch Loss: 11.958412\n",
            "Step 860: Minibatch Loss: 15.035862\n",
            "Step 862: Minibatch Loss: 13.458274\n",
            "Step 864: Minibatch Loss: 14.586282\n",
            "Step 866: Minibatch Loss: 12.594053\n",
            "Step 868: Minibatch Loss: 15.455242\n",
            "Step 870: Minibatch Loss: 13.620914\n",
            "Step 872: Minibatch Loss: 14.105824\n",
            "Step 874: Minibatch Loss: 13.058613\n",
            "Step 876: Minibatch Loss: 13.936400\n",
            "Step 878: Minibatch Loss: 12.853262\n",
            "Step 880: Minibatch Loss: 9.780458\n",
            "Step 882: Minibatch Loss: 16.426908\n",
            "Step 884: Minibatch Loss: 14.674363\n",
            "Step 886: Minibatch Loss: 13.077766\n",
            "Step 888: Minibatch Loss: 13.182612\n",
            "Step 890: Minibatch Loss: 12.293702\n",
            "Step 892: Minibatch Loss: 14.383752\n",
            "Step 894: Minibatch Loss: 14.010723\n",
            "Step 896: Minibatch Loss: 13.879520\n",
            "Step 898: Minibatch Loss: 13.982683\n",
            "Step 900: Minibatch Loss: 13.623528\n",
            "Step 902: Minibatch Loss: 14.900717\n",
            "Step 904: Minibatch Loss: 12.604237\n",
            "Step 906: Minibatch Loss: 12.855577\n",
            "Step 908: Minibatch Loss: 13.533530\n",
            "Step 910: Minibatch Loss: 13.198505\n",
            "Step 912: Minibatch Loss: 11.146155\n",
            "Step 914: Minibatch Loss: 13.383787\n",
            "Step 916: Minibatch Loss: 14.048264\n",
            "Step 918: Minibatch Loss: 12.792086\n",
            "Step 920: Minibatch Loss: 16.668736\n",
            "Step 922: Minibatch Loss: 16.380960\n",
            "Step 924: Minibatch Loss: 12.196212\n",
            "Step 926: Minibatch Loss: 15.397163\n",
            "Step 928: Minibatch Loss: 12.491401\n",
            "Step 930: Minibatch Loss: 11.358747\n",
            "Step 932: Minibatch Loss: 12.578130\n",
            "Step 934: Minibatch Loss: 14.942121\n",
            "Step 936: Minibatch Loss: 14.341536\n",
            "Step 938: Minibatch Loss: 11.940522\n",
            "Step 940: Minibatch Loss: 12.697118\n",
            "Step 942: Minibatch Loss: 12.289555\n",
            "Step 944: Minibatch Loss: 12.394498\n",
            "Step 946: Minibatch Loss: 13.830745\n",
            "Step 948: Minibatch Loss: 13.650296\n",
            "Step 950: Minibatch Loss: 11.315722\n",
            "Step 952: Minibatch Loss: 11.941043\n",
            "Step 954: Minibatch Loss: 13.157840\n",
            "Step 956: Minibatch Loss: 13.034408\n",
            "Step 958: Minibatch Loss: 15.759993\n",
            "Step 960: Minibatch Loss: 13.475844\n",
            "Step 962: Minibatch Loss: 16.226191\n",
            "Step 964: Minibatch Loss: 12.415318\n",
            "Step 966: Minibatch Loss: 14.708740\n",
            "Step 968: Minibatch Loss: 14.358010\n",
            "Step 970: Minibatch Loss: 10.873948\n",
            "Step 972: Minibatch Loss: 13.226996\n",
            "Step 974: Minibatch Loss: 11.506905\n",
            "Step 976: Minibatch Loss: 12.982702\n",
            "Step 978: Minibatch Loss: 13.143945\n",
            "Step 980: Minibatch Loss: 13.364577\n",
            "Step 982: Minibatch Loss: 15.039401\n",
            "Step 984: Minibatch Loss: 12.364129\n",
            "Step 986: Minibatch Loss: 13.175457\n",
            "Step 988: Minibatch Loss: 14.678021\n",
            "Step 990: Minibatch Loss: 14.853682\n",
            "Step 992: Minibatch Loss: 14.058123\n",
            "Step 994: Minibatch Loss: 11.027390\n",
            "Step 996: Minibatch Loss: 12.280916\n",
            "Step 998: Minibatch Loss: 11.570657\n",
            "Step 1000: Minibatch Loss: 15.840121\n",
            "Training for SNR= 2.5  sigma= 0.7498942093324559 iteratin: 0\n",
            "Step 1002: Minibatch Loss: 10.829546\n",
            "Step 1004: Minibatch Loss: 8.485227\n",
            "Step 1006: Minibatch Loss: 10.085777\n",
            "Step 1008: Minibatch Loss: 8.399647\n",
            "Step 1010: Minibatch Loss: 9.499749\n",
            "Step 1012: Minibatch Loss: 8.041839\n",
            "Step 1014: Minibatch Loss: 9.447034\n",
            "Step 1016: Minibatch Loss: 10.633214\n",
            "Step 1018: Minibatch Loss: 10.556611\n",
            "Step 1020: Minibatch Loss: 9.146118\n",
            "Step 1022: Minibatch Loss: 9.733566\n",
            "Step 1024: Minibatch Loss: 8.741636\n",
            "Step 1026: Minibatch Loss: 9.914480\n",
            "Step 1028: Minibatch Loss: 7.669547\n",
            "Step 1030: Minibatch Loss: 8.041316\n",
            "Step 1032: Minibatch Loss: 9.548016\n",
            "Step 1034: Minibatch Loss: 9.665279\n",
            "Step 1036: Minibatch Loss: 7.620151\n",
            "Step 1038: Minibatch Loss: 10.604291\n",
            "Step 1040: Minibatch Loss: 7.287623\n",
            "Step 1042: Minibatch Loss: 7.622252\n",
            "Step 1044: Minibatch Loss: 8.891334\n",
            "Step 1046: Minibatch Loss: 8.519783\n",
            "Step 1048: Minibatch Loss: 8.252342\n",
            "Step 1050: Minibatch Loss: 9.758168\n",
            "Step 1052: Minibatch Loss: 9.497439\n",
            "Step 1054: Minibatch Loss: 8.485297\n",
            "Step 1056: Minibatch Loss: 8.234979\n",
            "Step 1058: Minibatch Loss: 10.233098\n",
            "Step 1060: Minibatch Loss: 8.968575\n",
            "Step 1062: Minibatch Loss: 9.465481\n",
            "Step 1064: Minibatch Loss: 6.665624\n",
            "Step 1066: Minibatch Loss: 8.643268\n",
            "Step 1068: Minibatch Loss: 9.699471\n",
            "Step 1070: Minibatch Loss: 8.275995\n",
            "Step 1072: Minibatch Loss: 8.429358\n",
            "Step 1074: Minibatch Loss: 8.245284\n",
            "Step 1076: Minibatch Loss: 8.341318\n",
            "Step 1078: Minibatch Loss: 9.414564\n",
            "Step 1080: Minibatch Loss: 10.811652\n",
            "Step 1082: Minibatch Loss: 9.751489\n",
            "Step 1084: Minibatch Loss: 8.371619\n",
            "Step 1086: Minibatch Loss: 7.831036\n",
            "Step 1088: Minibatch Loss: 9.758801\n",
            "Step 1090: Minibatch Loss: 8.881143\n",
            "Step 1092: Minibatch Loss: 9.682624\n",
            "Step 1094: Minibatch Loss: 8.368738\n",
            "Step 1096: Minibatch Loss: 7.474802\n",
            "Step 1098: Minibatch Loss: 7.001020\n",
            "Step 1100: Minibatch Loss: 9.482936\n",
            "Step 1102: Minibatch Loss: 8.316486\n",
            "Step 1104: Minibatch Loss: 9.631086\n",
            "Step 1106: Minibatch Loss: 9.824947\n",
            "Step 1108: Minibatch Loss: 7.636298\n",
            "Step 1110: Minibatch Loss: 8.347967\n",
            "Step 1112: Minibatch Loss: 8.782570\n",
            "Step 1114: Minibatch Loss: 8.264385\n",
            "Step 1116: Minibatch Loss: 6.999794\n",
            "Step 1118: Minibatch Loss: 9.839076\n",
            "Step 1120: Minibatch Loss: 9.260460\n",
            "Step 1122: Minibatch Loss: 10.001548\n",
            "Step 1124: Minibatch Loss: 10.364105\n",
            "Step 1126: Minibatch Loss: 7.857141\n",
            "Step 1128: Minibatch Loss: 8.147914\n",
            "Step 1130: Minibatch Loss: 7.702509\n",
            "Step 1132: Minibatch Loss: 9.185493\n",
            "Step 1134: Minibatch Loss: 10.227383\n",
            "Step 1136: Minibatch Loss: 8.628844\n",
            "Step 1138: Minibatch Loss: 9.634496\n",
            "Step 1140: Minibatch Loss: 9.033027\n",
            "Step 1142: Minibatch Loss: 10.865300\n",
            "Step 1144: Minibatch Loss: 8.881541\n",
            "Step 1146: Minibatch Loss: 8.814712\n",
            "Step 1148: Minibatch Loss: 8.166471\n",
            "Step 1150: Minibatch Loss: 8.412547\n",
            "Step 1152: Minibatch Loss: 11.320279\n",
            "Step 1154: Minibatch Loss: 10.350776\n",
            "Step 1156: Minibatch Loss: 8.627834\n",
            "Step 1158: Minibatch Loss: 7.303164\n",
            "Step 1160: Minibatch Loss: 8.926944\n",
            "Step 1162: Minibatch Loss: 9.729048\n",
            "Step 1164: Minibatch Loss: 9.684476\n",
            "Step 1166: Minibatch Loss: 6.446909\n",
            "Step 1168: Minibatch Loss: 7.322157\n",
            "Step 1170: Minibatch Loss: 8.422935\n",
            "Step 1172: Minibatch Loss: 8.824563\n",
            "Step 1174: Minibatch Loss: 11.076164\n",
            "Step 1176: Minibatch Loss: 9.327137\n",
            "Step 1178: Minibatch Loss: 8.144090\n",
            "Step 1180: Minibatch Loss: 9.493699\n",
            "Step 1182: Minibatch Loss: 8.508474\n",
            "Step 1184: Minibatch Loss: 7.858661\n",
            "Step 1186: Minibatch Loss: 7.930868\n",
            "Step 1188: Minibatch Loss: 8.158285\n",
            "Step 1190: Minibatch Loss: 7.816940\n",
            "Step 1192: Minibatch Loss: 9.220834\n",
            "Step 1194: Minibatch Loss: 6.865024\n",
            "Step 1196: Minibatch Loss: 7.720381\n",
            "Step 1198: Minibatch Loss: 6.498546\n",
            "Step 1200: Minibatch Loss: 10.226167\n",
            "Training for SNR= 3.0  sigma= 0.7079457843841379 iteratin: 0\n",
            "Step 1202: Minibatch Loss: 6.291688\n",
            "Step 1204: Minibatch Loss: 6.567197\n",
            "Step 1206: Minibatch Loss: 5.799633\n",
            "Step 1208: Minibatch Loss: 5.483346\n",
            "Step 1210: Minibatch Loss: 6.615805\n",
            "Step 1212: Minibatch Loss: 5.790546\n",
            "Step 1214: Minibatch Loss: 5.214431\n",
            "Step 1216: Minibatch Loss: 4.753921\n",
            "Step 1218: Minibatch Loss: 6.481413\n",
            "Step 1220: Minibatch Loss: 5.576954\n",
            "Step 1222: Minibatch Loss: 5.828443\n",
            "Step 1224: Minibatch Loss: 6.847693\n",
            "Step 1226: Minibatch Loss: 5.537903\n",
            "Step 1228: Minibatch Loss: 7.082736\n",
            "Step 1230: Minibatch Loss: 5.303706\n",
            "Step 1232: Minibatch Loss: 7.016991\n",
            "Step 1234: Minibatch Loss: 6.640726\n",
            "Step 1236: Minibatch Loss: 5.683715\n",
            "Step 1238: Minibatch Loss: 8.239629\n",
            "Step 1240: Minibatch Loss: 6.991997\n",
            "Step 1242: Minibatch Loss: 5.691743\n",
            "Step 1244: Minibatch Loss: 4.368327\n",
            "Step 1246: Minibatch Loss: 4.616315\n",
            "Step 1248: Minibatch Loss: 7.040026\n",
            "Step 1250: Minibatch Loss: 6.211804\n",
            "Step 1252: Minibatch Loss: 7.851887\n",
            "Step 1254: Minibatch Loss: 4.809725\n",
            "Step 1256: Minibatch Loss: 5.637635\n",
            "Step 1258: Minibatch Loss: 6.709356\n",
            "Step 1260: Minibatch Loss: 5.100083\n",
            "Step 1262: Minibatch Loss: 4.895670\n",
            "Step 1264: Minibatch Loss: 5.832719\n",
            "Step 1266: Minibatch Loss: 5.374816\n",
            "Step 1268: Minibatch Loss: 5.052372\n",
            "Step 1270: Minibatch Loss: 7.181562\n",
            "Step 1272: Minibatch Loss: 7.529422\n",
            "Step 1274: Minibatch Loss: 5.504449\n",
            "Step 1276: Minibatch Loss: 5.435878\n",
            "Step 1278: Minibatch Loss: 5.515934\n",
            "Step 1280: Minibatch Loss: 6.123078\n",
            "Step 1282: Minibatch Loss: 7.315299\n",
            "Step 1284: Minibatch Loss: 7.452398\n",
            "Step 1286: Minibatch Loss: 5.849506\n",
            "Step 1288: Minibatch Loss: 5.330705\n",
            "Step 1290: Minibatch Loss: 6.359203\n",
            "Step 1292: Minibatch Loss: 7.129164\n",
            "Step 1294: Minibatch Loss: 4.929688\n",
            "Step 1296: Minibatch Loss: 5.918501\n",
            "Step 1298: Minibatch Loss: 5.670351\n",
            "Step 1300: Minibatch Loss: 4.415591\n",
            "Step 1302: Minibatch Loss: 4.675022\n",
            "Step 1304: Minibatch Loss: 5.971706\n",
            "Step 1306: Minibatch Loss: 5.354033\n",
            "Step 1308: Minibatch Loss: 6.576815\n",
            "Step 1310: Minibatch Loss: 6.389998\n",
            "Step 1312: Minibatch Loss: 6.914557\n",
            "Step 1314: Minibatch Loss: 5.914958\n",
            "Step 1316: Minibatch Loss: 6.911458\n",
            "Step 1318: Minibatch Loss: 6.347166\n",
            "Step 1320: Minibatch Loss: 4.648320\n",
            "Step 1322: Minibatch Loss: 5.132124\n",
            "Step 1324: Minibatch Loss: 6.028773\n",
            "Step 1326: Minibatch Loss: 5.956919\n",
            "Step 1328: Minibatch Loss: 5.712880\n",
            "Step 1330: Minibatch Loss: 6.695638\n",
            "Step 1332: Minibatch Loss: 5.813101\n",
            "Step 1334: Minibatch Loss: 6.839685\n",
            "Step 1336: Minibatch Loss: 4.888188\n",
            "Step 1338: Minibatch Loss: 5.355917\n",
            "Step 1340: Minibatch Loss: 6.951247\n",
            "Step 1342: Minibatch Loss: 5.654141\n",
            "Step 1344: Minibatch Loss: 4.989621\n",
            "Step 1346: Minibatch Loss: 5.504195\n",
            "Step 1348: Minibatch Loss: 6.159601\n",
            "Step 1350: Minibatch Loss: 5.273143\n",
            "Step 1352: Minibatch Loss: 6.292064\n",
            "Step 1354: Minibatch Loss: 5.913360\n",
            "Step 1356: Minibatch Loss: 5.679984\n",
            "Step 1358: Minibatch Loss: 8.548924\n",
            "Step 1360: Minibatch Loss: 8.220155\n",
            "Step 1362: Minibatch Loss: 4.153033\n",
            "Step 1364: Minibatch Loss: 5.275524\n",
            "Step 1366: Minibatch Loss: 5.125966\n",
            "Step 1368: Minibatch Loss: 5.940755\n",
            "Step 1370: Minibatch Loss: 5.587461\n",
            "Step 1372: Minibatch Loss: 5.201519\n",
            "Step 1374: Minibatch Loss: 5.177503\n",
            "Step 1376: Minibatch Loss: 5.292823\n",
            "Step 1378: Minibatch Loss: 6.347358\n",
            "Step 1380: Minibatch Loss: 5.971386\n",
            "Step 1382: Minibatch Loss: 5.580782\n",
            "Step 1384: Minibatch Loss: 5.275553\n",
            "Step 1386: Minibatch Loss: 4.904044\n",
            "Step 1388: Minibatch Loss: 5.493284\n",
            "Step 1390: Minibatch Loss: 5.559839\n",
            "Step 1392: Minibatch Loss: 4.444933\n",
            "Step 1394: Minibatch Loss: 5.617109\n",
            "Step 1396: Minibatch Loss: 5.812580\n",
            "Step 1398: Minibatch Loss: 4.507401\n",
            "Step 1400: Minibatch Loss: 6.173779\n",
            "Training for SNR= 3.5  sigma= 0.6683439175686147 iteratin: 0\n",
            "Step 1402: Minibatch Loss: 3.857546\n",
            "Step 1404: Minibatch Loss: 3.236195\n",
            "Step 1406: Minibatch Loss: 3.681082\n",
            "Step 1408: Minibatch Loss: 3.237875\n",
            "Step 1410: Minibatch Loss: 3.284277\n",
            "Step 1412: Minibatch Loss: 4.468584\n",
            "Step 1414: Minibatch Loss: 4.184848\n",
            "Step 1416: Minibatch Loss: 4.812768\n",
            "Step 1418: Minibatch Loss: 3.440989\n",
            "Step 1420: Minibatch Loss: 3.109329\n",
            "Step 1422: Minibatch Loss: 4.824678\n",
            "Step 1424: Minibatch Loss: 3.301524\n",
            "Step 1426: Minibatch Loss: 3.678625\n",
            "Step 1428: Minibatch Loss: 4.375256\n",
            "Step 1430: Minibatch Loss: 3.090868\n",
            "Step 1432: Minibatch Loss: 3.833825\n",
            "Step 1434: Minibatch Loss: 3.074429\n",
            "Step 1436: Minibatch Loss: 2.581078\n",
            "Step 1438: Minibatch Loss: 3.717881\n",
            "Step 1440: Minibatch Loss: 5.037251\n",
            "Step 1442: Minibatch Loss: 4.834556\n",
            "Step 1444: Minibatch Loss: 2.995963\n",
            "Step 1446: Minibatch Loss: 4.293949\n",
            "Step 1448: Minibatch Loss: 2.863501\n",
            "Step 1450: Minibatch Loss: 2.810217\n",
            "Step 1452: Minibatch Loss: 3.894284\n",
            "Step 1454: Minibatch Loss: 4.696063\n",
            "Step 1456: Minibatch Loss: 2.084301\n",
            "Step 1458: Minibatch Loss: 3.086219\n",
            "Step 1460: Minibatch Loss: 2.546795\n",
            "Step 1462: Minibatch Loss: 3.638154\n",
            "Step 1464: Minibatch Loss: 3.378258\n",
            "Step 1466: Minibatch Loss: 2.927027\n",
            "Step 1468: Minibatch Loss: 3.286864\n",
            "Step 1470: Minibatch Loss: 3.767037\n",
            "Step 1472: Minibatch Loss: 3.461240\n",
            "Step 1474: Minibatch Loss: 4.421477\n",
            "Step 1476: Minibatch Loss: 4.676069\n",
            "Step 1478: Minibatch Loss: 3.677965\n",
            "Step 1480: Minibatch Loss: 4.109778\n",
            "Step 1482: Minibatch Loss: 3.410556\n",
            "Step 1484: Minibatch Loss: 3.990435\n",
            "Step 1486: Minibatch Loss: 2.898138\n",
            "Step 1488: Minibatch Loss: 3.600395\n",
            "Step 1490: Minibatch Loss: 3.510940\n",
            "Step 1492: Minibatch Loss: 3.391456\n",
            "Step 1494: Minibatch Loss: 4.289497\n",
            "Step 1496: Minibatch Loss: 3.621496\n",
            "Step 1498: Minibatch Loss: 4.285798\n",
            "Step 1500: Minibatch Loss: 4.117667\n",
            "Step 1502: Minibatch Loss: 2.981173\n",
            "Step 1504: Minibatch Loss: 3.018481\n",
            "Step 1506: Minibatch Loss: 4.922434\n",
            "Step 1508: Minibatch Loss: 4.961518\n",
            "Step 1510: Minibatch Loss: 4.164756\n",
            "Step 1512: Minibatch Loss: 4.359735\n",
            "Step 1514: Minibatch Loss: 2.744076\n",
            "Step 1516: Minibatch Loss: 2.319247\n",
            "Step 1518: Minibatch Loss: 3.875622\n",
            "Step 1520: Minibatch Loss: 3.857906\n",
            "Step 1522: Minibatch Loss: 3.729764\n",
            "Step 1524: Minibatch Loss: 3.869734\n",
            "Step 1526: Minibatch Loss: 3.410990\n",
            "Step 1528: Minibatch Loss: 3.070058\n",
            "Step 1530: Minibatch Loss: 4.180957\n",
            "Step 1532: Minibatch Loss: 3.109124\n",
            "Step 1534: Minibatch Loss: 3.005881\n",
            "Step 1536: Minibatch Loss: 3.725783\n",
            "Step 1538: Minibatch Loss: 3.552943\n",
            "Step 1540: Minibatch Loss: 3.858587\n",
            "Step 1542: Minibatch Loss: 3.626264\n",
            "Step 1544: Minibatch Loss: 4.100430\n",
            "Step 1546: Minibatch Loss: 3.370887\n",
            "Step 1548: Minibatch Loss: 3.261340\n",
            "Step 1550: Minibatch Loss: 2.920703\n",
            "Step 1552: Minibatch Loss: 4.026279\n",
            "Step 1554: Minibatch Loss: 3.759510\n",
            "Step 1556: Minibatch Loss: 3.482601\n",
            "Step 1558: Minibatch Loss: 4.138369\n",
            "Step 1560: Minibatch Loss: 3.236032\n",
            "Step 1562: Minibatch Loss: 2.574563\n",
            "Step 1564: Minibatch Loss: 3.628692\n",
            "Step 1566: Minibatch Loss: 2.303041\n",
            "Step 1568: Minibatch Loss: 4.062764\n",
            "Step 1570: Minibatch Loss: 4.232377\n",
            "Step 1572: Minibatch Loss: 2.926658\n",
            "Step 1574: Minibatch Loss: 3.345743\n",
            "Step 1576: Minibatch Loss: 2.853847\n",
            "Step 1578: Minibatch Loss: 3.605426\n",
            "Step 1580: Minibatch Loss: 3.802540\n",
            "Step 1582: Minibatch Loss: 3.718585\n",
            "Step 1584: Minibatch Loss: 3.676852\n",
            "Step 1586: Minibatch Loss: 3.424672\n",
            "Step 1588: Minibatch Loss: 3.072586\n",
            "Step 1590: Minibatch Loss: 3.598099\n",
            "Step 1592: Minibatch Loss: 3.681358\n",
            "Step 1594: Minibatch Loss: 3.031193\n",
            "Step 1596: Minibatch Loss: 3.210883\n",
            "Step 1598: Minibatch Loss: 2.788291\n",
            "Step 1600: Minibatch Loss: 3.764424\n",
            "Training for SNR= 4.0  sigma= 0.6309573444801932 iteratin: 0\n",
            "Step 1602: Minibatch Loss: 3.337028\n",
            "Step 1604: Minibatch Loss: 1.770797\n",
            "Step 1606: Minibatch Loss: 2.840506\n",
            "Step 1608: Minibatch Loss: 1.951183\n",
            "Step 1610: Minibatch Loss: 2.547724\n",
            "Step 1612: Minibatch Loss: 2.580945\n",
            "Step 1614: Minibatch Loss: 2.474038\n",
            "Step 1616: Minibatch Loss: 2.931045\n",
            "Step 1618: Minibatch Loss: 1.239347\n",
            "Step 1620: Minibatch Loss: 1.471577\n",
            "Step 1622: Minibatch Loss: 2.895518\n",
            "Step 1624: Minibatch Loss: 2.655945\n",
            "Step 1626: Minibatch Loss: 2.769302\n",
            "Step 1628: Minibatch Loss: 2.157222\n",
            "Step 1630: Minibatch Loss: 2.736444\n",
            "Step 1632: Minibatch Loss: 1.822756\n",
            "Step 1634: Minibatch Loss: 1.792525\n",
            "Step 1636: Minibatch Loss: 3.299411\n",
            "Step 1638: Minibatch Loss: 2.594934\n",
            "Step 1640: Minibatch Loss: 2.308697\n",
            "Step 1642: Minibatch Loss: 3.210791\n",
            "Step 1644: Minibatch Loss: 1.368580\n",
            "Step 1646: Minibatch Loss: 2.020628\n",
            "Step 1648: Minibatch Loss: 2.480921\n",
            "Step 1650: Minibatch Loss: 2.790781\n",
            "Step 1652: Minibatch Loss: 2.461873\n",
            "Step 1654: Minibatch Loss: 2.730359\n",
            "Step 1656: Minibatch Loss: 1.139505\n",
            "Step 1658: Minibatch Loss: 2.084005\n",
            "Step 1660: Minibatch Loss: 2.475405\n",
            "Step 1662: Minibatch Loss: 2.076780\n",
            "Step 1664: Minibatch Loss: 2.293988\n",
            "Step 1666: Minibatch Loss: 2.531796\n",
            "Step 1668: Minibatch Loss: 1.932307\n",
            "Step 1670: Minibatch Loss: 1.898850\n",
            "Step 1672: Minibatch Loss: 2.636059\n",
            "Step 1674: Minibatch Loss: 3.533978\n",
            "Step 1676: Minibatch Loss: 2.121358\n",
            "Step 1678: Minibatch Loss: 1.788180\n",
            "Step 1680: Minibatch Loss: 1.781611\n",
            "Step 1682: Minibatch Loss: 1.961167\n",
            "Step 1684: Minibatch Loss: 2.841037\n",
            "Step 1686: Minibatch Loss: 2.761696\n",
            "Step 1688: Minibatch Loss: 1.333638\n",
            "Step 1690: Minibatch Loss: 1.958994\n",
            "Step 1692: Minibatch Loss: 3.455562\n",
            "Step 1694: Minibatch Loss: 2.853001\n",
            "Step 1696: Minibatch Loss: 1.717549\n",
            "Step 1698: Minibatch Loss: 2.975085\n",
            "Step 1700: Minibatch Loss: 2.213449\n",
            "Step 1702: Minibatch Loss: 1.759722\n",
            "Step 1704: Minibatch Loss: 1.705813\n",
            "Step 1706: Minibatch Loss: 2.493080\n",
            "Step 1708: Minibatch Loss: 2.885540\n",
            "Step 1710: Minibatch Loss: 2.880660\n",
            "Step 1712: Minibatch Loss: 2.160200\n",
            "Step 1714: Minibatch Loss: 2.227595\n",
            "Step 1716: Minibatch Loss: 2.462111\n",
            "Step 1718: Minibatch Loss: 2.572330\n",
            "Step 1720: Minibatch Loss: 2.791120\n",
            "Step 1722: Minibatch Loss: 1.769959\n",
            "Step 1724: Minibatch Loss: 1.711941\n",
            "Step 1726: Minibatch Loss: 2.218304\n",
            "Step 1728: Minibatch Loss: 2.976550\n",
            "Step 1730: Minibatch Loss: 1.397349\n",
            "Step 1732: Minibatch Loss: 1.710449\n",
            "Step 1734: Minibatch Loss: 2.141763\n",
            "Step 1736: Minibatch Loss: 1.898331\n",
            "Step 1738: Minibatch Loss: 2.172177\n",
            "Step 1740: Minibatch Loss: 1.211745\n",
            "Step 1742: Minibatch Loss: 2.202931\n",
            "Step 1744: Minibatch Loss: 2.433787\n",
            "Step 1746: Minibatch Loss: 1.661721\n",
            "Step 1748: Minibatch Loss: 2.496354\n",
            "Step 1750: Minibatch Loss: 1.950836\n",
            "Step 1752: Minibatch Loss: 2.226275\n",
            "Step 1754: Minibatch Loss: 2.137712\n",
            "Step 1756: Minibatch Loss: 1.601297\n",
            "Step 1758: Minibatch Loss: 2.402500\n",
            "Step 1760: Minibatch Loss: 0.825738\n",
            "Step 1762: Minibatch Loss: 1.913103\n",
            "Step 1764: Minibatch Loss: 2.692671\n",
            "Step 1766: Minibatch Loss: 1.815471\n",
            "Step 1768: Minibatch Loss: 2.393990\n",
            "Step 1770: Minibatch Loss: 2.199287\n",
            "Step 1772: Minibatch Loss: 2.540497\n",
            "Step 1774: Minibatch Loss: 1.810141\n",
            "Step 1776: Minibatch Loss: 2.173202\n",
            "Step 1778: Minibatch Loss: 2.257883\n",
            "Step 1780: Minibatch Loss: 1.500355\n",
            "Step 1782: Minibatch Loss: 1.947878\n",
            "Step 1784: Minibatch Loss: 3.106210\n",
            "Step 1786: Minibatch Loss: 3.210559\n",
            "Step 1788: Minibatch Loss: 2.511964\n",
            "Step 1790: Minibatch Loss: 2.706841\n",
            "Step 1792: Minibatch Loss: 2.672715\n",
            "Step 1794: Minibatch Loss: 2.132989\n",
            "Step 1796: Minibatch Loss: 2.008551\n",
            "Step 1798: Minibatch Loss: 0.774936\n",
            "Step 1800: Minibatch Loss: 2.058211\n",
            "Training for SNR= 4.5  sigma= 0.5956621435290105 iteratin: 0\n",
            "Step 1802: Minibatch Loss: 1.180081\n",
            "Step 1804: Minibatch Loss: 1.435959\n",
            "Step 1806: Minibatch Loss: 1.127577\n",
            "Step 1808: Minibatch Loss: 1.217497\n",
            "Step 1810: Minibatch Loss: 1.303695\n",
            "Step 1812: Minibatch Loss: 1.285486\n",
            "Step 1814: Minibatch Loss: 1.956803\n",
            "Step 1816: Minibatch Loss: 1.169931\n",
            "Step 1818: Minibatch Loss: 1.110903\n",
            "Step 1820: Minibatch Loss: 1.225944\n",
            "Step 1822: Minibatch Loss: 1.371121\n",
            "Step 1824: Minibatch Loss: 1.102301\n",
            "Step 1826: Minibatch Loss: 1.388000\n",
            "Step 1828: Minibatch Loss: 1.116871\n",
            "Step 1830: Minibatch Loss: 1.204339\n",
            "Step 1832: Minibatch Loss: 1.839943\n",
            "Step 1834: Minibatch Loss: 2.055715\n",
            "Step 1836: Minibatch Loss: 0.944634\n",
            "Step 1838: Minibatch Loss: 1.342047\n",
            "Step 1840: Minibatch Loss: 0.564322\n",
            "Step 1842: Minibatch Loss: 1.027451\n",
            "Step 1844: Minibatch Loss: 2.053519\n",
            "Step 1846: Minibatch Loss: 1.182697\n",
            "Step 1848: Minibatch Loss: 0.996569\n",
            "Step 1850: Minibatch Loss: 1.952158\n",
            "Step 1852: Minibatch Loss: 1.612263\n",
            "Step 1854: Minibatch Loss: 0.867224\n",
            "Step 1856: Minibatch Loss: 0.975824\n",
            "Step 1858: Minibatch Loss: 1.351109\n",
            "Step 1860: Minibatch Loss: 1.845547\n",
            "Step 1862: Minibatch Loss: 0.929297\n",
            "Step 1864: Minibatch Loss: 1.662286\n",
            "Step 1866: Minibatch Loss: 1.553748\n",
            "Step 1868: Minibatch Loss: 0.988063\n",
            "Step 1870: Minibatch Loss: 1.355108\n",
            "Step 1872: Minibatch Loss: 1.216430\n",
            "Step 1874: Minibatch Loss: 0.897079\n",
            "Step 1876: Minibatch Loss: 0.881486\n",
            "Step 1878: Minibatch Loss: 1.537688\n",
            "Step 1880: Minibatch Loss: 1.460093\n",
            "Step 1882: Minibatch Loss: 0.950381\n",
            "Step 1884: Minibatch Loss: 1.702045\n",
            "Step 1886: Minibatch Loss: 1.181366\n",
            "Step 1888: Minibatch Loss: 1.620912\n",
            "Step 1890: Minibatch Loss: 1.134080\n",
            "Step 1892: Minibatch Loss: 0.884221\n",
            "Step 1894: Minibatch Loss: 1.099843\n",
            "Step 1896: Minibatch Loss: 1.745935\n",
            "Step 1898: Minibatch Loss: 0.958351\n",
            "Step 1900: Minibatch Loss: 1.244712\n",
            "Step 1902: Minibatch Loss: 1.296676\n",
            "Step 1904: Minibatch Loss: 1.339352\n",
            "Step 1906: Minibatch Loss: 1.395340\n",
            "Step 1908: Minibatch Loss: 1.268342\n",
            "Step 1910: Minibatch Loss: 1.033750\n",
            "Step 1912: Minibatch Loss: 0.974486\n",
            "Step 1914: Minibatch Loss: 0.571687\n",
            "Step 1916: Minibatch Loss: 0.900656\n",
            "Step 1918: Minibatch Loss: 0.988309\n",
            "Step 1920: Minibatch Loss: 1.130548\n",
            "Step 1922: Minibatch Loss: 0.938712\n",
            "Step 1924: Minibatch Loss: 1.612638\n",
            "Step 1926: Minibatch Loss: 0.818339\n",
            "Step 1928: Minibatch Loss: 1.751411\n",
            "Step 1930: Minibatch Loss: 0.902788\n",
            "Step 1932: Minibatch Loss: 1.363666\n",
            "Step 1934: Minibatch Loss: 0.350761\n",
            "Step 1936: Minibatch Loss: 1.568112\n",
            "Step 1938: Minibatch Loss: 1.981066\n",
            "Step 1940: Minibatch Loss: 0.958621\n",
            "Step 1942: Minibatch Loss: 1.463720\n",
            "Step 1944: Minibatch Loss: 1.315837\n",
            "Step 1946: Minibatch Loss: 0.792139\n",
            "Step 1948: Minibatch Loss: 0.891042\n",
            "Step 1950: Minibatch Loss: 2.183777\n",
            "Step 1952: Minibatch Loss: 1.200145\n",
            "Step 1954: Minibatch Loss: 1.702333\n",
            "Step 1956: Minibatch Loss: 1.303445\n",
            "Step 1958: Minibatch Loss: 1.093910\n",
            "Step 1960: Minibatch Loss: 1.633934\n",
            "Step 1962: Minibatch Loss: 1.060364\n",
            "Step 1964: Minibatch Loss: 0.478209\n",
            "Step 1966: Minibatch Loss: 1.372714\n",
            "Step 1968: Minibatch Loss: 1.080453\n",
            "Step 1970: Minibatch Loss: 0.814524\n",
            "Step 1972: Minibatch Loss: 0.634691\n",
            "Step 1974: Minibatch Loss: 0.940395\n",
            "Step 1976: Minibatch Loss: 0.931503\n",
            "Step 1978: Minibatch Loss: 0.453898\n",
            "Step 1980: Minibatch Loss: 1.574905\n",
            "Step 1982: Minibatch Loss: 1.118339\n",
            "Step 1984: Minibatch Loss: 1.243633\n",
            "Step 1986: Minibatch Loss: 1.399833\n",
            "Step 1988: Minibatch Loss: 0.697620\n",
            "Step 1990: Minibatch Loss: 1.491854\n",
            "Step 1992: Minibatch Loss: 1.326096\n",
            "Step 1994: Minibatch Loss: 1.037116\n",
            "Step 1996: Minibatch Loss: 1.526343\n",
            "Step 1998: Minibatch Loss: 0.646020\n",
            "Step 2000: Minibatch Loss: 1.481016\n",
            "Training for SNR= 5.0  sigma= 0.5623413251903491 iteratin: 0\n",
            "Step 2002: Minibatch Loss: 0.363732\n",
            "Step 2004: Minibatch Loss: 0.389275\n",
            "Step 2006: Minibatch Loss: 0.452730\n",
            "Step 2008: Minibatch Loss: 0.689059\n",
            "Step 2010: Minibatch Loss: 1.534208\n",
            "Step 2012: Minibatch Loss: 0.745204\n",
            "Step 2014: Minibatch Loss: 0.644611\n",
            "Step 2016: Minibatch Loss: 0.522324\n",
            "Step 2018: Minibatch Loss: 0.565529\n",
            "Step 2020: Minibatch Loss: 0.840999\n",
            "Step 2022: Minibatch Loss: 0.219751\n",
            "Step 2024: Minibatch Loss: 0.573766\n",
            "Step 2026: Minibatch Loss: 0.629799\n",
            "Step 2028: Minibatch Loss: 0.888076\n",
            "Step 2030: Minibatch Loss: 0.639885\n",
            "Step 2032: Minibatch Loss: 0.320071\n",
            "Step 2034: Minibatch Loss: 0.721642\n",
            "Step 2036: Minibatch Loss: 0.672681\n",
            "Step 2038: Minibatch Loss: 0.808040\n",
            "Step 2040: Minibatch Loss: 0.423062\n",
            "Step 2042: Minibatch Loss: 0.910282\n",
            "Step 2044: Minibatch Loss: 0.289808\n",
            "Step 2046: Minibatch Loss: 0.897974\n",
            "Step 2048: Minibatch Loss: 0.372875\n",
            "Step 2050: Minibatch Loss: 0.317987\n",
            "Step 2052: Minibatch Loss: 0.882547\n",
            "Step 2054: Minibatch Loss: 0.747469\n",
            "Step 2056: Minibatch Loss: 0.418270\n",
            "Step 2058: Minibatch Loss: 0.433086\n",
            "Step 2060: Minibatch Loss: 1.296025\n",
            "Step 2062: Minibatch Loss: 0.412268\n",
            "Step 2064: Minibatch Loss: 0.932422\n",
            "Step 2066: Minibatch Loss: 0.945594\n",
            "Step 2068: Minibatch Loss: 1.216610\n",
            "Step 2070: Minibatch Loss: 1.019845\n",
            "Step 2072: Minibatch Loss: 0.875224\n",
            "Step 2074: Minibatch Loss: 0.508468\n",
            "Step 2076: Minibatch Loss: 0.544652\n",
            "Step 2078: Minibatch Loss: 0.713029\n",
            "Step 2080: Minibatch Loss: 0.441647\n",
            "Step 2082: Minibatch Loss: 0.691904\n",
            "Step 2084: Minibatch Loss: 0.354017\n",
            "Step 2086: Minibatch Loss: 0.828715\n",
            "Step 2088: Minibatch Loss: 0.639306\n",
            "Step 2090: Minibatch Loss: 0.421341\n",
            "Step 2092: Minibatch Loss: 1.180713\n",
            "Step 2094: Minibatch Loss: 0.985871\n",
            "Step 2096: Minibatch Loss: 0.491782\n",
            "Step 2098: Minibatch Loss: 0.400435\n",
            "Step 2100: Minibatch Loss: 0.681310\n",
            "Step 2102: Minibatch Loss: 0.672369\n",
            "Step 2104: Minibatch Loss: 1.160806\n",
            "Step 2106: Minibatch Loss: 0.355392\n",
            "Step 2108: Minibatch Loss: 0.579970\n",
            "Step 2110: Minibatch Loss: 0.906436\n",
            "Step 2112: Minibatch Loss: 1.523880\n",
            "Step 2114: Minibatch Loss: 0.830536\n",
            "Step 2116: Minibatch Loss: 1.133268\n",
            "Step 2118: Minibatch Loss: 0.258754\n",
            "Step 2120: Minibatch Loss: 0.425396\n",
            "Step 2122: Minibatch Loss: 1.159524\n",
            "Step 2124: Minibatch Loss: 0.700326\n",
            "Step 2126: Minibatch Loss: 0.927484\n",
            "Step 2128: Minibatch Loss: 0.609878\n",
            "Step 2130: Minibatch Loss: 1.318353\n",
            "Step 2132: Minibatch Loss: 0.725256\n",
            "Step 2134: Minibatch Loss: 0.486848\n",
            "Step 2136: Minibatch Loss: 0.501439\n",
            "Step 2138: Minibatch Loss: 0.715454\n",
            "Step 2140: Minibatch Loss: 0.508296\n",
            "Step 2142: Minibatch Loss: 0.868559\n",
            "Step 2144: Minibatch Loss: 1.171702\n",
            "Step 2146: Minibatch Loss: 0.672165\n",
            "Step 2148: Minibatch Loss: 0.226700\n",
            "Step 2150: Minibatch Loss: 0.470011\n",
            "Step 2152: Minibatch Loss: 0.459611\n",
            "Step 2154: Minibatch Loss: 1.041237\n",
            "Step 2156: Minibatch Loss: 1.191976\n",
            "Step 2158: Minibatch Loss: 1.013574\n",
            "Step 2160: Minibatch Loss: 0.813266\n",
            "Step 2162: Minibatch Loss: 0.712048\n",
            "Step 2164: Minibatch Loss: 0.764331\n",
            "Step 2166: Minibatch Loss: 0.723803\n",
            "Step 2168: Minibatch Loss: 0.328690\n",
            "Step 2170: Minibatch Loss: 0.680515\n",
            "Step 2172: Minibatch Loss: 0.783242\n",
            "Step 2174: Minibatch Loss: 1.468397\n",
            "Step 2176: Minibatch Loss: 0.554652\n",
            "Step 2178: Minibatch Loss: 0.608994\n",
            "Step 2180: Minibatch Loss: 0.990657\n",
            "Step 2182: Minibatch Loss: 0.745326\n",
            "Step 2184: Minibatch Loss: 0.861022\n",
            "Step 2186: Minibatch Loss: 0.985968\n",
            "Step 2188: Minibatch Loss: 0.898997\n",
            "Step 2190: Minibatch Loss: 0.505644\n",
            "Step 2192: Minibatch Loss: 0.253432\n",
            "Step 2194: Minibatch Loss: 0.583657\n",
            "Step 2196: Minibatch Loss: 0.367837\n",
            "Step 2198: Minibatch Loss: 0.509621\n",
            "Step 2200: Minibatch Loss: 0.532066\n",
            "Training for SNR= 5.5  sigma= 0.5308844442309884 iteratin: 0\n",
            "Step 2202: Minibatch Loss: 0.393092\n",
            "Loss= 0.009602947\n",
            "Early Stop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHByzQbTUqbv",
        "outputId": "e8b65df6-2379-4836-a2d2-f125a86ddb61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using trained model\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_dl_tensor  = numpy.array(())\n",
        "times_per_iter_dl_tensor = numpy.array(())\n",
        "\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  lrate = 0.001\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    input_message_xx = training_input_message_one_hot [i:i+1]\n",
        "    input_message_xx = input_message_xx.astype(\"float32\")\n",
        "    #,input_message_x_label:training_input_message [i]\n",
        "    encoded_message = train_sess.run ([dl_encoder_output], feed_dict={input_message_x:input_message_xx })\n",
        "    encoded_message = encoded_message[0][0]\n",
        "    #encoded_message = numpy.around(encoded_message[0][0]> 0).astype(int)\n",
        "    #print (encoded_message[0][0])\n",
        "    awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message})\n",
        "    #print (awgn_channel_output_message)\n",
        "    decoded_message = train_sess.run ([dl_decoder_only_output], feed_dict={input_channel_x:awgn_channel_output_message})\n",
        "    #print (\"input\", input_message[i])\n",
        "    #decoded_message = numpy.around(decoded_message[0][0]> 0).astype(int)\n",
        "    #rint (\"output\", decoded_message)\n",
        "    #print (\"output\", numpy.argmax(training_input_message_one_hot[i]), numpy.argmax(decoded_message[0][0]))\n",
        "    if (numpy.argmax(training_input_message_one_hot[i]) != numpy.argmax(decoded_message[0][0])):\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_dl_tensor=numpy.append(ber_per_iter_dl_tensor ,ber)\n",
        "  times_per_iter_dl_tensor=numpy.append(times_per_iter_dl_tensor, total_time)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 0.71s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 1.33s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 1.95s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 2.56s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.54\n",
            " -> Total Time: 6.55s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 0.60s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 1.21s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 1.85s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 2.48s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.45\n",
            " -> Total Time: 6.15s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.62s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 1.22s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 1.81s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 2.46s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.38\n",
            " -> Total Time: 6.10s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.60s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.23s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 1.84s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 2.48s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.33\n",
            " -> Total Time: 6.16s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.62s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.21s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 1.81s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.45s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.25\n",
            " -> Total Time: 6.09s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.62s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.28s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.97s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.63s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.20\n",
            " -> Total Time: 6.50s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.66s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 1.31s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.97s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 2.64s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.16\n",
            " -> Total Time: 6.58s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.64s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 1.29s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.96s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 2.61s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.11\n",
            " -> Total Time: 6.51s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.64s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 1.32s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 1.97s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 2.63s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.06\n",
            " -> Total Time: 6.57s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.67s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 1.33s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 2.00s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 2.66s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.05\n",
            " -> Total Time: 6.65s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.62s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 1.29s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 1.95s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 2.59s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 6.45s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.66s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 1.32s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 1.97s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 2.60s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 6.55s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.66s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 1.30s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 1.95s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 2.59s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 6.50s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.64s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 1.28s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 1.92s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 2.55s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 6.40s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.62s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 1.21s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 1.81s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 2.42s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 6.06s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.63s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 1.25s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 1.85s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 2.45s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 6.18s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.61s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 1.21s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 1.80s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 2.47s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 6.08s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.72s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 1.46s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 2.16s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 2.85s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 7.19s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.68s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 1.39s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 2.05s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 2.62s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 6.74s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.58s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 1.13s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 1.69s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 2.23s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 5.63s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syUQij3fuxRm",
        "outputId": "9a9b3a29-55ae-4281-fc73-62c27e64c960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1) = plt.subplots(1,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"ldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor,'', label=\"ai-dl\") # plot BER vs SNR\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "#ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "#ax2.set_xlabel('$E_b/$N_0$')\n",
        "#ax2.set_ylabel('Decoding Time [s]')\n",
        "#ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "#            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "#            xy=(1, 0.35), xycoords='axes fraction',\n",
        "#            xytext=(-20, 20), textcoords='offset pixels',\n",
        "#            horizontalalignment='right',\n",
        "#            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpIbD_FUw-iL",
        "outputId": "4bcaa9bb-ccb1-4a64-d1ef-0c9f647ed4be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "import numpy\n",
        "training_input_message = numpy.random.randint(2**input_message_length, size=(1,NUM_OF_INPUT_MESSAGE*10))\n",
        "training_input_message_one_hot = numpy.zeros((training_input_message.size, 2**input_message_length))\n",
        "training_input_message_one_hot[numpy.arange(training_input_message.size),training_input_message] = 1\n",
        "print(training_input_message_one_hot)\n",
        "print (training_input_message_one_hot.shape)\n",
        "print (training_input_message.shape)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(10000, 2048)\n",
            "(1, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "farYxiF5xJFj",
        "outputId": "645131d5-769a-4e76-b6bc-f4a9967c493b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using trained model\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_dl_tensor  = numpy.array(())\n",
        "times_per_iter_dl_tensor = numpy.array(())\n",
        "\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  lrate = 0.001\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    input_message_xx = training_input_message_one_hot [i:i+1]\n",
        "    input_message_xx = input_message_xx.astype(\"float32\")\n",
        "    #,input_message_x_label:training_input_message [i]\n",
        "    encoded_message = train_sess.run ([dl_encoder_output], feed_dict={input_message_x:input_message_xx })\n",
        "    encoded_message = encoded_message[0][0]\n",
        "    #encoded_message = numpy.around(encoded_message[0][0]> 0).astype(int)\n",
        "    #print (encoded_message[0][0])\n",
        "    awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message})\n",
        "    #print (awgn_channel_output_message)\n",
        "    decoded_message = train_sess.run ([dl_decoder_only_output], feed_dict={input_channel_x:awgn_channel_output_message})\n",
        "    #print (\"input\", input_message[i])\n",
        "    #decoded_message = numpy.around(decoded_message[0][0]> 0).astype(int)\n",
        "    #rint (\"output\", decoded_message)\n",
        "    #print (\"output\", numpy.argmax(training_input_message_one_hot[i]), numpy.argmax(decoded_message[0][0]))\n",
        "    if (numpy.argmax(training_input_message_one_hot[i]) != numpy.argmax(decoded_message[0][0])):\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_dl_tensor=numpy.append(ber_per_iter_dl_tensor ,ber)\n",
        "  times_per_iter_dl_tensor=numpy.append(times_per_iter_dl_tensor, total_time)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 0.63s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 1.25s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 1.86s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 2.46s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.55\n",
            " -> Total Time: 6.20s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 0.60s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 1.22s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 1.88s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 2.52s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.47\n",
            " -> Total Time: 6.21s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.60s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 1.23s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 1.86s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 2.51s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.40\n",
            " -> Total Time: 6.20s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.64s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.26s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 1.83s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 2.43s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.33\n",
            " -> Total Time: 6.16s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.59s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.21s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 1.82s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.44s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.24\n",
            " -> Total Time: 6.05s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.64s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.30s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.94s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.54s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.19\n",
            " -> Total Time: 6.43s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.63s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 1.27s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.89s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 2.55s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.14\n",
            " -> Total Time: 6.34s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.64s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 1.25s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.91s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 2.56s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.10\n",
            " -> Total Time: 6.36s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.64s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 1.33s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 1.96s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 2.58s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.07\n",
            " -> Total Time: 6.52s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.62s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 1.28s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 1.94s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 2.56s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.06\n",
            " -> Total Time: 6.40s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.60s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 1.21s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 1.83s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 2.46s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 6.10s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.63s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 1.25s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 1.84s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 2.50s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 6.23s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.60s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 1.24s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 1.90s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 2.55s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 6.29s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.64s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 1.31s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 1.97s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 2.63s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 6.55s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.64s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 1.28s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 1.89s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 2.48s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 6.28s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.61s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 1.22s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 1.84s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 2.49s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 6.16s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.62s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 1.21s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 1.84s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 2.45s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 6.11s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.60s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 1.23s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 1.83s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 2.46s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 6.11s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.59s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 1.21s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 1.81s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 2.41s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 6.03s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.59s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 1.21s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 1.86s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 2.46s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 6.12s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hV6_TllxLG_",
        "outputId": "89db8756-8b54-472b-8ada-d3ca5e2b7f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1) = plt.subplots(1,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"ldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor,'', label=\"ai-dl\") # plot BER vs SNR\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "#ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "#ax2.set_xlabel('$E_b/$N_0$')\n",
        "#ax2.set_ylabel('Decoding Time [s]')\n",
        "#ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "#            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "#            xy=(1, 0.35), xycoords='axes fraction',\n",
        "#            xytext=(-20, 20), textcoords='offset pixels',\n",
        "#            horizontalalignment='right',\n",
        "#            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsQhRxkXxML5"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}