{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MainModelKeras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamviji/project/blob/master/MainModelKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku11kjUKaO8X"
      },
      "source": [
        "Note:To Checkin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDSPPMfZ9czi",
        "outputId": "86a95789-716e-4bba-bb96-e559b8cf7d20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        }
      },
      "source": [
        "!rm -rf project\n",
        "!git clone https://github.com/iamviji/project.git\n",
        "!ls\n",
        "!ls project\n",
        "!pip install pyldpc\n",
        "!pip install scikit-commpy\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'project'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 53 (delta 11), reused 8 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (53/53), done.\n",
            "project  sample_data\n",
            "MainModel.ipynb  MainModelWithSingleBERTraining.ipynb  README.md  util.py\n",
            "Collecting pyldpc\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/aa/fd5495869c7106a638ae71aa497d7d266cae7f2a343d1f6a9d0e3a986e1e/pyldpc-0.7.9.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyldpc) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyldpc) (1.4.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from pyldpc) (0.48.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->pyldpc) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->pyldpc) (50.3.0)\n",
            "Building wheels for collected packages: pyldpc\n",
            "  Building wheel for pyldpc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyldpc: filename=pyldpc-0.7.9-cp36-none-any.whl size=14306 sha256=1d3b99a833677302be0cd37edbdf24e348cc7e02b3ca22dfa8bfbab9ec8f2fb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/7a/10/e94058ba8b0b6d98bf2719226d18d3dd6056525ad7b984c068\n",
            "Successfully built pyldpc\n",
            "Installing collected packages: pyldpc\n",
            "Successfully installed pyldpc-0.7.9\n",
            "Collecting scikit-commpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/b4/f7fa5bc8864e0ddbd3e7a2290b624b92690f53523474024915c33321802d/scikit_commpy-0.5.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->scikit-commpy) (1.15.0)\n",
            "Installing collected packages: scikit-commpy\n",
            "Successfully installed scikit-commpy-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QOuLqpdDgx2"
      },
      "source": [
        "import pyldpc\n",
        "import commpy\n",
        "import numpy \n",
        "import time\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior ()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YClXJbbr0lc7"
      },
      "source": [
        "SNR_BEGIN = 0\n",
        "SNR_END = 10\n",
        "SNR_STEP_SIZE = 0.5\n",
        "CHANEL_SIZE = 18\n",
        "NUM_OF_INPUT_MESSAGE = 1000\n",
        "LDPC_MAX_ITER = 100\n",
        "num_parity_check = 3\n",
        "num_bits_in_parity_check = 6 \n",
        "input_message_length =  0 # Caculated by channel encoder and initialized later"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvUzIMsB43i0"
      },
      "source": [
        "def timer_update(i,current,time_tot,tic_incr=500):\n",
        "    last = current\n",
        "    current = time.time()\n",
        "    t_diff = current-last\n",
        "    print('SNR: {:04.3f} - Iter: {} - Last {} iterations took {:03.2f}s'.format(snr,i+1,tic_incr,t_diff))\n",
        "    return time_tot + t_diff\n",
        "\n",
        "def Snr2Sigma(snr):\n",
        "  sigma = 10 ** (- snr / 20)\n",
        "  return sigma\n",
        "\n",
        "def pyldpc_encode (CodingMatrix, message):\n",
        "  rng = pyldpc.utils.check_random_state(seed=None)\n",
        "  d = pyldpc.utils.binaryproduct(CodingMatrix, message)\n",
        "  encoded_message = (-1) ** d\n",
        "  return encoded_message\n",
        "\n",
        "def pyldpc_decode (ParityCheckMatrix, CodingMatrix, message, snr, maxiter):\n",
        "  decoded_msg = pyldpc.decode(ParityCheckMatrix, message, snr, maxiter)\n",
        "  out_message = pyldpc.get_message(CodingMatrix, decoded_msg)\n",
        "  return out_message\n",
        "\n",
        "awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [CHANEL_SIZE])\n",
        "awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)\n",
        "\n",
        "init = tf.global_variables_initializer ()\n",
        "sess = tf.Session ()\n",
        "sess.run(init)\n",
        "\n",
        "def AWGNChannelOutput (xx, snr , s):\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  awgn_channel_output_message = s.run ([awgn_channel_output], feed_dict={noise_std_dev:sigma, channel_input:xx})\n",
        "  return awgn_channel_output_message"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jMQG-MZ_pXu",
        "outputId": "fe295c91-8abc-42db-81eb-e64235aca75e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "\n",
        "ParityCheckMatrix, CodingMatrix = pyldpc.make_ldpc(CHANEL_SIZE, num_parity_check, num_bits_in_parity_check, systematic=True, sparse=True)\n",
        "input_message_length = CodingMatrix.shape[1]\n",
        "print (\"input_message_size=\", input_message_length, \"channel_size=\",CHANEL_SIZE)\n",
        "print (\"input_message_size=\", CodingMatrix.shape[1], \"channel_size=\",CodingMatrix.shape[0])\n",
        "input_message = numpy.random.randint(2, size=(NUM_OF_INPUT_MESSAGE,input_message_length))\n",
        "print (input_message)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_message_size= 11 channel_size= 18\n",
            "input_message_size= 11 channel_size= 18\n",
            "[[0 1 0 ... 1 0 1]\n",
            " [1 0 1 ... 0 1 1]\n",
            " [0 1 1 ... 0 1 0]\n",
            " ...\n",
            " [0 1 0 ... 1 0 1]\n",
            " [1 1 1 ... 0 0 0]\n",
            " [0 1 1 ... 1 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WKg2HU2adgZ"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fL8ptL4aeOY"
      },
      "source": [
        "This section tries to compare BER and Time performance of PYLDPC in following 3 cases\n",
        "1. SNR Noise function provided in encoder function of pyldpc library (pyldpc.encode)\n",
        "2. SNR Noise function provided by commpy library (commpy.channels.awgn) \n",
        "3. SNR Noise function implemented using tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma5zUqFv0TH2",
        "outputId": "f4ca2699-7935-4ad4-f67d-d0ea31045865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using tensor flow based AWGN, to make sure that effect of it is same as AWGN\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_tensor  = numpy.array(())\n",
        "times_per_iter_tensor = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc_encode (CodingMatrix, input_message[i])\n",
        "    sigma = Snr2Sigma (snr)\n",
        "    awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message})[0]\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      #print (\"count=\",abs(decoded_message-input_message[i]).sum())\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_tensor=numpy.append(ber_per_iter_tensor ,ber)\n",
        "  times_per_iter_tensor=numpy.append(times_per_iter_tensor, total_time)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.87s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 3.83s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 5.72s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 7.64s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.61\n",
            " -> Total Time: 19.06s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.65s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 3.14s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 4.68s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 6.15s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.53\n",
            " -> Total Time: 15.62s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 1.33s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 2.49s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 3.93s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 5.15s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.45\n",
            " -> Total Time: 12.90s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 1.22s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 2.26s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 3.34s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 4.43s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.39\n",
            " -> Total Time: 11.25s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.88s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.80s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 2.72s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 3.67s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.32\n",
            " -> Total Time: 9.07s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.69s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.42s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 2.09s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.76s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.23\n",
            " -> Total Time: 6.96s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.58s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 1.21s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.76s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 2.33s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.15\n",
            " -> Total Time: 5.88s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.61s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 1.17s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.62s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 2.18s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.13\n",
            " -> Total Time: 5.57s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.41s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.82s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 1.21s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.66s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.08\n",
            " -> Total Time: 4.11s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.36s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.72s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 1.14s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.51s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.05\n",
            " -> Total Time: 3.73s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.41s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.76s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 1.18s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 1.55s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 3.89s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.33s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.65s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.98s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 1.39s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 3.35s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.34s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.65s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.97s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 1.31s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 3.26s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.32s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.67s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 1.00s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 1.32s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.31s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.32s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.63s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.93s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 1.24s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.13s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.32s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.65s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.97s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 1.28s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.22s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.32s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.62s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.94s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 1.25s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.13s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.31s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.61s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.92s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 1.27s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.10s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.30s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.62s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.92s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 1.24s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.08s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.31s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.61s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.92s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 1.22s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.06s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8dIFLg76c7O",
        "outputId": "fb8c75e4-e470-42ee-c15b-9bac36f36dfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using commpy based AWGN \n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_awgn  = numpy.array(())\n",
        "times_per_iter_awgn = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc_encode (CodingMatrix, input_message[i])\n",
        "    awgn_channel_output_message = commpy.channels.awgn(encoded_message, snr)\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_awgn=numpy.append(ber_per_iter_awgn ,ber)\n",
        "  times_per_iter_awgn=numpy.append(times_per_iter_awgn, total_time)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.76s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 3.52s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 5.26s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 6.89s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.62\n",
            " -> Total Time: 17.42s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.47s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.86s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 4.40s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 5.81s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.54\n",
            " -> Total Time: 14.54s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 1.16s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 2.31s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 3.43s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 4.79s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.44\n",
            " -> Total Time: 11.69s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.85s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.82s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.64s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 3.53s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.34\n",
            " -> Total Time: 8.84s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.69s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.52s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 2.26s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.97s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.32\n",
            " -> Total Time: 7.44s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.69s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.24s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.86s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.46s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.23\n",
            " -> Total Time: 6.25s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.51s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 1.03s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.56s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.90s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.18\n",
            " -> Total Time: 5.00s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.42s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.71s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.13s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.53s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.12\n",
            " -> Total Time: 3.79s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.30s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.62s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.95s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.27s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.08\n",
            " -> Total Time: 3.14s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.81s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.08s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.05\n",
            " -> Total Time: 2.62s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.05\n",
            " -> Total Time: 2.50s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.22s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.47s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 2.38s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.21s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.44s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.67s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.86s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.18s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.20s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.38s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.58s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.77s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.94s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.38s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.57s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.75s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.91s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.20s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.39s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.58s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.77s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.94s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.22s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.42s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.60s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.80s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.04s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.37s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.56s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.76s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.89s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.38s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.57s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.75s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.89s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.20s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.38s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.57s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.76s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.92s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ihPKJJk7Jj9",
        "outputId": "5e240ae5-fb76-43be-b54d-716a65b13f09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using tensor flow based AWGN, to make sure that effect of it is same as AWGN\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_pyldpc  = numpy.array(())\n",
        "times_per_iter_pyldpc = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc.encode (CodingMatrix, input_message[i], snr)\n",
        "    awgn_channel_output_message = encoded_message\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_pyldpc=numpy.append(ber_per_iter_pyldpc ,ber)\n",
        "  times_per_iter_pyldpc=numpy.append(times_per_iter_pyldpc, total_time)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.83s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 3.64s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 5.48s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 7.31s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.64\n",
            " -> Total Time: 18.26s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.37s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.90s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 4.51s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 5.92s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.55\n",
            " -> Total Time: 14.70s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 1.14s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 2.14s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 3.37s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 4.64s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.46\n",
            " -> Total Time: 11.29s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.95s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.72s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.76s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 3.54s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.35\n",
            " -> Total Time: 8.96s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.63s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.33s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 2.06s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.77s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.31\n",
            " -> Total Time: 6.79s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.69s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.38s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.91s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.50s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.22\n",
            " -> Total Time: 6.47s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.54s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.98s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.35s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.73s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.16\n",
            " -> Total Time: 4.60s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.30s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.71s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.11s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.59s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.14\n",
            " -> Total Time: 3.71s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.28s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.59s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.86s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.16s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.07\n",
            " -> Total Time: 2.88s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.31s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.55s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.84s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.08s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.05\n",
            " -> Total Time: 2.78s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.23s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.46s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.69s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.92s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 2.30s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.43s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.69s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 2.27s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.22s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.42s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.62s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.84s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.10s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.21s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.40s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.61s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.86s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.07s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.20s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.43s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.63s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.81s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.06s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.38s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.57s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.76s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.89s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.18s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.37s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.56s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.74s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.86s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.20s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.39s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.57s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.80s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.96s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.39s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.56s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.75s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.89s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.18s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.37s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.55s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.74s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.84s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR4-FOJ-BkAG",
        "outputId": "7ce861d8-d49e-4fd8-e72f-53e01e7a56c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "# Compare 3 AWGN(Tensorflow, CommPy, PYLDPC) Simulation on LDPC\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_pyldpc,'', label=\"pyldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"tensor\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_awgn,'', label=\"commpy-awgn\") # plot BER vs SNR\n",
        "\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "ax2.set_xlabel('$E_b/$N_0$')\n",
        "ax2.set_ylabel('Decoding Time [s]')\n",
        "ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "            xy=(1, 0.35), xycoords='axes fraction',\n",
        "            xytext=(-20, 20), textcoords='offset pixels',\n",
        "            horizontalalignment='right',\n",
        "            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBl0pWc1oF_r",
        "outputId": "61272fdf-a404-4b4f-8dff-3379ae612a18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "training_input_message = numpy.random.randint(2, size=(NUM_OF_INPUT_MESSAGE*10,input_message_length))\n",
        "print (training_input_message)\n",
        "print (len(training_input_message))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 ... 1 1 1]\n",
            " [1 1 1 ... 0 0 1]\n",
            " [1 1 1 ... 1 1 0]\n",
            " ...\n",
            " [0 1 0 ... 1 1 0]\n",
            " [0 0 1 ... 1 0 1]\n",
            " [0 1 0 ... 0 1 1]]\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggY5VwudaRwR"
      },
      "source": [
        "<B>Conclussion:</B>\n",
        "      It proved that tensorflow behaves similar to AWGN noise channel provided by pyldpc, commpy. But tensor flow based one takes adds little more time delay. This need to be offseted if we are comparing performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOeuNfeLCgfb"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, GaussianNoise\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior ()\n",
        "\n",
        "input_message_x = Input(shape=(input_message_length,))\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "enc_layer1 = Dense(CHANEL_SIZE, activation='tanh')(input_message_x)\n",
        "enc_layer2 = Dense(CHANEL_SIZE, activation='sigmoid')(enc_layer1)\n",
        "#encoded2 = Dense(CHANEL_SIZE, activation='sigmoid')(encoded1)\n",
        "# this model maps an input to its encoded representation\n",
        "enc_layer3 =  enc_layer2 / tf.sqrt(tf.reduce_mean(tf.square(enc_layer2)))\n",
        "#enc_layer2 = tf.round(enc_layer1)\n",
        "encoder = Model(input_message_x, enc_layer3)\n",
        "\n",
        "awgn_channel = GaussianNoise(Snr2Sigma(7.0),input_shape=(CHANEL_SIZE,))\n",
        "\n",
        "# create a placeholder for an encoded (32-dimensional) input\n",
        "encoded_input = Input(shape=(CHANEL_SIZE,))\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "dec_layer1 = Dense(CHANEL_SIZE, activation='tanh')(encoded_input)\n",
        "dec_layer2 = Dense(input_message_length, activation='sigmoid')(dec_layer1)\n",
        "# this model maps an encoded input to its decoder representation\n",
        "decoder = Model(encoded_input, dec_layer2)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgXpqxjrnJ-F",
        "outputId": "72823659-dde6-4940-9bc7-bfe8eb29d174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(encoder.summary())\n",
        "print(decoder.summary())\n",
        "print(autoencoder.summary())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 11)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 18)           216         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 18)           342         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Square (TensorFlowO multiple             0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mean (TensorFlowOpL multiple             0           tf_op_layer_Square[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sqrt (TensorFlowOpL multiple             0           tf_op_layer_Mean[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_truediv (TensorFlow multiple             0           dense_1[0][0]                    \n",
            "                                                                 tf_op_layer_Sqrt[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 558\n",
            "Trainable params: 558\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 18)]              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 18)                342       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 11)                209       \n",
            "=================================================================\n",
            "Total params: 551\n",
            "Trainable params: 551\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 11)]              0         \n",
            "_________________________________________________________________\n",
            "functional_1 (Functional)    (None, 18)                558       \n",
            "_________________________________________________________________\n",
            "gaussian_noise (GaussianNois (None, 18)                0         \n",
            "_________________________________________________________________\n",
            "functional_3 (Functional)    (None, 11)                551       \n",
            "=================================================================\n",
            "Total params: 1,109\n",
            "Trainable params: 1,109\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOXLOYLu8aML",
        "outputId": "73067cf0-09fd-45ba-9aa5-e4810cbd5769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import keras\n",
        "\n",
        "#def custom_losff_fucntion (act, pred):\n",
        "#  return (tf.reduce_mean(-1*(act * tf.log(pred) + (1-act)*tf.log(1-pred))))\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "autoencoder.compile(optimizer=opt, loss='binary_crossentropy')\n",
        "#autoencoder.compile(optimizer=opt, loss=custom_losff_fucntion)\n",
        "#loss='mean_squared_error'\n",
        "#for snr in (numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)):\n",
        "for snr in (numpy.arange (-2, 12, SNR_STEP_SIZE)):\n",
        "  sigma = 1.0*Snr2Sigma (snr)\n",
        "  print (\"Training for SNR=\", snr, \" sigma=\", sigma) \n",
        "  awgn_channel = GaussianNoise(sigma,input_shape=(CHANEL_SIZE,))\n",
        "  autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))\n",
        "  opt = keras.optimizers.Adam(learning_rate=0.002)\n",
        "  autoencoder.compile(optimizer=opt, loss='binary_crossentropy')\n",
        "  autoencoder.fit(training_input_message, training_input_message,\n",
        "                #epochs=50, original\n",
        "                epochs=20,\n",
        "                batch_size=500,\n",
        "                shuffle=False,\n",
        "                validation_data=(input_message, input_message))\n",
        "  \n",
        "  "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for SNR= -2.0  sigma= 1.2589254117941673\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 1s 144us/sample - loss: 1.6177 - val_loss: 2.6812e-05\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 1.5637 - val_loss: 4.1468e-05\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 1.5074 - val_loss: 8.9438e-05\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 1.4452 - val_loss: 2.9844e-04\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 1.3870 - val_loss: 9.3741e-04\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 1.3165 - val_loss: 0.0024\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 1.2505 - val_loss: 0.0053\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 1.2056 - val_loss: 0.0088\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 1.1426 - val_loss: 0.0136\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 1.0771 - val_loss: 0.0199\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.9857 - val_loss: 0.0285\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.9259 - val_loss: 0.0415\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.8773 - val_loss: 0.0679\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.8059 - val_loss: 0.1110\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.7729 - val_loss: 0.1392\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.7388 - val_loss: 0.1490\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.7005 - val_loss: 0.1616\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.6884 - val_loss: 0.1749\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.6588 - val_loss: 0.1863\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.6464 - val_loss: 0.1987\n",
            "Training for SNR= -1.5  sigma= 1.1885022274370185\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 1s 144us/sample - loss: 0.5782 - val_loss: 0.1997\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.5718 - val_loss: 0.2074\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.5629 - val_loss: 0.2140\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.5483 - val_loss: 0.2104\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.5422 - val_loss: 0.2077\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.5386 - val_loss: 0.2120\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.5276 - val_loss: 0.2091\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.5161 - val_loss: 0.2058\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.5124 - val_loss: 0.2065\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.5070 - val_loss: 0.2079\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4980 - val_loss: 0.2077\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4943 - val_loss: 0.2099\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4885 - val_loss: 0.2108\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.4847 - val_loss: 0.2117\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4800 - val_loss: 0.2141\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.4776 - val_loss: 0.2131\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4794 - val_loss: 0.2150\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4752 - val_loss: 0.2135\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4743 - val_loss: 0.2168\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4726 - val_loss: 0.2179\n",
            "Training for SNR= -1.0  sigma= 1.1220184543019633\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 1s 150us/sample - loss: 0.4485 - val_loss: 0.1896\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4452 - val_loss: 0.1868\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4446 - val_loss: 0.1891\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4419 - val_loss: 0.1884\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4382 - val_loss: 0.1865\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4424 - val_loss: 0.1918\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4370 - val_loss: 0.1893\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4384 - val_loss: 0.1900\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4380 - val_loss: 0.1899\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4349 - val_loss: 0.1904\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4387 - val_loss: 0.1938\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4358 - val_loss: 0.1925\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4360 - val_loss: 0.1922\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4348 - val_loss: 0.1932\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4349 - val_loss: 0.1923\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4326 - val_loss: 0.1935\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4338 - val_loss: 0.1944\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4316 - val_loss: 0.1946\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4322 - val_loss: 0.1968\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4313 - val_loss: 0.1932\n",
            "Training for SNR= -0.5  sigma= 1.0592537251772889\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 152us/sample - loss: 0.4095 - val_loss: 0.1694\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4054 - val_loss: 0.1654\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4037 - val_loss: 0.1651\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4047 - val_loss: 0.1653\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4060 - val_loss: 0.1649\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4041 - val_loss: 0.1656\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4034 - val_loss: 0.1669\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4047 - val_loss: 0.1676\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4040 - val_loss: 0.1663\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4064 - val_loss: 0.1702\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4037 - val_loss: 0.1695\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4054 - val_loss: 0.1676\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4062 - val_loss: 0.1715\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4033 - val_loss: 0.1679\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4006 - val_loss: 0.1657\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4043 - val_loss: 0.1714\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4007 - val_loss: 0.1691\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4012 - val_loss: 0.1686\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4056 - val_loss: 0.1733\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.4044 - val_loss: 0.1718\n",
            "Training for SNR= 0.0  sigma= 1.0\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 150us/sample - loss: 0.3774 - val_loss: 0.1436\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3777 - val_loss: 0.1424\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3778 - val_loss: 0.1437\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3748 - val_loss: 0.1425\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3750 - val_loss: 0.1400\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3776 - val_loss: 0.1420\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3763 - val_loss: 0.1430\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3759 - val_loss: 0.1431\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3730 - val_loss: 0.1409\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3759 - val_loss: 0.1422\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3792 - val_loss: 0.1440\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3772 - val_loss: 0.1441\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3755 - val_loss: 0.1428\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3733 - val_loss: 0.1416\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3772 - val_loss: 0.1429\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3772 - val_loss: 0.1445\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3748 - val_loss: 0.1426\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3770 - val_loss: 0.1438\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3770 - val_loss: 0.1448\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3745 - val_loss: 0.1440\n",
            "Training for SNR= 0.5  sigma= 0.9440608762859234\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 154us/sample - loss: 0.3480 - val_loss: 0.1185\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3479 - val_loss: 0.1153\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3479 - val_loss: 0.1145\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3519 - val_loss: 0.1159\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3521 - val_loss: 0.1186\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.3503 - val_loss: 0.1184\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3481 - val_loss: 0.1172\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3490 - val_loss: 0.1187\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3458 - val_loss: 0.1147\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3454 - val_loss: 0.1141\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3469 - val_loss: 0.1158\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3494 - val_loss: 0.1159\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3522 - val_loss: 0.1176\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3480 - val_loss: 0.1159\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3490 - val_loss: 0.1176\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3463 - val_loss: 0.1164\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3466 - val_loss: 0.1155\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3479 - val_loss: 0.1177\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3491 - val_loss: 0.1163\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3476 - val_loss: 0.1161\n",
            "Training for SNR= 1.0  sigma= 0.8912509381337456\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 153us/sample - loss: 0.3242 - val_loss: 0.0950\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3191 - val_loss: 0.0909\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3228 - val_loss: 0.0913\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3216 - val_loss: 0.0923\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3206 - val_loss: 0.0924\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3224 - val_loss: 0.0919\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3176 - val_loss: 0.0910\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3175 - val_loss: 0.0896\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3243 - val_loss: 0.0926\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3249 - val_loss: 0.0931\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3243 - val_loss: 0.0949\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3230 - val_loss: 0.0932\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3192 - val_loss: 0.0910\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3244 - val_loss: 0.0921\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3210 - val_loss: 0.0927\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.3199 - val_loss: 0.0917\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3187 - val_loss: 0.0921\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3222 - val_loss: 0.0919\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3202 - val_loss: 0.0917\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.3207 - val_loss: 0.0919\n",
            "Training for SNR= 1.5  sigma= 0.8413951416451951\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 158us/sample - loss: 0.2949 - val_loss: 0.0766\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2938 - val_loss: 0.0716\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2929 - val_loss: 0.0702\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2916 - val_loss: 0.0701\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2910 - val_loss: 0.0693\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2926 - val_loss: 0.0702\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2899 - val_loss: 0.0696\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2912 - val_loss: 0.0695\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2936 - val_loss: 0.0700\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2932 - val_loss: 0.0708\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2921 - val_loss: 0.0710\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2952 - val_loss: 0.0710\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2926 - val_loss: 0.0708\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2953 - val_loss: 0.0707\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2944 - val_loss: 0.0706\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2941 - val_loss: 0.0720\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2963 - val_loss: 0.0717\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2934 - val_loss: 0.0707\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2944 - val_loss: 0.0712\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2924 - val_loss: 0.0699\n",
            "Training for SNR= 2.0  sigma= 0.7943282347242815\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 158us/sample - loss: 0.2633 - val_loss: 0.0577\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2647 - val_loss: 0.0541\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2647 - val_loss: 0.0525\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2636 - val_loss: 0.0526\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2648 - val_loss: 0.0531\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2658 - val_loss: 0.0529\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2656 - val_loss: 0.0525\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2649 - val_loss: 0.0530\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2638 - val_loss: 0.0528\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2684 - val_loss: 0.0529\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2634 - val_loss: 0.0521\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2639 - val_loss: 0.0520\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2658 - val_loss: 0.0522\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2651 - val_loss: 0.0523\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2669 - val_loss: 0.0525\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2651 - val_loss: 0.0518\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2676 - val_loss: 0.0518\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2659 - val_loss: 0.0526\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2618 - val_loss: 0.0520\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2605 - val_loss: 0.0504\n",
            "Training for SNR= 2.5  sigma= 0.7498942093324559\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 162us/sample - loss: 0.2374 - val_loss: 0.0434\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2354 - val_loss: 0.0398\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2338 - val_loss: 0.0385\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2344 - val_loss: 0.0374\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2356 - val_loss: 0.0373\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2387 - val_loss: 0.0378\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2375 - val_loss: 0.0384\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2355 - val_loss: 0.0380\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2389 - val_loss: 0.0379\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2353 - val_loss: 0.0374\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2388 - val_loss: 0.0373\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2320 - val_loss: 0.0369\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2345 - val_loss: 0.0365\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2358 - val_loss: 0.0365\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2344 - val_loss: 0.0363\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2349 - val_loss: 0.0367\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2310 - val_loss: 0.0357\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2343 - val_loss: 0.0360\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2377 - val_loss: 0.0367\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2362 - val_loss: 0.0365\n",
            "Training for SNR= 3.0  sigma= 0.7079457843841379\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 159us/sample - loss: 0.2103 - val_loss: 0.0320\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2077 - val_loss: 0.0284\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2077 - val_loss: 0.0270\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2076 - val_loss: 0.0269\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2084 - val_loss: 0.0266\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2096 - val_loss: 0.0267\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2075 - val_loss: 0.0262\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2061 - val_loss: 0.0257\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2057 - val_loss: 0.0259\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2092 - val_loss: 0.0258\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2093 - val_loss: 0.0258\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2088 - val_loss: 0.0261\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2090 - val_loss: 0.0261\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2063 - val_loss: 0.0253\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2079 - val_loss: 0.0254\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.2063 - val_loss: 0.0255\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2040 - val_loss: 0.0253\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2103 - val_loss: 0.0252\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2053 - val_loss: 0.0251\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.2073 - val_loss: 0.0250\n",
            "Training for SNR= 3.5  sigma= 0.6683439175686147\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 163us/sample - loss: 0.1828 - val_loss: 0.0214\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1819 - val_loss: 0.0196\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1822 - val_loss: 0.0185\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1794 - val_loss: 0.0179\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1803 - val_loss: 0.0176\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1806 - val_loss: 0.0177\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1816 - val_loss: 0.0178\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1790 - val_loss: 0.0174\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1795 - val_loss: 0.0172\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1798 - val_loss: 0.0170\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1810 - val_loss: 0.0173\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1834 - val_loss: 0.0173\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1836 - val_loss: 0.0177\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1790 - val_loss: 0.0173\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1775 - val_loss: 0.0170\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1811 - val_loss: 0.0168\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1800 - val_loss: 0.0166\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1789 - val_loss: 0.0165\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1831 - val_loss: 0.0168\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1809 - val_loss: 0.0169\n",
            "Training for SNR= 4.0  sigma= 0.6309573444801932\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 164us/sample - loss: 0.1546 - val_loss: 0.0142\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1574 - val_loss: 0.0130\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1546 - val_loss: 0.0124\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1562 - val_loss: 0.0119\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1551 - val_loss: 0.0117\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1537 - val_loss: 0.0114\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1545 - val_loss: 0.0115\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1540 - val_loss: 0.0111\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1537 - val_loss: 0.0110\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1554 - val_loss: 0.0110\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1550 - val_loss: 0.0112\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1522 - val_loss: 0.0109\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1541 - val_loss: 0.0107\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1546 - val_loss: 0.0107\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1536 - val_loss: 0.0109\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1566 - val_loss: 0.0108\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1546 - val_loss: 0.0110\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1533 - val_loss: 0.0108\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1527 - val_loss: 0.0105\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1532 - val_loss: 0.0106\n",
            "Training for SNR= 4.5  sigma= 0.5956621435290105\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 167us/sample - loss: 0.1318 - val_loss: 0.0091\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1295 - val_loss: 0.0083\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1306 - val_loss: 0.0077\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1295 - val_loss: 0.0074\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1314 - val_loss: 0.0072\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1323 - val_loss: 0.0070\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1293 - val_loss: 0.0070\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1308 - val_loss: 0.0068\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1309 - val_loss: 0.0067\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1304 - val_loss: 0.0067\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1312 - val_loss: 0.0067\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1304 - val_loss: 0.0067\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1289 - val_loss: 0.0066\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1312 - val_loss: 0.0065\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1296 - val_loss: 0.0064\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1314 - val_loss: 0.0065\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1320 - val_loss: 0.0065\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1293 - val_loss: 0.0065\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1295 - val_loss: 0.0063\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1318 - val_loss: 0.0065\n",
            "Training for SNR= 5.0  sigma= 0.5623413251903491\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 167us/sample - loss: 0.1080 - val_loss: 0.0056\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1071 - val_loss: 0.0051\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1065 - val_loss: 0.0047\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1070 - val_loss: 0.0045\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1084 - val_loss: 0.0045\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1081 - val_loss: 0.0043\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1080 - val_loss: 0.0041\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1060 - val_loss: 0.0040\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1067 - val_loss: 0.0039\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1057 - val_loss: 0.0038\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.1070 - val_loss: 0.0037\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1065 - val_loss: 0.0036\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1072 - val_loss: 0.0036\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1047 - val_loss: 0.0036\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1062 - val_loss: 0.0035\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1065 - val_loss: 0.0035\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1075 - val_loss: 0.0036\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.1078 - val_loss: 0.0035\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1048 - val_loss: 0.0035\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1078 - val_loss: 0.0035\n",
            "Training for SNR= 5.5  sigma= 0.5308844442309884\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 167us/sample - loss: 0.0882 - val_loss: 0.0031\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0864 - val_loss: 0.0028\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0889 - val_loss: 0.0026\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0868 - val_loss: 0.0024\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0867 - val_loss: 0.0024\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0849 - val_loss: 0.0023\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0853 - val_loss: 0.0022\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0872 - val_loss: 0.0022\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0850 - val_loss: 0.0021\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0868 - val_loss: 0.0021\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0857 - val_loss: 0.0020\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0870 - val_loss: 0.0020\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0843 - val_loss: 0.0019\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0834 - val_loss: 0.0019\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0854 - val_loss: 0.0019\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0855 - val_loss: 0.0019\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0877 - val_loss: 0.0019\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0887 - val_loss: 0.0019\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0859 - val_loss: 0.0019\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0858 - val_loss: 0.0019\n",
            "Training for SNR= 6.0  sigma= 0.5011872336272722\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 171us/sample - loss: 0.0664 - val_loss: 0.0016\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0687 - val_loss: 0.0015\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0681 - val_loss: 0.0014\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0677 - val_loss: 0.0013\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0686 - val_loss: 0.0013\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0655 - val_loss: 0.0012\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0680 - val_loss: 0.0012\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0674 - val_loss: 0.0011\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0672 - val_loss: 0.0011\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0668 - val_loss: 0.0011\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0666 - val_loss: 0.0011\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0657 - val_loss: 0.0010\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0705 - val_loss: 0.0010\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0684 - val_loss: 0.0010\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0671 - val_loss: 9.9996e-04\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0683 - val_loss: 0.0010\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0665 - val_loss: 9.8260e-04\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0689 - val_loss: 9.9751e-04\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0689 - val_loss: 9.7522e-04\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0653 - val_loss: 9.6581e-04\n",
            "Training for SNR= 6.5  sigma= 0.47315125896148047\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 173us/sample - loss: 0.0524 - val_loss: 8.7905e-04\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0513 - val_loss: 8.0123e-04\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0511 - val_loss: 7.5583e-04\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0515 - val_loss: 7.0333e-04\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0510 - val_loss: 6.7891e-04\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0506 - val_loss: 6.3360e-04\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0500 - val_loss: 6.0689e-04\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0524 - val_loss: 6.0110e-04\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0522 - val_loss: 5.8134e-04\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0517 - val_loss: 5.5941e-04\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0526 - val_loss: 5.3863e-04\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0518 - val_loss: 5.3140e-04\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0523 - val_loss: 5.3171e-04\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0520 - val_loss: 5.1864e-04\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0533 - val_loss: 5.1593e-04\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0526 - val_loss: 5.1223e-04\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0516 - val_loss: 5.1225e-04\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0519 - val_loss: 5.0288e-04\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0518 - val_loss: 4.8960e-04\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0504 - val_loss: 4.8256e-04\n",
            "Training for SNR= 7.0  sigma= 0.44668359215096315\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 175us/sample - loss: 0.0382 - val_loss: 4.4384e-04\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0395 - val_loss: 4.1318e-04\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0377 - val_loss: 3.8432e-04\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0370 - val_loss: 3.6381e-04\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0384 - val_loss: 3.4538e-04\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0378 - val_loss: 3.3165e-04\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0387 - val_loss: 3.1886e-04\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0385 - val_loss: 3.2045e-04\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0393 - val_loss: 3.0281e-04\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0384 - val_loss: 2.9487e-04\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0381 - val_loss: 2.8772e-04\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0370 - val_loss: 2.7876e-04\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0395 - val_loss: 2.7585e-04\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0390 - val_loss: 2.7399e-04\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0384 - val_loss: 2.6595e-04\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0389 - val_loss: 2.6212e-04\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0388 - val_loss: 2.5837e-04\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0391 - val_loss: 2.5690e-04\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0394 - val_loss: 2.5741e-04\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0379 - val_loss: 2.5248e-04\n",
            "Training for SNR= 7.5  sigma= 0.4216965034285822\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 176us/sample - loss: 0.0287 - val_loss: 2.2761e-04\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0274 - val_loss: 2.1682e-04\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0282 - val_loss: 2.0166e-04\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0290 - val_loss: 1.9820e-04\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0275 - val_loss: 1.8537e-04\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0290 - val_loss: 1.7815e-04\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0267 - val_loss: 1.7184e-04\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0292 - val_loss: 1.6849e-04\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0287 - val_loss: 1.7442e-04\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0277 - val_loss: 1.6255e-04\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0284 - val_loss: 1.6127e-04\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0276 - val_loss: 1.5767e-04\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0274 - val_loss: 1.5163e-04\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0281 - val_loss: 1.4623e-04\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0273 - val_loss: 1.4470e-04\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0283 - val_loss: 1.3824e-04\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0292 - val_loss: 1.3835e-04\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0269 - val_loss: 1.3895e-04\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0273 - val_loss: 1.3499e-04\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0285 - val_loss: 1.3730e-04\n",
            "Training for SNR= 8.0  sigma= 0.3981071705534972\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 180us/sample - loss: 0.0205 - val_loss: 1.2545e-04\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0199 - val_loss: 1.1837e-04\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0194 - val_loss: 1.1177e-04\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0193 - val_loss: 1.0706e-04\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0193 - val_loss: 1.0474e-04\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0182 - val_loss: 9.8581e-05\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0189 - val_loss: 9.6726e-05\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0188 - val_loss: 9.3858e-05\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0184 - val_loss: 8.7359e-05\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0190 - val_loss: 8.4735e-05\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0185 - val_loss: 8.1401e-05\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0185 - val_loss: 7.9739e-05\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0199 - val_loss: 7.9767e-05\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0184 - val_loss: 7.6273e-05\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0187 - val_loss: 7.4739e-05\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0191 - val_loss: 7.5322e-05\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0198 - val_loss: 7.0907e-05\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0192 - val_loss: 7.1554e-05\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0193 - val_loss: 6.7484e-05\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0195 - val_loss: 6.8604e-05\n",
            "Training for SNR= 8.5  sigma= 0.3758374042884442\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 179us/sample - loss: 0.0130 - val_loss: 6.5464e-05\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0134 - val_loss: 6.3122e-05\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0131 - val_loss: 6.1307e-05\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0124 - val_loss: 5.7942e-05\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0129 - val_loss: 5.5984e-05\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0128 - val_loss: 5.3208e-05\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0134 - val_loss: 5.1418e-05\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0129 - val_loss: 4.9860e-05\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0133 - val_loss: 4.9127e-05\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0130 - val_loss: 4.8842e-05\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0134 - val_loss: 4.6671e-05\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0131 - val_loss: 4.4770e-05\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0117 - val_loss: 4.4635e-05\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0132 - val_loss: 4.4309e-05\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0133 - val_loss: 4.2769e-05\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0134 - val_loss: 4.3136e-05\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0122 - val_loss: 4.2673e-05\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0127 - val_loss: 4.0582e-05\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0120 - val_loss: 3.9371e-05\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0131 - val_loss: 3.8414e-05\n",
            "Training for SNR= 9.0  sigma= 0.35481338923357547\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 179us/sample - loss: 0.0084 - val_loss: 3.7902e-05\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0081 - val_loss: 3.5923e-05\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0088 - val_loss: 3.4431e-05\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0091 - val_loss: 3.4087e-05\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0089 - val_loss: 3.2817e-05\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0079 - val_loss: 3.2116e-05\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0080 - val_loss: 3.0903e-05\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0081 - val_loss: 3.0518e-05\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0081 - val_loss: 2.9905e-05\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0080 - val_loss: 2.8762e-05\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0081 - val_loss: 2.8733e-05\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0079 - val_loss: 2.8641e-05\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0079 - val_loss: 2.6216e-05\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0082 - val_loss: 2.6487e-05\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0081 - val_loss: 2.5904e-05\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0087 - val_loss: 2.5213e-05\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0074 - val_loss: 2.4184e-05\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0077 - val_loss: 2.4421e-05\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0080 - val_loss: 2.3342e-05\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0088 - val_loss: 2.3520e-05\n",
            "Training for SNR= 9.5  sigma= 0.33496543915782767\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 186us/sample - loss: 0.0048 - val_loss: 2.2954e-05\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0055 - val_loss: 2.2089e-05\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0049 - val_loss: 2.0921e-05\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0055 - val_loss: 2.0538e-05\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0050 - val_loss: 2.0755e-05\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0051 - val_loss: 1.9190e-05\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0052 - val_loss: 1.9486e-05\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0048 - val_loss: 1.8234e-05\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0044 - val_loss: 1.7729e-05\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0048 - val_loss: 1.7457e-05\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0046 - val_loss: 1.6693e-05\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0052 - val_loss: 1.6427e-05\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0049 - val_loss: 1.6178e-05\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0046 - val_loss: 1.5961e-05\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0050 - val_loss: 1.5904e-05\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0054 - val_loss: 1.5434e-05\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0048 - val_loss: 1.5251e-05\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0049 - val_loss: 1.4642e-05\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0048 - val_loss: 1.4088e-05\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 10us/sample - loss: 0.0053 - val_loss: 1.4409e-05\n",
            "Training for SNR= 10.0  sigma= 0.31622776601683794\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 186us/sample - loss: 0.0031 - val_loss: 1.4258e-05\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0032 - val_loss: 1.3467e-05\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0027 - val_loss: 1.3076e-05\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0031 - val_loss: 1.2963e-05\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0030 - val_loss: 1.2206e-05\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0031 - val_loss: 1.2601e-05\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0033 - val_loss: 1.1728e-05\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0035 - val_loss: 1.1847e-05\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0030 - val_loss: 1.1287e-05\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0033 - val_loss: 1.1534e-05\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0030 - val_loss: 1.0921e-05\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0030 - val_loss: 1.0858e-05\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0030 - val_loss: 1.0713e-05\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0027 - val_loss: 1.0238e-05\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0028 - val_loss: 1.0543e-05\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0031 - val_loss: 9.8944e-06\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0032 - val_loss: 9.7508e-06\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0030 - val_loss: 9.8446e-06\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0029 - val_loss: 1.0274e-05\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0033 - val_loss: 9.6632e-06\n",
            "Training for SNR= 10.5  sigma= 0.29853826189179594\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 189us/sample - loss: 0.0017 - val_loss: 9.6335e-06\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0019 - val_loss: 9.2581e-06\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0018 - val_loss: 8.8930e-06\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0017 - val_loss: 9.3526e-06\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0015 - val_loss: 8.6143e-06\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0017 - val_loss: 8.5133e-06\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0017 - val_loss: 8.3094e-06\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0016 - val_loss: 8.3552e-06\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0016 - val_loss: 7.8596e-06\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0018 - val_loss: 7.8725e-06\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0018 - val_loss: 7.7077e-06\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0018 - val_loss: 7.6498e-06\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0016 - val_loss: 7.5758e-06\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0015 - val_loss: 7.6834e-06\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0019 - val_loss: 6.9636e-06\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0013 - val_loss: 7.1367e-06\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0017 - val_loss: 7.0650e-06\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0015 - val_loss: 6.8139e-06\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0017 - val_loss: 6.8198e-06\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0013 - val_loss: 6.9014e-06\n",
            "Training for SNR= 11.0  sigma= 0.28183829312644537\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 188us/sample - loss: 0.0011 - val_loss: 6.6943e-06\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 8.4647e-04 - val_loss: 6.5382e-06\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 8.4754e-04 - val_loss: 6.4562e-06\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 9.1043e-04 - val_loss: 6.3278e-06\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0012 - val_loss: 6.3571e-06\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 7.9429e-04 - val_loss: 6.1042e-06\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 9.0524e-04 - val_loss: 6.1345e-06\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 9.0014e-04 - val_loss: 6.0564e-06\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 8.7920e-04 - val_loss: 5.8743e-06\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 9.1271e-04 - val_loss: 5.7651e-06\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 8.1792e-04 - val_loss: 5.6711e-06\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 7.4923e-04 - val_loss: 5.4501e-06\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0010 - val_loss: 5.7006e-06\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 8.2754e-04 - val_loss: 5.3850e-06\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 8.1326e-04 - val_loss: 5.3017e-06\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0011 - val_loss: 5.5245e-06\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 7.2456e-04 - val_loss: 5.1957e-06\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 9.3052e-04 - val_loss: 5.1051e-06\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 8.8723e-04 - val_loss: 4.9960e-06\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 8.3788e-04 - val_loss: 4.9963e-06\n",
            "Training for SNR= 11.5  sigma= 0.26607250597988097\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 191us/sample - loss: 3.9279e-04 - val_loss: 4.8341e-06\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 3.3946e-04 - val_loss: 4.6047e-06\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 4.1548e-04 - val_loss: 4.6691e-06\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 4.8395e-04 - val_loss: 4.5625e-06\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 4.0301e-04 - val_loss: 4.4531e-06\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 4.5491e-04 - val_loss: 4.4192e-06\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 4.3691e-04 - val_loss: 4.3698e-06\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 4.9241e-04 - val_loss: 4.2458e-06\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 3.1607e-04 - val_loss: 4.3245e-06\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 5.5754e-04 - val_loss: 4.1383e-06\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 5.4793e-04 - val_loss: 3.9980e-06\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 5.4991e-04 - val_loss: 4.1006e-06\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 4.5773e-04 - val_loss: 3.9255e-06\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 5.0402e-04 - val_loss: 4.0479e-06\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 4.7434e-04 - val_loss: 3.8581e-06\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 4.0258e-04 - val_loss: 3.8937e-06\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 4.8628e-04 - val_loss: 3.7332e-06\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 5.2613e-04 - val_loss: 3.7889e-06\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 3.5948e-04 - val_loss: 4.0189e-06\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 4.7380e-04 - val_loss: 3.7489e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHByzQbTUqbv",
        "outputId": "9308975a-b532-48c3-be93-630d985a9166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using trained model\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_dl_tensor  = numpy.array(())\n",
        "times_per_iter_dl_tensor = numpy.array(())\n",
        "\n",
        "#awgn_channel_tx = GaussianNoise(0.5,input_shape=(CHANEL_SIZE,))\n",
        "\n",
        "#awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [CHANEL_SIZE])\n",
        "#awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "#awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "#awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)\n",
        "\n",
        "\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    input_message_xx = input_message [i:i+1]\n",
        "    #print (\"input\", input_message_xx)\n",
        "    encoded_message = encoder.predict(input_message_xx)\n",
        "    #encoded_message = numpy.around(encoded_message > 0.5).astype(int)\n",
        "    #print(encoded_message)\n",
        "    #print (\"encoded\", encoded_message)\n",
        "    #noised_message = awgn_channel.predict (encoded_message)\n",
        "    noised_message = commpy.channels.awgn(encoded_message, snr)\n",
        "    #awgn_channel = GaussianNoise(sigma,input_shape=(CHANEL_SIZE,))\n",
        "    #noised_message = awgn_channel.predict(encoded_message)\n",
        "    #noised_message = awgn_layer (encoded_message)\n",
        "    #awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:0.5, awgn_channel_input:encoded_message[0]})\n",
        "    #print(noised_message)\n",
        "    decoded_message = decoder.predict(noised_message)\n",
        "    decoded_message = numpy.around(decoded_message[0]).astype(int)\n",
        "    #decoded_message = numpy.around(decoded_message > 0.5).astype(int)\n",
        "    #print (\".\")\n",
        "    #autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))\n",
        "    #decoded_message = autoencoder.predict(input_message_xx)\n",
        "    #print (\"output\", decoded_message)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "      #print (\"Error\")\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_dl_tensor=numpy.append(ber_per_iter_dl_tensor ,ber)\n",
        "  times_per_iter_dl_tensor=numpy.append(times_per_iter_dl_tensor, total_time)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.36s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 3.04s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 4.67s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 6.04s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.67\n",
            " -> Total Time: 15.11s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.44s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.83s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 4.14s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 5.57s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.66\n",
            " -> Total Time: 13.98s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 1.35s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 2.73s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 4.13s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 5.59s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.67\n",
            " -> Total Time: 13.79s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 1.57s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 3.04s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 4.31s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 5.96s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.65\n",
            " -> Total Time: 14.88s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 2.01s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 3.94s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 5.73s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 7.66s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.58\n",
            " -> Total Time: 19.33s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 2.68s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 5.19s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 7.64s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 9.56s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.61\n",
            " -> Total Time: 25.06s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 1.54s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 3.10s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 4.72s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 6.28s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.60\n",
            " -> Total Time: 15.64s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 1.48s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 3.08s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 4.57s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 6.07s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.56\n",
            " -> Total Time: 15.20s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 1.66s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 3.27s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 4.77s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 6.16s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.51\n",
            " -> Total Time: 15.86s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 1.47s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 3.05s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 4.63s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 6.20s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.51\n",
            " -> Total Time: 15.34s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 1.38s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 2.93s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 4.40s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 5.87s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.50\n",
            " -> Total Time: 14.59s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 1.47s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 3.01s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 4.62s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 6.00s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.48\n",
            " -> Total Time: 15.10s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 1.50s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 3.03s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 4.39s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 5.81s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.44\n",
            " -> Total Time: 14.73s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 1.39s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 3.06s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 4.56s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 6.09s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.44\n",
            " -> Total Time: 15.10s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 1.45s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 2.97s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 4.48s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 6.07s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.43\n",
            " -> Total Time: 14.98s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 1.43s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 2.96s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 4.45s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 6.08s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.38\n",
            " -> Total Time: 14.92s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 1.44s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 2.88s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 4.35s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 5.99s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.37\n",
            " -> Total Time: 14.67s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 1.57s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 3.15s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 4.61s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 6.09s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.32\n",
            " -> Total Time: 15.42s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 1.50s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 2.99s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 4.48s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 5.80s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.32\n",
            " -> Total Time: 14.76s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 1.40s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 2.83s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 4.45s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 6.08s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.28\n",
            " -> Total Time: 14.76s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syUQij3fuxRm",
        "outputId": "7219389d-c606-43a6-ecd7-781e7a5f8345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1) = plt.subplots(1,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"ldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor,'', label=\"ai-dl\") # plot BER vs SNR\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "#ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "#ax2.set_xlabel('$E_b/$N_0$')\n",
        "#ax2.set_ylabel('Decoding Time [s]')\n",
        "#ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "#            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "#            xy=(1, 0.35), xycoords='axes fraction',\n",
        "#            xytext=(-20, 20), textcoords='offset pixels',\n",
        "#            horizontalalignment='right',\n",
        "#            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}