{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MainModelKeras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamviji/project/blob/master/MainModelKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku11kjUKaO8X"
      },
      "source": [
        "Note:To Checkin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDSPPMfZ9czi",
        "outputId": "86a95789-716e-4bba-bb96-e559b8cf7d20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        }
      },
      "source": [
        "!rm -rf project\n",
        "!git clone https://github.com/iamviji/project.git\n",
        "!ls\n",
        "!ls project\n",
        "!pip install pyldpc\n",
        "!pip install scikit-commpy\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'project'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 53 (delta 11), reused 8 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (53/53), done.\n",
            "project  sample_data\n",
            "MainModel.ipynb  MainModelWithSingleBERTraining.ipynb  README.md  util.py\n",
            "Collecting pyldpc\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/aa/fd5495869c7106a638ae71aa497d7d266cae7f2a343d1f6a9d0e3a986e1e/pyldpc-0.7.9.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyldpc) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyldpc) (1.4.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from pyldpc) (0.48.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->pyldpc) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->pyldpc) (50.3.0)\n",
            "Building wheels for collected packages: pyldpc\n",
            "  Building wheel for pyldpc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyldpc: filename=pyldpc-0.7.9-cp36-none-any.whl size=14306 sha256=1d3b99a833677302be0cd37edbdf24e348cc7e02b3ca22dfa8bfbab9ec8f2fb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/7a/10/e94058ba8b0b6d98bf2719226d18d3dd6056525ad7b984c068\n",
            "Successfully built pyldpc\n",
            "Installing collected packages: pyldpc\n",
            "Successfully installed pyldpc-0.7.9\n",
            "Collecting scikit-commpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/b4/f7fa5bc8864e0ddbd3e7a2290b624b92690f53523474024915c33321802d/scikit_commpy-0.5.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->scikit-commpy) (1.15.0)\n",
            "Installing collected packages: scikit-commpy\n",
            "Successfully installed scikit-commpy-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QOuLqpdDgx2"
      },
      "source": [
        "import pyldpc\n",
        "import commpy\n",
        "import numpy \n",
        "import time\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior ()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YClXJbbr0lc7"
      },
      "source": [
        "SNR_BEGIN = 0\n",
        "SNR_END = 10\n",
        "SNR_STEP_SIZE = 0.5\n",
        "CHANEL_SIZE = 18\n",
        "NUM_OF_INPUT_MESSAGE = 1000\n",
        "LDPC_MAX_ITER = 100\n",
        "num_parity_check = 3\n",
        "num_bits_in_parity_check = 6 \n",
        "input_message_length =  0 # Caculated by channel encoder and initialized later"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvUzIMsB43i0"
      },
      "source": [
        "def timer_update(i,current,time_tot,tic_incr=500):\n",
        "    last = current\n",
        "    current = time.time()\n",
        "    t_diff = current-last\n",
        "    print('SNR: {:04.3f} - Iter: {} - Last {} iterations took {:03.2f}s'.format(snr,i+1,tic_incr,t_diff))\n",
        "    return time_tot + t_diff\n",
        "\n",
        "def Snr2Sigma(snr):\n",
        "  sigma = 10 ** (- snr / 20)\n",
        "  return sigma\n",
        "\n",
        "def pyldpc_encode (CodingMatrix, message):\n",
        "  rng = pyldpc.utils.check_random_state(seed=None)\n",
        "  d = pyldpc.utils.binaryproduct(CodingMatrix, message)\n",
        "  encoded_message = (-1) ** d\n",
        "  return encoded_message\n",
        "\n",
        "def pyldpc_decode (ParityCheckMatrix, CodingMatrix, message, snr, maxiter):\n",
        "  decoded_msg = pyldpc.decode(ParityCheckMatrix, message, snr, maxiter)\n",
        "  out_message = pyldpc.get_message(CodingMatrix, decoded_msg)\n",
        "  return out_message\n",
        "\n",
        "awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [CHANEL_SIZE])\n",
        "awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)\n",
        "\n",
        "init = tf.global_variables_initializer ()\n",
        "sess = tf.Session ()\n",
        "sess.run(init)\n",
        "\n",
        "def AWGNChannelOutput (xx, snr , s):\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  awgn_channel_output_message = s.run ([awgn_channel_output], feed_dict={noise_std_dev:sigma, channel_input:xx})\n",
        "  return awgn_channel_output_message"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jMQG-MZ_pXu",
        "outputId": "fe295c91-8abc-42db-81eb-e64235aca75e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "\n",
        "ParityCheckMatrix, CodingMatrix = pyldpc.make_ldpc(CHANEL_SIZE, num_parity_check, num_bits_in_parity_check, systematic=True, sparse=True)\n",
        "input_message_length = CodingMatrix.shape[1]\n",
        "print (\"input_message_size=\", input_message_length, \"channel_size=\",CHANEL_SIZE)\n",
        "print (\"input_message_size=\", CodingMatrix.shape[1], \"channel_size=\",CodingMatrix.shape[0])\n",
        "input_message = numpy.random.randint(2, size=(NUM_OF_INPUT_MESSAGE,input_message_length))\n",
        "print (input_message)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_message_size= 11 channel_size= 18\n",
            "input_message_size= 11 channel_size= 18\n",
            "[[0 1 0 ... 1 0 1]\n",
            " [1 0 1 ... 0 1 1]\n",
            " [0 1 1 ... 0 1 0]\n",
            " ...\n",
            " [0 1 0 ... 1 0 1]\n",
            " [1 1 1 ... 0 0 0]\n",
            " [0 1 1 ... 1 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WKg2HU2adgZ"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fL8ptL4aeOY"
      },
      "source": [
        "This section tries to compare BER and Time performance of PYLDPC in following 3 cases\n",
        "1. SNR Noise function provided in encoder function of pyldpc library (pyldpc.encode)\n",
        "2. SNR Noise function provided by commpy library (commpy.channels.awgn) \n",
        "3. SNR Noise function implemented using tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma5zUqFv0TH2",
        "outputId": "f4ca2699-7935-4ad4-f67d-d0ea31045865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using tensor flow based AWGN, to make sure that effect of it is same as AWGN\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_tensor  = numpy.array(())\n",
        "times_per_iter_tensor = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc_encode (CodingMatrix, input_message[i])\n",
        "    sigma = Snr2Sigma (snr)\n",
        "    awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message})[0]\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      #print (\"count=\",abs(decoded_message-input_message[i]).sum())\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_tensor=numpy.append(ber_per_iter_tensor ,ber)\n",
        "  times_per_iter_tensor=numpy.append(times_per_iter_tensor, total_time)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.87s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 3.83s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 5.72s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 7.64s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.61\n",
            " -> Total Time: 19.06s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.65s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 3.14s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 4.68s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 6.15s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.53\n",
            " -> Total Time: 15.62s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 1.33s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 2.49s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 3.93s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 5.15s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.45\n",
            " -> Total Time: 12.90s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 1.22s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 2.26s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 3.34s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 4.43s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.39\n",
            " -> Total Time: 11.25s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.88s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.80s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 2.72s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 3.67s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.32\n",
            " -> Total Time: 9.07s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.69s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.42s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 2.09s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.76s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.23\n",
            " -> Total Time: 6.96s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.58s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 1.21s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.76s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 2.33s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.15\n",
            " -> Total Time: 5.88s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.61s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 1.17s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.62s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 2.18s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.13\n",
            " -> Total Time: 5.57s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.41s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.82s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 1.21s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.66s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.08\n",
            " -> Total Time: 4.11s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.36s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.72s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 1.14s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.51s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.05\n",
            " -> Total Time: 3.73s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.41s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.76s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 1.18s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 1.55s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 3.89s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.33s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.65s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.98s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 1.39s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 3.35s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.34s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.65s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.97s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 1.31s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 3.26s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.32s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.67s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 1.00s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 1.32s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.31s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.32s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.63s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.93s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 1.24s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.13s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.32s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.65s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.97s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 1.28s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.22s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.32s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.62s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.94s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 1.25s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.13s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.31s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.61s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.92s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 1.27s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.10s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.30s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.62s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.92s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 1.24s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.08s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.31s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.61s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.92s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 1.22s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.06s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8dIFLg76c7O",
        "outputId": "fb8c75e4-e470-42ee-c15b-9bac36f36dfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using commpy based AWGN \n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_awgn  = numpy.array(())\n",
        "times_per_iter_awgn = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc_encode (CodingMatrix, input_message[i])\n",
        "    awgn_channel_output_message = commpy.channels.awgn(encoded_message, snr)\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_awgn=numpy.append(ber_per_iter_awgn ,ber)\n",
        "  times_per_iter_awgn=numpy.append(times_per_iter_awgn, total_time)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.76s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 3.52s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 5.26s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 6.89s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.62\n",
            " -> Total Time: 17.42s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.47s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.86s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 4.40s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 5.81s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.54\n",
            " -> Total Time: 14.54s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 1.16s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 2.31s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 3.43s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 4.79s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.44\n",
            " -> Total Time: 11.69s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.85s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.82s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.64s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 3.53s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.34\n",
            " -> Total Time: 8.84s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.69s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.52s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 2.26s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.97s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.32\n",
            " -> Total Time: 7.44s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.69s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.24s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.86s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.46s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.23\n",
            " -> Total Time: 6.25s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.51s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 1.03s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.56s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.90s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.18\n",
            " -> Total Time: 5.00s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.42s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.71s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.13s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.53s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.12\n",
            " -> Total Time: 3.79s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.30s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.62s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.95s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.27s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.08\n",
            " -> Total Time: 3.14s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.81s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.08s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.05\n",
            " -> Total Time: 2.62s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.05\n",
            " -> Total Time: 2.50s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.22s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.47s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 2.38s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.21s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.44s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.67s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.86s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.18s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.20s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.38s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.58s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.77s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.94s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.38s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.57s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.75s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.91s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.20s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.39s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.58s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.77s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.94s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.22s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.42s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.60s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.80s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.04s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.37s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.56s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.76s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.89s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.38s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.57s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.75s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.89s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.20s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.38s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.57s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.76s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.92s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ihPKJJk7Jj9",
        "outputId": "5e240ae5-fb76-43be-b54d-716a65b13f09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using tensor flow based AWGN, to make sure that effect of it is same as AWGN\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_pyldpc  = numpy.array(())\n",
        "times_per_iter_pyldpc = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc.encode (CodingMatrix, input_message[i], snr)\n",
        "    awgn_channel_output_message = encoded_message\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_pyldpc=numpy.append(ber_per_iter_pyldpc ,ber)\n",
        "  times_per_iter_pyldpc=numpy.append(times_per_iter_pyldpc, total_time)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.83s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 3.64s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 5.48s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 7.31s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.64\n",
            " -> Total Time: 18.26s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.37s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.90s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 4.51s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 5.92s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.55\n",
            " -> Total Time: 14.70s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 1.14s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 2.14s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 3.37s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 4.64s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.46\n",
            " -> Total Time: 11.29s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.95s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.72s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.76s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 3.54s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.35\n",
            " -> Total Time: 8.96s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.63s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.33s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 2.06s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.77s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.31\n",
            " -> Total Time: 6.79s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.69s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.38s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.91s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.50s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.22\n",
            " -> Total Time: 6.47s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.54s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.98s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.35s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.73s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.16\n",
            " -> Total Time: 4.60s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.30s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.71s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.11s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.59s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.14\n",
            " -> Total Time: 3.71s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.28s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.59s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.86s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.16s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.07\n",
            " -> Total Time: 2.88s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.31s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.55s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.84s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.08s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.05\n",
            " -> Total Time: 2.78s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.23s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.46s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.69s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.92s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 2.30s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.43s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.69s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 2.27s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.22s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.42s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.62s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.84s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.10s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.21s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.40s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.61s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.86s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.07s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.20s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.43s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.63s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.81s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.06s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.38s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.57s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.76s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.89s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.18s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.37s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.56s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.74s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.86s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.20s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.39s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.57s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.80s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.96s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.39s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.56s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.75s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.89s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.18s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.37s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.55s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.74s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.84s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR4-FOJ-BkAG",
        "outputId": "7ce861d8-d49e-4fd8-e72f-53e01e7a56c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "# Compare 3 AWGN(Tensorflow, CommPy, PYLDPC) Simulation on LDPC\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_pyldpc,'', label=\"pyldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"tensor\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_awgn,'', label=\"commpy-awgn\") # plot BER vs SNR\n",
        "\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "ax2.set_xlabel('$E_b/$N_0$')\n",
        "ax2.set_ylabel('Decoding Time [s]')\n",
        "ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "            xy=(1, 0.35), xycoords='axes fraction',\n",
        "            xytext=(-20, 20), textcoords='offset pixels',\n",
        "            horizontalalignment='right',\n",
        "            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBl0pWc1oF_r",
        "outputId": "61272fdf-a404-4b4f-8dff-3379ae612a18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "training_input_message = numpy.random.randint(2, size=(NUM_OF_INPUT_MESSAGE*10,input_message_length))\n",
        "print (training_input_message)\n",
        "print (len(training_input_message))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 ... 1 1 1]\n",
            " [1 1 1 ... 0 0 1]\n",
            " [1 1 1 ... 1 1 0]\n",
            " ...\n",
            " [0 1 0 ... 1 1 0]\n",
            " [0 0 1 ... 1 0 1]\n",
            " [0 1 0 ... 0 1 1]]\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggY5VwudaRwR"
      },
      "source": [
        "<B>Conclussion:</B>\n",
        "      It proved that tensorflow behaves similar to AWGN noise channel provided by pyldpc, commpy. But tensor flow based one takes adds little more time delay. This need to be offseted if we are comparing performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOeuNfeLCgfb"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, GaussianNoise\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior ()\n",
        "\n",
        "input_message_x = Input(shape=(input_message_length,))\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "enc_layer1 = Dense(CHANEL_SIZE, activation='tanh')(input_message_x)\n",
        "enc_layer2 = Dense(CHANEL_SIZE, activation='sigmoid')(enc_layer1)\n",
        "#encoded2 = Dense(CHANEL_SIZE, activation='sigmoid')(encoded1)\n",
        "# this model maps an input to its encoded representation\n",
        "enc_layer3 =  enc_layer2 / tf.sqrt(tf.reduce_mean(tf.square(enc_layer2)))\n",
        "#enc_layer2 = tf.round(enc_layer1)\n",
        "encoder = Model(input_message_x, enc_layer3)\n",
        "\n",
        "awgn_channel = GaussianNoise(Snr2Sigma(7.0),input_shape=(CHANEL_SIZE,))\n",
        "\n",
        "# create a placeholder for an encoded (32-dimensional) input\n",
        "encoded_input = Input(shape=(CHANEL_SIZE,))\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "dec_layer1 = Dense(CHANEL_SIZE, activation='tanh')(encoded_input)\n",
        "dec_layer2 = Dense(input_message_length, activation='sigmoid')(dec_layer1)\n",
        "# this model maps an encoded input to its decoder representation\n",
        "decoder = Model(encoded_input, dec_layer2)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgXpqxjrnJ-F",
        "outputId": "72823659-dde6-4940-9bc7-bfe8eb29d174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        }
      },
      "source": [
        "print(encoder.summary())\n",
        "print(decoder.summary())\n",
        "print(autoencoder.summary())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 11)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 18)           216         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 18)           342         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Square (TensorFlowO multiple             0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mean (TensorFlowOpL multiple             0           tf_op_layer_Square[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sqrt (TensorFlowOpL multiple             0           tf_op_layer_Mean[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_truediv (TensorFlow multiple             0           dense_1[0][0]                    \n",
            "                                                                 tf_op_layer_Sqrt[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 558\n",
            "Trainable params: 558\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 18)]              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 18)                342       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 11)                209       \n",
            "=================================================================\n",
            "Total params: 551\n",
            "Trainable params: 551\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 11)]              0         \n",
            "_________________________________________________________________\n",
            "functional_1 (Functional)    (None, 18)                558       \n",
            "_________________________________________________________________\n",
            "gaussian_noise (GaussianNois (None, 18)                0         \n",
            "_________________________________________________________________\n",
            "functional_3 (Functional)    (None, 11)                551       \n",
            "=================================================================\n",
            "Total params: 1,109\n",
            "Trainable params: 1,109\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOXLOYLu8aML",
        "outputId": "0d52a5a9-c85d-475c-829b-6671c73b56e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import keras\n",
        "\n",
        "#def custom_losff_fucntion (act, pred):\n",
        "#  return (tf.reduce_mean(-1*(act * tf.log(pred) + (1-act)*tf.log(1-pred))))\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "autoencoder.compile(optimizer=opt, loss='binary_crossentropy')\n",
        "#autoencoder.compile(optimizer=opt, loss=custom_losff_fucntion)\n",
        "#loss='mean_squared_error'\n",
        "#for snr in (numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)):\n",
        "for snr in (numpy.arange (0, 3, SNR_STEP_SIZE)):\n",
        "  sigma = 1.0*Snr2Sigma (snr)\n",
        "  print (\"Training for SNR=\", snr, \" sigma=\", sigma) \n",
        "  #awgn_channel = GaussianNoise(sigma,input_shape=(CHANEL_SIZE,))\n",
        "  #autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))\n",
        "  #autoencoder.compile(optimizer=opt, loss='mean_squared_error')\n",
        "  autoencoder.fit(training_input_message, training_input_message,\n",
        "                #epochs=50, original\n",
        "                epochs=100,\n",
        "                batch_size=500,\n",
        "                shuffle=False,\n",
        "                validation_data=(input_message, input_message))\n",
        "  "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for SNR= 0.0  sigma= 1.0\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "10000/10000 [==============================] - ETA: 0s - loss: 0.7327WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "10000/10000 [==============================] - 0s 9us/sample - loss: 0.7327 - val_loss: 0.6909\n",
            "Epoch 2/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.6927 - val_loss: 0.6640\n",
            "Epoch 3/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.6722 - val_loss: 0.6448\n",
            "Epoch 4/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.6563 - val_loss: 0.6235\n",
            "Epoch 5/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.6363 - val_loss: 0.6000\n",
            "Epoch 6/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.6172 - val_loss: 0.5763\n",
            "Epoch 7/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.5964 - val_loss: 0.5542\n",
            "Epoch 8/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.5790 - val_loss: 0.5337\n",
            "Epoch 9/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.5612 - val_loss: 0.5140\n",
            "Epoch 10/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.5446 - val_loss: 0.4947\n",
            "Epoch 11/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.5288 - val_loss: 0.4755\n",
            "Epoch 12/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.5110 - val_loss: 0.4561\n",
            "Epoch 13/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.4963 - val_loss: 0.4366\n",
            "Epoch 14/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.4801 - val_loss: 0.4171\n",
            "Epoch 15/100\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.4650 - val_loss: 0.3981\n",
            "Epoch 16/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.4484 - val_loss: 0.3801\n",
            "Epoch 17/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.4350 - val_loss: 0.3639\n",
            "Epoch 18/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.4218 - val_loss: 0.3491\n",
            "Epoch 19/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.4090 - val_loss: 0.3357\n",
            "Epoch 20/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.3985 - val_loss: 0.3232\n",
            "Epoch 21/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.3894 - val_loss: 0.3117\n",
            "Epoch 22/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.3788 - val_loss: 0.3007\n",
            "Epoch 23/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.3702 - val_loss: 0.2905\n",
            "Epoch 24/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.3629 - val_loss: 0.2812\n",
            "Epoch 25/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.3534 - val_loss: 0.2721\n",
            "Epoch 26/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.3469 - val_loss: 0.2635\n",
            "Epoch 27/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.3400 - val_loss: 0.2552\n",
            "Epoch 28/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.3331 - val_loss: 0.2473\n",
            "Epoch 29/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.3275 - val_loss: 0.2399\n",
            "Epoch 30/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.3202 - val_loss: 0.2330\n",
            "Epoch 31/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.3163 - val_loss: 0.2267\n",
            "Epoch 32/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.3100 - val_loss: 0.2213\n",
            "Epoch 33/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.3066 - val_loss: 0.2165\n",
            "Epoch 34/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.3027 - val_loss: 0.2121\n",
            "Epoch 35/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2994 - val_loss: 0.2083\n",
            "Epoch 36/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.2966 - val_loss: 0.2048\n",
            "Epoch 37/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2940 - val_loss: 0.2016\n",
            "Epoch 38/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.2923 - val_loss: 0.1987\n",
            "Epoch 39/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2891 - val_loss: 0.1958\n",
            "Epoch 40/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2882 - val_loss: 0.1930\n",
            "Epoch 41/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.2850 - val_loss: 0.1907\n",
            "Epoch 42/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2835 - val_loss: 0.1883\n",
            "Epoch 43/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.2803 - val_loss: 0.1862\n",
            "Epoch 44/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2797 - val_loss: 0.1840\n",
            "Epoch 45/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.2769 - val_loss: 0.1822\n",
            "Epoch 46/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2762 - val_loss: 0.1801\n",
            "Epoch 47/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2745 - val_loss: 0.1781\n",
            "Epoch 48/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2731 - val_loss: 0.1763\n",
            "Epoch 49/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2711 - val_loss: 0.1746\n",
            "Epoch 50/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2691 - val_loss: 0.1730\n",
            "Epoch 51/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.2691 - val_loss: 0.1714\n",
            "Epoch 52/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2668 - val_loss: 0.1698\n",
            "Epoch 53/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2670 - val_loss: 0.1684\n",
            "Epoch 54/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.2651 - val_loss: 0.1671\n",
            "Epoch 55/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2619 - val_loss: 0.1657\n",
            "Epoch 56/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.2604 - val_loss: 0.1638\n",
            "Epoch 57/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.2602 - val_loss: 0.1623\n",
            "Epoch 58/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.2606 - val_loss: 0.1609\n",
            "Epoch 59/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.2578 - val_loss: 0.1596\n",
            "Epoch 60/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2573 - val_loss: 0.1578\n",
            "Epoch 61/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2562 - val_loss: 0.1562\n",
            "Epoch 62/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2531 - val_loss: 0.1548\n",
            "Epoch 63/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.2511 - val_loss: 0.1532\n",
            "Epoch 64/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2504 - val_loss: 0.1515\n",
            "Epoch 65/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2481 - val_loss: 0.1493\n",
            "Epoch 66/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.2478 - val_loss: 0.1470\n",
            "Epoch 67/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2439 - val_loss: 0.1445\n",
            "Epoch 68/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2426 - val_loss: 0.1420\n",
            "Epoch 69/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2408 - val_loss: 0.1388\n",
            "Epoch 70/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.2388 - val_loss: 0.1355\n",
            "Epoch 71/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2343 - val_loss: 0.1321\n",
            "Epoch 72/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2298 - val_loss: 0.1285\n",
            "Epoch 73/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2294 - val_loss: 0.1253\n",
            "Epoch 74/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.2245 - val_loss: 0.1216\n",
            "Epoch 75/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2201 - val_loss: 0.1190\n",
            "Epoch 76/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2203 - val_loss: 0.1166\n",
            "Epoch 77/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2173 - val_loss: 0.1146\n",
            "Epoch 78/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.2166 - val_loss: 0.1130\n",
            "Epoch 79/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.2137 - val_loss: 0.1115\n",
            "Epoch 80/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2136 - val_loss: 0.1098\n",
            "Epoch 81/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.2120 - val_loss: 0.1085\n",
            "Epoch 82/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.2114 - val_loss: 0.1073\n",
            "Epoch 83/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2097 - val_loss: 0.1059\n",
            "Epoch 84/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.2084 - val_loss: 0.1045\n",
            "Epoch 85/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2067 - val_loss: 0.1035\n",
            "Epoch 86/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2064 - val_loss: 0.1025\n",
            "Epoch 87/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2037 - val_loss: 0.1017\n",
            "Epoch 88/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2044 - val_loss: 0.1006\n",
            "Epoch 89/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2025 - val_loss: 0.0994\n",
            "Epoch 90/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2027 - val_loss: 0.0985\n",
            "Epoch 91/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2025 - val_loss: 0.0977\n",
            "Epoch 92/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.2016 - val_loss: 0.0969\n",
            "Epoch 93/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.2002 - val_loss: 0.0961\n",
            "Epoch 94/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1984 - val_loss: 0.0950\n",
            "Epoch 95/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1980 - val_loss: 0.0942\n",
            "Epoch 96/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1986 - val_loss: 0.0936\n",
            "Epoch 97/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1974 - val_loss: 0.0928\n",
            "Epoch 98/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1963 - val_loss: 0.0921\n",
            "Epoch 99/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1940 - val_loss: 0.0910\n",
            "Epoch 100/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1923 - val_loss: 0.0904\n",
            "Training for SNR= 0.5  sigma= 0.9440608762859234\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1941 - val_loss: 0.0898\n",
            "Epoch 2/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1923 - val_loss: 0.0889\n",
            "Epoch 3/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1924 - val_loss: 0.0886\n",
            "Epoch 4/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1907 - val_loss: 0.0876\n",
            "Epoch 5/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1896 - val_loss: 0.0870\n",
            "Epoch 6/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1892 - val_loss: 0.0866\n",
            "Epoch 7/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1876 - val_loss: 0.0857\n",
            "Epoch 8/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1870 - val_loss: 0.0853\n",
            "Epoch 9/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1869 - val_loss: 0.0845\n",
            "Epoch 10/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1861 - val_loss: 0.0840\n",
            "Epoch 11/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1835 - val_loss: 0.0830\n",
            "Epoch 12/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1848 - val_loss: 0.0825\n",
            "Epoch 13/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1830 - val_loss: 0.0819\n",
            "Epoch 14/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1829 - val_loss: 0.0813\n",
            "Epoch 15/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1825 - val_loss: 0.0808\n",
            "Epoch 16/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1805 - val_loss: 0.0801\n",
            "Epoch 17/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.1809 - val_loss: 0.0793\n",
            "Epoch 18/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1795 - val_loss: 0.0789\n",
            "Epoch 19/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.1770 - val_loss: 0.0783\n",
            "Epoch 20/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1775 - val_loss: 0.0777\n",
            "Epoch 21/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1778 - val_loss: 0.0774\n",
            "Epoch 22/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1753 - val_loss: 0.0766\n",
            "Epoch 23/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1756 - val_loss: 0.0759\n",
            "Epoch 24/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1737 - val_loss: 0.0755\n",
            "Epoch 25/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1748 - val_loss: 0.0751\n",
            "Epoch 26/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1737 - val_loss: 0.0746\n",
            "Epoch 27/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1738 - val_loss: 0.0741\n",
            "Epoch 28/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.1726 - val_loss: 0.0738\n",
            "Epoch 29/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1698 - val_loss: 0.0732\n",
            "Epoch 30/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1734 - val_loss: 0.0728\n",
            "Epoch 31/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1710 - val_loss: 0.0722\n",
            "Epoch 32/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1688 - val_loss: 0.0718\n",
            "Epoch 33/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1674 - val_loss: 0.0711\n",
            "Epoch 34/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1687 - val_loss: 0.0708\n",
            "Epoch 35/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.1682 - val_loss: 0.0705\n",
            "Epoch 36/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1665 - val_loss: 0.0700\n",
            "Epoch 37/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1680 - val_loss: 0.0696\n",
            "Epoch 38/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1654 - val_loss: 0.0692\n",
            "Epoch 39/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1638 - val_loss: 0.0687\n",
            "Epoch 40/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1639 - val_loss: 0.0684\n",
            "Epoch 41/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1636 - val_loss: 0.0680\n",
            "Epoch 42/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1648 - val_loss: 0.0678\n",
            "Epoch 43/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1626 - val_loss: 0.0673\n",
            "Epoch 44/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1614 - val_loss: 0.0670\n",
            "Epoch 45/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1618 - val_loss: 0.0667\n",
            "Epoch 46/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1630 - val_loss: 0.0663\n",
            "Epoch 47/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1608 - val_loss: 0.0663\n",
            "Epoch 48/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1598 - val_loss: 0.0657\n",
            "Epoch 49/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1599 - val_loss: 0.0653\n",
            "Epoch 50/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1593 - val_loss: 0.0651\n",
            "Epoch 51/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1582 - val_loss: 0.0645\n",
            "Epoch 52/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1593 - val_loss: 0.0643\n",
            "Epoch 53/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1572 - val_loss: 0.0639\n",
            "Epoch 54/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1571 - val_loss: 0.0640\n",
            "Epoch 55/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1580 - val_loss: 0.0635\n",
            "Epoch 56/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1561 - val_loss: 0.0630\n",
            "Epoch 57/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1554 - val_loss: 0.0628\n",
            "Epoch 58/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1549 - val_loss: 0.0628\n",
            "Epoch 59/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1554 - val_loss: 0.0624\n",
            "Epoch 60/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.1540 - val_loss: 0.0621\n",
            "Epoch 61/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1538 - val_loss: 0.0620\n",
            "Epoch 62/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1539 - val_loss: 0.0617\n",
            "Epoch 63/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1530 - val_loss: 0.0613\n",
            "Epoch 64/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.1506 - val_loss: 0.0612\n",
            "Epoch 65/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1536 - val_loss: 0.0609\n",
            "Epoch 66/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1512 - val_loss: 0.0607\n",
            "Epoch 67/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1505 - val_loss: 0.0604\n",
            "Epoch 68/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1510 - val_loss: 0.0602\n",
            "Epoch 69/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1505 - val_loss: 0.0599\n",
            "Epoch 70/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1483 - val_loss: 0.0598\n",
            "Epoch 71/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1495 - val_loss: 0.0595\n",
            "Epoch 72/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1506 - val_loss: 0.0594\n",
            "Epoch 73/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1473 - val_loss: 0.0592\n",
            "Epoch 74/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1494 - val_loss: 0.0591\n",
            "Epoch 75/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1483 - val_loss: 0.0587\n",
            "Epoch 76/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1489 - val_loss: 0.0586\n",
            "Epoch 77/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1465 - val_loss: 0.0584\n",
            "Epoch 78/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1458 - val_loss: 0.0582\n",
            "Epoch 79/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1464 - val_loss: 0.0581\n",
            "Epoch 80/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.1450 - val_loss: 0.0578\n",
            "Epoch 81/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1464 - val_loss: 0.0577\n",
            "Epoch 82/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1451 - val_loss: 0.0575\n",
            "Epoch 83/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1439 - val_loss: 0.0575\n",
            "Epoch 84/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1447 - val_loss: 0.0571\n",
            "Epoch 85/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1445 - val_loss: 0.0570\n",
            "Epoch 86/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1435 - val_loss: 0.0568\n",
            "Epoch 87/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1427 - val_loss: 0.0565\n",
            "Epoch 88/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1421 - val_loss: 0.0564\n",
            "Epoch 89/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1430 - val_loss: 0.0561\n",
            "Epoch 90/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1418 - val_loss: 0.0559\n",
            "Epoch 91/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1406 - val_loss: 0.0556\n",
            "Epoch 92/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1401 - val_loss: 0.0554\n",
            "Epoch 93/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1405 - val_loss: 0.0552\n",
            "Epoch 94/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1397 - val_loss: 0.0549\n",
            "Epoch 95/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.1395 - val_loss: 0.0547\n",
            "Epoch 96/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1386 - val_loss: 0.0546\n",
            "Epoch 97/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1391 - val_loss: 0.0544\n",
            "Epoch 98/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1378 - val_loss: 0.0541\n",
            "Epoch 99/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.1365 - val_loss: 0.0535\n",
            "Epoch 100/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1381 - val_loss: 0.0532\n",
            "Training for SNR= 1.0  sigma= 0.8912509381337456\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1382 - val_loss: 0.0531\n",
            "Epoch 2/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1371 - val_loss: 0.0526\n",
            "Epoch 3/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1359 - val_loss: 0.0522\n",
            "Epoch 4/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1351 - val_loss: 0.0519\n",
            "Epoch 5/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1353 - val_loss: 0.0517\n",
            "Epoch 6/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1334 - val_loss: 0.0511\n",
            "Epoch 7/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1342 - val_loss: 0.0506\n",
            "Epoch 8/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1316 - val_loss: 0.0501\n",
            "Epoch 9/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1339 - val_loss: 0.0494\n",
            "Epoch 10/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1337 - val_loss: 0.0489\n",
            "Epoch 11/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1314 - val_loss: 0.0484\n",
            "Epoch 12/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1293 - val_loss: 0.0474\n",
            "Epoch 13/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1305 - val_loss: 0.0468\n",
            "Epoch 14/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1291 - val_loss: 0.0459\n",
            "Epoch 15/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.1280 - val_loss: 0.0450\n",
            "Epoch 16/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1282 - val_loss: 0.0440\n",
            "Epoch 17/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1271 - val_loss: 0.0431\n",
            "Epoch 18/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1250 - val_loss: 0.0417\n",
            "Epoch 19/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1256 - val_loss: 0.0409\n",
            "Epoch 20/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1237 - val_loss: 0.0400\n",
            "Epoch 21/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1251 - val_loss: 0.0389\n",
            "Epoch 22/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1216 - val_loss: 0.0378\n",
            "Epoch 23/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1225 - val_loss: 0.0367\n",
            "Epoch 24/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1199 - val_loss: 0.0354\n",
            "Epoch 25/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1206 - val_loss: 0.0341\n",
            "Epoch 26/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1183 - val_loss: 0.0331\n",
            "Epoch 27/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1179 - val_loss: 0.0320\n",
            "Epoch 28/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1175 - val_loss: 0.0309\n",
            "Epoch 29/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1156 - val_loss: 0.0297\n",
            "Epoch 30/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1156 - val_loss: 0.0289\n",
            "Epoch 31/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1138 - val_loss: 0.0277\n",
            "Epoch 32/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1115 - val_loss: 0.0270\n",
            "Epoch 33/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1124 - val_loss: 0.0259\n",
            "Epoch 34/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1114 - val_loss: 0.0254\n",
            "Epoch 35/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1121 - val_loss: 0.0246\n",
            "Epoch 36/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1100 - val_loss: 0.0237\n",
            "Epoch 37/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1099 - val_loss: 0.0230\n",
            "Epoch 38/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1077 - val_loss: 0.0223\n",
            "Epoch 39/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1083 - val_loss: 0.0218\n",
            "Epoch 40/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1086 - val_loss: 0.0213\n",
            "Epoch 41/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1081 - val_loss: 0.0207\n",
            "Epoch 42/100\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.1069 - val_loss: 0.0200\n",
            "Epoch 43/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1045 - val_loss: 0.0195\n",
            "Epoch 44/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1041 - val_loss: 0.0189\n",
            "Epoch 45/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1043 - val_loss: 0.0185\n",
            "Epoch 46/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1038 - val_loss: 0.0180\n",
            "Epoch 47/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1040 - val_loss: 0.0179\n",
            "Epoch 48/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1025 - val_loss: 0.0173\n",
            "Epoch 49/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1019 - val_loss: 0.0167\n",
            "Epoch 50/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1042 - val_loss: 0.0166\n",
            "Epoch 51/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1029 - val_loss: 0.0163\n",
            "Epoch 52/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1018 - val_loss: 0.0157\n",
            "Epoch 53/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.1039 - val_loss: 0.0155\n",
            "Epoch 54/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1010 - val_loss: 0.0150\n",
            "Epoch 55/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.1026 - val_loss: 0.0148\n",
            "Epoch 56/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.1028 - val_loss: 0.0146\n",
            "Epoch 57/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0999 - val_loss: 0.0144\n",
            "Epoch 58/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0987 - val_loss: 0.0141\n",
            "Epoch 59/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0997 - val_loss: 0.0138\n",
            "Epoch 60/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0995 - val_loss: 0.0137\n",
            "Epoch 61/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0993 - val_loss: 0.0134\n",
            "Epoch 62/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0982 - val_loss: 0.0132\n",
            "Epoch 63/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0991 - val_loss: 0.0130\n",
            "Epoch 64/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0973 - val_loss: 0.0126\n",
            "Epoch 65/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0983 - val_loss: 0.0126\n",
            "Epoch 66/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0976 - val_loss: 0.0124\n",
            "Epoch 67/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0974 - val_loss: 0.0121\n",
            "Epoch 68/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0986 - val_loss: 0.0118\n",
            "Epoch 69/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0963 - val_loss: 0.0118\n",
            "Epoch 70/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0977 - val_loss: 0.0117\n",
            "Epoch 71/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0971 - val_loss: 0.0114\n",
            "Epoch 72/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0967 - val_loss: 0.0111\n",
            "Epoch 73/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0968 - val_loss: 0.0109\n",
            "Epoch 74/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0950 - val_loss: 0.0109\n",
            "Epoch 75/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0962 - val_loss: 0.0107\n",
            "Epoch 76/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0936 - val_loss: 0.0105\n",
            "Epoch 77/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0957 - val_loss: 0.0103\n",
            "Epoch 78/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0950 - val_loss: 0.0102\n",
            "Epoch 79/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0939 - val_loss: 0.0100\n",
            "Epoch 80/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0944 - val_loss: 0.0098\n",
            "Epoch 81/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0953 - val_loss: 0.0096\n",
            "Epoch 82/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0926 - val_loss: 0.0096\n",
            "Epoch 83/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0926 - val_loss: 0.0095\n",
            "Epoch 84/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0919 - val_loss: 0.0093\n",
            "Epoch 85/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0935 - val_loss: 0.0091\n",
            "Epoch 86/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0946 - val_loss: 0.0090\n",
            "Epoch 87/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0913 - val_loss: 0.0090\n",
            "Epoch 88/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0917 - val_loss: 0.0088\n",
            "Epoch 89/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0923 - val_loss: 0.0088\n",
            "Epoch 90/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0907 - val_loss: 0.0085\n",
            "Epoch 91/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0913 - val_loss: 0.0084\n",
            "Epoch 92/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0891 - val_loss: 0.0083\n",
            "Epoch 93/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0893 - val_loss: 0.0083\n",
            "Epoch 94/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0879 - val_loss: 0.0082\n",
            "Epoch 95/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0887 - val_loss: 0.0080\n",
            "Epoch 96/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0867 - val_loss: 0.0079\n",
            "Epoch 97/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0879 - val_loss: 0.0078\n",
            "Epoch 98/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0877 - val_loss: 0.0077\n",
            "Epoch 99/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0868 - val_loss: 0.0076\n",
            "Epoch 100/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0903 - val_loss: 0.0075\n",
            "Training for SNR= 1.5  sigma= 0.8413951416451951\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0891 - val_loss: 0.0074\n",
            "Epoch 2/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0889 - val_loss: 0.0073\n",
            "Epoch 3/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0882 - val_loss: 0.0073\n",
            "Epoch 4/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0864 - val_loss: 0.0072\n",
            "Epoch 5/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0888 - val_loss: 0.0071\n",
            "Epoch 6/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0880 - val_loss: 0.0069\n",
            "Epoch 7/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0868 - val_loss: 0.0069\n",
            "Epoch 8/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0856 - val_loss: 0.0068\n",
            "Epoch 9/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0871 - val_loss: 0.0067\n",
            "Epoch 10/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0855 - val_loss: 0.0066\n",
            "Epoch 11/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0850 - val_loss: 0.0066\n",
            "Epoch 12/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0859 - val_loss: 0.0065\n",
            "Epoch 13/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0857 - val_loss: 0.0064\n",
            "Epoch 14/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0848 - val_loss: 0.0064\n",
            "Epoch 15/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0853 - val_loss: 0.0063\n",
            "Epoch 16/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0849 - val_loss: 0.0062\n",
            "Epoch 17/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0840 - val_loss: 0.0061\n",
            "Epoch 18/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0850 - val_loss: 0.0061\n",
            "Epoch 19/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0831 - val_loss: 0.0060\n",
            "Epoch 20/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0839 - val_loss: 0.0059\n",
            "Epoch 21/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0831 - val_loss: 0.0059\n",
            "Epoch 22/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0838 - val_loss: 0.0058\n",
            "Epoch 23/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0816 - val_loss: 0.0057\n",
            "Epoch 24/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0835 - val_loss: 0.0057\n",
            "Epoch 25/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0832 - val_loss: 0.0056\n",
            "Epoch 26/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0835 - val_loss: 0.0055\n",
            "Epoch 27/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0830 - val_loss: 0.0055\n",
            "Epoch 28/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0825 - val_loss: 0.0054\n",
            "Epoch 29/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0814 - val_loss: 0.0054\n",
            "Epoch 30/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0813 - val_loss: 0.0054\n",
            "Epoch 31/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0816 - val_loss: 0.0052\n",
            "Epoch 32/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0820 - val_loss: 0.0053\n",
            "Epoch 33/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0831 - val_loss: 0.0052\n",
            "Epoch 34/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0810 - val_loss: 0.0051\n",
            "Epoch 35/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0814 - val_loss: 0.0051\n",
            "Epoch 36/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0801 - val_loss: 0.0050\n",
            "Epoch 37/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0820 - val_loss: 0.0050\n",
            "Epoch 38/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0814 - val_loss: 0.0050\n",
            "Epoch 39/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0797 - val_loss: 0.0050\n",
            "Epoch 40/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0813 - val_loss: 0.0049\n",
            "Epoch 41/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0825 - val_loss: 0.0048\n",
            "Epoch 42/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0799 - val_loss: 0.0048\n",
            "Epoch 43/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0809 - val_loss: 0.0047\n",
            "Epoch 44/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0802 - val_loss: 0.0047\n",
            "Epoch 45/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0799 - val_loss: 0.0047\n",
            "Epoch 46/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0802 - val_loss: 0.0047\n",
            "Epoch 47/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0804 - val_loss: 0.0046\n",
            "Epoch 48/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0782 - val_loss: 0.0045\n",
            "Epoch 49/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0793 - val_loss: 0.0045\n",
            "Epoch 50/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0792 - val_loss: 0.0045\n",
            "Epoch 51/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0779 - val_loss: 0.0044\n",
            "Epoch 52/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0786 - val_loss: 0.0043\n",
            "Epoch 53/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0803 - val_loss: 0.0043\n",
            "Epoch 54/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0794 - val_loss: 0.0043\n",
            "Epoch 55/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0761 - val_loss: 0.0043\n",
            "Epoch 56/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0770 - val_loss: 0.0041\n",
            "Epoch 57/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0782 - val_loss: 0.0042\n",
            "Epoch 58/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0762 - val_loss: 0.0041\n",
            "Epoch 59/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0769 - val_loss: 0.0041\n",
            "Epoch 60/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0777 - val_loss: 0.0040\n",
            "Epoch 61/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0764 - val_loss: 0.0040\n",
            "Epoch 62/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0772 - val_loss: 0.0039\n",
            "Epoch 63/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0758 - val_loss: 0.0039\n",
            "Epoch 64/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0755 - val_loss: 0.0038\n",
            "Epoch 65/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0752 - val_loss: 0.0038\n",
            "Epoch 66/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0761 - val_loss: 0.0038\n",
            "Epoch 67/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0760 - val_loss: 0.0038\n",
            "Epoch 68/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0752 - val_loss: 0.0037\n",
            "Epoch 69/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0774 - val_loss: 0.0037\n",
            "Epoch 70/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0751 - val_loss: 0.0036\n",
            "Epoch 71/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0752 - val_loss: 0.0036\n",
            "Epoch 72/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0744 - val_loss: 0.0036\n",
            "Epoch 73/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0744 - val_loss: 0.0035\n",
            "Epoch 74/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0748 - val_loss: 0.0035\n",
            "Epoch 75/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0724 - val_loss: 0.0035\n",
            "Epoch 76/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0748 - val_loss: 0.0034\n",
            "Epoch 77/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0746 - val_loss: 0.0034\n",
            "Epoch 78/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0766 - val_loss: 0.0034\n",
            "Epoch 79/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0756 - val_loss: 0.0034\n",
            "Epoch 80/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0745 - val_loss: 0.0033\n",
            "Epoch 81/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0742 - val_loss: 0.0033\n",
            "Epoch 82/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0741 - val_loss: 0.0033\n",
            "Epoch 83/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0754 - val_loss: 0.0032\n",
            "Epoch 84/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0740 - val_loss: 0.0033\n",
            "Epoch 85/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0748 - val_loss: 0.0032\n",
            "Epoch 86/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0728 - val_loss: 0.0032\n",
            "Epoch 87/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0735 - val_loss: 0.0032\n",
            "Epoch 88/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0737 - val_loss: 0.0031\n",
            "Epoch 89/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0745 - val_loss: 0.0031\n",
            "Epoch 90/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0731 - val_loss: 0.0032\n",
            "Epoch 91/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0738 - val_loss: 0.0031\n",
            "Epoch 92/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0720 - val_loss: 0.0031\n",
            "Epoch 93/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0718 - val_loss: 0.0030\n",
            "Epoch 94/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0728 - val_loss: 0.0030\n",
            "Epoch 95/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0721 - val_loss: 0.0030\n",
            "Epoch 96/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0714 - val_loss: 0.0030\n",
            "Epoch 97/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0716 - val_loss: 0.0030\n",
            "Epoch 98/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0722 - val_loss: 0.0029\n",
            "Epoch 99/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0729 - val_loss: 0.0029\n",
            "Epoch 100/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0721 - val_loss: 0.0029\n",
            "Training for SNR= 2.0  sigma= 0.7943282347242815\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0719 - val_loss: 0.0029\n",
            "Epoch 2/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0715 - val_loss: 0.0029\n",
            "Epoch 3/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0717 - val_loss: 0.0028\n",
            "Epoch 4/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0714 - val_loss: 0.0028\n",
            "Epoch 5/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0706 - val_loss: 0.0028\n",
            "Epoch 6/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0719 - val_loss: 0.0028\n",
            "Epoch 7/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0724 - val_loss: 0.0027\n",
            "Epoch 8/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0708 - val_loss: 0.0028\n",
            "Epoch 9/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0714 - val_loss: 0.0028\n",
            "Epoch 10/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0719 - val_loss: 0.0027\n",
            "Epoch 11/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0695 - val_loss: 0.0027\n",
            "Epoch 12/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0702 - val_loss: 0.0027\n",
            "Epoch 13/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0725 - val_loss: 0.0027\n",
            "Epoch 14/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0721 - val_loss: 0.0027\n",
            "Epoch 15/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0713 - val_loss: 0.0027\n",
            "Epoch 16/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0708 - val_loss: 0.0026\n",
            "Epoch 17/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0690 - val_loss: 0.0026\n",
            "Epoch 18/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0708 - val_loss: 0.0026\n",
            "Epoch 19/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0708 - val_loss: 0.0026\n",
            "Epoch 20/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0703 - val_loss: 0.0026\n",
            "Epoch 21/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0711 - val_loss: 0.0026\n",
            "Epoch 22/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0708 - val_loss: 0.0025\n",
            "Epoch 23/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0694 - val_loss: 0.0025\n",
            "Epoch 24/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0695 - val_loss: 0.0025\n",
            "Epoch 25/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0695 - val_loss: 0.0025\n",
            "Epoch 26/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0700 - val_loss: 0.0024\n",
            "Epoch 27/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0697 - val_loss: 0.0024\n",
            "Epoch 28/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0697 - val_loss: 0.0024\n",
            "Epoch 29/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0689 - val_loss: 0.0024\n",
            "Epoch 30/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0707 - val_loss: 0.0024\n",
            "Epoch 31/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0706 - val_loss: 0.0024\n",
            "Epoch 32/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0688 - val_loss: 0.0024\n",
            "Epoch 33/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0696 - val_loss: 0.0024\n",
            "Epoch 34/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0713 - val_loss: 0.0024\n",
            "Epoch 35/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0705 - val_loss: 0.0023\n",
            "Epoch 36/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0703 - val_loss: 0.0023\n",
            "Epoch 37/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0696 - val_loss: 0.0024\n",
            "Epoch 38/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0687 - val_loss: 0.0023\n",
            "Epoch 39/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0675 - val_loss: 0.0023\n",
            "Epoch 40/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0697 - val_loss: 0.0023\n",
            "Epoch 41/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0664 - val_loss: 0.0023\n",
            "Epoch 42/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0678 - val_loss: 0.0023\n",
            "Epoch 43/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0689 - val_loss: 0.0023\n",
            "Epoch 44/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0688 - val_loss: 0.0023\n",
            "Epoch 45/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0688 - val_loss: 0.0022\n",
            "Epoch 46/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0687 - val_loss: 0.0022\n",
            "Epoch 47/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0680 - val_loss: 0.0022\n",
            "Epoch 48/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0694 - val_loss: 0.0022\n",
            "Epoch 49/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0667 - val_loss: 0.0022\n",
            "Epoch 50/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0683 - val_loss: 0.0022\n",
            "Epoch 51/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0683 - val_loss: 0.0022\n",
            "Epoch 52/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0683 - val_loss: 0.0022\n",
            "Epoch 53/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0679 - val_loss: 0.0021\n",
            "Epoch 54/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0660 - val_loss: 0.0021\n",
            "Epoch 55/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0697 - val_loss: 0.0021\n",
            "Epoch 56/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0696 - val_loss: 0.0021\n",
            "Epoch 57/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0687 - val_loss: 0.0021\n",
            "Epoch 58/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0674 - val_loss: 0.0021\n",
            "Epoch 59/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0686 - val_loss: 0.0021\n",
            "Epoch 60/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0673 - val_loss: 0.0021\n",
            "Epoch 61/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0700 - val_loss: 0.0021\n",
            "Epoch 62/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0680 - val_loss: 0.0021\n",
            "Epoch 63/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0662 - val_loss: 0.0021\n",
            "Epoch 64/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0683 - val_loss: 0.0021\n",
            "Epoch 65/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0672 - val_loss: 0.0020\n",
            "Epoch 66/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0680 - val_loss: 0.0021\n",
            "Epoch 67/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0671 - val_loss: 0.0020\n",
            "Epoch 68/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0691 - val_loss: 0.0020\n",
            "Epoch 69/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0683 - val_loss: 0.0020\n",
            "Epoch 70/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0672 - val_loss: 0.0020\n",
            "Epoch 71/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0679 - val_loss: 0.0020\n",
            "Epoch 72/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0679 - val_loss: 0.0020\n",
            "Epoch 73/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0671 - val_loss: 0.0020\n",
            "Epoch 74/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0671 - val_loss: 0.0020\n",
            "Epoch 75/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0685 - val_loss: 0.0020\n",
            "Epoch 76/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0666 - val_loss: 0.0020\n",
            "Epoch 77/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0661 - val_loss: 0.0020\n",
            "Epoch 78/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0667 - val_loss: 0.0020\n",
            "Epoch 79/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0665 - val_loss: 0.0020\n",
            "Epoch 80/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0660 - val_loss: 0.0020\n",
            "Epoch 81/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0684 - val_loss: 0.0019\n",
            "Epoch 82/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0673 - val_loss: 0.0019\n",
            "Epoch 83/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0672 - val_loss: 0.0019\n",
            "Epoch 84/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0668 - val_loss: 0.0019\n",
            "Epoch 85/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0662 - val_loss: 0.0019\n",
            "Epoch 86/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0676 - val_loss: 0.0019\n",
            "Epoch 87/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0673 - val_loss: 0.0019\n",
            "Epoch 88/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0661 - val_loss: 0.0019\n",
            "Epoch 89/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0673 - val_loss: 0.0019\n",
            "Epoch 90/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0666 - val_loss: 0.0019\n",
            "Epoch 91/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0681 - val_loss: 0.0019\n",
            "Epoch 92/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0661 - val_loss: 0.0019\n",
            "Epoch 93/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0667 - val_loss: 0.0019\n",
            "Epoch 94/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0665 - val_loss: 0.0019\n",
            "Epoch 95/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0658 - val_loss: 0.0018\n",
            "Epoch 96/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0669 - val_loss: 0.0018\n",
            "Epoch 97/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0675 - val_loss: 0.0019\n",
            "Epoch 98/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0678 - val_loss: 0.0018\n",
            "Epoch 99/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0669 - val_loss: 0.0018\n",
            "Epoch 100/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0679 - val_loss: 0.0018\n",
            "Training for SNR= 2.5  sigma= 0.7498942093324559\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0661 - val_loss: 0.0018\n",
            "Epoch 2/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0663 - val_loss: 0.0018\n",
            "Epoch 3/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0665 - val_loss: 0.0018\n",
            "Epoch 4/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0664 - val_loss: 0.0018\n",
            "Epoch 5/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0662 - val_loss: 0.0018\n",
            "Epoch 6/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0666 - val_loss: 0.0018\n",
            "Epoch 7/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0679 - val_loss: 0.0018\n",
            "Epoch 8/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0651 - val_loss: 0.0018\n",
            "Epoch 9/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0653 - val_loss: 0.0018\n",
            "Epoch 10/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0674 - val_loss: 0.0018\n",
            "Epoch 11/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0661 - val_loss: 0.0018\n",
            "Epoch 12/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0650 - val_loss: 0.0018\n",
            "Epoch 13/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0640 - val_loss: 0.0017\n",
            "Epoch 14/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0659 - val_loss: 0.0017\n",
            "Epoch 15/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0645 - val_loss: 0.0017\n",
            "Epoch 16/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0657 - val_loss: 0.0017\n",
            "Epoch 17/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0649 - val_loss: 0.0017\n",
            "Epoch 18/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0655 - val_loss: 0.0017\n",
            "Epoch 19/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0658 - val_loss: 0.0017\n",
            "Epoch 20/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0664 - val_loss: 0.0017\n",
            "Epoch 21/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0661 - val_loss: 0.0017\n",
            "Epoch 22/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0669 - val_loss: 0.0017\n",
            "Epoch 23/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0654 - val_loss: 0.0017\n",
            "Epoch 24/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0645 - val_loss: 0.0017\n",
            "Epoch 25/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0660 - val_loss: 0.0017\n",
            "Epoch 26/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0659 - val_loss: 0.0017\n",
            "Epoch 27/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0644 - val_loss: 0.0017\n",
            "Epoch 28/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0666 - val_loss: 0.0017\n",
            "Epoch 29/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0646 - val_loss: 0.0016\n",
            "Epoch 30/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0664 - val_loss: 0.0016\n",
            "Epoch 31/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0646 - val_loss: 0.0017\n",
            "Epoch 32/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0654 - val_loss: 0.0016\n",
            "Epoch 33/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0633 - val_loss: 0.0016\n",
            "Epoch 34/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0671 - val_loss: 0.0016\n",
            "Epoch 35/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0661 - val_loss: 0.0016\n",
            "Epoch 36/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0661 - val_loss: 0.0016\n",
            "Epoch 37/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0641 - val_loss: 0.0016\n",
            "Epoch 38/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0650 - val_loss: 0.0016\n",
            "Epoch 39/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0656 - val_loss: 0.0016\n",
            "Epoch 40/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0636 - val_loss: 0.0016\n",
            "Epoch 41/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0654 - val_loss: 0.0016\n",
            "Epoch 42/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0637 - val_loss: 0.0016\n",
            "Epoch 43/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0636 - val_loss: 0.0016\n",
            "Epoch 44/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0646 - val_loss: 0.0016\n",
            "Epoch 45/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0639 - val_loss: 0.0016\n",
            "Epoch 46/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0651 - val_loss: 0.0016\n",
            "Epoch 47/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0652 - val_loss: 0.0016\n",
            "Epoch 48/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0653 - val_loss: 0.0016\n",
            "Epoch 49/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0654 - val_loss: 0.0016\n",
            "Epoch 50/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0646 - val_loss: 0.0015\n",
            "Epoch 51/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0647 - val_loss: 0.0016\n",
            "Epoch 52/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0606 - val_loss: 0.0016\n",
            "Epoch 53/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0656 - val_loss: 0.0016\n",
            "Epoch 54/100\n",
            "10000/10000 [==============================] - 0s 8us/sample - loss: 0.0651 - val_loss: 0.0015\n",
            "Epoch 55/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0653 - val_loss: 0.0015\n",
            "Epoch 56/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0636 - val_loss: 0.0015\n",
            "Epoch 57/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0645 - val_loss: 0.0015\n",
            "Epoch 58/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0644 - val_loss: 0.0015\n",
            "Epoch 59/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0659 - val_loss: 0.0015\n",
            "Epoch 60/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0630 - val_loss: 0.0015\n",
            "Epoch 61/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0640 - val_loss: 0.0015\n",
            "Epoch 62/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0639 - val_loss: 0.0015\n",
            "Epoch 63/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0645 - val_loss: 0.0015\n",
            "Epoch 64/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0644 - val_loss: 0.0015\n",
            "Epoch 65/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0660 - val_loss: 0.0015\n",
            "Epoch 66/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0631 - val_loss: 0.0015\n",
            "Epoch 67/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0615 - val_loss: 0.0015\n",
            "Epoch 68/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0638 - val_loss: 0.0015\n",
            "Epoch 69/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0643 - val_loss: 0.0015\n",
            "Epoch 70/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0633 - val_loss: 0.0015\n",
            "Epoch 71/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0631 - val_loss: 0.0015\n",
            "Epoch 72/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0646 - val_loss: 0.0015\n",
            "Epoch 73/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0644 - val_loss: 0.0015\n",
            "Epoch 74/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0643 - val_loss: 0.0015\n",
            "Epoch 75/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0636 - val_loss: 0.0015\n",
            "Epoch 76/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0634 - val_loss: 0.0014\n",
            "Epoch 77/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0621 - val_loss: 0.0014\n",
            "Epoch 78/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0636 - val_loss: 0.0014\n",
            "Epoch 79/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0646 - val_loss: 0.0014\n",
            "Epoch 80/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0635 - val_loss: 0.0014\n",
            "Epoch 81/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0650 - val_loss: 0.0014\n",
            "Epoch 82/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0639 - val_loss: 0.0014\n",
            "Epoch 83/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0642 - val_loss: 0.0014\n",
            "Epoch 84/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0641 - val_loss: 0.0014\n",
            "Epoch 85/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0630 - val_loss: 0.0014\n",
            "Epoch 86/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0638 - val_loss: 0.0014\n",
            "Epoch 87/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0640 - val_loss: 0.0014\n",
            "Epoch 88/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0626 - val_loss: 0.0014\n",
            "Epoch 89/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0640 - val_loss: 0.0014\n",
            "Epoch 90/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0633 - val_loss: 0.0014\n",
            "Epoch 91/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0630 - val_loss: 0.0014\n",
            "Epoch 92/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0644 - val_loss: 0.0014\n",
            "Epoch 93/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0638 - val_loss: 0.0014\n",
            "Epoch 94/100\n",
            "10000/10000 [==============================] - 0s 7us/sample - loss: 0.0638 - val_loss: 0.0014\n",
            "Epoch 95/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0627 - val_loss: 0.0014\n",
            "Epoch 96/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0635 - val_loss: 0.0014\n",
            "Epoch 97/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0640 - val_loss: 0.0014\n",
            "Epoch 98/100\n",
            "10000/10000 [==============================] - 0s 5us/sample - loss: 0.0610 - val_loss: 0.0014\n",
            "Epoch 99/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0634 - val_loss: 0.0014\n",
            "Epoch 100/100\n",
            "10000/10000 [==============================] - 0s 6us/sample - loss: 0.0646 - val_loss: 0.0014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHByzQbTUqbv",
        "outputId": "d4a84037-c885-4aa1-b92f-5158cc72faa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using trained model\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_dl_tensor  = numpy.array(())\n",
        "times_per_iter_dl_tensor = numpy.array(())\n",
        "\n",
        "#awgn_channel_tx = GaussianNoise(0.5,input_shape=(CHANEL_SIZE,))\n",
        "\n",
        "#awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [CHANEL_SIZE])\n",
        "#awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "#awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "#awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)\n",
        "\n",
        "\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    input_message_xx = input_message [i:i+1]\n",
        "    #print (\"input\", input_message_xx)\n",
        "    encoded_message = encoder.predict(input_message_xx)\n",
        "    #encoded_message = numpy.around(encoded_message > 0.5).astype(int)\n",
        "    #print(encoded_message)\n",
        "    #print (\"encoded\", encoded_message)\n",
        "    #noised_message = awgn_channel.predict (encoded_message)\n",
        "    noised_message = commpy.channels.awgn(encoded_message, snr)\n",
        "    #awgn_channel = GaussianNoise(sigma,input_shape=(CHANEL_SIZE,))\n",
        "    #noised_message = awgn_channel.predict(encoded_message)\n",
        "    #noised_message = awgn_layer (encoded_message)\n",
        "    #awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:0.5, awgn_channel_input:encoded_message[0]})\n",
        "    #print(noised_message)\n",
        "    decoded_message = decoder.predict(noised_message)\n",
        "    decoded_message = numpy.around(decoded_message[0]).astype(int)\n",
        "    #decoded_message = numpy.around(decoded_message > 0.5).astype(int)\n",
        "    #print (\".\")\n",
        "    #autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))\n",
        "    #decoded_message = autoencoder.predict(input_message_xx)\n",
        "    #print (\"output\", decoded_message)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "      #print (\"Error\")\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_dl_tensor=numpy.append(ber_per_iter_dl_tensor ,ber)\n",
        "  times_per_iter_dl_tensor=numpy.append(times_per_iter_dl_tensor, total_time)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 0.53s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 1.03s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 1.53s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 2.02s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.85\n",
            " -> Total Time: 5.11s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 0.52s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 1.00s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 1.49s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 1.99s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.82\n",
            " -> Total Time: 5.01s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.49s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 0.99s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 1.48s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 1.97s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.81\n",
            " -> Total Time: 4.93s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.50s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 0.99s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 1.49s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 1.98s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.81\n",
            " -> Total Time: 4.97s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.50s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 0.99s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 1.49s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 1.97s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.81\n",
            " -> Total Time: 4.95s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.50s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 0.99s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.48s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 1.96s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.78\n",
            " -> Total Time: 4.94s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.48s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.96s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.49s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.98s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.77\n",
            " -> Total Time: 4.91s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.52s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 1.03s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.53s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 2.01s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.75\n",
            " -> Total Time: 5.09s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.48s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.98s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 1.46s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.96s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.75\n",
            " -> Total Time: 4.89s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.48s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.97s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 1.45s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.94s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.74\n",
            " -> Total Time: 4.85s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.48s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.97s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 1.46s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 1.97s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.71\n",
            " -> Total Time: 4.88s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.51s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 1.00s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 1.48s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 1.97s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.71\n",
            " -> Total Time: 4.96s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.50s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 1.01s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 1.51s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 2.01s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.70\n",
            " -> Total Time: 5.03s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.49s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.98s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 1.46s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 1.94s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.68\n",
            " -> Total Time: 4.87s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.49s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.98s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 1.49s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 2.02s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.67\n",
            " -> Total Time: 4.98s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.47s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.96s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 1.44s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 1.96s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.63\n",
            " -> Total Time: 4.83s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.51s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 1.01s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 1.49s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 1.98s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.61\n",
            " -> Total Time: 4.99s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.48s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.96s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 1.45s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 1.95s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.60\n",
            " -> Total Time: 4.83s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.49s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.98s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 1.47s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 1.97s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.60\n",
            " -> Total Time: 4.91s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.48s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.97s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 1.46s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 1.95s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.52\n",
            " -> Total Time: 4.86s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syUQij3fuxRm",
        "outputId": "55d2f404-f87b-4852-a174-c062f04d4afc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1) = plt.subplots(1,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"ldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor,'', label=\"ai-dl\") # plot BER vs SNR\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "#ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "#ax2.set_xlabel('$E_b/$N_0$')\n",
        "#ax2.set_ylabel('Decoding Time [s]')\n",
        "#ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "#            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "#            xy=(1, 0.35), xycoords='axes fraction',\n",
        "#            xytext=(-20, 20), textcoords='offset pixels',\n",
        "#            horizontalalignment='right',\n",
        "#            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}