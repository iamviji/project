{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MainModelKeras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamviji/project/blob/master/MainModelKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku11kjUKaO8X"
      },
      "source": [
        "Note:To Checkin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDSPPMfZ9czi",
        "outputId": "86a95789-716e-4bba-bb96-e559b8cf7d20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        }
      },
      "source": [
        "!rm -rf project\n",
        "!git clone https://github.com/iamviji/project.git\n",
        "!ls\n",
        "!ls project\n",
        "!pip install pyldpc\n",
        "!pip install scikit-commpy\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'project'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 53 (delta 11), reused 8 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (53/53), done.\n",
            "project  sample_data\n",
            "MainModel.ipynb  MainModelWithSingleBERTraining.ipynb  README.md  util.py\n",
            "Collecting pyldpc\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/aa/fd5495869c7106a638ae71aa497d7d266cae7f2a343d1f6a9d0e3a986e1e/pyldpc-0.7.9.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyldpc) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyldpc) (1.4.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from pyldpc) (0.48.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->pyldpc) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->pyldpc) (50.3.0)\n",
            "Building wheels for collected packages: pyldpc\n",
            "  Building wheel for pyldpc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyldpc: filename=pyldpc-0.7.9-cp36-none-any.whl size=14306 sha256=1d3b99a833677302be0cd37edbdf24e348cc7e02b3ca22dfa8bfbab9ec8f2fb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/7a/10/e94058ba8b0b6d98bf2719226d18d3dd6056525ad7b984c068\n",
            "Successfully built pyldpc\n",
            "Installing collected packages: pyldpc\n",
            "Successfully installed pyldpc-0.7.9\n",
            "Collecting scikit-commpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/b4/f7fa5bc8864e0ddbd3e7a2290b624b92690f53523474024915c33321802d/scikit_commpy-0.5.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->scikit-commpy) (1.15.0)\n",
            "Installing collected packages: scikit-commpy\n",
            "Successfully installed scikit-commpy-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QOuLqpdDgx2"
      },
      "source": [
        "import pyldpc\n",
        "import commpy\n",
        "import numpy \n",
        "import time\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior ()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YClXJbbr0lc7"
      },
      "source": [
        "SNR_BEGIN = 0\n",
        "SNR_END = 10\n",
        "SNR_STEP_SIZE = 0.5\n",
        "CHANEL_SIZE = 18\n",
        "NUM_OF_INPUT_MESSAGE = 1000\n",
        "LDPC_MAX_ITER = 100\n",
        "num_parity_check = 3\n",
        "num_bits_in_parity_check = 6 \n",
        "input_message_length =  0 # Caculated by channel encoder and initialized later"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvUzIMsB43i0"
      },
      "source": [
        "def timer_update(i,current,time_tot,tic_incr=500):\n",
        "    last = current\n",
        "    current = time.time()\n",
        "    t_diff = current-last\n",
        "    print('SNR: {:04.3f} - Iter: {} - Last {} iterations took {:03.2f}s'.format(snr,i+1,tic_incr,t_diff))\n",
        "    return time_tot + t_diff\n",
        "\n",
        "def Snr2Sigma(snr):\n",
        "  sigma = 10 ** (- snr / 20)\n",
        "  return sigma\n",
        "\n",
        "def pyldpc_encode (CodingMatrix, message):\n",
        "  rng = pyldpc.utils.check_random_state(seed=None)\n",
        "  d = pyldpc.utils.binaryproduct(CodingMatrix, message)\n",
        "  encoded_message = (-1) ** d\n",
        "  return encoded_message\n",
        "\n",
        "def pyldpc_decode (ParityCheckMatrix, CodingMatrix, message, snr, maxiter):\n",
        "  decoded_msg = pyldpc.decode(ParityCheckMatrix, message, snr, maxiter)\n",
        "  out_message = pyldpc.get_message(CodingMatrix, decoded_msg)\n",
        "  return out_message\n",
        "\n",
        "awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [CHANEL_SIZE])\n",
        "awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)\n",
        "\n",
        "init = tf.global_variables_initializer ()\n",
        "sess = tf.Session ()\n",
        "sess.run(init)\n",
        "\n",
        "def AWGNChannelOutput (xx, snr , s):\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  awgn_channel_output_message = s.run ([awgn_channel_output], feed_dict={noise_std_dev:sigma, channel_input:xx})\n",
        "  return awgn_channel_output_message"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jMQG-MZ_pXu",
        "outputId": "fe295c91-8abc-42db-81eb-e64235aca75e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "\n",
        "ParityCheckMatrix, CodingMatrix = pyldpc.make_ldpc(CHANEL_SIZE, num_parity_check, num_bits_in_parity_check, systematic=True, sparse=True)\n",
        "input_message_length = CodingMatrix.shape[1]\n",
        "print (\"input_message_size=\", input_message_length, \"channel_size=\",CHANEL_SIZE)\n",
        "print (\"input_message_size=\", CodingMatrix.shape[1], \"channel_size=\",CodingMatrix.shape[0])\n",
        "input_message = numpy.random.randint(2, size=(NUM_OF_INPUT_MESSAGE,input_message_length))\n",
        "print (input_message)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_message_size= 11 channel_size= 18\n",
            "input_message_size= 11 channel_size= 18\n",
            "[[0 1 0 ... 1 0 1]\n",
            " [1 0 1 ... 0 1 1]\n",
            " [0 1 1 ... 0 1 0]\n",
            " ...\n",
            " [0 1 0 ... 1 0 1]\n",
            " [1 1 1 ... 0 0 0]\n",
            " [0 1 1 ... 1 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WKg2HU2adgZ"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fL8ptL4aeOY"
      },
      "source": [
        "This section tries to compare BER and Time performance of PYLDPC in following 3 cases\n",
        "1. SNR Noise function provided in encoder function of pyldpc library (pyldpc.encode)\n",
        "2. SNR Noise function provided by commpy library (commpy.channels.awgn) \n",
        "3. SNR Noise function implemented using tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma5zUqFv0TH2",
        "outputId": "f4ca2699-7935-4ad4-f67d-d0ea31045865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using tensor flow based AWGN, to make sure that effect of it is same as AWGN\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_tensor  = numpy.array(())\n",
        "times_per_iter_tensor = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc_encode (CodingMatrix, input_message[i])\n",
        "    sigma = Snr2Sigma (snr)\n",
        "    awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message})[0]\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      #print (\"count=\",abs(decoded_message-input_message[i]).sum())\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_tensor=numpy.append(ber_per_iter_tensor ,ber)\n",
        "  times_per_iter_tensor=numpy.append(times_per_iter_tensor, total_time)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.87s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 3.83s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 5.72s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 7.64s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.61\n",
            " -> Total Time: 19.06s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.65s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 3.14s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 4.68s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 6.15s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.53\n",
            " -> Total Time: 15.62s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 1.33s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 2.49s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 3.93s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 5.15s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.45\n",
            " -> Total Time: 12.90s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 1.22s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 2.26s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 3.34s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 4.43s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.39\n",
            " -> Total Time: 11.25s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.88s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.80s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 2.72s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 3.67s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.32\n",
            " -> Total Time: 9.07s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.69s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.42s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 2.09s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.76s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.23\n",
            " -> Total Time: 6.96s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.58s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 1.21s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.76s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 2.33s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.15\n",
            " -> Total Time: 5.88s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.61s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 1.17s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.62s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 2.18s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.13\n",
            " -> Total Time: 5.57s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.41s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.82s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 1.21s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.66s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.08\n",
            " -> Total Time: 4.11s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.36s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.72s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 1.14s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.51s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.05\n",
            " -> Total Time: 3.73s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.41s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.76s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 1.18s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 1.55s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 3.89s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.33s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.65s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.98s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 1.39s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 3.35s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.34s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.65s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.97s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 1.31s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 3.26s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.32s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.67s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 1.00s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 1.32s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.31s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.32s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.63s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.93s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 1.24s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.13s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.32s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.65s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.97s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 1.28s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.22s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.32s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.62s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.94s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 1.25s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.13s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.31s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.61s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.92s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 1.27s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.10s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.30s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.62s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.92s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 1.24s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.08s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.31s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.61s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.92s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 1.22s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 3.06s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8dIFLg76c7O",
        "outputId": "fb8c75e4-e470-42ee-c15b-9bac36f36dfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using commpy based AWGN \n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_awgn  = numpy.array(())\n",
        "times_per_iter_awgn = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc_encode (CodingMatrix, input_message[i])\n",
        "    awgn_channel_output_message = commpy.channels.awgn(encoded_message, snr)\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_awgn=numpy.append(ber_per_iter_awgn ,ber)\n",
        "  times_per_iter_awgn=numpy.append(times_per_iter_awgn, total_time)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.76s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 3.52s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 5.26s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 6.89s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.62\n",
            " -> Total Time: 17.42s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.47s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.86s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 4.40s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 5.81s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.54\n",
            " -> Total Time: 14.54s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 1.16s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 2.31s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 3.43s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 4.79s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.44\n",
            " -> Total Time: 11.69s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.85s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.82s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.64s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 3.53s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.34\n",
            " -> Total Time: 8.84s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.69s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.52s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 2.26s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.97s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.32\n",
            " -> Total Time: 7.44s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.69s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.24s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.86s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.46s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.23\n",
            " -> Total Time: 6.25s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.51s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 1.03s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.56s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.90s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.18\n",
            " -> Total Time: 5.00s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.42s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.71s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.13s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.53s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.12\n",
            " -> Total Time: 3.79s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.30s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.62s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.95s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.27s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.08\n",
            " -> Total Time: 3.14s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.81s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.08s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.05\n",
            " -> Total Time: 2.62s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.05\n",
            " -> Total Time: 2.50s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.22s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.47s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 2.38s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.21s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.44s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.67s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.86s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.18s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.20s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.38s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.58s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.77s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.94s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.38s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.57s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.75s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.91s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.20s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.39s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.58s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.77s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.94s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.22s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.42s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.60s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.80s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.04s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.37s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.56s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.76s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.89s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.38s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.57s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.75s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.89s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.20s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.38s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.57s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.76s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.92s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ihPKJJk7Jj9",
        "outputId": "5e240ae5-fb76-43be-b54d-716a65b13f09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using tensor flow based AWGN, to make sure that effect of it is same as AWGN\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_pyldpc  = numpy.array(())\n",
        "times_per_iter_pyldpc = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc.encode (CodingMatrix, input_message[i], snr)\n",
        "    awgn_channel_output_message = encoded_message\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_pyldpc=numpy.append(ber_per_iter_pyldpc ,ber)\n",
        "  times_per_iter_pyldpc=numpy.append(times_per_iter_pyldpc, total_time)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.83s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 3.64s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 5.48s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 7.31s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.64\n",
            " -> Total Time: 18.26s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.37s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.90s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 4.51s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 5.92s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.55\n",
            " -> Total Time: 14.70s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 1.14s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 2.14s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 3.37s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 4.64s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.46\n",
            " -> Total Time: 11.29s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.95s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.72s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.76s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 3.54s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.35\n",
            " -> Total Time: 8.96s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.63s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.33s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 2.06s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.77s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.31\n",
            " -> Total Time: 6.79s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.69s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.38s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.91s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.50s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.22\n",
            " -> Total Time: 6.47s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.54s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.98s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.35s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.73s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.16\n",
            " -> Total Time: 4.60s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.30s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.71s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.11s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.59s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.14\n",
            " -> Total Time: 3.71s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.28s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.59s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.86s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.16s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.07\n",
            " -> Total Time: 2.88s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.31s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.55s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.84s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.08s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.05\n",
            " -> Total Time: 2.78s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.23s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.46s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.69s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.92s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 2.30s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.43s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.69s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 2.27s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.22s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.42s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.62s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.84s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.10s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.21s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.40s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.61s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.86s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.07s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.20s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.43s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.63s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.81s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.06s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.38s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.57s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.76s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.89s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.18s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.37s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.56s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.74s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.86s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.20s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.39s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.57s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.80s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.96s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.19s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.39s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.56s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.75s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.89s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.18s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.37s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.55s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.74s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.84s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR4-FOJ-BkAG",
        "outputId": "7ce861d8-d49e-4fd8-e72f-53e01e7a56c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "# Compare 3 AWGN(Tensorflow, CommPy, PYLDPC) Simulation on LDPC\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_pyldpc,'', label=\"pyldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"tensor\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_awgn,'', label=\"commpy-awgn\") # plot BER vs SNR\n",
        "\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "ax2.set_xlabel('$E_b/$N_0$')\n",
        "ax2.set_ylabel('Decoding Time [s]')\n",
        "ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "            xy=(1, 0.35), xycoords='axes fraction',\n",
        "            xytext=(-20, 20), textcoords='offset pixels',\n",
        "            horizontalalignment='right',\n",
        "            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBl0pWc1oF_r",
        "outputId": "61272fdf-a404-4b4f-8dff-3379ae612a18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "training_input_message = numpy.random.randint(2, size=(NUM_OF_INPUT_MESSAGE*10,input_message_length))\n",
        "print (training_input_message)\n",
        "print (len(training_input_message))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 ... 1 1 1]\n",
            " [1 1 1 ... 0 0 1]\n",
            " [1 1 1 ... 1 1 0]\n",
            " ...\n",
            " [0 1 0 ... 1 1 0]\n",
            " [0 0 1 ... 1 0 1]\n",
            " [0 1 0 ... 0 1 1]]\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggY5VwudaRwR"
      },
      "source": [
        "<B>Conclussion:</B>\n",
        "      It proved that tensorflow behaves similar to AWGN noise channel provided by pyldpc, commpy. But tensor flow based one takes adds little more time delay. This need to be offseted if we are comparing performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOeuNfeLCgfb"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, GaussianNoise\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior ()\n",
        "\n",
        "input_message_x = Input(shape=(input_message_length,))\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "enc_layer1 = Dense(CHANEL_SIZE, activation='tanh')(input_message_x)\n",
        "enc_layer2 = Dense(CHANEL_SIZE, activation='sigmoid')(enc_layer1)\n",
        "#encoded2 = Dense(CHANEL_SIZE, activation='sigmoid')(encoded1)\n",
        "# this model maps an input to its encoded representation\n",
        "enc_layer3 =  enc_layer2 / tf.sqrt(tf.reduce_mean(tf.square(enc_layer2)))\n",
        "#enc_layer2 = tf.round(enc_layer1)\n",
        "encoder = Model(input_message_x, enc_layer3)\n",
        "\n",
        "awgn_channel = GaussianNoise(Snr2Sigma(7.0),input_shape=(CHANEL_SIZE,))\n",
        "\n",
        "# create a placeholder for an encoded (32-dimensional) input\n",
        "encoded_input = Input(shape=(CHANEL_SIZE,))\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "dec_layer1 = Dense(CHANEL_SIZE, activation='tanh')(encoded_input)\n",
        "dec_layer2 = Dense(input_message_length, activation='sigmoid')(dec_layer1)\n",
        "# this model maps an encoded input to its decoder representation\n",
        "decoder = Model(encoded_input, dec_layer2)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgXpqxjrnJ-F",
        "outputId": "72823659-dde6-4940-9bc7-bfe8eb29d174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(encoder.summary())\n",
        "print(decoder.summary())\n",
        "print(autoencoder.summary())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 11)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 18)           216         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 18)           342         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Square (TensorFlowO multiple             0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mean (TensorFlowOpL multiple             0           tf_op_layer_Square[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sqrt (TensorFlowOpL multiple             0           tf_op_layer_Mean[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_truediv (TensorFlow multiple             0           dense_1[0][0]                    \n",
            "                                                                 tf_op_layer_Sqrt[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 558\n",
            "Trainable params: 558\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 18)]              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 18)                342       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 11)                209       \n",
            "=================================================================\n",
            "Total params: 551\n",
            "Trainable params: 551\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 11)]              0         \n",
            "_________________________________________________________________\n",
            "functional_1 (Functional)    (None, 18)                558       \n",
            "_________________________________________________________________\n",
            "gaussian_noise (GaussianNois (None, 18)                0         \n",
            "_________________________________________________________________\n",
            "functional_3 (Functional)    (None, 11)                551       \n",
            "=================================================================\n",
            "Total params: 1,109\n",
            "Trainable params: 1,109\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOXLOYLu8aML",
        "outputId": "8ea145b4-dbec-44f6-881b-17003f80fec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import keras\n",
        "\n",
        "#def custom_losff_fucntion (act, pred):\n",
        "#  return (tf.reduce_mean(-1*(act * tf.log(pred) + (1-act)*tf.log(1-pred))))\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "autoencoder.compile(optimizer=opt, loss='binary_crossentropy')\n",
        "#autoencoder.compile(optimizer=opt, loss=custom_losff_fucntion)\n",
        "#loss='mean_squared_error'\n",
        "#for snr in (numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)):\n",
        "for snr in (numpy.arange (-2, 12, SNR_STEP_SIZE)):\n",
        "  sigma = 1.0*Snr2Sigma (snr)\n",
        "  print (\"Training for SNR=\", snr, \" sigma=\", sigma) \n",
        "  awgn_channel = GaussianNoise(sigma,input_shape=(CHANEL_SIZE,))\n",
        "  autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))\n",
        "  opt = keras.optimizers.Adam(learning_rate=0.003)\n",
        "  autoencoder.compile(optimizer=opt, loss='binary_crossentropy')\n",
        "  autoencoder.fit(training_input_message, training_input_message,\n",
        "                #epochs=50, original\n",
        "                epochs=20,\n",
        "                batch_size=500,\n",
        "                shuffle=False,\n",
        "                validation_data=(input_message, input_message))\n",
        "  \n",
        "  "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for SNR= -2.0  sigma= 1.2589254117941673\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 195us/sample - loss: 1.8743 - val_loss: 5.0112e-06\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 1.8001 - val_loss: 7.3856e-06\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 1.6890 - val_loss: 1.3882e-05\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 1.6242 - val_loss: 1.0822e-04\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 1.5019 - val_loss: 0.0019\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 1.4028 - val_loss: 0.0085\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 1.2711 - val_loss: 0.0175\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 1.1539 - val_loss: 0.0352\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 1.0456 - val_loss: 0.0612\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.9395 - val_loss: 0.1182\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.8406 - val_loss: 0.1725\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.8036 - val_loss: 0.1787\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.7705 - val_loss: 0.1963\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.7402 - val_loss: 0.2117\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.7117 - val_loss: 0.2258\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.6912 - val_loss: 0.2449\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.6559 - val_loss: 0.2585\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.6524 - val_loss: 0.2662\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.6371 - val_loss: 0.2760\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.6263 - val_loss: 0.2825\n",
            "Training for SNR= -1.5  sigma= 1.1885022274370185\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 198us/sample - loss: 0.5891 - val_loss: 0.2712\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.5729 - val_loss: 0.2687\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.5598 - val_loss: 0.2630\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.5561 - val_loss: 0.2650\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.5447 - val_loss: 0.2609\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.5373 - val_loss: 0.2571\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.5296 - val_loss: 0.2655\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.5270 - val_loss: 0.2633\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.5156 - val_loss: 0.2631\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.5123 - val_loss: 0.2503\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.5098 - val_loss: 0.2399\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.5047 - val_loss: 0.2355\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.5011 - val_loss: 0.2301\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4985 - val_loss: 0.2290\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4952 - val_loss: 0.2214\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4906 - val_loss: 0.2224\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4883 - val_loss: 0.2209\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4839 - val_loss: 0.2200\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4884 - val_loss: 0.2236\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4794 - val_loss: 0.2240\n",
            "Training for SNR= -1.0  sigma= 1.1220184543019633\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 201us/sample - loss: 0.4573 - val_loss: 0.1987\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4520 - val_loss: 0.1995\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4496 - val_loss: 0.2017\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4488 - val_loss: 0.2053\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4472 - val_loss: 0.2024\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4469 - val_loss: 0.2065\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4434 - val_loss: 0.2055\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4391 - val_loss: 0.2008\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4411 - val_loss: 0.2056\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4429 - val_loss: 0.2075\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4415 - val_loss: 0.2089\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4435 - val_loss: 0.2104\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4411 - val_loss: 0.2136\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4422 - val_loss: 0.2114\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4397 - val_loss: 0.2125\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4389 - val_loss: 0.2148\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4352 - val_loss: 0.2115\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4343 - val_loss: 0.2092\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4361 - val_loss: 0.2156\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4341 - val_loss: 0.2136\n",
            "Training for SNR= -0.5  sigma= 1.0592537251772889\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 201us/sample - loss: 0.4101 - val_loss: 0.1857\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4099 - val_loss: 0.1886\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4110 - val_loss: 0.1906\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4067 - val_loss: 0.1896\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 11us/sample - loss: 0.4099 - val_loss: 0.1916\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4064 - val_loss: 0.1888\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4083 - val_loss: 0.1878\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4102 - val_loss: 0.1951\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4081 - val_loss: 0.1924\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4093 - val_loss: 0.1904\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4105 - val_loss: 0.1913\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4074 - val_loss: 0.1927\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4081 - val_loss: 0.1918\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4090 - val_loss: 0.1934\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4069 - val_loss: 0.1919\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.4085 - val_loss: 0.1913\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4109 - val_loss: 0.1964\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.4081 - val_loss: 0.1941\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4086 - val_loss: 0.1933\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.4081 - val_loss: 0.1945\n",
            "Training for SNR= 0.0  sigma= 1.0\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 202us/sample - loss: 0.3862 - val_loss: 0.1695\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3840 - val_loss: 0.1696\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3847 - val_loss: 0.1709\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3837 - val_loss: 0.1700\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3835 - val_loss: 0.1682\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3849 - val_loss: 0.1718\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.3846 - val_loss: 0.1712\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.3837 - val_loss: 0.1681\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3842 - val_loss: 0.1677\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3865 - val_loss: 0.1708\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3855 - val_loss: 0.1716\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3818 - val_loss: 0.1672\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3822 - val_loss: 0.1688\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.3858 - val_loss: 0.1709\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3851 - val_loss: 0.1710\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3836 - val_loss: 0.1709\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3851 - val_loss: 0.1705\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3820 - val_loss: 0.1707\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3861 - val_loss: 0.1701\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3831 - val_loss: 0.1698\n",
            "Training for SNR= 0.5  sigma= 0.9440608762859234\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 211us/sample - loss: 0.3614 - val_loss: 0.1493\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3604 - val_loss: 0.1471\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3580 - val_loss: 0.1480\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3563 - val_loss: 0.1455\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3558 - val_loss: 0.1453\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 16us/sample - loss: 0.3578 - val_loss: 0.1470\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3589 - val_loss: 0.1479\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3594 - val_loss: 0.1468\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3583 - val_loss: 0.1461\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3581 - val_loss: 0.1460\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3618 - val_loss: 0.1489\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.3583 - val_loss: 0.1489\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3593 - val_loss: 0.1481\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3602 - val_loss: 0.1487\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3596 - val_loss: 0.1502\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3548 - val_loss: 0.1453\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3568 - val_loss: 0.1467\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.3584 - val_loss: 0.1476\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3560 - val_loss: 0.1473\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3593 - val_loss: 0.1465\n",
            "Training for SNR= 1.0  sigma= 0.8912509381337456\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 204us/sample - loss: 0.3332 - val_loss: 0.1296\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3348 - val_loss: 0.1273\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3321 - val_loss: 0.1282\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3324 - val_loss: 0.1282\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3343 - val_loss: 0.1288\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3293 - val_loss: 0.1261\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.3294 - val_loss: 0.1259\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3344 - val_loss: 0.1286\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3322 - val_loss: 0.1281\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3302 - val_loss: 0.1273\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3305 - val_loss: 0.1270\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3357 - val_loss: 0.1287\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.3314 - val_loss: 0.1281\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3330 - val_loss: 0.1272\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3314 - val_loss: 0.1281\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3333 - val_loss: 0.1290\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3311 - val_loss: 0.1281\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3299 - val_loss: 0.1260\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3339 - val_loss: 0.1297\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3335 - val_loss: 0.1288\n",
            "Training for SNR= 1.5  sigma= 0.8413951416451951\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 209us/sample - loss: 0.3060 - val_loss: 0.1136\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.3053 - val_loss: 0.1108\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.3061 - val_loss: 0.1110\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3047 - val_loss: 0.1106\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3057 - val_loss: 0.1106\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3049 - val_loss: 0.1105\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3053 - val_loss: 0.1110\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3072 - val_loss: 0.1120\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.3055 - val_loss: 0.1110\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.3064 - val_loss: 0.1104\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3049 - val_loss: 0.1103\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3060 - val_loss: 0.1104\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3043 - val_loss: 0.1105\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.3033 - val_loss: 0.1099\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3070 - val_loss: 0.1112\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.3074 - val_loss: 0.1121\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3044 - val_loss: 0.1110\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3049 - val_loss: 0.1112\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3035 - val_loss: 0.1105\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.3037 - val_loss: 0.1101\n",
            "Training for SNR= 2.0  sigma= 0.7943282347242815\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 213us/sample - loss: 0.2828 - val_loss: 0.1001\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2811 - val_loss: 0.0982\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2797 - val_loss: 0.0976\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2807 - val_loss: 0.0974\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2837 - val_loss: 0.0996\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2798 - val_loss: 0.0978\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2780 - val_loss: 0.0958\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2787 - val_loss: 0.0972\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2771 - val_loss: 0.0973\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2802 - val_loss: 0.0973\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2798 - val_loss: 0.0965\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2772 - val_loss: 0.0958\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2798 - val_loss: 0.0961\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2796 - val_loss: 0.0965\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2784 - val_loss: 0.0965\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2786 - val_loss: 0.0965\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2802 - val_loss: 0.0975\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2803 - val_loss: 0.0977\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2782 - val_loss: 0.0966\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2798 - val_loss: 0.0969\n",
            "Training for SNR= 2.5  sigma= 0.7498942093324559\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 211us/sample - loss: 0.2544 - val_loss: 0.0885\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2537 - val_loss: 0.0873\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2573 - val_loss: 0.0875\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2536 - val_loss: 0.0874\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2545 - val_loss: 0.0865\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2542 - val_loss: 0.0868\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2566 - val_loss: 0.0871\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 16us/sample - loss: 0.2517 - val_loss: 0.0864\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.2538 - val_loss: 0.0869\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2539 - val_loss: 0.0860\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2526 - val_loss: 0.0861\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2527 - val_loss: 0.0864\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2535 - val_loss: 0.0868\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2540 - val_loss: 0.0869\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2536 - val_loss: 0.0867\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2532 - val_loss: 0.0865\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2541 - val_loss: 0.0864\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2539 - val_loss: 0.0871\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2515 - val_loss: 0.0865\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2517 - val_loss: 0.0861\n",
            "Training for SNR= 3.0  sigma= 0.7079457843841379\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 215us/sample - loss: 0.2313 - val_loss: 0.0811\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2301 - val_loss: 0.0799\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2284 - val_loss: 0.0793\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2289 - val_loss: 0.0791\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2290 - val_loss: 0.0792\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2288 - val_loss: 0.0793\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2306 - val_loss: 0.0794\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2297 - val_loss: 0.0790\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2261 - val_loss: 0.0789\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2307 - val_loss: 0.0790\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2288 - val_loss: 0.0793\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2274 - val_loss: 0.0788\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2266 - val_loss: 0.0786\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2256 - val_loss: 0.0788\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2291 - val_loss: 0.0788\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2276 - val_loss: 0.0785\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2285 - val_loss: 0.0784\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2284 - val_loss: 0.0786\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2282 - val_loss: 0.0789\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2301 - val_loss: 0.0786\n",
            "Training for SNR= 3.5  sigma= 0.6683439175686147\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 218us/sample - loss: 0.2057 - val_loss: 0.0754\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2071 - val_loss: 0.0743\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.2048 - val_loss: 0.0738\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2027 - val_loss: 0.0736\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2043 - val_loss: 0.0734\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2071 - val_loss: 0.0732\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2050 - val_loss: 0.0733\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2035 - val_loss: 0.0733\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2064 - val_loss: 0.0733\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2050 - val_loss: 0.0735\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.2071 - val_loss: 0.0734\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2055 - val_loss: 0.0734\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2065 - val_loss: 0.0734\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2044 - val_loss: 0.0734\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2039 - val_loss: 0.0731\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2060 - val_loss: 0.0730\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2036 - val_loss: 0.0729\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2060 - val_loss: 0.0728\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2058 - val_loss: 0.0731\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.2065 - val_loss: 0.0733\n",
            "Training for SNR= 4.0  sigma= 0.6309573444801932\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 224us/sample - loss: 0.1818 - val_loss: 0.0707\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1820 - val_loss: 0.0702\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1830 - val_loss: 0.0698\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1820 - val_loss: 0.0694\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1834 - val_loss: 0.0695\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1825 - val_loss: 0.0695\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1808 - val_loss: 0.0692\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1841 - val_loss: 0.0693\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1844 - val_loss: 0.0694\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1828 - val_loss: 0.0693\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1837 - val_loss: 0.0690\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1822 - val_loss: 0.0691\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.1809 - val_loss: 0.0690\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1814 - val_loss: 0.0689\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1830 - val_loss: 0.0688\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1834 - val_loss: 0.0690\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1832 - val_loss: 0.0691\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1810 - val_loss: 0.0688\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1838 - val_loss: 0.0691\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1818 - val_loss: 0.0689\n",
            "Training for SNR= 4.5  sigma= 0.5956621435290105\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 225us/sample - loss: 0.1615 - val_loss: 0.0679\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1621 - val_loss: 0.0674\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1600 - val_loss: 0.0669\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1622 - val_loss: 0.0668\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1619 - val_loss: 0.0667\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1607 - val_loss: 0.0666\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1611 - val_loss: 0.0666\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1614 - val_loss: 0.0666\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1605 - val_loss: 0.0666\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1624 - val_loss: 0.0667\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1624 - val_loss: 0.0664\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1583 - val_loss: 0.0664\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1590 - val_loss: 0.0662\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1578 - val_loss: 0.0662\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1623 - val_loss: 0.0663\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1609 - val_loss: 0.0664\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1605 - val_loss: 0.0664\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.1643 - val_loss: 0.0663\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1618 - val_loss: 0.0663\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 16us/sample - loss: 0.1611 - val_loss: 0.0664\n",
            "Training for SNR= 5.0  sigma= 0.5623413251903491\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 235us/sample - loss: 0.1416 - val_loss: 0.0657\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1417 - val_loss: 0.0654\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.1419 - val_loss: 0.0650\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.1426 - val_loss: 0.0649\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.1414 - val_loss: 0.0649\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1409 - val_loss: 0.0648\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1421 - val_loss: 0.0647\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.1413 - val_loss: 0.0646\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.1424 - val_loss: 0.0647\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.1427 - val_loss: 0.0646\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 16us/sample - loss: 0.1429 - val_loss: 0.0648\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.1414 - val_loss: 0.0645\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1411 - val_loss: 0.0644\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1413 - val_loss: 0.0646\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1408 - val_loss: 0.0645\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1399 - val_loss: 0.0645\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1428 - val_loss: 0.0645\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1420 - val_loss: 0.0645\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1439 - val_loss: 0.0644\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1412 - val_loss: 0.0643\n",
            "Training for SNR= 5.5  sigma= 0.5308844442309884\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 224us/sample - loss: 0.1273 - val_loss: 0.0640\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1258 - val_loss: 0.0641\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1255 - val_loss: 0.0639\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1256 - val_loss: 0.0637\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1260 - val_loss: 0.0637\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1264 - val_loss: 0.0636\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1264 - val_loss: 0.0635\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1230 - val_loss: 0.0633\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1260 - val_loss: 0.0637\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1248 - val_loss: 0.0637\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.1250 - val_loss: 0.0634\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1253 - val_loss: 0.0634\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1258 - val_loss: 0.0632\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1233 - val_loss: 0.0634\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1242 - val_loss: 0.0633\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1244 - val_loss: 0.0632\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1239 - val_loss: 0.0629\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1273 - val_loss: 0.0632\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1236 - val_loss: 0.0629\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.1250 - val_loss: 0.0630\n",
            "Training for SNR= 6.0  sigma= 0.5011872336272722\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 230us/sample - loss: 0.1109 - val_loss: 0.0628\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1113 - val_loss: 0.0625\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1094 - val_loss: 0.0623\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1121 - val_loss: 0.0621\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1112 - val_loss: 0.0621\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1102 - val_loss: 0.0617\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1092 - val_loss: 0.0617\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1095 - val_loss: 0.0615\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1084 - val_loss: 0.0610\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.1104 - val_loss: 0.0606\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1112 - val_loss: 0.0592\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1081 - val_loss: 0.0528\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1041 - val_loss: 0.0424\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0982 - val_loss: 0.0289\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0896 - val_loss: 0.0114\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0769 - val_loss: 0.0021\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0740 - val_loss: 0.0011\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0706 - val_loss: 9.1189e-04\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0709 - val_loss: 9.3314e-04\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0701 - val_loss: 9.8531e-04\n",
            "Training for SNR= 6.5  sigma= 0.47315125896148047\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 229us/sample - loss: 0.0550 - val_loss: 7.8187e-04\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0549 - val_loss: 7.0024e-04\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0538 - val_loss: 6.5959e-04\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0520 - val_loss: 6.0480e-04\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0546 - val_loss: 5.8034e-04\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0528 - val_loss: 5.7081e-04\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0532 - val_loss: 5.5473e-04\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0527 - val_loss: 5.3800e-04\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0537 - val_loss: 5.3552e-04\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0533 - val_loss: 5.1613e-04\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0527 - val_loss: 5.2220e-04\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0531 - val_loss: 4.9340e-04\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0523 - val_loss: 4.8417e-04\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0514 - val_loss: 4.7923e-04\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0538 - val_loss: 4.6705e-04\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0519 - val_loss: 4.4963e-04\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0496 - val_loss: 4.3885e-04\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0542 - val_loss: 4.5025e-04\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0523 - val_loss: 4.3764e-04\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0529 - val_loss: 4.3447e-04\n",
            "Training for SNR= 7.0  sigma= 0.44668359215096315\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 234us/sample - loss: 0.0403 - val_loss: 3.9794e-04\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0379 - val_loss: 3.5338e-04\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0395 - val_loss: 3.2702e-04\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0396 - val_loss: 3.1310e-04\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0410 - val_loss: 2.9828e-04\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0379 - val_loss: 2.8506e-04\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0391 - val_loss: 2.7073e-04\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0376 - val_loss: 2.6024e-04\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0393 - val_loss: 2.5501e-04\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0399 - val_loss: 2.5610e-04\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0378 - val_loss: 2.4164e-04\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0399 - val_loss: 2.3636e-04\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0384 - val_loss: 2.3595e-04\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0383 - val_loss: 2.2549e-04\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0397 - val_loss: 2.2828e-04\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0379 - val_loss: 2.1643e-04\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0384 - val_loss: 2.1863e-04\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0383 - val_loss: 2.1194e-04\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0396 - val_loss: 2.1218e-04\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0394 - val_loss: 2.0803e-04\n",
            "Training for SNR= 7.5  sigma= 0.4216965034285822\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 236us/sample - loss: 0.0292 - val_loss: 1.9278e-04\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0289 - val_loss: 1.7484e-04\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0286 - val_loss: 1.6575e-04\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0287 - val_loss: 1.5792e-04\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0279 - val_loss: 1.5008e-04\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0282 - val_loss: 1.4186e-04\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0284 - val_loss: 1.3883e-04\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0280 - val_loss: 1.3382e-04\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0280 - val_loss: 1.3026e-04\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0272 - val_loss: 1.2329e-04\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0283 - val_loss: 1.2544e-04\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0283 - val_loss: 1.1944e-04\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0287 - val_loss: 1.1593e-04\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0271 - val_loss: 1.1892e-04\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0272 - val_loss: 1.1548e-04\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0279 - val_loss: 1.1186e-04\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0275 - val_loss: 1.1037e-04\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0284 - val_loss: 1.0912e-04\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0288 - val_loss: 1.0769e-04\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0282 - val_loss: 1.0568e-04\n",
            "Training for SNR= 8.0  sigma= 0.3981071705534972\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 237us/sample - loss: 0.0203 - val_loss: 9.9352e-05\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0192 - val_loss: 9.0447e-05\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0199 - val_loss: 8.5624e-05\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0194 - val_loss: 8.0049e-05\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0190 - val_loss: 7.6579e-05\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0200 - val_loss: 7.5707e-05\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0190 - val_loss: 7.1260e-05\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0202 - val_loss: 6.9259e-05\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0207 - val_loss: 6.8667e-05\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0203 - val_loss: 6.6624e-05\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0189 - val_loss: 6.3850e-05\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0201 - val_loss: 6.2295e-05\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0194 - val_loss: 6.1410e-05\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0192 - val_loss: 6.1496e-05\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0205 - val_loss: 6.1004e-05\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0209 - val_loss: 6.1802e-05\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0195 - val_loss: 5.7823e-05\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0193 - val_loss: 5.8233e-05\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0197 - val_loss: 5.6929e-05\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0198 - val_loss: 5.7304e-05\n",
            "Training for SNR= 8.5  sigma= 0.3758374042884442\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 238us/sample - loss: 0.0133 - val_loss: 5.3914e-05\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0130 - val_loss: 4.9743e-05\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0140 - val_loss: 4.6965e-05\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0130 - val_loss: 4.5224e-05\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0131 - val_loss: 4.3121e-05\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0133 - val_loss: 4.1423e-05\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0127 - val_loss: 4.0847e-05\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0130 - val_loss: 3.9610e-05\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0132 - val_loss: 3.9646e-05\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0147 - val_loss: 3.8830e-05\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0142 - val_loss: 3.7376e-05\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0122 - val_loss: 3.6443e-05\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0121 - val_loss: 3.4504e-05\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0137 - val_loss: 3.3745e-05\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0132 - val_loss: 3.3956e-05\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0122 - val_loss: 3.2546e-05\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0121 - val_loss: 3.1952e-05\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0125 - val_loss: 3.1675e-05\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0127 - val_loss: 3.1470e-05\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0132 - val_loss: 3.0581e-05\n",
            "Training for SNR= 9.0  sigma= 0.35481338923357547\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 235us/sample - loss: 0.0087 - val_loss: 2.8969e-05\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0078 - val_loss: 2.7979e-05\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0082 - val_loss: 2.7069e-05\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0093 - val_loss: 2.6412e-05\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0084 - val_loss: 2.6644e-05\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0081 - val_loss: 2.4623e-05\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0081 - val_loss: 2.3885e-05\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0083 - val_loss: 2.2476e-05\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0080 - val_loss: 2.2171e-05\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0081 - val_loss: 2.1934e-05\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0081 - val_loss: 2.0778e-05\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0090 - val_loss: 2.0400e-05\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0086 - val_loss: 2.0151e-05\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0089 - val_loss: 2.0407e-05\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0075 - val_loss: 1.9890e-05\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0077 - val_loss: 1.9929e-05\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0080 - val_loss: 1.8751e-05\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0086 - val_loss: 1.8051e-05\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0083 - val_loss: 1.7326e-05\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0079 - val_loss: 1.7338e-05\n",
            "Training for SNR= 9.5  sigma= 0.33496543915782767\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 240us/sample - loss: 0.0050 - val_loss: 1.7143e-05\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0049 - val_loss: 1.6321e-05\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0052 - val_loss: 1.5715e-05\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0055 - val_loss: 1.5268e-05\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0053 - val_loss: 1.4785e-05\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0049 - val_loss: 1.4330e-05\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0055 - val_loss: 1.4269e-05\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0056 - val_loss: 1.4009e-05\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0048 - val_loss: 1.3645e-05\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0051 - val_loss: 1.2782e-05\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0047 - val_loss: 1.2974e-05\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0062 - val_loss: 1.2605e-05\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0050 - val_loss: 1.2666e-05\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0050 - val_loss: 1.2404e-05\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0048 - val_loss: 1.1967e-05\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0057 - val_loss: 1.1675e-05\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0056 - val_loss: 1.1777e-05\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0045 - val_loss: 1.1316e-05\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0052 - val_loss: 1.0983e-05\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0051 - val_loss: 1.1088e-05\n",
            "Training for SNR= 10.0  sigma= 0.31622776601683794\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 241us/sample - loss: 0.0032 - val_loss: 1.0630e-05\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0030 - val_loss: 1.0454e-05\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0028 - val_loss: 1.0296e-05\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0030 - val_loss: 1.0088e-05\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0028 - val_loss: 9.7930e-06\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0030 - val_loss: 9.3830e-06\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0030 - val_loss: 9.5305e-06\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0029 - val_loss: 8.9993e-06\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0034 - val_loss: 8.9190e-06\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0032 - val_loss: 8.6047e-06\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0031 - val_loss: 8.5919e-06\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0030 - val_loss: 8.6284e-06\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0031 - val_loss: 8.2661e-06\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0035 - val_loss: 8.3323e-06\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0029 - val_loss: 8.1808e-06\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0028 - val_loss: 7.9088e-06\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0034 - val_loss: 7.8545e-06\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0033 - val_loss: 7.9879e-06\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0029 - val_loss: 7.7724e-06\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 16us/sample - loss: 0.0033 - val_loss: 7.4446e-06\n",
            "Training for SNR= 10.5  sigma= 0.29853826189179594\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 243us/sample - loss: 0.0017 - val_loss: 7.3431e-06\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0018 - val_loss: 7.3033e-06\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0017 - val_loss: 6.9508e-06\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0020 - val_loss: 7.2297e-06\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0018 - val_loss: 6.7832e-06\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0017 - val_loss: 6.8001e-06\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0018 - val_loss: 6.5804e-06\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0017 - val_loss: 6.4587e-06\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0016 - val_loss: 6.4491e-06\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0015 - val_loss: 6.0705e-06\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0019 - val_loss: 6.2033e-06\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0017 - val_loss: 6.1136e-06\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0016 - val_loss: 5.7906e-06\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0016 - val_loss: 5.9187e-06\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0019 - val_loss: 5.8030e-06\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0016 - val_loss: 5.6297e-06\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0014 - val_loss: 5.6816e-06\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0017 - val_loss: 5.6229e-06\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0018 - val_loss: 5.5792e-06\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0018 - val_loss: 5.4316e-06\n",
            "Training for SNR= 11.0  sigma= 0.28183829312644537\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 243us/sample - loss: 9.7266e-04 - val_loss: 5.1821e-06\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 7.8312e-04 - val_loss: 5.1507e-06\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 8.9759e-04 - val_loss: 5.1344e-06\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 8.0797e-04 - val_loss: 5.0295e-06\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0011 - val_loss: 4.6382e-06\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 9.8030e-04 - val_loss: 4.6706e-06\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 8.0034e-04 - val_loss: 4.5697e-06\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 16us/sample - loss: 0.0010 - val_loss: 4.6730e-06\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 9.5679e-04 - val_loss: 4.5060e-06\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0010 - val_loss: 4.6149e-06\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0011 - val_loss: 4.6111e-06\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 9.0851e-04 - val_loss: 4.5176e-06\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0011 - val_loss: 4.2791e-06\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 8.4629e-04 - val_loss: 4.2911e-06\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 9.3245e-04 - val_loss: 4.1833e-06\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 8.7051e-04 - val_loss: 3.9831e-06\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 7.7489e-04 - val_loss: 4.0836e-06\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0010 - val_loss: 3.9225e-06\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0010 - val_loss: 3.8507e-06\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0010 - val_loss: 3.8332e-06\n",
            "Training for SNR= 11.5  sigma= 0.26607250597988097\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "10000/10000 [==============================] - 2s 245us/sample - loss: 4.6389e-04 - val_loss: 3.8402e-06\n",
            "Epoch 2/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 6.2008e-04 - val_loss: 3.8494e-06\n",
            "Epoch 3/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 4.7621e-04 - val_loss: 3.7622e-06\n",
            "Epoch 4/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 4.1550e-04 - val_loss: 3.7250e-06\n",
            "Epoch 5/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 4.9105e-04 - val_loss: 3.6931e-06\n",
            "Epoch 6/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 5.3831e-04 - val_loss: 3.5446e-06\n",
            "Epoch 7/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 4.2690e-04 - val_loss: 3.4469e-06\n",
            "Epoch 8/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 4.3623e-04 - val_loss: 3.4250e-06\n",
            "Epoch 9/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 4.8186e-04 - val_loss: 3.2875e-06\n",
            "Epoch 10/20\n",
            "10000/10000 [==============================] - 0s 13us/sample - loss: 4.9594e-04 - val_loss: 3.2872e-06\n",
            "Epoch 11/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 5.1705e-04 - val_loss: 3.3346e-06\n",
            "Epoch 12/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 5.4799e-04 - val_loss: 3.3428e-06\n",
            "Epoch 13/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 5.0565e-04 - val_loss: 3.2097e-06\n",
            "Epoch 14/20\n",
            "10000/10000 [==============================] - 0s 17us/sample - loss: 4.1758e-04 - val_loss: 3.2712e-06\n",
            "Epoch 15/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 6.2760e-04 - val_loss: 3.1341e-06\n",
            "Epoch 16/20\n",
            "10000/10000 [==============================] - 0s 14us/sample - loss: 4.7848e-04 - val_loss: 2.9730e-06\n",
            "Epoch 17/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 5.2984e-04 - val_loss: 3.0439e-06\n",
            "Epoch 18/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 4.3272e-04 - val_loss: 2.9976e-06\n",
            "Epoch 19/20\n",
            "10000/10000 [==============================] - 0s 16us/sample - loss: 4.4340e-04 - val_loss: 2.9804e-06\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 15us/sample - loss: 5.2302e-04 - val_loss: 2.9975e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHByzQbTUqbv",
        "outputId": "d95bb66a-5167-4d4b-84c9-d5dfbd0b97b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using trained model\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_dl_tensor  = numpy.array(())\n",
        "times_per_iter_dl_tensor = numpy.array(())\n",
        "\n",
        "#awgn_channel_tx = GaussianNoise(0.5,input_shape=(CHANEL_SIZE,))\n",
        "\n",
        "#awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [CHANEL_SIZE])\n",
        "#awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "#awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "#awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)\n",
        "\n",
        "\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    input_message_xx = input_message [i:i+1]\n",
        "    #print (\"input\", input_message_xx)\n",
        "    encoded_message = encoder.predict(input_message_xx)\n",
        "    #encoded_message = numpy.around(encoded_message > 0.5).astype(int)\n",
        "    #print(encoded_message)\n",
        "    #print (\"encoded\", encoded_message)\n",
        "    #noised_message = awgn_channel.predict (encoded_message)\n",
        "    noised_message = commpy.channels.awgn(encoded_message, snr)\n",
        "    #awgn_channel = GaussianNoise(sigma,input_shape=(CHANEL_SIZE,))\n",
        "    #noised_message = awgn_channel.predict(encoded_message)\n",
        "    #noised_message = awgn_layer (encoded_message)\n",
        "    #awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:0.5, awgn_channel_input:encoded_message[0]})\n",
        "    #print(noised_message)\n",
        "    decoded_message = decoder.predict(noised_message)\n",
        "    decoded_message = numpy.around(decoded_message[0]).astype(int)\n",
        "    #decoded_message = numpy.around(decoded_message > 0.5).astype(int)\n",
        "    #print (\".\")\n",
        "    #autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))\n",
        "    #decoded_message = autoencoder.predict(input_message_xx)\n",
        "    #print (\"output\", decoded_message)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "      #print (\"Error\")\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_dl_tensor=numpy.append(ber_per_iter_dl_tensor ,ber)\n",
        "  times_per_iter_dl_tensor=numpy.append(times_per_iter_dl_tensor, total_time)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 2.19s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 4.37s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 6.45s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 8.49s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.62\n",
            " -> Total Time: 21.50s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 2.15s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 4.34s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 6.46s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 8.50s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.63\n",
            " -> Total Time: 21.44s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 2.19s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 4.21s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 6.26s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 8.35s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.61\n",
            " -> Total Time: 21.00s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 2.15s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 4.21s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 6.31s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 8.50s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.57\n",
            " -> Total Time: 21.16s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 1.91s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 3.94s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 6.07s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 8.17s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.57\n",
            " -> Total Time: 20.09s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 2.06s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 4.12s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 6.24s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 8.25s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.56\n",
            " -> Total Time: 20.68s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 2.10s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 4.28s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 6.38s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 8.36s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.50\n",
            " -> Total Time: 21.11s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 2.14s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 4.29s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 6.46s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 8.57s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.49\n",
            " -> Total Time: 21.47s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 2.05s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 4.34s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 6.62s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 8.82s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.47\n",
            " -> Total Time: 21.84s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 1.99s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 4.08s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 6.15s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 8.26s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.45\n",
            " -> Total Time: 20.48s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 2.05s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 4.00s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 6.02s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 8.22s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.45\n",
            " -> Total Time: 20.28s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 2.21s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 4.35s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 6.46s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 8.61s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.44\n",
            " -> Total Time: 21.63s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 1.99s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 4.04s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 6.12s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 8.28s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.40\n",
            " -> Total Time: 20.43s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 2.11s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 4.15s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 6.20s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 8.13s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.39\n",
            " -> Total Time: 20.60s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 2.05s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 4.03s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 6.22s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 8.31s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.37\n",
            " -> Total Time: 20.61s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 1.97s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 4.03s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 6.18s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 8.30s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.33\n",
            " -> Total Time: 20.48s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 2.16s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 4.26s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 6.30s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 8.36s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.31\n",
            " -> Total Time: 21.09s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 2.13s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 4.26s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 6.39s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 8.66s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.27\n",
            " -> Total Time: 21.44s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 2.26s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 4.39s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 6.50s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 8.55s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.25\n",
            " -> Total Time: 21.70s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 2.09s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 4.27s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 6.48s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 8.65s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.22\n",
            " -> Total Time: 21.49s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syUQij3fuxRm",
        "outputId": "42dfa3e7-b223-4d2a-ee8d-e74c1b625d21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1) = plt.subplots(1,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"ldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor,'', label=\"ai-dl\") # plot BER vs SNR\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "#ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "#ax2.set_xlabel('$E_b/$N_0$')\n",
        "#ax2.set_ylabel('Decoding Time [s]')\n",
        "#ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "#            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "#            xy=(1, 0.35), xycoords='axes fraction',\n",
        "#            xytext=(-20, 20), textcoords='offset pixels',\n",
        "#            horizontalalignment='right',\n",
        "#            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}