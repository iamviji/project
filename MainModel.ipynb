{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MainModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamviji/project/blob/master/MainModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDSPPMfZ9czi",
        "outputId": "a03d11ce-a4c4-415a-a663-22c8cc9a9866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "!rm -rf project\n",
        "!git clone https://github.com/iamviji/project.git\n",
        "!ls\n",
        "!ls project\n",
        "!pip install pyldpc\n",
        "!pip install scikit-commpy\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'project'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 68 (delta 19), reused 8 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (68/68), done.\n",
            "project  sample_data\n",
            "MainModel.ipynb       MainModelOneHotMethod.ipynb\t    README.md\n",
            "MainModelKeras.ipynb  MainModelWithSingleBERTraining.ipynb  util.py\n",
            "Collecting pyldpc\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/aa/fd5495869c7106a638ae71aa497d7d266cae7f2a343d1f6a9d0e3a986e1e/pyldpc-0.7.9.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyldpc) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyldpc) (1.4.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from pyldpc) (0.48.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->pyldpc) (50.3.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->pyldpc) (0.31.0)\n",
            "Building wheels for collected packages: pyldpc\n",
            "  Building wheel for pyldpc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyldpc: filename=pyldpc-0.7.9-cp36-none-any.whl size=14306 sha256=25cbe0ef5dde908b9c247704e92ee59839573379b5cebd272d15a7673c6644e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/7a/10/e94058ba8b0b6d98bf2719226d18d3dd6056525ad7b984c068\n",
            "Successfully built pyldpc\n",
            "Installing collected packages: pyldpc\n",
            "Successfully installed pyldpc-0.7.9\n",
            "Collecting scikit-commpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/b4/f7fa5bc8864e0ddbd3e7a2290b624b92690f53523474024915c33321802d/scikit_commpy-0.5.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-commpy) (1.18.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-commpy) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->scikit-commpy) (1.15.0)\n",
            "Installing collected packages: scikit-commpy\n",
            "Successfully installed scikit-commpy-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QOuLqpdDgx2",
        "outputId": "dfaa5a58-aabc-47d1-8869-6ed215e8313d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import pyldpc\n",
        "import commpy\n",
        "import numpy \n",
        "import time\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior ()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YClXJbbr0lc7"
      },
      "source": [
        "SNR_BEGIN = 0\n",
        "SNR_END = 10\n",
        "SNR_STEP_SIZE = 0.5\n",
        "CHANEL_SIZE = 18\n",
        "NUM_OF_INPUT_MESSAGE = 1000\n",
        "LDPC_MAX_ITER = 100\n",
        "num_parity_check = 3\n",
        "num_bits_in_parity_check = 6 \n",
        "input_message_length =  0 # Caculated by channel encoder and initialized later"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvUzIMsB43i0"
      },
      "source": [
        "def timer_update(i,current,time_tot,tic_incr=500):\n",
        "    last = current\n",
        "    current = time.time()\n",
        "    t_diff = current-last\n",
        "    print('SNR: {:04.3f} - Iter: {} - Last {} iterations took {:03.2f}s'.format(snr,i+1,tic_incr,t_diff))\n",
        "    return time_tot + t_diff\n",
        "\n",
        "def Snr2Sigma(snr):\n",
        "  sigma = 10 ** (- snr / 20)\n",
        "  return sigma\n",
        "\n",
        "def pyldpc_encode (CodingMatrix, message):\n",
        "  rng = pyldpc.utils.check_random_state(seed=None)\n",
        "  d = pyldpc.utils.binaryproduct(CodingMatrix, message)\n",
        "  encoded_message = (-1) ** d\n",
        "  return encoded_message\n",
        "\n",
        "def pyldpc_decode (ParityCheckMatrix, CodingMatrix, message, snr, maxiter):\n",
        "  decoded_msg = pyldpc.decode(ParityCheckMatrix, message, snr, maxiter)\n",
        "  out_message = pyldpc.get_message(CodingMatrix, decoded_msg)\n",
        "  return out_message\n",
        "\n",
        "awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [CHANEL_SIZE])\n",
        "awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)\n",
        "\n",
        "init = tf.global_variables_initializer ()\n",
        "sess = tf.Session ()\n",
        "sess.run(init)\n",
        "\n",
        "def AWGNChannelOutput (xx, snr , s):\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  awgn_channel_output_message = s.run ([awgn_channel_output], feed_dict={noise_std_dev:sigma, channel_input:xx})\n",
        "  return awgn_channel_output_message"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jMQG-MZ_pXu",
        "outputId": "bac72019-7960-4208-c99b-ce615e274504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "\n",
        "ParityCheckMatrix, CodingMatrix = pyldpc.make_ldpc(CHANEL_SIZE, num_parity_check, num_bits_in_parity_check, systematic=True, sparse=True)\n",
        "input_message_length = CodingMatrix.shape[1]\n",
        "print (\"input_message_size=\", input_message_length, \"channel_size=\",CHANEL_SIZE)\n",
        "print (\"input_message_size=\", CodingMatrix.shape[1], \"channel_size=\",CodingMatrix.shape[0])\n",
        "input_message = numpy.random.randint(2, size=(NUM_OF_INPUT_MESSAGE,input_message_length))\n",
        "print (input_message)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_message_size= 11 channel_size= 18\n",
            "input_message_size= 11 channel_size= 18\n",
            "[[1 0 1 ... 0 1 0]\n",
            " [0 0 1 ... 0 1 0]\n",
            " [1 0 0 ... 1 0 0]\n",
            " ...\n",
            " [0 1 0 ... 1 0 0]\n",
            " [1 1 0 ... 0 1 0]\n",
            " [1 0 1 ... 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WKg2HU2adgZ"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fL8ptL4aeOY"
      },
      "source": [
        "This section tries to compare BER and Time performance of PYLDPC in following 3 cases\n",
        "1. SNR Noise function provided in encoder function of pyldpc library (pyldpc.encode)\n",
        "2. SNR Noise function provided by commpy library (commpy.channels.awgn) \n",
        "3. SNR Noise function implemented using tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma5zUqFv0TH2",
        "outputId": "60fe8c9b-8c14-4092-9be8-094e37927250",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using tensor flow based AWGN, to make sure that effect of it is same as AWGN\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_tensor  = numpy.array(())\n",
        "times_per_iter_tensor = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc_encode (CodingMatrix, input_message[i])\n",
        "    sigma = Snr2Sigma (snr)\n",
        "    awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message})[0]\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      #print (\"count=\",abs(decoded_message-input_message[i]).sum())\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_tensor=numpy.append(ber_per_iter_tensor ,ber)\n",
        "  times_per_iter_tensor=numpy.append(times_per_iter_tensor, total_time)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.75s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 3.31s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 4.74s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 6.27s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.60\n",
            " -> Total Time: 16.06s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.18s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.46s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 3.80s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 5.11s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.54\n",
            " -> Total Time: 12.55s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 1.16s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 2.25s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 3.42s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 4.41s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.42\n",
            " -> Total Time: 11.24s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.92s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.81s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.66s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 3.49s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.37\n",
            " -> Total Time: 8.88s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.75s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.56s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 2.21s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.96s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.30\n",
            " -> Total Time: 7.49s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.57s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.13s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.71s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.25s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.22\n",
            " -> Total Time: 5.66s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.45s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.99s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.53s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 2.08s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.16\n",
            " -> Total Time: 5.06s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.38s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.80s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 1.21s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.68s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.12\n",
            " -> Total Time: 4.08s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.46s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.83s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 1.24s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.63s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.12\n",
            " -> Total Time: 4.16s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.32s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.60s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.93s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.24s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.05\n",
            " -> Total Time: 3.08s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.28s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.58s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.88s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 1.20s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 2.94s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.29s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.56s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.84s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 1.15s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 2.84s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.28s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.55s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.83s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 1.10s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.76s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.29s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.55s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.82s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 1.09s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.74s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.54s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.83s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 1.10s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.73s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.80s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 1.09s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.68s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.81s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 1.10s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.69s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.79s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 1.04s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.61s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.80s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 1.06s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.64s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.80s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 1.07s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.66s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8dIFLg76c7O",
        "outputId": "81abffbb-b27f-4c3d-fb93-aea854cdce6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using commpy based AWGN \n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_awgn  = numpy.array(())\n",
        "times_per_iter_awgn = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc_encode (CodingMatrix, input_message[i])\n",
        "    awgn_channel_output_message = commpy.channels.awgn(encoded_message, snr)\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_awgn=numpy.append(ber_per_iter_awgn ,ber)\n",
        "  times_per_iter_awgn=numpy.append(times_per_iter_awgn, total_time)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.22s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 2.72s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 4.18s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 5.78s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.61\n",
            " -> Total Time: 13.91s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.23s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.39s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 3.63s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 4.88s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.54\n",
            " -> Total Time: 12.13s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.85s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 1.75s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 2.70s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 3.52s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.43\n",
            " -> Total Time: 8.82s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.76s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.59s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.33s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 2.97s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.34\n",
            " -> Total Time: 7.65s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.56s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.15s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 1.72s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.27s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.27\n",
            " -> Total Time: 5.69s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.52s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.01s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.58s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 2.07s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.26\n",
            " -> Total Time: 5.18s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.41s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.89s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.20s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.56s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.17\n",
            " -> Total Time: 4.06s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.30s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.63s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 0.97s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.26s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.12\n",
            " -> Total Time: 3.17s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.76s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.02s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.08\n",
            " -> Total Time: 2.53s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.46s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.06\n",
            " -> Total Time: 2.41s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.44s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.64s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.87s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 2.19s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.39s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.59s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.77s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 1.93s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.18s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.37s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.53s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.69s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.77s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.34s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.52s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.68s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.70s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.33s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.49s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.66s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.64s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.33s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.49s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.66s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.65s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.34s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.50s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.67s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.67s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.48s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.64s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.60s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.48s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.64s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.59s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.49s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.65s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.63s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ihPKJJk7Jj9",
        "outputId": "afac564b-e639-4a9e-9d7e-6752ae0164b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using tensor flow based AWGN, to make sure that effect of it is same as AWGN\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_pyldpc  = numpy.array(())\n",
        "times_per_iter_pyldpc = numpy.array(())\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    encoded_message = pyldpc.encode (CodingMatrix, input_message[i], snr)\n",
        "    awgn_channel_output_message = encoded_message\n",
        "    decoded_message = pyldpc_decode(ParityCheckMatrix, CodingMatrix, awgn_channel_output_message, snr, LDPC_MAX_ITER)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_pyldpc=numpy.append(ber_per_iter_pyldpc ,ber)\n",
        "  times_per_iter_pyldpc=numpy.append(times_per_iter_pyldpc, total_time)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyldpc/decoder.py:63: UserWarning: Decoding stopped before convergence. You may want\n",
            "                       to increase maxiter\n",
            "  to increase maxiter\"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 1.34s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 2.80s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 4.24s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 5.65s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.61\n",
            " -> Total Time: 14.04s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 1.17s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 2.59s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 3.72s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 4.88s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.54\n",
            " -> Total Time: 12.35s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 1.02s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 1.97s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 2.91s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 3.83s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.45\n",
            " -> Total Time: 9.73s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.76s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.53s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.42s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 3.21s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.36\n",
            " -> Total Time: 7.93s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.67s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.24s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 1.87s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 2.47s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.29\n",
            " -> Total Time: 6.25s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.40s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 0.78s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 1.23s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 1.80s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.23\n",
            " -> Total Time: 4.20s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.31s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.77s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 1.11s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.45s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.17\n",
            " -> Total Time: 3.65s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.36s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.66s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 0.96s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.29s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.12\n",
            " -> Total Time: 3.26s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.09\n",
            " -> Total Time: 2.54s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.39s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.60s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 0.84s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 2.01s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.20s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.41s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.58s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.76s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 1.94s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.34s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.53s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.71s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 1.75s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.33s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.50s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.68s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.66s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.33s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.49s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.66s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.62s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.48s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.64s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 1.59s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.48s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.65s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.60s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.16s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.47s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.63s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.59s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.15s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.31s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.46s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 0.62s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.55s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.34s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.49s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.65s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.64s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.17s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.32s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.48s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.63s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 1.60s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR4-FOJ-BkAG",
        "outputId": "ef4f0e9a-3c1f-4349-97b4-2841d40c5cc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "# Compare 3 AWGN(Tensorflow, CommPy, PYLDPC) Simulation on LDPC\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_pyldpc,'', label=\"pyldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"tensor\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_awgn,'', label=\"commpy-awgn\") # plot BER vs SNR\n",
        "\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "ax2.set_xlabel('$E_b/$N_0$')\n",
        "ax2.set_ylabel('Decoding Time [s]')\n",
        "ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "            xy=(1, 0.35), xycoords='axes fraction',\n",
        "            xytext=(-20, 20), textcoords='offset pixels',\n",
        "            horizontalalignment='right',\n",
        "            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggY5VwudaRwR"
      },
      "source": [
        "<B>Conclussion:</B>\n",
        "      It proved that tensorflow behaves similar to AWGN noise channel provided by pyldpc, commpy. But tensor flow based one takes adds little more time delay. This need to be offseted if we are comparing performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOeuNfeLCgfb",
        "outputId": "560b2368-4a89-4781-928c-b7c5f715bfe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Define Model \n",
        "\n",
        "# input_message_length is initialized by ldpc encoder\n",
        "num_hidden_1 = CHANEL_SIZE\n",
        "print (\"input_message_length=\", input_message_length)\n",
        "\n",
        "input_message_x = tf.placeholder(\"float32\", [None, input_message_length], name=\"input_message_x\")\n",
        "awgn_noise_std_dev_x = tf.placeholder(\"float32\", name =\"awgn_noise_std_dev\")\n",
        "input_channel_x = tf.placeholder(\"float32\", [None, CHANEL_SIZE], name=\"input_channel_x\")\n",
        "\n",
        "weights = {\n",
        "  \"encoder_l1\" : tf.Variable (tf.random_uniform([input_message_length, num_hidden_1], -1, 1), name=\"encoder_l1_weights\"),\n",
        "  \"decoder_l1\" : tf.Variable (tf.random_uniform([num_hidden_1, input_message_length], -1, 1), name=\"decoder_l1_weights\"),\n",
        "  \"decoder_l2\" : tf.Variable (tf.random_uniform([input_message_length, input_message_length], -1, 1), name=\"decoder_l2_weights\"),\n",
        "}\n",
        "\n",
        "biases = {\n",
        "  \"encoder_l1\" : tf.Variable (tf.random_uniform([num_hidden_1], -1,1), name=\"encoder_l1_bias\"),\n",
        "  \"decoder_l1\" : tf.Variable (tf.random_uniform([input_message_length], -1,1), name=\"decoder_l1_bias\"),\n",
        "  \"decoder_l2\" : tf.Variable (tf.random_uniform([input_message_length], -1,1), name=\"decoder_l2_bias\"),\n",
        "}\n",
        "\n",
        "def dl_encoder (x):\n",
        "  layer_1 = tf.nn.tanh (tf.matmul(x, weights['encoder_l1']) + biases['encoder_l1'])\n",
        "  #layer_2 = tf.round(layer_1)\n",
        "  layer_2 =  layer_1 / tf.sqrt(tf.reduce_mean(tf.square(layer_1)))\n",
        "  #layer_2 =  tf.nn.relu(layer_1)\n",
        "  return layer_2\n",
        "\n",
        "def dl_decoder (x):\n",
        "  layer_1 = tf.nn.tanh (tf.matmul(x, weights['decoder_l1']) + biases['decoder_l1'])\n",
        "  layer_2 = tf.nn.sigmoid (tf.matmul(layer_1, weights['decoder_l2']) + biases['decoder_l2'])\n",
        "  return layer_2\n",
        "\n",
        "def awgn_layer(x):\n",
        "  awgn_noise = tf.random.normal(tf.shape(x), stddev=awgn_noise_std_dev_x,  name=\"awgn_noise\")\n",
        "  awgn_channel_output = tf.add(x, awgn_noise, name =\"x_and_noise\")\n",
        "  return awgn_channel_output\n",
        "\n",
        "\n",
        "dl_encoder_output = dl_encoder(input_message_x)\n",
        "dl_decoder_input = awgn_layer(dl_encoder_output)\n",
        "#awgn_noise = tf.random.normal(tf.shape(dl_encoder_output), stddev=awgn_noise_std_dev,  name=\"awgn_noise\")\n",
        "#dl_decoder_input = tf.add(dl_encoder_output, awgn_noise, name =\"x_and_noise\")\n",
        "dl_decoder_output = dl_decoder (dl_decoder_input)\n",
        "dl_decoder_only_output = dl_decoder(input_channel_x)\n",
        "\n",
        "\n",
        "loss1 = tf.reduce_mean (-1 * (input_message_x*tf.log(dl_decoder_output) + (1 - input_message_x)*tf.log(1 - dl_decoder_output) ))\n",
        "lr = tf.placeholder(dtype=tf.float32,shape=[])\n",
        "rms_optimizer = tf.train.AdamOptimizer(learning_rate=0.00007).minimize (loss1)\n",
        "rms_optimizer1 = tf.train.AdamOptimizer(learning_rate=0.001).minimize (loss1)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_message_length= 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IquiCDkRMGYq",
        "outputId": "538cad8b-cf40-45d0-ab8a-f233c84d6d1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import random\n",
        "training_input_message = numpy.random.randint(2, size=(2**input_message_length,input_message_length))\n",
        "\n",
        "complete_input = 1\n",
        "if (complete_input):\n",
        "  #training_input_message = numpy.zeros (size=(2**input_message_length,input_message_length))\n",
        "  for i in range(training_input_message.shape[1]):\n",
        "    for j in range(input_message_length):\n",
        "      if (i >= 2**j):\n",
        "        training_input_message[i][j] = 1\n",
        "      else:  \n",
        "        training_input_message[i][j] = 0\n",
        "  random.shuffle(training_input_message)\n",
        "\n",
        "print (training_input_message)\n",
        "print (len(training_input_message))\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 1 ... 1 0 0]\n",
            " [0 1 0 ... 1 1 0]\n",
            " [0 0 1 ... 1 1 1]]\n",
            "2048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOXLOYLu8aML",
        "outputId": "68578c18-a56f-4a18-bba8-928a28e19bbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 200\n",
        "\n",
        "# Training\n",
        "train_init = tf.global_variables_initializer ()\n",
        "train_sess = tf.Session ()\n",
        "\n",
        "epochs = 50\n",
        "outer_ephocs = 1\n",
        "display_step = epochs/2\n",
        "num_of_batches = len(training_input_message) / batch_size\n",
        "print (\"batch_size:\", batch_size, \"num_of_batcches:\", num_of_batches)\n",
        "train_sess.run(train_init)\n",
        "l = 0\n",
        "lrate = 0.1\n",
        "\n",
        "for oe in range(outer_ephocs):\n",
        "  for snr in (numpy.arange (SNR_BEGIN, SNR_END, 0.25)):\n",
        "    sigma = 1.0*Snr2Sigma (snr)\n",
        "    print (\"Training for SNR=\", snr, \" sigma=\", sigma) \n",
        "    if (snr < 6): opt = rms_optimizer1\n",
        "    else : opt = rms_optimizer \n",
        "    for e in range(epochs):\n",
        "      for j in range (int(num_of_batches)):\n",
        "        x_train_batch = training_input_message [j*batch_size:(j+1)*batch_size]\n",
        "        x_train_batch_float = x_train_batch.astype(\"float32\")\n",
        "        _, l = train_sess.run ([opt, loss1], feed_dict={input_message_x:x_train_batch_float, awgn_noise_std_dev_x:sigma, lr:lrate})\n",
        "        if (l < 0.5): lrate = 0.001\n",
        "        if (l < 0.25): lrate = 0.00005\n",
        "        if j % display_step == 0:\n",
        "          print('Step %i: Minibatch Loss: %f' % (i*display_step+j, l))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch_size: 200 num_of_batcches: 10.24\n",
            "Training for SNR= 0.0  sigma= 1.0\n",
            "Step 3825: Minibatch Loss: 1.076185\n",
            "Step 3825: Minibatch Loss: 1.052594\n",
            "Step 3825: Minibatch Loss: 1.052495\n",
            "Step 3825: Minibatch Loss: 0.987120\n",
            "Step 3825: Minibatch Loss: 0.978465\n",
            "Step 3825: Minibatch Loss: 0.943497\n",
            "Step 3825: Minibatch Loss: 0.938027\n",
            "Step 3825: Minibatch Loss: 0.916269\n",
            "Step 3825: Minibatch Loss: 0.906800\n",
            "Step 3825: Minibatch Loss: 0.885019\n",
            "Step 3825: Minibatch Loss: 0.837268\n",
            "Step 3825: Minibatch Loss: 0.848925\n",
            "Step 3825: Minibatch Loss: 0.827011\n",
            "Step 3825: Minibatch Loss: 0.807421\n",
            "Step 3825: Minibatch Loss: 0.793502\n",
            "Step 3825: Minibatch Loss: 0.789432\n",
            "Step 3825: Minibatch Loss: 0.767688\n",
            "Step 3825: Minibatch Loss: 0.769750\n",
            "Step 3825: Minibatch Loss: 0.755347\n",
            "Step 3825: Minibatch Loss: 0.760643\n",
            "Step 3825: Minibatch Loss: 0.768927\n",
            "Step 3825: Minibatch Loss: 0.753698\n",
            "Step 3825: Minibatch Loss: 0.738158\n",
            "Step 3825: Minibatch Loss: 0.736296\n",
            "Step 3825: Minibatch Loss: 0.730609\n",
            "Step 3825: Minibatch Loss: 0.723975\n",
            "Step 3825: Minibatch Loss: 0.706733\n",
            "Step 3825: Minibatch Loss: 0.716333\n",
            "Step 3825: Minibatch Loss: 0.726125\n",
            "Step 3825: Minibatch Loss: 0.704065\n",
            "Step 3825: Minibatch Loss: 0.705075\n",
            "Step 3825: Minibatch Loss: 0.687991\n",
            "Step 3825: Minibatch Loss: 0.687723\n",
            "Step 3825: Minibatch Loss: 0.688911\n",
            "Step 3825: Minibatch Loss: 0.683071\n",
            "Step 3825: Minibatch Loss: 0.676757\n",
            "Step 3825: Minibatch Loss: 0.675450\n",
            "Step 3825: Minibatch Loss: 0.688171\n",
            "Step 3825: Minibatch Loss: 0.675984\n",
            "Step 3825: Minibatch Loss: 0.653893\n",
            "Step 3825: Minibatch Loss: 0.672883\n",
            "Step 3825: Minibatch Loss: 0.654620\n",
            "Step 3825: Minibatch Loss: 0.659175\n",
            "Step 3825: Minibatch Loss: 0.655758\n",
            "Step 3825: Minibatch Loss: 0.656335\n",
            "Step 3825: Minibatch Loss: 0.641623\n",
            "Step 3825: Minibatch Loss: 0.634232\n",
            "Step 3825: Minibatch Loss: 0.634835\n",
            "Step 3825: Minibatch Loss: 0.633482\n",
            "Step 3825: Minibatch Loss: 0.635811\n",
            "Training for SNR= 0.25  sigma= 0.9716279515771061\n",
            "Step 3825: Minibatch Loss: 0.619401\n",
            "Step 3825: Minibatch Loss: 0.626384\n",
            "Step 3825: Minibatch Loss: 0.605083\n",
            "Step 3825: Minibatch Loss: 0.607384\n",
            "Step 3825: Minibatch Loss: 0.601185\n",
            "Step 3825: Minibatch Loss: 0.603400\n",
            "Step 3825: Minibatch Loss: 0.596263\n",
            "Step 3825: Minibatch Loss: 0.586853\n",
            "Step 3825: Minibatch Loss: 0.592279\n",
            "Step 3825: Minibatch Loss: 0.580339\n",
            "Step 3825: Minibatch Loss: 0.575114\n",
            "Step 3825: Minibatch Loss: 0.566747\n",
            "Step 3825: Minibatch Loss: 0.573839\n",
            "Step 3825: Minibatch Loss: 0.565532\n",
            "Step 3825: Minibatch Loss: 0.560336\n",
            "Step 3825: Minibatch Loss: 0.558456\n",
            "Step 3825: Minibatch Loss: 0.560052\n",
            "Step 3825: Minibatch Loss: 0.550891\n",
            "Step 3825: Minibatch Loss: 0.540999\n",
            "Step 3825: Minibatch Loss: 0.544764\n",
            "Step 3825: Minibatch Loss: 0.540421\n",
            "Step 3825: Minibatch Loss: 0.528560\n",
            "Step 3825: Minibatch Loss: 0.521823\n",
            "Step 3825: Minibatch Loss: 0.522174\n",
            "Step 3825: Minibatch Loss: 0.517527\n",
            "Step 3825: Minibatch Loss: 0.520760\n",
            "Step 3825: Minibatch Loss: 0.509956\n",
            "Step 3825: Minibatch Loss: 0.512109\n",
            "Step 3825: Minibatch Loss: 0.500699\n",
            "Step 3825: Minibatch Loss: 0.518045\n",
            "Step 3825: Minibatch Loss: 0.506968\n",
            "Step 3825: Minibatch Loss: 0.504951\n",
            "Step 3825: Minibatch Loss: 0.503287\n",
            "Step 3825: Minibatch Loss: 0.493744\n",
            "Step 3825: Minibatch Loss: 0.480933\n",
            "Step 3825: Minibatch Loss: 0.488432\n",
            "Step 3825: Minibatch Loss: 0.491017\n",
            "Step 3825: Minibatch Loss: 0.494698\n",
            "Step 3825: Minibatch Loss: 0.473670\n",
            "Step 3825: Minibatch Loss: 0.478845\n",
            "Step 3825: Minibatch Loss: 0.480333\n",
            "Step 3825: Minibatch Loss: 0.468932\n",
            "Step 3825: Minibatch Loss: 0.472172\n",
            "Step 3825: Minibatch Loss: 0.469973\n",
            "Step 3825: Minibatch Loss: 0.468144\n",
            "Step 3825: Minibatch Loss: 0.479264\n",
            "Step 3825: Minibatch Loss: 0.473565\n",
            "Step 3825: Minibatch Loss: 0.466959\n",
            "Step 3825: Minibatch Loss: 0.471223\n",
            "Step 3825: Minibatch Loss: 0.451348\n",
            "Training for SNR= 0.5  sigma= 0.9440608762859234\n",
            "Step 3825: Minibatch Loss: 0.456192\n",
            "Step 3825: Minibatch Loss: 0.437575\n",
            "Step 3825: Minibatch Loss: 0.448115\n",
            "Step 3825: Minibatch Loss: 0.441345\n",
            "Step 3825: Minibatch Loss: 0.442454\n",
            "Step 3825: Minibatch Loss: 0.442327\n",
            "Step 3825: Minibatch Loss: 0.441634\n",
            "Step 3825: Minibatch Loss: 0.434017\n",
            "Step 3825: Minibatch Loss: 0.444736\n",
            "Step 3825: Minibatch Loss: 0.433392\n",
            "Step 3825: Minibatch Loss: 0.434419\n",
            "Step 3825: Minibatch Loss: 0.438172\n",
            "Step 3825: Minibatch Loss: 0.421648\n",
            "Step 3825: Minibatch Loss: 0.423391\n",
            "Step 3825: Minibatch Loss: 0.425638\n",
            "Step 3825: Minibatch Loss: 0.430298\n",
            "Step 3825: Minibatch Loss: 0.419849\n",
            "Step 3825: Minibatch Loss: 0.432744\n",
            "Step 3825: Minibatch Loss: 0.416904\n",
            "Step 3825: Minibatch Loss: 0.422531\n",
            "Step 3825: Minibatch Loss: 0.422699\n",
            "Step 3825: Minibatch Loss: 0.412514\n",
            "Step 3825: Minibatch Loss: 0.422887\n",
            "Step 3825: Minibatch Loss: 0.411433\n",
            "Step 3825: Minibatch Loss: 0.412146\n",
            "Step 3825: Minibatch Loss: 0.397592\n",
            "Step 3825: Minibatch Loss: 0.399151\n",
            "Step 3825: Minibatch Loss: 0.410539\n",
            "Step 3825: Minibatch Loss: 0.397569\n",
            "Step 3825: Minibatch Loss: 0.390171\n",
            "Step 3825: Minibatch Loss: 0.392225\n",
            "Step 3825: Minibatch Loss: 0.392144\n",
            "Step 3825: Minibatch Loss: 0.382772\n",
            "Step 3825: Minibatch Loss: 0.390097\n",
            "Step 3825: Minibatch Loss: 0.404293\n",
            "Step 3825: Minibatch Loss: 0.388875\n",
            "Step 3825: Minibatch Loss: 0.389413\n",
            "Step 3825: Minibatch Loss: 0.383200\n",
            "Step 3825: Minibatch Loss: 0.395534\n",
            "Step 3825: Minibatch Loss: 0.392894\n",
            "Step 3825: Minibatch Loss: 0.385358\n",
            "Step 3825: Minibatch Loss: 0.369929\n",
            "Step 3825: Minibatch Loss: 0.380461\n",
            "Step 3825: Minibatch Loss: 0.374776\n",
            "Step 3825: Minibatch Loss: 0.376772\n",
            "Step 3825: Minibatch Loss: 0.374640\n",
            "Step 3825: Minibatch Loss: 0.372092\n",
            "Step 3825: Minibatch Loss: 0.369548\n",
            "Step 3825: Minibatch Loss: 0.366980\n",
            "Step 3825: Minibatch Loss: 0.367297\n",
            "Training for SNR= 0.75  sigma= 0.9172759353897796\n",
            "Step 3825: Minibatch Loss: 0.356426\n",
            "Step 3825: Minibatch Loss: 0.363215\n",
            "Step 3825: Minibatch Loss: 0.353420\n",
            "Step 3825: Minibatch Loss: 0.363655\n",
            "Step 3825: Minibatch Loss: 0.352790\n",
            "Step 3825: Minibatch Loss: 0.361900\n",
            "Step 3825: Minibatch Loss: 0.342172\n",
            "Step 3825: Minibatch Loss: 0.343282\n",
            "Step 3825: Minibatch Loss: 0.359671\n",
            "Step 3825: Minibatch Loss: 0.357412\n",
            "Step 3825: Minibatch Loss: 0.356873\n",
            "Step 3825: Minibatch Loss: 0.341513\n",
            "Step 3825: Minibatch Loss: 0.346963\n",
            "Step 3825: Minibatch Loss: 0.332160\n",
            "Step 3825: Minibatch Loss: 0.337207\n",
            "Step 3825: Minibatch Loss: 0.348404\n",
            "Step 3825: Minibatch Loss: 0.348893\n",
            "Step 3825: Minibatch Loss: 0.333444\n",
            "Step 3825: Minibatch Loss: 0.338157\n",
            "Step 3825: Minibatch Loss: 0.332855\n",
            "Step 3825: Minibatch Loss: 0.327448\n",
            "Step 3825: Minibatch Loss: 0.319917\n",
            "Step 3825: Minibatch Loss: 0.343097\n",
            "Step 3825: Minibatch Loss: 0.329791\n",
            "Step 3825: Minibatch Loss: 0.331504\n",
            "Step 3825: Minibatch Loss: 0.337741\n",
            "Step 3825: Minibatch Loss: 0.338529\n",
            "Step 3825: Minibatch Loss: 0.327106\n",
            "Step 3825: Minibatch Loss: 0.340746\n",
            "Step 3825: Minibatch Loss: 0.321568\n",
            "Step 3825: Minibatch Loss: 0.315091\n",
            "Step 3825: Minibatch Loss: 0.319753\n",
            "Step 3825: Minibatch Loss: 0.325628\n",
            "Step 3825: Minibatch Loss: 0.333893\n",
            "Step 3825: Minibatch Loss: 0.315151\n",
            "Step 3825: Minibatch Loss: 0.316360\n",
            "Step 3825: Minibatch Loss: 0.310672\n",
            "Step 3825: Minibatch Loss: 0.317589\n",
            "Step 3825: Minibatch Loss: 0.319984\n",
            "Step 3825: Minibatch Loss: 0.329605\n",
            "Step 3825: Minibatch Loss: 0.307936\n",
            "Step 3825: Minibatch Loss: 0.334933\n",
            "Step 3825: Minibatch Loss: 0.312308\n",
            "Step 3825: Minibatch Loss: 0.325739\n",
            "Step 3825: Minibatch Loss: 0.310878\n",
            "Step 3825: Minibatch Loss: 0.321651\n",
            "Step 3825: Minibatch Loss: 0.309594\n",
            "Step 3825: Minibatch Loss: 0.312951\n",
            "Step 3825: Minibatch Loss: 0.321338\n",
            "Step 3825: Minibatch Loss: 0.303007\n",
            "Training for SNR= 1.0  sigma= 0.8912509381337456\n",
            "Step 3825: Minibatch Loss: 0.289140\n",
            "Step 3825: Minibatch Loss: 0.302722\n",
            "Step 3825: Minibatch Loss: 0.294964\n",
            "Step 3825: Minibatch Loss: 0.304201\n",
            "Step 3825: Minibatch Loss: 0.313981\n",
            "Step 3825: Minibatch Loss: 0.290168\n",
            "Step 3825: Minibatch Loss: 0.294820\n",
            "Step 3825: Minibatch Loss: 0.294598\n",
            "Step 3825: Minibatch Loss: 0.294028\n",
            "Step 3825: Minibatch Loss: 0.293559\n",
            "Step 3825: Minibatch Loss: 0.288815\n",
            "Step 3825: Minibatch Loss: 0.290656\n",
            "Step 3825: Minibatch Loss: 0.293552\n",
            "Step 3825: Minibatch Loss: 0.301666\n",
            "Step 3825: Minibatch Loss: 0.288779\n",
            "Step 3825: Minibatch Loss: 0.287536\n",
            "Step 3825: Minibatch Loss: 0.283682\n",
            "Step 3825: Minibatch Loss: 0.283975\n",
            "Step 3825: Minibatch Loss: 0.287941\n",
            "Step 3825: Minibatch Loss: 0.279041\n",
            "Step 3825: Minibatch Loss: 0.283133\n",
            "Step 3825: Minibatch Loss: 0.294519\n",
            "Step 3825: Minibatch Loss: 0.285462\n",
            "Step 3825: Minibatch Loss: 0.278079\n",
            "Step 3825: Minibatch Loss: 0.278090\n",
            "Step 3825: Minibatch Loss: 0.267174\n",
            "Step 3825: Minibatch Loss: 0.281189\n",
            "Step 3825: Minibatch Loss: 0.265044\n",
            "Step 3825: Minibatch Loss: 0.280722\n",
            "Step 3825: Minibatch Loss: 0.285138\n",
            "Step 3825: Minibatch Loss: 0.279362\n",
            "Step 3825: Minibatch Loss: 0.273022\n",
            "Step 3825: Minibatch Loss: 0.277699\n",
            "Step 3825: Minibatch Loss: 0.269494\n",
            "Step 3825: Minibatch Loss: 0.275093\n",
            "Step 3825: Minibatch Loss: 0.273698\n",
            "Step 3825: Minibatch Loss: 0.278551\n",
            "Step 3825: Minibatch Loss: 0.265290\n",
            "Step 3825: Minibatch Loss: 0.264327\n",
            "Step 3825: Minibatch Loss: 0.272834\n",
            "Step 3825: Minibatch Loss: 0.264499\n",
            "Step 3825: Minibatch Loss: 0.259533\n",
            "Step 3825: Minibatch Loss: 0.276970\n",
            "Step 3825: Minibatch Loss: 0.261449\n",
            "Step 3825: Minibatch Loss: 0.266792\n",
            "Step 3825: Minibatch Loss: 0.264866\n",
            "Step 3825: Minibatch Loss: 0.270553\n",
            "Step 3825: Minibatch Loss: 0.262468\n",
            "Step 3825: Minibatch Loss: 0.260008\n",
            "Step 3825: Minibatch Loss: 0.262183\n",
            "Training for SNR= 1.25  sigma= 0.8659643233600653\n",
            "Step 3825: Minibatch Loss: 0.260096\n",
            "Step 3825: Minibatch Loss: 0.255469\n",
            "Step 3825: Minibatch Loss: 0.255018\n",
            "Step 3825: Minibatch Loss: 0.248161\n",
            "Step 3825: Minibatch Loss: 0.249402\n",
            "Step 3825: Minibatch Loss: 0.249484\n",
            "Step 3825: Minibatch Loss: 0.256129\n",
            "Step 3825: Minibatch Loss: 0.253455\n",
            "Step 3825: Minibatch Loss: 0.257424\n",
            "Step 3825: Minibatch Loss: 0.241513\n",
            "Step 3825: Minibatch Loss: 0.255116\n",
            "Step 3825: Minibatch Loss: 0.258084\n",
            "Step 3825: Minibatch Loss: 0.244653\n",
            "Step 3825: Minibatch Loss: 0.251415\n",
            "Step 3825: Minibatch Loss: 0.245410\n",
            "Step 3825: Minibatch Loss: 0.245257\n",
            "Step 3825: Minibatch Loss: 0.241717\n",
            "Step 3825: Minibatch Loss: 0.258605\n",
            "Step 3825: Minibatch Loss: 0.250029\n",
            "Step 3825: Minibatch Loss: 0.256392\n",
            "Step 3825: Minibatch Loss: 0.239131\n",
            "Step 3825: Minibatch Loss: 0.242434\n",
            "Step 3825: Minibatch Loss: 0.238387\n",
            "Step 3825: Minibatch Loss: 0.242923\n",
            "Step 3825: Minibatch Loss: 0.230836\n",
            "Step 3825: Minibatch Loss: 0.242803\n",
            "Step 3825: Minibatch Loss: 0.248375\n",
            "Step 3825: Minibatch Loss: 0.247271\n",
            "Step 3825: Minibatch Loss: 0.242877\n",
            "Step 3825: Minibatch Loss: 0.238012\n",
            "Step 3825: Minibatch Loss: 0.245198\n",
            "Step 3825: Minibatch Loss: 0.240650\n",
            "Step 3825: Minibatch Loss: 0.230740\n",
            "Step 3825: Minibatch Loss: 0.238290\n",
            "Step 3825: Minibatch Loss: 0.236830\n",
            "Step 3825: Minibatch Loss: 0.244929\n",
            "Step 3825: Minibatch Loss: 0.235108\n",
            "Step 3825: Minibatch Loss: 0.246068\n",
            "Step 3825: Minibatch Loss: 0.243452\n",
            "Step 3825: Minibatch Loss: 0.245379\n",
            "Step 3825: Minibatch Loss: 0.234266\n",
            "Step 3825: Minibatch Loss: 0.228002\n",
            "Step 3825: Minibatch Loss: 0.235290\n",
            "Step 3825: Minibatch Loss: 0.230419\n",
            "Step 3825: Minibatch Loss: 0.244323\n",
            "Step 3825: Minibatch Loss: 0.234643\n",
            "Step 3825: Minibatch Loss: 0.235539\n",
            "Step 3825: Minibatch Loss: 0.235663\n",
            "Step 3825: Minibatch Loss: 0.229335\n",
            "Step 3825: Minibatch Loss: 0.225290\n",
            "Training for SNR= 1.5  sigma= 0.8413951416451951\n",
            "Step 3825: Minibatch Loss: 0.231088\n",
            "Step 3825: Minibatch Loss: 0.225806\n",
            "Step 3825: Minibatch Loss: 0.233103\n",
            "Step 3825: Minibatch Loss: 0.226346\n",
            "Step 3825: Minibatch Loss: 0.219765\n",
            "Step 3825: Minibatch Loss: 0.233432\n",
            "Step 3825: Minibatch Loss: 0.228044\n",
            "Step 3825: Minibatch Loss: 0.215398\n",
            "Step 3825: Minibatch Loss: 0.214100\n",
            "Step 3825: Minibatch Loss: 0.218038\n",
            "Step 3825: Minibatch Loss: 0.208633\n",
            "Step 3825: Minibatch Loss: 0.205489\n",
            "Step 3825: Minibatch Loss: 0.216363\n",
            "Step 3825: Minibatch Loss: 0.218405\n",
            "Step 3825: Minibatch Loss: 0.225041\n",
            "Step 3825: Minibatch Loss: 0.214134\n",
            "Step 3825: Minibatch Loss: 0.215170\n",
            "Step 3825: Minibatch Loss: 0.217373\n",
            "Step 3825: Minibatch Loss: 0.206750\n",
            "Step 3825: Minibatch Loss: 0.217755\n",
            "Step 3825: Minibatch Loss: 0.214905\n",
            "Step 3825: Minibatch Loss: 0.215889\n",
            "Step 3825: Minibatch Loss: 0.211311\n",
            "Step 3825: Minibatch Loss: 0.214594\n",
            "Step 3825: Minibatch Loss: 0.199071\n",
            "Step 3825: Minibatch Loss: 0.217441\n",
            "Step 3825: Minibatch Loss: 0.199176\n",
            "Step 3825: Minibatch Loss: 0.216401\n",
            "Step 3825: Minibatch Loss: 0.224232\n",
            "Step 3825: Minibatch Loss: 0.210621\n",
            "Step 3825: Minibatch Loss: 0.213204\n",
            "Step 3825: Minibatch Loss: 0.212328\n",
            "Step 3825: Minibatch Loss: 0.219051\n",
            "Step 3825: Minibatch Loss: 0.217230\n",
            "Step 3825: Minibatch Loss: 0.222906\n",
            "Step 3825: Minibatch Loss: 0.215253\n",
            "Step 3825: Minibatch Loss: 0.200345\n",
            "Step 3825: Minibatch Loss: 0.213141\n",
            "Step 3825: Minibatch Loss: 0.211948\n",
            "Step 3825: Minibatch Loss: 0.221456\n",
            "Step 3825: Minibatch Loss: 0.216738\n",
            "Step 3825: Minibatch Loss: 0.214517\n",
            "Step 3825: Minibatch Loss: 0.216971\n",
            "Step 3825: Minibatch Loss: 0.195849\n",
            "Step 3825: Minibatch Loss: 0.230529\n",
            "Step 3825: Minibatch Loss: 0.218515\n",
            "Step 3825: Minibatch Loss: 0.210281\n",
            "Step 3825: Minibatch Loss: 0.200072\n",
            "Step 3825: Minibatch Loss: 0.215566\n",
            "Step 3825: Minibatch Loss: 0.213339\n",
            "Training for SNR= 1.75  sigma= 0.81752303794365\n",
            "Step 3825: Minibatch Loss: 0.194634\n",
            "Step 3825: Minibatch Loss: 0.202124\n",
            "Step 3825: Minibatch Loss: 0.185205\n",
            "Step 3825: Minibatch Loss: 0.212166\n",
            "Step 3825: Minibatch Loss: 0.205823\n",
            "Step 3825: Minibatch Loss: 0.203646\n",
            "Step 3825: Minibatch Loss: 0.207440\n",
            "Step 3825: Minibatch Loss: 0.197204\n",
            "Step 3825: Minibatch Loss: 0.192885\n",
            "Step 3825: Minibatch Loss: 0.204243\n",
            "Step 3825: Minibatch Loss: 0.195528\n",
            "Step 3825: Minibatch Loss: 0.198993\n",
            "Step 3825: Minibatch Loss: 0.196883\n",
            "Step 3825: Minibatch Loss: 0.188021\n",
            "Step 3825: Minibatch Loss: 0.208112\n",
            "Step 3825: Minibatch Loss: 0.204148\n",
            "Step 3825: Minibatch Loss: 0.206602\n",
            "Step 3825: Minibatch Loss: 0.202600\n",
            "Step 3825: Minibatch Loss: 0.204740\n",
            "Step 3825: Minibatch Loss: 0.194712\n",
            "Step 3825: Minibatch Loss: 0.208259\n",
            "Step 3825: Minibatch Loss: 0.189383\n",
            "Step 3825: Minibatch Loss: 0.203864\n",
            "Step 3825: Minibatch Loss: 0.190085\n",
            "Step 3825: Minibatch Loss: 0.192391\n",
            "Step 3825: Minibatch Loss: 0.192184\n",
            "Step 3825: Minibatch Loss: 0.197056\n",
            "Step 3825: Minibatch Loss: 0.201322\n",
            "Step 3825: Minibatch Loss: 0.193688\n",
            "Step 3825: Minibatch Loss: 0.194630\n",
            "Step 3825: Minibatch Loss: 0.196656\n",
            "Step 3825: Minibatch Loss: 0.193217\n",
            "Step 3825: Minibatch Loss: 0.195922\n",
            "Step 3825: Minibatch Loss: 0.200077\n",
            "Step 3825: Minibatch Loss: 0.205424\n",
            "Step 3825: Minibatch Loss: 0.196464\n",
            "Step 3825: Minibatch Loss: 0.203609\n",
            "Step 3825: Minibatch Loss: 0.196360\n",
            "Step 3825: Minibatch Loss: 0.213177\n",
            "Step 3825: Minibatch Loss: 0.189747\n",
            "Step 3825: Minibatch Loss: 0.202173\n",
            "Step 3825: Minibatch Loss: 0.191740\n",
            "Step 3825: Minibatch Loss: 0.202737\n",
            "Step 3825: Minibatch Loss: 0.202288\n",
            "Step 3825: Minibatch Loss: 0.190449\n",
            "Step 3825: Minibatch Loss: 0.195915\n",
            "Step 3825: Minibatch Loss: 0.189330\n",
            "Step 3825: Minibatch Loss: 0.192866\n",
            "Step 3825: Minibatch Loss: 0.199116\n",
            "Step 3825: Minibatch Loss: 0.193556\n",
            "Training for SNR= 2.0  sigma= 0.7943282347242815\n",
            "Step 3825: Minibatch Loss: 0.189294\n",
            "Step 3825: Minibatch Loss: 0.196801\n",
            "Step 3825: Minibatch Loss: 0.180281\n",
            "Step 3825: Minibatch Loss: 0.177103\n",
            "Step 3825: Minibatch Loss: 0.186412\n",
            "Step 3825: Minibatch Loss: 0.192499\n",
            "Step 3825: Minibatch Loss: 0.181026\n",
            "Step 3825: Minibatch Loss: 0.176318\n",
            "Step 3825: Minibatch Loss: 0.187683\n",
            "Step 3825: Minibatch Loss: 0.195992\n",
            "Step 3825: Minibatch Loss: 0.182799\n",
            "Step 3825: Minibatch Loss: 0.179179\n",
            "Step 3825: Minibatch Loss: 0.180770\n",
            "Step 3825: Minibatch Loss: 0.181860\n",
            "Step 3825: Minibatch Loss: 0.177769\n",
            "Step 3825: Minibatch Loss: 0.198146\n",
            "Step 3825: Minibatch Loss: 0.186349\n",
            "Step 3825: Minibatch Loss: 0.182932\n",
            "Step 3825: Minibatch Loss: 0.183795\n",
            "Step 3825: Minibatch Loss: 0.180270\n",
            "Step 3825: Minibatch Loss: 0.172883\n",
            "Step 3825: Minibatch Loss: 0.181054\n",
            "Step 3825: Minibatch Loss: 0.173335\n",
            "Step 3825: Minibatch Loss: 0.185870\n",
            "Step 3825: Minibatch Loss: 0.174431\n",
            "Step 3825: Minibatch Loss: 0.190123\n",
            "Step 3825: Minibatch Loss: 0.184205\n",
            "Step 3825: Minibatch Loss: 0.184776\n",
            "Step 3825: Minibatch Loss: 0.184655\n",
            "Step 3825: Minibatch Loss: 0.177118\n",
            "Step 3825: Minibatch Loss: 0.193016\n",
            "Step 3825: Minibatch Loss: 0.181388\n",
            "Step 3825: Minibatch Loss: 0.183700\n",
            "Step 3825: Minibatch Loss: 0.188577\n",
            "Step 3825: Minibatch Loss: 0.181809\n",
            "Step 3825: Minibatch Loss: 0.182752\n",
            "Step 3825: Minibatch Loss: 0.177808\n",
            "Step 3825: Minibatch Loss: 0.176664\n",
            "Step 3825: Minibatch Loss: 0.165935\n",
            "Step 3825: Minibatch Loss: 0.192973\n",
            "Step 3825: Minibatch Loss: 0.179346\n",
            "Step 3825: Minibatch Loss: 0.186384\n",
            "Step 3825: Minibatch Loss: 0.181620\n",
            "Step 3825: Minibatch Loss: 0.178972\n",
            "Step 3825: Minibatch Loss: 0.178493\n",
            "Step 3825: Minibatch Loss: 0.188836\n",
            "Step 3825: Minibatch Loss: 0.190163\n",
            "Step 3825: Minibatch Loss: 0.186764\n",
            "Step 3825: Minibatch Loss: 0.182157\n",
            "Step 3825: Minibatch Loss: 0.181196\n",
            "Training for SNR= 2.25  sigma= 0.7717915155850124\n",
            "Step 3825: Minibatch Loss: 0.157893\n",
            "Step 3825: Minibatch Loss: 0.167713\n",
            "Step 3825: Minibatch Loss: 0.174366\n",
            "Step 3825: Minibatch Loss: 0.182378\n",
            "Step 3825: Minibatch Loss: 0.171024\n",
            "Step 3825: Minibatch Loss: 0.169063\n",
            "Step 3825: Minibatch Loss: 0.169170\n",
            "Step 3825: Minibatch Loss: 0.173378\n",
            "Step 3825: Minibatch Loss: 0.164148\n",
            "Step 3825: Minibatch Loss: 0.184170\n",
            "Step 3825: Minibatch Loss: 0.161570\n",
            "Step 3825: Minibatch Loss: 0.160190\n",
            "Step 3825: Minibatch Loss: 0.172643\n",
            "Step 3825: Minibatch Loss: 0.173187\n",
            "Step 3825: Minibatch Loss: 0.163117\n",
            "Step 3825: Minibatch Loss: 0.163934\n",
            "Step 3825: Minibatch Loss: 0.177771\n",
            "Step 3825: Minibatch Loss: 0.174536\n",
            "Step 3825: Minibatch Loss: 0.179840\n",
            "Step 3825: Minibatch Loss: 0.168614\n",
            "Step 3825: Minibatch Loss: 0.162105\n",
            "Step 3825: Minibatch Loss: 0.178870\n",
            "Step 3825: Minibatch Loss: 0.172521\n",
            "Step 3825: Minibatch Loss: 0.160381\n",
            "Step 3825: Minibatch Loss: 0.175981\n",
            "Step 3825: Minibatch Loss: 0.169770\n",
            "Step 3825: Minibatch Loss: 0.165846\n",
            "Step 3825: Minibatch Loss: 0.152915\n",
            "Step 3825: Minibatch Loss: 0.175580\n",
            "Step 3825: Minibatch Loss: 0.170972\n",
            "Step 3825: Minibatch Loss: 0.171979\n",
            "Step 3825: Minibatch Loss: 0.165892\n",
            "Step 3825: Minibatch Loss: 0.174230\n",
            "Step 3825: Minibatch Loss: 0.176248\n",
            "Step 3825: Minibatch Loss: 0.168591\n",
            "Step 3825: Minibatch Loss: 0.166136\n",
            "Step 3825: Minibatch Loss: 0.183695\n",
            "Step 3825: Minibatch Loss: 0.157814\n",
            "Step 3825: Minibatch Loss: 0.160949\n",
            "Step 3825: Minibatch Loss: 0.158621\n",
            "Step 3825: Minibatch Loss: 0.156118\n",
            "Step 3825: Minibatch Loss: 0.177025\n",
            "Step 3825: Minibatch Loss: 0.172612\n",
            "Step 3825: Minibatch Loss: 0.169602\n",
            "Step 3825: Minibatch Loss: 0.165759\n",
            "Step 3825: Minibatch Loss: 0.169681\n",
            "Step 3825: Minibatch Loss: 0.161926\n",
            "Step 3825: Minibatch Loss: 0.164545\n",
            "Step 3825: Minibatch Loss: 0.168237\n",
            "Step 3825: Minibatch Loss: 0.162447\n",
            "Training for SNR= 2.5  sigma= 0.7498942093324559\n",
            "Step 3825: Minibatch Loss: 0.157622\n",
            "Step 3825: Minibatch Loss: 0.169438\n",
            "Step 3825: Minibatch Loss: 0.151631\n",
            "Step 3825: Minibatch Loss: 0.156796\n",
            "Step 3825: Minibatch Loss: 0.167089\n",
            "Step 3825: Minibatch Loss: 0.156272\n",
            "Step 3825: Minibatch Loss: 0.147211\n",
            "Step 3825: Minibatch Loss: 0.152563\n",
            "Step 3825: Minibatch Loss: 0.149164\n",
            "Step 3825: Minibatch Loss: 0.162104\n",
            "Step 3825: Minibatch Loss: 0.153192\n",
            "Step 3825: Minibatch Loss: 0.165354\n",
            "Step 3825: Minibatch Loss: 0.146975\n",
            "Step 3825: Minibatch Loss: 0.163705\n",
            "Step 3825: Minibatch Loss: 0.155793\n",
            "Step 3825: Minibatch Loss: 0.152916\n",
            "Step 3825: Minibatch Loss: 0.153462\n",
            "Step 3825: Minibatch Loss: 0.169450\n",
            "Step 3825: Minibatch Loss: 0.164208\n",
            "Step 3825: Minibatch Loss: 0.148636\n",
            "Step 3825: Minibatch Loss: 0.150757\n",
            "Step 3825: Minibatch Loss: 0.149883\n",
            "Step 3825: Minibatch Loss: 0.161304\n",
            "Step 3825: Minibatch Loss: 0.153464\n",
            "Step 3825: Minibatch Loss: 0.156886\n",
            "Step 3825: Minibatch Loss: 0.164015\n",
            "Step 3825: Minibatch Loss: 0.154208\n",
            "Step 3825: Minibatch Loss: 0.156490\n",
            "Step 3825: Minibatch Loss: 0.165685\n",
            "Step 3825: Minibatch Loss: 0.146478\n",
            "Step 3825: Minibatch Loss: 0.153695\n",
            "Step 3825: Minibatch Loss: 0.156278\n",
            "Step 3825: Minibatch Loss: 0.157196\n",
            "Step 3825: Minibatch Loss: 0.157569\n",
            "Step 3825: Minibatch Loss: 0.155995\n",
            "Step 3825: Minibatch Loss: 0.162593\n",
            "Step 3825: Minibatch Loss: 0.142158\n",
            "Step 3825: Minibatch Loss: 0.155364\n",
            "Step 3825: Minibatch Loss: 0.153969\n",
            "Step 3825: Minibatch Loss: 0.149092\n",
            "Step 3825: Minibatch Loss: 0.149161\n",
            "Step 3825: Minibatch Loss: 0.153506\n",
            "Step 3825: Minibatch Loss: 0.156653\n",
            "Step 3825: Minibatch Loss: 0.157724\n",
            "Step 3825: Minibatch Loss: 0.152643\n",
            "Step 3825: Minibatch Loss: 0.161029\n",
            "Step 3825: Minibatch Loss: 0.173182\n",
            "Step 3825: Minibatch Loss: 0.150204\n",
            "Step 3825: Minibatch Loss: 0.154478\n",
            "Step 3825: Minibatch Loss: 0.163255\n",
            "Training for SNR= 2.75  sigma= 0.7286181745132277\n",
            "Step 3825: Minibatch Loss: 0.150773\n",
            "Step 3825: Minibatch Loss: 0.149973\n",
            "Step 3825: Minibatch Loss: 0.145424\n",
            "Step 3825: Minibatch Loss: 0.149192\n",
            "Step 3825: Minibatch Loss: 0.145692\n",
            "Step 3825: Minibatch Loss: 0.145167\n",
            "Step 3825: Minibatch Loss: 0.141191\n",
            "Step 3825: Minibatch Loss: 0.147557\n",
            "Step 3825: Minibatch Loss: 0.148898\n",
            "Step 3825: Minibatch Loss: 0.149238\n",
            "Step 3825: Minibatch Loss: 0.158520\n",
            "Step 3825: Minibatch Loss: 0.135323\n",
            "Step 3825: Minibatch Loss: 0.135523\n",
            "Step 3825: Minibatch Loss: 0.137838\n",
            "Step 3825: Minibatch Loss: 0.140733\n",
            "Step 3825: Minibatch Loss: 0.139350\n",
            "Step 3825: Minibatch Loss: 0.151076\n",
            "Step 3825: Minibatch Loss: 0.145050\n",
            "Step 3825: Minibatch Loss: 0.159727\n",
            "Step 3825: Minibatch Loss: 0.136878\n",
            "Step 3825: Minibatch Loss: 0.142747\n",
            "Step 3825: Minibatch Loss: 0.145815\n",
            "Step 3825: Minibatch Loss: 0.132138\n",
            "Step 3825: Minibatch Loss: 0.141402\n",
            "Step 3825: Minibatch Loss: 0.151524\n",
            "Step 3825: Minibatch Loss: 0.147448\n",
            "Step 3825: Minibatch Loss: 0.147308\n",
            "Step 3825: Minibatch Loss: 0.148396\n",
            "Step 3825: Minibatch Loss: 0.147033\n",
            "Step 3825: Minibatch Loss: 0.147882\n",
            "Step 3825: Minibatch Loss: 0.150184\n",
            "Step 3825: Minibatch Loss: 0.136497\n",
            "Step 3825: Minibatch Loss: 0.136433\n",
            "Step 3825: Minibatch Loss: 0.139968\n",
            "Step 3825: Minibatch Loss: 0.144706\n",
            "Step 3825: Minibatch Loss: 0.148188\n",
            "Step 3825: Minibatch Loss: 0.143328\n",
            "Step 3825: Minibatch Loss: 0.151688\n",
            "Step 3825: Minibatch Loss: 0.130636\n",
            "Step 3825: Minibatch Loss: 0.135959\n",
            "Step 3825: Minibatch Loss: 0.151201\n",
            "Step 3825: Minibatch Loss: 0.142972\n",
            "Step 3825: Minibatch Loss: 0.145823\n",
            "Step 3825: Minibatch Loss: 0.148699\n",
            "Step 3825: Minibatch Loss: 0.140096\n",
            "Step 3825: Minibatch Loss: 0.157147\n",
            "Step 3825: Minibatch Loss: 0.150500\n",
            "Step 3825: Minibatch Loss: 0.150665\n",
            "Step 3825: Minibatch Loss: 0.139579\n",
            "Step 3825: Minibatch Loss: 0.145344\n",
            "Training for SNR= 3.0  sigma= 0.7079457843841379\n",
            "Step 3825: Minibatch Loss: 0.133776\n",
            "Step 3825: Minibatch Loss: 0.138954\n",
            "Step 3825: Minibatch Loss: 0.130784\n",
            "Step 3825: Minibatch Loss: 0.129936\n",
            "Step 3825: Minibatch Loss: 0.127326\n",
            "Step 3825: Minibatch Loss: 0.138292\n",
            "Step 3825: Minibatch Loss: 0.120481\n",
            "Step 3825: Minibatch Loss: 0.123517\n",
            "Step 3825: Minibatch Loss: 0.137268\n",
            "Step 3825: Minibatch Loss: 0.140386\n",
            "Step 3825: Minibatch Loss: 0.124215\n",
            "Step 3825: Minibatch Loss: 0.126974\n",
            "Step 3825: Minibatch Loss: 0.142333\n",
            "Step 3825: Minibatch Loss: 0.135722\n",
            "Step 3825: Minibatch Loss: 0.129304\n",
            "Step 3825: Minibatch Loss: 0.140029\n",
            "Step 3825: Minibatch Loss: 0.133268\n",
            "Step 3825: Minibatch Loss: 0.122008\n",
            "Step 3825: Minibatch Loss: 0.134449\n",
            "Step 3825: Minibatch Loss: 0.135167\n",
            "Step 3825: Minibatch Loss: 0.134806\n",
            "Step 3825: Minibatch Loss: 0.124134\n",
            "Step 3825: Minibatch Loss: 0.138818\n",
            "Step 3825: Minibatch Loss: 0.129128\n",
            "Step 3825: Minibatch Loss: 0.130857\n",
            "Step 3825: Minibatch Loss: 0.130647\n",
            "Step 3825: Minibatch Loss: 0.142177\n",
            "Step 3825: Minibatch Loss: 0.133662\n",
            "Step 3825: Minibatch Loss: 0.126852\n",
            "Step 3825: Minibatch Loss: 0.123068\n",
            "Step 3825: Minibatch Loss: 0.119801\n",
            "Step 3825: Minibatch Loss: 0.117682\n",
            "Step 3825: Minibatch Loss: 0.128010\n",
            "Step 3825: Minibatch Loss: 0.126881\n",
            "Step 3825: Minibatch Loss: 0.122423\n",
            "Step 3825: Minibatch Loss: 0.133605\n",
            "Step 3825: Minibatch Loss: 0.139176\n",
            "Step 3825: Minibatch Loss: 0.141243\n",
            "Step 3825: Minibatch Loss: 0.124618\n",
            "Step 3825: Minibatch Loss: 0.131377\n",
            "Step 3825: Minibatch Loss: 0.135528\n",
            "Step 3825: Minibatch Loss: 0.133621\n",
            "Step 3825: Minibatch Loss: 0.121335\n",
            "Step 3825: Minibatch Loss: 0.125598\n",
            "Step 3825: Minibatch Loss: 0.139589\n",
            "Step 3825: Minibatch Loss: 0.132126\n",
            "Step 3825: Minibatch Loss: 0.124571\n",
            "Step 3825: Minibatch Loss: 0.129796\n",
            "Step 3825: Minibatch Loss: 0.136486\n",
            "Step 3825: Minibatch Loss: 0.139642\n",
            "Training for SNR= 3.25  sigma= 0.6878599123088076\n",
            "Step 3825: Minibatch Loss: 0.119111\n",
            "Step 3825: Minibatch Loss: 0.125719\n",
            "Step 3825: Minibatch Loss: 0.120993\n",
            "Step 3825: Minibatch Loss: 0.118997\n",
            "Step 3825: Minibatch Loss: 0.121097\n",
            "Step 3825: Minibatch Loss: 0.118942\n",
            "Step 3825: Minibatch Loss: 0.131294\n",
            "Step 3825: Minibatch Loss: 0.108198\n",
            "Step 3825: Minibatch Loss: 0.122240\n",
            "Step 3825: Minibatch Loss: 0.119600\n",
            "Step 3825: Minibatch Loss: 0.117698\n",
            "Step 3825: Minibatch Loss: 0.125168\n",
            "Step 3825: Minibatch Loss: 0.123100\n",
            "Step 3825: Minibatch Loss: 0.112491\n",
            "Step 3825: Minibatch Loss: 0.125360\n",
            "Step 3825: Minibatch Loss: 0.115305\n",
            "Step 3825: Minibatch Loss: 0.130933\n",
            "Step 3825: Minibatch Loss: 0.126787\n",
            "Step 3825: Minibatch Loss: 0.125368\n",
            "Step 3825: Minibatch Loss: 0.123431\n",
            "Step 3825: Minibatch Loss: 0.129830\n",
            "Step 3825: Minibatch Loss: 0.112135\n",
            "Step 3825: Minibatch Loss: 0.124430\n",
            "Step 3825: Minibatch Loss: 0.122360\n",
            "Step 3825: Minibatch Loss: 0.122920\n",
            "Step 3825: Minibatch Loss: 0.124437\n",
            "Step 3825: Minibatch Loss: 0.132910\n",
            "Step 3825: Minibatch Loss: 0.119038\n",
            "Step 3825: Minibatch Loss: 0.128754\n",
            "Step 3825: Minibatch Loss: 0.118987\n",
            "Step 3825: Minibatch Loss: 0.123744\n",
            "Step 3825: Minibatch Loss: 0.123148\n",
            "Step 3825: Minibatch Loss: 0.118995\n",
            "Step 3825: Minibatch Loss: 0.128639\n",
            "Step 3825: Minibatch Loss: 0.129840\n",
            "Step 3825: Minibatch Loss: 0.124927\n",
            "Step 3825: Minibatch Loss: 0.119127\n",
            "Step 3825: Minibatch Loss: 0.126226\n",
            "Step 3825: Minibatch Loss: 0.125680\n",
            "Step 3825: Minibatch Loss: 0.121707\n",
            "Step 3825: Minibatch Loss: 0.116493\n",
            "Step 3825: Minibatch Loss: 0.109623\n",
            "Step 3825: Minibatch Loss: 0.123656\n",
            "Step 3825: Minibatch Loss: 0.115372\n",
            "Step 3825: Minibatch Loss: 0.126185\n",
            "Step 3825: Minibatch Loss: 0.116561\n",
            "Step 3825: Minibatch Loss: 0.117118\n",
            "Step 3825: Minibatch Loss: 0.114535\n",
            "Step 3825: Minibatch Loss: 0.128126\n",
            "Step 3825: Minibatch Loss: 0.126443\n",
            "Training for SNR= 3.5  sigma= 0.6683439175686147\n",
            "Step 3825: Minibatch Loss: 0.116196\n",
            "Step 3825: Minibatch Loss: 0.109101\n",
            "Step 3825: Minibatch Loss: 0.111074\n",
            "Step 3825: Minibatch Loss: 0.119517\n",
            "Step 3825: Minibatch Loss: 0.119459\n",
            "Step 3825: Minibatch Loss: 0.110355\n",
            "Step 3825: Minibatch Loss: 0.117459\n",
            "Step 3825: Minibatch Loss: 0.111034\n",
            "Step 3825: Minibatch Loss: 0.120469\n",
            "Step 3825: Minibatch Loss: 0.112280\n",
            "Step 3825: Minibatch Loss: 0.103412\n",
            "Step 3825: Minibatch Loss: 0.104757\n",
            "Step 3825: Minibatch Loss: 0.107144\n",
            "Step 3825: Minibatch Loss: 0.109451\n",
            "Step 3825: Minibatch Loss: 0.103838\n",
            "Step 3825: Minibatch Loss: 0.111390\n",
            "Step 3825: Minibatch Loss: 0.116356\n",
            "Step 3825: Minibatch Loss: 0.108354\n",
            "Step 3825: Minibatch Loss: 0.116846\n",
            "Step 3825: Minibatch Loss: 0.112801\n",
            "Step 3825: Minibatch Loss: 0.104304\n",
            "Step 3825: Minibatch Loss: 0.105301\n",
            "Step 3825: Minibatch Loss: 0.107811\n",
            "Step 3825: Minibatch Loss: 0.105511\n",
            "Step 3825: Minibatch Loss: 0.099480\n",
            "Step 3825: Minibatch Loss: 0.103012\n",
            "Step 3825: Minibatch Loss: 0.118935\n",
            "Step 3825: Minibatch Loss: 0.108195\n",
            "Step 3825: Minibatch Loss: 0.112925\n",
            "Step 3825: Minibatch Loss: 0.111113\n",
            "Step 3825: Minibatch Loss: 0.094697\n",
            "Step 3825: Minibatch Loss: 0.109434\n",
            "Step 3825: Minibatch Loss: 0.110610\n",
            "Step 3825: Minibatch Loss: 0.108831\n",
            "Step 3825: Minibatch Loss: 0.102307\n",
            "Step 3825: Minibatch Loss: 0.107620\n",
            "Step 3825: Minibatch Loss: 0.106974\n",
            "Step 3825: Minibatch Loss: 0.122092\n",
            "Step 3825: Minibatch Loss: 0.100570\n",
            "Step 3825: Minibatch Loss: 0.101225\n",
            "Step 3825: Minibatch Loss: 0.108582\n",
            "Step 3825: Minibatch Loss: 0.108336\n",
            "Step 3825: Minibatch Loss: 0.107152\n",
            "Step 3825: Minibatch Loss: 0.102165\n",
            "Step 3825: Minibatch Loss: 0.107830\n",
            "Step 3825: Minibatch Loss: 0.102549\n",
            "Step 3825: Minibatch Loss: 0.093234\n",
            "Step 3825: Minibatch Loss: 0.107812\n",
            "Step 3825: Minibatch Loss: 0.096672\n",
            "Step 3825: Minibatch Loss: 0.103754\n",
            "Training for SNR= 3.75  sigma= 0.6493816315762113\n",
            "Step 3825: Minibatch Loss: 0.086972\n",
            "Step 3825: Minibatch Loss: 0.103645\n",
            "Step 3825: Minibatch Loss: 0.085871\n",
            "Step 3825: Minibatch Loss: 0.083619\n",
            "Step 3825: Minibatch Loss: 0.085431\n",
            "Step 3825: Minibatch Loss: 0.099451\n",
            "Step 3825: Minibatch Loss: 0.085874\n",
            "Step 3825: Minibatch Loss: 0.089344\n",
            "Step 3825: Minibatch Loss: 0.091715\n",
            "Step 3825: Minibatch Loss: 0.100814\n",
            "Step 3825: Minibatch Loss: 0.083452\n",
            "Step 3825: Minibatch Loss: 0.084924\n",
            "Step 3825: Minibatch Loss: 0.073514\n",
            "Step 3825: Minibatch Loss: 0.086937\n",
            "Step 3825: Minibatch Loss: 0.093982\n",
            "Step 3825: Minibatch Loss: 0.099049\n",
            "Step 3825: Minibatch Loss: 0.080137\n",
            "Step 3825: Minibatch Loss: 0.094232\n",
            "Step 3825: Minibatch Loss: 0.077647\n",
            "Step 3825: Minibatch Loss: 0.085597\n",
            "Step 3825: Minibatch Loss: 0.080083\n",
            "Step 3825: Minibatch Loss: 0.074057\n",
            "Step 3825: Minibatch Loss: 0.079601\n",
            "Step 3825: Minibatch Loss: 0.081970\n",
            "Step 3825: Minibatch Loss: 0.080968\n",
            "Step 3825: Minibatch Loss: 0.077319\n",
            "Step 3825: Minibatch Loss: 0.086722\n",
            "Step 3825: Minibatch Loss: 0.074817\n",
            "Step 3825: Minibatch Loss: 0.084180\n",
            "Step 3825: Minibatch Loss: 0.075935\n",
            "Step 3825: Minibatch Loss: 0.090676\n",
            "Step 3825: Minibatch Loss: 0.072217\n",
            "Step 3825: Minibatch Loss: 0.094100\n",
            "Step 3825: Minibatch Loss: 0.085861\n",
            "Step 3825: Minibatch Loss: 0.094543\n",
            "Step 3825: Minibatch Loss: 0.080459\n",
            "Step 3825: Minibatch Loss: 0.092040\n",
            "Step 3825: Minibatch Loss: 0.077346\n",
            "Step 3825: Minibatch Loss: 0.100372\n",
            "Step 3825: Minibatch Loss: 0.073833\n",
            "Step 3825: Minibatch Loss: 0.075441\n",
            "Step 3825: Minibatch Loss: 0.083245\n",
            "Step 3825: Minibatch Loss: 0.079690\n",
            "Step 3825: Minibatch Loss: 0.078618\n",
            "Step 3825: Minibatch Loss: 0.073708\n",
            "Step 3825: Minibatch Loss: 0.079917\n",
            "Step 3825: Minibatch Loss: 0.093314\n",
            "Step 3825: Minibatch Loss: 0.080172\n",
            "Step 3825: Minibatch Loss: 0.085960\n",
            "Step 3825: Minibatch Loss: 0.088260\n",
            "Training for SNR= 4.0  sigma= 0.6309573444801932\n",
            "Step 3825: Minibatch Loss: 0.073033\n",
            "Step 3825: Minibatch Loss: 0.069295\n",
            "Step 3825: Minibatch Loss: 0.067134\n",
            "Step 3825: Minibatch Loss: 0.072480\n",
            "Step 3825: Minibatch Loss: 0.075085\n",
            "Step 3825: Minibatch Loss: 0.069962\n",
            "Step 3825: Minibatch Loss: 0.079606\n",
            "Step 3825: Minibatch Loss: 0.066116\n",
            "Step 3825: Minibatch Loss: 0.078519\n",
            "Step 3825: Minibatch Loss: 0.069829\n",
            "Step 3825: Minibatch Loss: 0.068143\n",
            "Step 3825: Minibatch Loss: 0.072991\n",
            "Step 3825: Minibatch Loss: 0.068584\n",
            "Step 3825: Minibatch Loss: 0.072184\n",
            "Step 3825: Minibatch Loss: 0.067994\n",
            "Step 3825: Minibatch Loss: 0.067920\n",
            "Step 3825: Minibatch Loss: 0.065055\n",
            "Step 3825: Minibatch Loss: 0.072430\n",
            "Step 3825: Minibatch Loss: 0.071229\n",
            "Step 3825: Minibatch Loss: 0.062637\n",
            "Step 3825: Minibatch Loss: 0.076367\n",
            "Step 3825: Minibatch Loss: 0.064091\n",
            "Step 3825: Minibatch Loss: 0.070946\n",
            "Step 3825: Minibatch Loss: 0.070366\n",
            "Step 3825: Minibatch Loss: 0.081669\n",
            "Step 3825: Minibatch Loss: 0.074935\n",
            "Step 3825: Minibatch Loss: 0.072183\n",
            "Step 3825: Minibatch Loss: 0.072858\n",
            "Step 3825: Minibatch Loss: 0.064624\n",
            "Step 3825: Minibatch Loss: 0.076939\n",
            "Step 3825: Minibatch Loss: 0.078586\n",
            "Step 3825: Minibatch Loss: 0.070957\n",
            "Step 3825: Minibatch Loss: 0.072699\n",
            "Step 3825: Minibatch Loss: 0.074675\n",
            "Step 3825: Minibatch Loss: 0.066195\n",
            "Step 3825: Minibatch Loss: 0.069305\n",
            "Step 3825: Minibatch Loss: 0.072768\n",
            "Step 3825: Minibatch Loss: 0.072502\n",
            "Step 3825: Minibatch Loss: 0.065622\n",
            "Step 3825: Minibatch Loss: 0.069632\n",
            "Step 3825: Minibatch Loss: 0.067245\n",
            "Step 3825: Minibatch Loss: 0.074972\n",
            "Step 3825: Minibatch Loss: 0.074301\n",
            "Step 3825: Minibatch Loss: 0.072069\n",
            "Step 3825: Minibatch Loss: 0.066669\n",
            "Step 3825: Minibatch Loss: 0.061280\n",
            "Step 3825: Minibatch Loss: 0.066198\n",
            "Step 3825: Minibatch Loss: 0.072242\n",
            "Step 3825: Minibatch Loss: 0.061625\n",
            "Step 3825: Minibatch Loss: 0.070020\n",
            "Training for SNR= 4.25  sigma= 0.6130557921498208\n",
            "Step 3825: Minibatch Loss: 0.057064\n",
            "Step 3825: Minibatch Loss: 0.056699\n",
            "Step 3825: Minibatch Loss: 0.062536\n",
            "Step 3825: Minibatch Loss: 0.057326\n",
            "Step 3825: Minibatch Loss: 0.069752\n",
            "Step 3825: Minibatch Loss: 0.057540\n",
            "Step 3825: Minibatch Loss: 0.057904\n",
            "Step 3825: Minibatch Loss: 0.059714\n",
            "Step 3825: Minibatch Loss: 0.061492\n",
            "Step 3825: Minibatch Loss: 0.066710\n",
            "Step 3825: Minibatch Loss: 0.061400\n",
            "Step 3825: Minibatch Loss: 0.057642\n",
            "Step 3825: Minibatch Loss: 0.064587\n",
            "Step 3825: Minibatch Loss: 0.067345\n",
            "Step 3825: Minibatch Loss: 0.054531\n",
            "Step 3825: Minibatch Loss: 0.051947\n",
            "Step 3825: Minibatch Loss: 0.054864\n",
            "Step 3825: Minibatch Loss: 0.053494\n",
            "Step 3825: Minibatch Loss: 0.060810\n",
            "Step 3825: Minibatch Loss: 0.059672\n",
            "Step 3825: Minibatch Loss: 0.054408\n",
            "Step 3825: Minibatch Loss: 0.065140\n",
            "Step 3825: Minibatch Loss: 0.065806\n",
            "Step 3825: Minibatch Loss: 0.049528\n",
            "Step 3825: Minibatch Loss: 0.056792\n",
            "Step 3825: Minibatch Loss: 0.056464\n",
            "Step 3825: Minibatch Loss: 0.042525\n",
            "Step 3825: Minibatch Loss: 0.060200\n",
            "Step 3825: Minibatch Loss: 0.065093\n",
            "Step 3825: Minibatch Loss: 0.065531\n",
            "Step 3825: Minibatch Loss: 0.059210\n",
            "Step 3825: Minibatch Loss: 0.056733\n",
            "Step 3825: Minibatch Loss: 0.063751\n",
            "Step 3825: Minibatch Loss: 0.050883\n",
            "Step 3825: Minibatch Loss: 0.055801\n",
            "Step 3825: Minibatch Loss: 0.055627\n",
            "Step 3825: Minibatch Loss: 0.056109\n",
            "Step 3825: Minibatch Loss: 0.059697\n",
            "Step 3825: Minibatch Loss: 0.063704\n",
            "Step 3825: Minibatch Loss: 0.066648\n",
            "Step 3825: Minibatch Loss: 0.060985\n",
            "Step 3825: Minibatch Loss: 0.053771\n",
            "Step 3825: Minibatch Loss: 0.063494\n",
            "Step 3825: Minibatch Loss: 0.052758\n",
            "Step 3825: Minibatch Loss: 0.057445\n",
            "Step 3825: Minibatch Loss: 0.055270\n",
            "Step 3825: Minibatch Loss: 0.053932\n",
            "Step 3825: Minibatch Loss: 0.067176\n",
            "Step 3825: Minibatch Loss: 0.056527\n",
            "Step 3825: Minibatch Loss: 0.057971\n",
            "Training for SNR= 4.5  sigma= 0.5956621435290105\n",
            "Step 3825: Minibatch Loss: 0.048002\n",
            "Step 3825: Minibatch Loss: 0.055401\n",
            "Step 3825: Minibatch Loss: 0.054693\n",
            "Step 3825: Minibatch Loss: 0.059845\n",
            "Step 3825: Minibatch Loss: 0.063036\n",
            "Step 3825: Minibatch Loss: 0.051732\n",
            "Step 3825: Minibatch Loss: 0.055766\n",
            "Step 3825: Minibatch Loss: 0.048275\n",
            "Step 3825: Minibatch Loss: 0.052606\n",
            "Step 3825: Minibatch Loss: 0.061985\n",
            "Step 3825: Minibatch Loss: 0.048933\n",
            "Step 3825: Minibatch Loss: 0.045673\n",
            "Step 3825: Minibatch Loss: 0.053130\n",
            "Step 3825: Minibatch Loss: 0.050665\n",
            "Step 3825: Minibatch Loss: 0.052294\n",
            "Step 3825: Minibatch Loss: 0.047232\n",
            "Step 3825: Minibatch Loss: 0.053681\n",
            "Step 3825: Minibatch Loss: 0.053560\n",
            "Step 3825: Minibatch Loss: 0.059764\n",
            "Step 3825: Minibatch Loss: 0.059565\n",
            "Step 3825: Minibatch Loss: 0.059774\n",
            "Step 3825: Minibatch Loss: 0.050332\n",
            "Step 3825: Minibatch Loss: 0.056195\n",
            "Step 3825: Minibatch Loss: 0.048510\n",
            "Step 3825: Minibatch Loss: 0.049221\n",
            "Step 3825: Minibatch Loss: 0.052783\n",
            "Step 3825: Minibatch Loss: 0.047007\n",
            "Step 3825: Minibatch Loss: 0.060009\n",
            "Step 3825: Minibatch Loss: 0.050041\n",
            "Step 3825: Minibatch Loss: 0.049846\n",
            "Step 3825: Minibatch Loss: 0.051318\n",
            "Step 3825: Minibatch Loss: 0.049618\n",
            "Step 3825: Minibatch Loss: 0.051629\n",
            "Step 3825: Minibatch Loss: 0.053690\n",
            "Step 3825: Minibatch Loss: 0.056490\n",
            "Step 3825: Minibatch Loss: 0.053537\n",
            "Step 3825: Minibatch Loss: 0.042730\n",
            "Step 3825: Minibatch Loss: 0.052910\n",
            "Step 3825: Minibatch Loss: 0.054373\n",
            "Step 3825: Minibatch Loss: 0.046076\n",
            "Step 3825: Minibatch Loss: 0.047304\n",
            "Step 3825: Minibatch Loss: 0.046249\n",
            "Step 3825: Minibatch Loss: 0.042423\n",
            "Step 3825: Minibatch Loss: 0.051058\n",
            "Step 3825: Minibatch Loss: 0.052362\n",
            "Step 3825: Minibatch Loss: 0.059173\n",
            "Step 3825: Minibatch Loss: 0.054196\n",
            "Step 3825: Minibatch Loss: 0.045017\n",
            "Step 3825: Minibatch Loss: 0.049588\n",
            "Step 3825: Minibatch Loss: 0.051475\n",
            "Training for SNR= 4.75  sigma= 0.5787619883491206\n",
            "Step 3825: Minibatch Loss: 0.043363\n",
            "Step 3825: Minibatch Loss: 0.049464\n",
            "Step 3825: Minibatch Loss: 0.045521\n",
            "Step 3825: Minibatch Loss: 0.043884\n",
            "Step 3825: Minibatch Loss: 0.049742\n",
            "Step 3825: Minibatch Loss: 0.043703\n",
            "Step 3825: Minibatch Loss: 0.048615\n",
            "Step 3825: Minibatch Loss: 0.044813\n",
            "Step 3825: Minibatch Loss: 0.040292\n",
            "Step 3825: Minibatch Loss: 0.047992\n",
            "Step 3825: Minibatch Loss: 0.047243\n",
            "Step 3825: Minibatch Loss: 0.039505\n",
            "Step 3825: Minibatch Loss: 0.046851\n",
            "Step 3825: Minibatch Loss: 0.040696\n",
            "Step 3825: Minibatch Loss: 0.050371\n",
            "Step 3825: Minibatch Loss: 0.048478\n",
            "Step 3825: Minibatch Loss: 0.045461\n",
            "Step 3825: Minibatch Loss: 0.045989\n",
            "Step 3825: Minibatch Loss: 0.040251\n",
            "Step 3825: Minibatch Loss: 0.036171\n",
            "Step 3825: Minibatch Loss: 0.045769\n",
            "Step 3825: Minibatch Loss: 0.044111\n",
            "Step 3825: Minibatch Loss: 0.046060\n",
            "Step 3825: Minibatch Loss: 0.055801\n",
            "Step 3825: Minibatch Loss: 0.035022\n",
            "Step 3825: Minibatch Loss: 0.049163\n",
            "Step 3825: Minibatch Loss: 0.051357\n",
            "Step 3825: Minibatch Loss: 0.046450\n",
            "Step 3825: Minibatch Loss: 0.050392\n",
            "Step 3825: Minibatch Loss: 0.046793\n",
            "Step 3825: Minibatch Loss: 0.054655\n",
            "Step 3825: Minibatch Loss: 0.049901\n",
            "Step 3825: Minibatch Loss: 0.041791\n",
            "Step 3825: Minibatch Loss: 0.042417\n",
            "Step 3825: Minibatch Loss: 0.045347\n",
            "Step 3825: Minibatch Loss: 0.046237\n",
            "Step 3825: Minibatch Loss: 0.044856\n",
            "Step 3825: Minibatch Loss: 0.050498\n",
            "Step 3825: Minibatch Loss: 0.039134\n",
            "Step 3825: Minibatch Loss: 0.047340\n",
            "Step 3825: Minibatch Loss: 0.036288\n",
            "Step 3825: Minibatch Loss: 0.046039\n",
            "Step 3825: Minibatch Loss: 0.050599\n",
            "Step 3825: Minibatch Loss: 0.050239\n",
            "Step 3825: Minibatch Loss: 0.047896\n",
            "Step 3825: Minibatch Loss: 0.040978\n",
            "Step 3825: Minibatch Loss: 0.041337\n",
            "Step 3825: Minibatch Loss: 0.046951\n",
            "Step 3825: Minibatch Loss: 0.049760\n",
            "Step 3825: Minibatch Loss: 0.051494\n",
            "Training for SNR= 5.0  sigma= 0.5623413251903491\n",
            "Step 3825: Minibatch Loss: 0.041394\n",
            "Step 3825: Minibatch Loss: 0.040240\n",
            "Step 3825: Minibatch Loss: 0.034312\n",
            "Step 3825: Minibatch Loss: 0.031505\n",
            "Step 3825: Minibatch Loss: 0.032998\n",
            "Step 3825: Minibatch Loss: 0.035861\n",
            "Step 3825: Minibatch Loss: 0.038723\n",
            "Step 3825: Minibatch Loss: 0.039830\n",
            "Step 3825: Minibatch Loss: 0.030187\n",
            "Step 3825: Minibatch Loss: 0.049828\n",
            "Step 3825: Minibatch Loss: 0.046183\n",
            "Step 3825: Minibatch Loss: 0.037405\n",
            "Step 3825: Minibatch Loss: 0.041922\n",
            "Step 3825: Minibatch Loss: 0.039414\n",
            "Step 3825: Minibatch Loss: 0.036447\n",
            "Step 3825: Minibatch Loss: 0.034653\n",
            "Step 3825: Minibatch Loss: 0.041977\n",
            "Step 3825: Minibatch Loss: 0.035369\n",
            "Step 3825: Minibatch Loss: 0.043626\n",
            "Step 3825: Minibatch Loss: 0.030085\n",
            "Step 3825: Minibatch Loss: 0.039080\n",
            "Step 3825: Minibatch Loss: 0.037121\n",
            "Step 3825: Minibatch Loss: 0.037624\n",
            "Step 3825: Minibatch Loss: 0.032367\n",
            "Step 3825: Minibatch Loss: 0.037782\n",
            "Step 3825: Minibatch Loss: 0.046571\n",
            "Step 3825: Minibatch Loss: 0.049244\n",
            "Step 3825: Minibatch Loss: 0.037250\n",
            "Step 3825: Minibatch Loss: 0.037620\n",
            "Step 3825: Minibatch Loss: 0.029531\n",
            "Step 3825: Minibatch Loss: 0.031843\n",
            "Step 3825: Minibatch Loss: 0.046706\n",
            "Step 3825: Minibatch Loss: 0.038280\n",
            "Step 3825: Minibatch Loss: 0.046585\n",
            "Step 3825: Minibatch Loss: 0.035036\n",
            "Step 3825: Minibatch Loss: 0.044487\n",
            "Step 3825: Minibatch Loss: 0.043341\n",
            "Step 3825: Minibatch Loss: 0.030762\n",
            "Step 3825: Minibatch Loss: 0.044551\n",
            "Step 3825: Minibatch Loss: 0.036681\n",
            "Step 3825: Minibatch Loss: 0.038237\n",
            "Step 3825: Minibatch Loss: 0.031011\n",
            "Step 3825: Minibatch Loss: 0.041789\n",
            "Step 3825: Minibatch Loss: 0.036323\n",
            "Step 3825: Minibatch Loss: 0.046616\n",
            "Step 3825: Minibatch Loss: 0.036362\n",
            "Step 3825: Minibatch Loss: 0.033408\n",
            "Step 3825: Minibatch Loss: 0.042053\n",
            "Step 3825: Minibatch Loss: 0.035112\n",
            "Step 3825: Minibatch Loss: 0.039688\n",
            "Training for SNR= 5.25  sigma= 0.5463865498818542\n",
            "Step 3825: Minibatch Loss: 0.038953\n",
            "Step 3825: Minibatch Loss: 0.025886\n",
            "Step 3825: Minibatch Loss: 0.030813\n",
            "Step 3825: Minibatch Loss: 0.036083\n",
            "Step 3825: Minibatch Loss: 0.032622\n",
            "Step 3825: Minibatch Loss: 0.035780\n",
            "Step 3825: Minibatch Loss: 0.031034\n",
            "Step 3825: Minibatch Loss: 0.030818\n",
            "Step 3825: Minibatch Loss: 0.027232\n",
            "Step 3825: Minibatch Loss: 0.036009\n",
            "Step 3825: Minibatch Loss: 0.040189\n",
            "Step 3825: Minibatch Loss: 0.040684\n",
            "Step 3825: Minibatch Loss: 0.033541\n",
            "Step 3825: Minibatch Loss: 0.034745\n",
            "Step 3825: Minibatch Loss: 0.041786\n",
            "Step 3825: Minibatch Loss: 0.036568\n",
            "Step 3825: Minibatch Loss: 0.037360\n",
            "Step 3825: Minibatch Loss: 0.034394\n",
            "Step 3825: Minibatch Loss: 0.033197\n",
            "Step 3825: Minibatch Loss: 0.033106\n",
            "Step 3825: Minibatch Loss: 0.036365\n",
            "Step 3825: Minibatch Loss: 0.033883\n",
            "Step 3825: Minibatch Loss: 0.039401\n",
            "Step 3825: Minibatch Loss: 0.029993\n",
            "Step 3825: Minibatch Loss: 0.030907\n",
            "Step 3825: Minibatch Loss: 0.033025\n",
            "Step 3825: Minibatch Loss: 0.035583\n",
            "Step 3825: Minibatch Loss: 0.031221\n",
            "Step 3825: Minibatch Loss: 0.030375\n",
            "Step 3825: Minibatch Loss: 0.033787\n",
            "Step 3825: Minibatch Loss: 0.040725\n",
            "Step 3825: Minibatch Loss: 0.031766\n",
            "Step 3825: Minibatch Loss: 0.025477\n",
            "Step 3825: Minibatch Loss: 0.037466\n",
            "Step 3825: Minibatch Loss: 0.028231\n",
            "Step 3825: Minibatch Loss: 0.033934\n",
            "Step 3825: Minibatch Loss: 0.034548\n",
            "Step 3825: Minibatch Loss: 0.038046\n",
            "Step 3825: Minibatch Loss: 0.033827\n",
            "Step 3825: Minibatch Loss: 0.031386\n",
            "Step 3825: Minibatch Loss: 0.030565\n",
            "Step 3825: Minibatch Loss: 0.036923\n",
            "Step 3825: Minibatch Loss: 0.032411\n",
            "Step 3825: Minibatch Loss: 0.036326\n",
            "Step 3825: Minibatch Loss: 0.035214\n",
            "Step 3825: Minibatch Loss: 0.031911\n",
            "Step 3825: Minibatch Loss: 0.034241\n",
            "Step 3825: Minibatch Loss: 0.030841\n",
            "Step 3825: Minibatch Loss: 0.032472\n",
            "Step 3825: Minibatch Loss: 0.037767\n",
            "Training for SNR= 5.5  sigma= 0.5308844442309884\n",
            "Step 3825: Minibatch Loss: 0.026935\n",
            "Step 3825: Minibatch Loss: 0.032106\n",
            "Step 3825: Minibatch Loss: 0.036959\n",
            "Step 3825: Minibatch Loss: 0.030444\n",
            "Step 3825: Minibatch Loss: 0.037222\n",
            "Step 3825: Minibatch Loss: 0.023149\n",
            "Step 3825: Minibatch Loss: 0.031470\n",
            "Step 3825: Minibatch Loss: 0.027379\n",
            "Step 3825: Minibatch Loss: 0.035303\n",
            "Step 3825: Minibatch Loss: 0.028082\n",
            "Step 3825: Minibatch Loss: 0.022527\n",
            "Step 3825: Minibatch Loss: 0.027652\n",
            "Step 3825: Minibatch Loss: 0.025695\n",
            "Step 3825: Minibatch Loss: 0.029377\n",
            "Step 3825: Minibatch Loss: 0.030543\n",
            "Step 3825: Minibatch Loss: 0.026477\n",
            "Step 3825: Minibatch Loss: 0.030830\n",
            "Step 3825: Minibatch Loss: 0.026691\n",
            "Step 3825: Minibatch Loss: 0.028093\n",
            "Step 3825: Minibatch Loss: 0.019493\n",
            "Step 3825: Minibatch Loss: 0.033175\n",
            "Step 3825: Minibatch Loss: 0.025167\n",
            "Step 3825: Minibatch Loss: 0.031594\n",
            "Step 3825: Minibatch Loss: 0.031136\n",
            "Step 3825: Minibatch Loss: 0.024479\n",
            "Step 3825: Minibatch Loss: 0.027980\n",
            "Step 3825: Minibatch Loss: 0.025075\n",
            "Step 3825: Minibatch Loss: 0.032662\n",
            "Step 3825: Minibatch Loss: 0.032616\n",
            "Step 3825: Minibatch Loss: 0.025051\n",
            "Step 3825: Minibatch Loss: 0.022887\n",
            "Step 3825: Minibatch Loss: 0.027005\n",
            "Step 3825: Minibatch Loss: 0.024122\n",
            "Step 3825: Minibatch Loss: 0.033035\n",
            "Step 3825: Minibatch Loss: 0.028739\n",
            "Step 3825: Minibatch Loss: 0.027057\n",
            "Step 3825: Minibatch Loss: 0.026233\n",
            "Step 3825: Minibatch Loss: 0.022931\n",
            "Step 3825: Minibatch Loss: 0.030206\n",
            "Step 3825: Minibatch Loss: 0.027127\n",
            "Step 3825: Minibatch Loss: 0.027857\n",
            "Step 3825: Minibatch Loss: 0.026479\n",
            "Step 3825: Minibatch Loss: 0.020726\n",
            "Step 3825: Minibatch Loss: 0.027917\n",
            "Step 3825: Minibatch Loss: 0.027709\n",
            "Step 3825: Minibatch Loss: 0.033090\n",
            "Step 3825: Minibatch Loss: 0.034819\n",
            "Step 3825: Minibatch Loss: 0.025949\n",
            "Step 3825: Minibatch Loss: 0.028564\n",
            "Step 3825: Minibatch Loss: 0.024029\n",
            "Training for SNR= 5.75  sigma= 0.5158221650723057\n",
            "Step 3825: Minibatch Loss: 0.020833\n",
            "Step 3825: Minibatch Loss: 0.022220\n",
            "Step 3825: Minibatch Loss: 0.026927\n",
            "Step 3825: Minibatch Loss: 0.027111\n",
            "Step 3825: Minibatch Loss: 0.024028\n",
            "Step 3825: Minibatch Loss: 0.025651\n",
            "Step 3825: Minibatch Loss: 0.022045\n",
            "Step 3825: Minibatch Loss: 0.022699\n",
            "Step 3825: Minibatch Loss: 0.021933\n",
            "Step 3825: Minibatch Loss: 0.022799\n",
            "Step 3825: Minibatch Loss: 0.026458\n",
            "Step 3825: Minibatch Loss: 0.020153\n",
            "Step 3825: Minibatch Loss: 0.019839\n",
            "Step 3825: Minibatch Loss: 0.024866\n",
            "Step 3825: Minibatch Loss: 0.018386\n",
            "Step 3825: Minibatch Loss: 0.017000\n",
            "Step 3825: Minibatch Loss: 0.019339\n",
            "Step 3825: Minibatch Loss: 0.023761\n",
            "Step 3825: Minibatch Loss: 0.019870\n",
            "Step 3825: Minibatch Loss: 0.021488\n",
            "Step 3825: Minibatch Loss: 0.020627\n",
            "Step 3825: Minibatch Loss: 0.021562\n",
            "Step 3825: Minibatch Loss: 0.020918\n",
            "Step 3825: Minibatch Loss: 0.022603\n",
            "Step 3825: Minibatch Loss: 0.023984\n",
            "Step 3825: Minibatch Loss: 0.022960\n",
            "Step 3825: Minibatch Loss: 0.022869\n",
            "Step 3825: Minibatch Loss: 0.024609\n",
            "Step 3825: Minibatch Loss: 0.026548\n",
            "Step 3825: Minibatch Loss: 0.026434\n",
            "Step 3825: Minibatch Loss: 0.019708\n",
            "Step 3825: Minibatch Loss: 0.027509\n",
            "Step 3825: Minibatch Loss: 0.021230\n",
            "Step 3825: Minibatch Loss: 0.026510\n",
            "Step 3825: Minibatch Loss: 0.017891\n",
            "Step 3825: Minibatch Loss: 0.020859\n",
            "Step 3825: Minibatch Loss: 0.021961\n",
            "Step 3825: Minibatch Loss: 0.027990\n",
            "Step 3825: Minibatch Loss: 0.029185\n",
            "Step 3825: Minibatch Loss: 0.025252\n",
            "Step 3825: Minibatch Loss: 0.021771\n",
            "Step 3825: Minibatch Loss: 0.021426\n",
            "Step 3825: Minibatch Loss: 0.019203\n",
            "Step 3825: Minibatch Loss: 0.023278\n",
            "Step 3825: Minibatch Loss: 0.024011\n",
            "Step 3825: Minibatch Loss: 0.021796\n",
            "Step 3825: Minibatch Loss: 0.020276\n",
            "Step 3825: Minibatch Loss: 0.022299\n",
            "Step 3825: Minibatch Loss: 0.018872\n",
            "Step 3825: Minibatch Loss: 0.024175\n",
            "Training for SNR= 6.0  sigma= 0.5011872336272722\n",
            "Step 3825: Minibatch Loss: 0.020856\n",
            "Step 3825: Minibatch Loss: 0.023747\n",
            "Step 3825: Minibatch Loss: 0.023262\n",
            "Step 3825: Minibatch Loss: 0.017571\n",
            "Step 3825: Minibatch Loss: 0.020914\n",
            "Step 3825: Minibatch Loss: 0.020082\n",
            "Step 3825: Minibatch Loss: 0.023502\n",
            "Step 3825: Minibatch Loss: 0.024040\n",
            "Step 3825: Minibatch Loss: 0.016204\n",
            "Step 3825: Minibatch Loss: 0.017647\n",
            "Step 3825: Minibatch Loss: 0.019959\n",
            "Step 3825: Minibatch Loss: 0.023407\n",
            "Step 3825: Minibatch Loss: 0.022924\n",
            "Step 3825: Minibatch Loss: 0.022468\n",
            "Step 3825: Minibatch Loss: 0.016831\n",
            "Step 3825: Minibatch Loss: 0.014001\n",
            "Step 3825: Minibatch Loss: 0.017900\n",
            "Step 3825: Minibatch Loss: 0.019993\n",
            "Step 3825: Minibatch Loss: 0.023057\n",
            "Step 3825: Minibatch Loss: 0.025208\n",
            "Step 3825: Minibatch Loss: 0.018844\n",
            "Step 3825: Minibatch Loss: 0.019558\n",
            "Step 3825: Minibatch Loss: 0.018682\n",
            "Step 3825: Minibatch Loss: 0.022959\n",
            "Step 3825: Minibatch Loss: 0.021950\n",
            "Step 3825: Minibatch Loss: 0.018494\n",
            "Step 3825: Minibatch Loss: 0.021489\n",
            "Step 3825: Minibatch Loss: 0.021942\n",
            "Step 3825: Minibatch Loss: 0.020832\n",
            "Step 3825: Minibatch Loss: 0.020604\n",
            "Step 3825: Minibatch Loss: 0.023163\n",
            "Step 3825: Minibatch Loss: 0.019349\n",
            "Step 3825: Minibatch Loss: 0.017861\n",
            "Step 3825: Minibatch Loss: 0.019572\n",
            "Step 3825: Minibatch Loss: 0.021573\n",
            "Step 3825: Minibatch Loss: 0.019434\n",
            "Step 3825: Minibatch Loss: 0.017107\n",
            "Step 3825: Minibatch Loss: 0.019653\n",
            "Step 3825: Minibatch Loss: 0.019891\n",
            "Step 3825: Minibatch Loss: 0.015587\n",
            "Step 3825: Minibatch Loss: 0.024618\n",
            "Step 3825: Minibatch Loss: 0.017720\n",
            "Step 3825: Minibatch Loss: 0.019415\n",
            "Step 3825: Minibatch Loss: 0.022203\n",
            "Step 3825: Minibatch Loss: 0.018098\n",
            "Step 3825: Minibatch Loss: 0.030994\n",
            "Step 3825: Minibatch Loss: 0.022665\n",
            "Step 3825: Minibatch Loss: 0.021039\n",
            "Step 3825: Minibatch Loss: 0.019416\n",
            "Step 3825: Minibatch Loss: 0.018025\n",
            "Training for SNR= 6.25  sigma= 0.4869675251658631\n",
            "Step 3825: Minibatch Loss: 0.013225\n",
            "Step 3825: Minibatch Loss: 0.018758\n",
            "Step 3825: Minibatch Loss: 0.019806\n",
            "Step 3825: Minibatch Loss: 0.017607\n",
            "Step 3825: Minibatch Loss: 0.017860\n",
            "Step 3825: Minibatch Loss: 0.015276\n",
            "Step 3825: Minibatch Loss: 0.020176\n",
            "Step 3825: Minibatch Loss: 0.016960\n",
            "Step 3825: Minibatch Loss: 0.015471\n",
            "Step 3825: Minibatch Loss: 0.015135\n",
            "Step 3825: Minibatch Loss: 0.016732\n",
            "Step 3825: Minibatch Loss: 0.021368\n",
            "Step 3825: Minibatch Loss: 0.019729\n",
            "Step 3825: Minibatch Loss: 0.016457\n",
            "Step 3825: Minibatch Loss: 0.017470\n",
            "Step 3825: Minibatch Loss: 0.018351\n",
            "Step 3825: Minibatch Loss: 0.019424\n",
            "Step 3825: Minibatch Loss: 0.017960\n",
            "Step 3825: Minibatch Loss: 0.016457\n",
            "Step 3825: Minibatch Loss: 0.014282\n",
            "Step 3825: Minibatch Loss: 0.016103\n",
            "Step 3825: Minibatch Loss: 0.018322\n",
            "Step 3825: Minibatch Loss: 0.018053\n",
            "Step 3825: Minibatch Loss: 0.015015\n",
            "Step 3825: Minibatch Loss: 0.016133\n",
            "Step 3825: Minibatch Loss: 0.020961\n",
            "Step 3825: Minibatch Loss: 0.015122\n",
            "Step 3825: Minibatch Loss: 0.018454\n",
            "Step 3825: Minibatch Loss: 0.017115\n",
            "Step 3825: Minibatch Loss: 0.016979\n",
            "Step 3825: Minibatch Loss: 0.018714\n",
            "Step 3825: Minibatch Loss: 0.021860\n",
            "Step 3825: Minibatch Loss: 0.016438\n",
            "Step 3825: Minibatch Loss: 0.019905\n",
            "Step 3825: Minibatch Loss: 0.015156\n",
            "Step 3825: Minibatch Loss: 0.022518\n",
            "Step 3825: Minibatch Loss: 0.017877\n",
            "Step 3825: Minibatch Loss: 0.019482\n",
            "Step 3825: Minibatch Loss: 0.018309\n",
            "Step 3825: Minibatch Loss: 0.023097\n",
            "Step 3825: Minibatch Loss: 0.022696\n",
            "Step 3825: Minibatch Loss: 0.017021\n",
            "Step 3825: Minibatch Loss: 0.015015\n",
            "Step 3825: Minibatch Loss: 0.020802\n",
            "Step 3825: Minibatch Loss: 0.013045\n",
            "Step 3825: Minibatch Loss: 0.021139\n",
            "Step 3825: Minibatch Loss: 0.015811\n",
            "Step 3825: Minibatch Loss: 0.018680\n",
            "Step 3825: Minibatch Loss: 0.013207\n",
            "Step 3825: Minibatch Loss: 0.014696\n",
            "Training for SNR= 6.5  sigma= 0.47315125896148047\n",
            "Step 3825: Minibatch Loss: 0.013629\n",
            "Step 3825: Minibatch Loss: 0.018799\n",
            "Step 3825: Minibatch Loss: 0.011923\n",
            "Step 3825: Minibatch Loss: 0.013917\n",
            "Step 3825: Minibatch Loss: 0.013970\n",
            "Step 3825: Minibatch Loss: 0.015304\n",
            "Step 3825: Minibatch Loss: 0.015039\n",
            "Step 3825: Minibatch Loss: 0.015632\n",
            "Step 3825: Minibatch Loss: 0.014273\n",
            "Step 3825: Minibatch Loss: 0.016570\n",
            "Step 3825: Minibatch Loss: 0.013899\n",
            "Step 3825: Minibatch Loss: 0.013952\n",
            "Step 3825: Minibatch Loss: 0.018354\n",
            "Step 3825: Minibatch Loss: 0.013462\n",
            "Step 3825: Minibatch Loss: 0.016278\n",
            "Step 3825: Minibatch Loss: 0.018665\n",
            "Step 3825: Minibatch Loss: 0.013372\n",
            "Step 3825: Minibatch Loss: 0.015092\n",
            "Step 3825: Minibatch Loss: 0.018086\n",
            "Step 3825: Minibatch Loss: 0.020220\n",
            "Step 3825: Minibatch Loss: 0.015649\n",
            "Step 3825: Minibatch Loss: 0.016455\n",
            "Step 3825: Minibatch Loss: 0.015487\n",
            "Step 3825: Minibatch Loss: 0.016629\n",
            "Step 3825: Minibatch Loss: 0.014291\n",
            "Step 3825: Minibatch Loss: 0.014884\n",
            "Step 3825: Minibatch Loss: 0.015526\n",
            "Step 3825: Minibatch Loss: 0.013145\n",
            "Step 3825: Minibatch Loss: 0.016704\n",
            "Step 3825: Minibatch Loss: 0.016640\n",
            "Step 3825: Minibatch Loss: 0.015517\n",
            "Step 3825: Minibatch Loss: 0.013360\n",
            "Step 3825: Minibatch Loss: 0.015313\n",
            "Step 3825: Minibatch Loss: 0.020599\n",
            "Step 3825: Minibatch Loss: 0.015808\n",
            "Step 3825: Minibatch Loss: 0.015198\n",
            "Step 3825: Minibatch Loss: 0.020848\n",
            "Step 3825: Minibatch Loss: 0.013350\n",
            "Step 3825: Minibatch Loss: 0.012039\n",
            "Step 3825: Minibatch Loss: 0.014683\n",
            "Step 3825: Minibatch Loss: 0.013481\n",
            "Step 3825: Minibatch Loss: 0.018196\n",
            "Step 3825: Minibatch Loss: 0.019417\n",
            "Step 3825: Minibatch Loss: 0.015003\n",
            "Step 3825: Minibatch Loss: 0.011546\n",
            "Step 3825: Minibatch Loss: 0.019619\n",
            "Step 3825: Minibatch Loss: 0.014922\n",
            "Step 3825: Minibatch Loss: 0.015443\n",
            "Step 3825: Minibatch Loss: 0.018874\n",
            "Step 3825: Minibatch Loss: 0.013690\n",
            "Training for SNR= 6.75  sigma= 0.4597269885308722\n",
            "Step 3825: Minibatch Loss: 0.015751\n",
            "Step 3825: Minibatch Loss: 0.016360\n",
            "Step 3825: Minibatch Loss: 0.014346\n",
            "Step 3825: Minibatch Loss: 0.010867\n",
            "Step 3825: Minibatch Loss: 0.013115\n",
            "Step 3825: Minibatch Loss: 0.014055\n",
            "Step 3825: Minibatch Loss: 0.014310\n",
            "Step 3825: Minibatch Loss: 0.014000\n",
            "Step 3825: Minibatch Loss: 0.010885\n",
            "Step 3825: Minibatch Loss: 0.012817\n",
            "Step 3825: Minibatch Loss: 0.009770\n",
            "Step 3825: Minibatch Loss: 0.012900\n",
            "Step 3825: Minibatch Loss: 0.012982\n",
            "Step 3825: Minibatch Loss: 0.013088\n",
            "Step 3825: Minibatch Loss: 0.010253\n",
            "Step 3825: Minibatch Loss: 0.015869\n",
            "Step 3825: Minibatch Loss: 0.015097\n",
            "Step 3825: Minibatch Loss: 0.014529\n",
            "Step 3825: Minibatch Loss: 0.019400\n",
            "Step 3825: Minibatch Loss: 0.014591\n",
            "Step 3825: Minibatch Loss: 0.013648\n",
            "Step 3825: Minibatch Loss: 0.013726\n",
            "Step 3825: Minibatch Loss: 0.018203\n",
            "Step 3825: Minibatch Loss: 0.013170\n",
            "Step 3825: Minibatch Loss: 0.011268\n",
            "Step 3825: Minibatch Loss: 0.012196\n",
            "Step 3825: Minibatch Loss: 0.012715\n",
            "Step 3825: Minibatch Loss: 0.012274\n",
            "Step 3825: Minibatch Loss: 0.014786\n",
            "Step 3825: Minibatch Loss: 0.014031\n",
            "Step 3825: Minibatch Loss: 0.013000\n",
            "Step 3825: Minibatch Loss: 0.014193\n",
            "Step 3825: Minibatch Loss: 0.015885\n",
            "Step 3825: Minibatch Loss: 0.012345\n",
            "Step 3825: Minibatch Loss: 0.010729\n",
            "Step 3825: Minibatch Loss: 0.014002\n",
            "Step 3825: Minibatch Loss: 0.011188\n",
            "Step 3825: Minibatch Loss: 0.011453\n",
            "Step 3825: Minibatch Loss: 0.013579\n",
            "Step 3825: Minibatch Loss: 0.015084\n",
            "Step 3825: Minibatch Loss: 0.015218\n",
            "Step 3825: Minibatch Loss: 0.011651\n",
            "Step 3825: Minibatch Loss: 0.012971\n",
            "Step 3825: Minibatch Loss: 0.017174\n",
            "Step 3825: Minibatch Loss: 0.013235\n",
            "Step 3825: Minibatch Loss: 0.014667\n",
            "Step 3825: Minibatch Loss: 0.012011\n",
            "Step 3825: Minibatch Loss: 0.013876\n",
            "Step 3825: Minibatch Loss: 0.017126\n",
            "Step 3825: Minibatch Loss: 0.013360\n",
            "Training for SNR= 7.0  sigma= 0.44668359215096315\n",
            "Step 3825: Minibatch Loss: 0.009517\n",
            "Step 3825: Minibatch Loss: 0.013547\n",
            "Step 3825: Minibatch Loss: 0.011034\n",
            "Step 3825: Minibatch Loss: 0.009693\n",
            "Step 3825: Minibatch Loss: 0.012554\n",
            "Step 3825: Minibatch Loss: 0.010107\n",
            "Step 3825: Minibatch Loss: 0.011029\n",
            "Step 3825: Minibatch Loss: 0.010526\n",
            "Step 3825: Minibatch Loss: 0.012368\n",
            "Step 3825: Minibatch Loss: 0.011830\n",
            "Step 3825: Minibatch Loss: 0.011783\n",
            "Step 3825: Minibatch Loss: 0.010587\n",
            "Step 3825: Minibatch Loss: 0.012385\n",
            "Step 3825: Minibatch Loss: 0.014434\n",
            "Step 3825: Minibatch Loss: 0.010934\n",
            "Step 3825: Minibatch Loss: 0.009127\n",
            "Step 3825: Minibatch Loss: 0.009899\n",
            "Step 3825: Minibatch Loss: 0.010443\n",
            "Step 3825: Minibatch Loss: 0.012811\n",
            "Step 3825: Minibatch Loss: 0.011760\n",
            "Step 3825: Minibatch Loss: 0.010531\n",
            "Step 3825: Minibatch Loss: 0.012762\n",
            "Step 3825: Minibatch Loss: 0.011148\n",
            "Step 3825: Minibatch Loss: 0.011891\n",
            "Step 3825: Minibatch Loss: 0.012992\n",
            "Step 3825: Minibatch Loss: 0.010368\n",
            "Step 3825: Minibatch Loss: 0.010153\n",
            "Step 3825: Minibatch Loss: 0.014029\n",
            "Step 3825: Minibatch Loss: 0.013603\n",
            "Step 3825: Minibatch Loss: 0.012489\n",
            "Step 3825: Minibatch Loss: 0.012073\n",
            "Step 3825: Minibatch Loss: 0.010368\n",
            "Step 3825: Minibatch Loss: 0.015435\n",
            "Step 3825: Minibatch Loss: 0.013675\n",
            "Step 3825: Minibatch Loss: 0.012047\n",
            "Step 3825: Minibatch Loss: 0.013424\n",
            "Step 3825: Minibatch Loss: 0.009105\n",
            "Step 3825: Minibatch Loss: 0.014189\n",
            "Step 3825: Minibatch Loss: 0.008450\n",
            "Step 3825: Minibatch Loss: 0.011083\n",
            "Step 3825: Minibatch Loss: 0.009102\n",
            "Step 3825: Minibatch Loss: 0.013204\n",
            "Step 3825: Minibatch Loss: 0.009986\n",
            "Step 3825: Minibatch Loss: 0.009839\n",
            "Step 3825: Minibatch Loss: 0.012409\n",
            "Step 3825: Minibatch Loss: 0.013159\n",
            "Step 3825: Minibatch Loss: 0.011959\n",
            "Step 3825: Minibatch Loss: 0.013982\n",
            "Step 3825: Minibatch Loss: 0.012271\n",
            "Step 3825: Minibatch Loss: 0.013297\n",
            "Training for SNR= 7.25  sigma= 0.4340102636447438\n",
            "Step 3825: Minibatch Loss: 0.009467\n",
            "Step 3825: Minibatch Loss: 0.008857\n",
            "Step 3825: Minibatch Loss: 0.010922\n",
            "Step 3825: Minibatch Loss: 0.012227\n",
            "Step 3825: Minibatch Loss: 0.012862\n",
            "Step 3825: Minibatch Loss: 0.009597\n",
            "Step 3825: Minibatch Loss: 0.010377\n",
            "Step 3825: Minibatch Loss: 0.010213\n",
            "Step 3825: Minibatch Loss: 0.012950\n",
            "Step 3825: Minibatch Loss: 0.012061\n",
            "Step 3825: Minibatch Loss: 0.010340\n",
            "Step 3825: Minibatch Loss: 0.013351\n",
            "Step 3825: Minibatch Loss: 0.011005\n",
            "Step 3825: Minibatch Loss: 0.008696\n",
            "Step 3825: Minibatch Loss: 0.011292\n",
            "Step 3825: Minibatch Loss: 0.008957\n",
            "Step 3825: Minibatch Loss: 0.010337\n",
            "Step 3825: Minibatch Loss: 0.010379\n",
            "Step 3825: Minibatch Loss: 0.007987\n",
            "Step 3825: Minibatch Loss: 0.011724\n",
            "Step 3825: Minibatch Loss: 0.010592\n",
            "Step 3825: Minibatch Loss: 0.010973\n",
            "Step 3825: Minibatch Loss: 0.011756\n",
            "Step 3825: Minibatch Loss: 0.011721\n",
            "Step 3825: Minibatch Loss: 0.011257\n",
            "Step 3825: Minibatch Loss: 0.010422\n",
            "Step 3825: Minibatch Loss: 0.011791\n",
            "Step 3825: Minibatch Loss: 0.009643\n",
            "Step 3825: Minibatch Loss: 0.008236\n",
            "Step 3825: Minibatch Loss: 0.010896\n",
            "Step 3825: Minibatch Loss: 0.009766\n",
            "Step 3825: Minibatch Loss: 0.010665\n",
            "Step 3825: Minibatch Loss: 0.009454\n",
            "Step 3825: Minibatch Loss: 0.010253\n",
            "Step 3825: Minibatch Loss: 0.014672\n",
            "Step 3825: Minibatch Loss: 0.010918\n",
            "Step 3825: Minibatch Loss: 0.011373\n",
            "Step 3825: Minibatch Loss: 0.011535\n",
            "Step 3825: Minibatch Loss: 0.009364\n",
            "Step 3825: Minibatch Loss: 0.009929\n",
            "Step 3825: Minibatch Loss: 0.008139\n",
            "Step 3825: Minibatch Loss: 0.009667\n",
            "Step 3825: Minibatch Loss: 0.012213\n",
            "Step 3825: Minibatch Loss: 0.011404\n",
            "Step 3825: Minibatch Loss: 0.009468\n",
            "Step 3825: Minibatch Loss: 0.011398\n",
            "Step 3825: Minibatch Loss: 0.008944\n",
            "Step 3825: Minibatch Loss: 0.010766\n",
            "Step 3825: Minibatch Loss: 0.007891\n",
            "Step 3825: Minibatch Loss: 0.011582\n",
            "Training for SNR= 7.5  sigma= 0.4216965034285822\n",
            "Step 3825: Minibatch Loss: 0.009218\n",
            "Step 3825: Minibatch Loss: 0.007214\n",
            "Step 3825: Minibatch Loss: 0.009607\n",
            "Step 3825: Minibatch Loss: 0.008738\n",
            "Step 3825: Minibatch Loss: 0.010321\n",
            "Step 3825: Minibatch Loss: 0.008057\n",
            "Step 3825: Minibatch Loss: 0.009426\n",
            "Step 3825: Minibatch Loss: 0.011305\n",
            "Step 3825: Minibatch Loss: 0.008127\n",
            "Step 3825: Minibatch Loss: 0.009804\n",
            "Step 3825: Minibatch Loss: 0.012477\n",
            "Step 3825: Minibatch Loss: 0.009464\n",
            "Step 3825: Minibatch Loss: 0.008940\n",
            "Step 3825: Minibatch Loss: 0.008890\n",
            "Step 3825: Minibatch Loss: 0.011302\n",
            "Step 3825: Minibatch Loss: 0.010361\n",
            "Step 3825: Minibatch Loss: 0.008157\n",
            "Step 3825: Minibatch Loss: 0.008307\n",
            "Step 3825: Minibatch Loss: 0.010595\n",
            "Step 3825: Minibatch Loss: 0.009821\n",
            "Step 3825: Minibatch Loss: 0.010797\n",
            "Step 3825: Minibatch Loss: 0.011538\n",
            "Step 3825: Minibatch Loss: 0.009482\n",
            "Step 3825: Minibatch Loss: 0.009151\n",
            "Step 3825: Minibatch Loss: 0.008225\n",
            "Step 3825: Minibatch Loss: 0.007973\n",
            "Step 3825: Minibatch Loss: 0.009256\n",
            "Step 3825: Minibatch Loss: 0.008705\n",
            "Step 3825: Minibatch Loss: 0.007980\n",
            "Step 3825: Minibatch Loss: 0.010099\n",
            "Step 3825: Minibatch Loss: 0.008598\n",
            "Step 3825: Minibatch Loss: 0.008181\n",
            "Step 3825: Minibatch Loss: 0.010651\n",
            "Step 3825: Minibatch Loss: 0.011040\n",
            "Step 3825: Minibatch Loss: 0.008934\n",
            "Step 3825: Minibatch Loss: 0.008768\n",
            "Step 3825: Minibatch Loss: 0.009211\n",
            "Step 3825: Minibatch Loss: 0.011938\n",
            "Step 3825: Minibatch Loss: 0.007845\n",
            "Step 3825: Minibatch Loss: 0.007832\n",
            "Step 3825: Minibatch Loss: 0.008082\n",
            "Step 3825: Minibatch Loss: 0.009793\n",
            "Step 3825: Minibatch Loss: 0.009654\n",
            "Step 3825: Minibatch Loss: 0.006831\n",
            "Step 3825: Minibatch Loss: 0.011344\n",
            "Step 3825: Minibatch Loss: 0.007194\n",
            "Step 3825: Minibatch Loss: 0.009677\n",
            "Step 3825: Minibatch Loss: 0.009532\n",
            "Step 3825: Minibatch Loss: 0.010626\n",
            "Step 3825: Minibatch Loss: 0.008188\n",
            "Training for SNR= 7.75  sigma= 0.40973210981354147\n",
            "Step 3825: Minibatch Loss: 0.006985\n",
            "Step 3825: Minibatch Loss: 0.007642\n",
            "Step 3825: Minibatch Loss: 0.007839\n",
            "Step 3825: Minibatch Loss: 0.007545\n",
            "Step 3825: Minibatch Loss: 0.008909\n",
            "Step 3825: Minibatch Loss: 0.010181\n",
            "Step 3825: Minibatch Loss: 0.009436\n",
            "Step 3825: Minibatch Loss: 0.007760\n",
            "Step 3825: Minibatch Loss: 0.008717\n",
            "Step 3825: Minibatch Loss: 0.008050\n",
            "Step 3825: Minibatch Loss: 0.007798\n",
            "Step 3825: Minibatch Loss: 0.007381\n",
            "Step 3825: Minibatch Loss: 0.009515\n",
            "Step 3825: Minibatch Loss: 0.008971\n",
            "Step 3825: Minibatch Loss: 0.010014\n",
            "Step 3825: Minibatch Loss: 0.008429\n",
            "Step 3825: Minibatch Loss: 0.006822\n",
            "Step 3825: Minibatch Loss: 0.008703\n",
            "Step 3825: Minibatch Loss: 0.008351\n",
            "Step 3825: Minibatch Loss: 0.007817\n",
            "Step 3825: Minibatch Loss: 0.006981\n",
            "Step 3825: Minibatch Loss: 0.010287\n",
            "Step 3825: Minibatch Loss: 0.008188\n",
            "Step 3825: Minibatch Loss: 0.008490\n",
            "Step 3825: Minibatch Loss: 0.006495\n",
            "Step 3825: Minibatch Loss: 0.007294\n",
            "Step 3825: Minibatch Loss: 0.007708\n",
            "Step 3825: Minibatch Loss: 0.009925\n",
            "Step 3825: Minibatch Loss: 0.008116\n",
            "Step 3825: Minibatch Loss: 0.008071\n",
            "Step 3825: Minibatch Loss: 0.008458\n",
            "Step 3825: Minibatch Loss: 0.008723\n",
            "Step 3825: Minibatch Loss: 0.009066\n",
            "Step 3825: Minibatch Loss: 0.006881\n",
            "Step 3825: Minibatch Loss: 0.007410\n",
            "Step 3825: Minibatch Loss: 0.010013\n",
            "Step 3825: Minibatch Loss: 0.008662\n",
            "Step 3825: Minibatch Loss: 0.008918\n",
            "Step 3825: Minibatch Loss: 0.007404\n",
            "Step 3825: Minibatch Loss: 0.010301\n",
            "Step 3825: Minibatch Loss: 0.010371\n",
            "Step 3825: Minibatch Loss: 0.008847\n",
            "Step 3825: Minibatch Loss: 0.009932\n",
            "Step 3825: Minibatch Loss: 0.008336\n",
            "Step 3825: Minibatch Loss: 0.008922\n",
            "Step 3825: Minibatch Loss: 0.007217\n",
            "Step 3825: Minibatch Loss: 0.007776\n",
            "Step 3825: Minibatch Loss: 0.008854\n",
            "Step 3825: Minibatch Loss: 0.008662\n",
            "Step 3825: Minibatch Loss: 0.007057\n",
            "Training for SNR= 8.0  sigma= 0.3981071705534972\n",
            "Step 3825: Minibatch Loss: 0.006646\n",
            "Step 3825: Minibatch Loss: 0.007313\n",
            "Step 3825: Minibatch Loss: 0.007177\n",
            "Step 3825: Minibatch Loss: 0.008410\n",
            "Step 3825: Minibatch Loss: 0.007789\n",
            "Step 3825: Minibatch Loss: 0.006771\n",
            "Step 3825: Minibatch Loss: 0.007150\n",
            "Step 3825: Minibatch Loss: 0.008055\n",
            "Step 3825: Minibatch Loss: 0.008466\n",
            "Step 3825: Minibatch Loss: 0.006191\n",
            "Step 3825: Minibatch Loss: 0.006037\n",
            "Step 3825: Minibatch Loss: 0.008985\n",
            "Step 3825: Minibatch Loss: 0.007035\n",
            "Step 3825: Minibatch Loss: 0.009911\n",
            "Step 3825: Minibatch Loss: 0.007835\n",
            "Step 3825: Minibatch Loss: 0.008091\n",
            "Step 3825: Minibatch Loss: 0.008031\n",
            "Step 3825: Minibatch Loss: 0.006291\n",
            "Step 3825: Minibatch Loss: 0.006749\n",
            "Step 3825: Minibatch Loss: 0.007881\n",
            "Step 3825: Minibatch Loss: 0.006365\n",
            "Step 3825: Minibatch Loss: 0.006372\n",
            "Step 3825: Minibatch Loss: 0.006204\n",
            "Step 3825: Minibatch Loss: 0.008783\n",
            "Step 3825: Minibatch Loss: 0.008221\n",
            "Step 3825: Minibatch Loss: 0.005760\n",
            "Step 3825: Minibatch Loss: 0.006889\n",
            "Step 3825: Minibatch Loss: 0.008295\n",
            "Step 3825: Minibatch Loss: 0.006867\n",
            "Step 3825: Minibatch Loss: 0.006625\n",
            "Step 3825: Minibatch Loss: 0.006433\n",
            "Step 3825: Minibatch Loss: 0.006097\n",
            "Step 3825: Minibatch Loss: 0.007818\n",
            "Step 3825: Minibatch Loss: 0.006559\n",
            "Step 3825: Minibatch Loss: 0.008252\n",
            "Step 3825: Minibatch Loss: 0.005512\n",
            "Step 3825: Minibatch Loss: 0.008282\n",
            "Step 3825: Minibatch Loss: 0.007799\n",
            "Step 3825: Minibatch Loss: 0.007376\n",
            "Step 3825: Minibatch Loss: 0.008173\n",
            "Step 3825: Minibatch Loss: 0.006776\n",
            "Step 3825: Minibatch Loss: 0.010607\n",
            "Step 3825: Minibatch Loss: 0.008416\n",
            "Step 3825: Minibatch Loss: 0.009620\n",
            "Step 3825: Minibatch Loss: 0.008102\n",
            "Step 3825: Minibatch Loss: 0.008803\n",
            "Step 3825: Minibatch Loss: 0.007101\n",
            "Step 3825: Minibatch Loss: 0.006446\n",
            "Step 3825: Minibatch Loss: 0.007591\n",
            "Step 3825: Minibatch Loss: 0.006262\n",
            "Training for SNR= 8.25  sigma= 0.3868120546330522\n",
            "Step 3825: Minibatch Loss: 0.005699\n",
            "Step 3825: Minibatch Loss: 0.007710\n",
            "Step 3825: Minibatch Loss: 0.006728\n",
            "Step 3825: Minibatch Loss: 0.005510\n",
            "Step 3825: Minibatch Loss: 0.006813\n",
            "Step 3825: Minibatch Loss: 0.006381\n",
            "Step 3825: Minibatch Loss: 0.006261\n",
            "Step 3825: Minibatch Loss: 0.005721\n",
            "Step 3825: Minibatch Loss: 0.005389\n",
            "Step 3825: Minibatch Loss: 0.005736\n",
            "Step 3825: Minibatch Loss: 0.007379\n",
            "Step 3825: Minibatch Loss: 0.006527\n",
            "Step 3825: Minibatch Loss: 0.006595\n",
            "Step 3825: Minibatch Loss: 0.006875\n",
            "Step 3825: Minibatch Loss: 0.006567\n",
            "Step 3825: Minibatch Loss: 0.006304\n",
            "Step 3825: Minibatch Loss: 0.007459\n",
            "Step 3825: Minibatch Loss: 0.006265\n",
            "Step 3825: Minibatch Loss: 0.006757\n",
            "Step 3825: Minibatch Loss: 0.006237\n",
            "Step 3825: Minibatch Loss: 0.006379\n",
            "Step 3825: Minibatch Loss: 0.006752\n",
            "Step 3825: Minibatch Loss: 0.007826\n",
            "Step 3825: Minibatch Loss: 0.006554\n",
            "Step 3825: Minibatch Loss: 0.006485\n",
            "Step 3825: Minibatch Loss: 0.006948\n",
            "Step 3825: Minibatch Loss: 0.006937\n",
            "Step 3825: Minibatch Loss: 0.005670\n",
            "Step 3825: Minibatch Loss: 0.006602\n",
            "Step 3825: Minibatch Loss: 0.007917\n",
            "Step 3825: Minibatch Loss: 0.006543\n",
            "Step 3825: Minibatch Loss: 0.006407\n",
            "Step 3825: Minibatch Loss: 0.007419\n",
            "Step 3825: Minibatch Loss: 0.005620\n",
            "Step 3825: Minibatch Loss: 0.006481\n",
            "Step 3825: Minibatch Loss: 0.006589\n",
            "Step 3825: Minibatch Loss: 0.006340\n",
            "Step 3825: Minibatch Loss: 0.006307\n",
            "Step 3825: Minibatch Loss: 0.006449\n",
            "Step 3825: Minibatch Loss: 0.006036\n",
            "Step 3825: Minibatch Loss: 0.008008\n",
            "Step 3825: Minibatch Loss: 0.005323\n",
            "Step 3825: Minibatch Loss: 0.005904\n",
            "Step 3825: Minibatch Loss: 0.006133\n",
            "Step 3825: Minibatch Loss: 0.006217\n",
            "Step 3825: Minibatch Loss: 0.005823\n",
            "Step 3825: Minibatch Loss: 0.005961\n",
            "Step 3825: Minibatch Loss: 0.005115\n",
            "Step 3825: Minibatch Loss: 0.005481\n",
            "Step 3825: Minibatch Loss: 0.005022\n",
            "Training for SNR= 8.5  sigma= 0.3758374042884442\n",
            "Step 3825: Minibatch Loss: 0.006326\n",
            "Step 3825: Minibatch Loss: 0.006043\n",
            "Step 3825: Minibatch Loss: 0.005819\n",
            "Step 3825: Minibatch Loss: 0.005851\n",
            "Step 3825: Minibatch Loss: 0.005326\n",
            "Step 3825: Minibatch Loss: 0.005323\n",
            "Step 3825: Minibatch Loss: 0.005023\n",
            "Step 3825: Minibatch Loss: 0.006822\n",
            "Step 3825: Minibatch Loss: 0.007396\n",
            "Step 3825: Minibatch Loss: 0.006410\n",
            "Step 3825: Minibatch Loss: 0.005991\n",
            "Step 3825: Minibatch Loss: 0.005811\n",
            "Step 3825: Minibatch Loss: 0.006879\n",
            "Step 3825: Minibatch Loss: 0.005961\n",
            "Step 3825: Minibatch Loss: 0.006389\n",
            "Step 3825: Minibatch Loss: 0.006287\n",
            "Step 3825: Minibatch Loss: 0.005318\n",
            "Step 3825: Minibatch Loss: 0.004772\n",
            "Step 3825: Minibatch Loss: 0.005202\n",
            "Step 3825: Minibatch Loss: 0.005746\n",
            "Step 3825: Minibatch Loss: 0.006238\n",
            "Step 3825: Minibatch Loss: 0.005986\n",
            "Step 3825: Minibatch Loss: 0.005552\n",
            "Step 3825: Minibatch Loss: 0.006020\n",
            "Step 3825: Minibatch Loss: 0.006096\n",
            "Step 3825: Minibatch Loss: 0.004772\n",
            "Step 3825: Minibatch Loss: 0.006674\n",
            "Step 3825: Minibatch Loss: 0.005432\n",
            "Step 3825: Minibatch Loss: 0.005504\n",
            "Step 3825: Minibatch Loss: 0.005300\n",
            "Step 3825: Minibatch Loss: 0.006727\n",
            "Step 3825: Minibatch Loss: 0.005950\n",
            "Step 3825: Minibatch Loss: 0.005464\n",
            "Step 3825: Minibatch Loss: 0.005825\n",
            "Step 3825: Minibatch Loss: 0.005122\n",
            "Step 3825: Minibatch Loss: 0.005437\n",
            "Step 3825: Minibatch Loss: 0.007566\n",
            "Step 3825: Minibatch Loss: 0.006605\n",
            "Step 3825: Minibatch Loss: 0.004980\n",
            "Step 3825: Minibatch Loss: 0.007322\n",
            "Step 3825: Minibatch Loss: 0.005684\n",
            "Step 3825: Minibatch Loss: 0.006066\n",
            "Step 3825: Minibatch Loss: 0.006091\n",
            "Step 3825: Minibatch Loss: 0.005491\n",
            "Step 3825: Minibatch Loss: 0.005537\n",
            "Step 3825: Minibatch Loss: 0.005469\n",
            "Step 3825: Minibatch Loss: 0.007329\n",
            "Step 3825: Minibatch Loss: 0.005120\n",
            "Step 3825: Minibatch Loss: 0.005879\n",
            "Step 3825: Minibatch Loss: 0.005958\n",
            "Training for SNR= 8.75  sigma= 0.3651741272548377\n",
            "Step 3825: Minibatch Loss: 0.004534\n",
            "Step 3825: Minibatch Loss: 0.005131\n",
            "Step 3825: Minibatch Loss: 0.005726\n",
            "Step 3825: Minibatch Loss: 0.005195\n",
            "Step 3825: Minibatch Loss: 0.005558\n",
            "Step 3825: Minibatch Loss: 0.005519\n",
            "Step 3825: Minibatch Loss: 0.006287\n",
            "Step 3825: Minibatch Loss: 0.005903\n",
            "Step 3825: Minibatch Loss: 0.005711\n",
            "Step 3825: Minibatch Loss: 0.005153\n",
            "Step 3825: Minibatch Loss: 0.004937\n",
            "Step 3825: Minibatch Loss: 0.005307\n",
            "Step 3825: Minibatch Loss: 0.006293\n",
            "Step 3825: Minibatch Loss: 0.005508\n",
            "Step 3825: Minibatch Loss: 0.006313\n",
            "Step 3825: Minibatch Loss: 0.004464\n",
            "Step 3825: Minibatch Loss: 0.005123\n",
            "Step 3825: Minibatch Loss: 0.005711\n",
            "Step 3825: Minibatch Loss: 0.005501\n",
            "Step 3825: Minibatch Loss: 0.005178\n",
            "Step 3825: Minibatch Loss: 0.006899\n",
            "Step 3825: Minibatch Loss: 0.004922\n",
            "Step 3825: Minibatch Loss: 0.005720\n",
            "Step 3825: Minibatch Loss: 0.005091\n",
            "Step 3825: Minibatch Loss: 0.005100\n",
            "Step 3825: Minibatch Loss: 0.005232\n",
            "Step 3825: Minibatch Loss: 0.005271\n",
            "Step 3825: Minibatch Loss: 0.005261\n",
            "Step 3825: Minibatch Loss: 0.004876\n",
            "Step 3825: Minibatch Loss: 0.005073\n",
            "Step 3825: Minibatch Loss: 0.004820\n",
            "Step 3825: Minibatch Loss: 0.005994\n",
            "Step 3825: Minibatch Loss: 0.004935\n",
            "Step 3825: Minibatch Loss: 0.005667\n",
            "Step 3825: Minibatch Loss: 0.004793\n",
            "Step 3825: Minibatch Loss: 0.005319\n",
            "Step 3825: Minibatch Loss: 0.005278\n",
            "Step 3825: Minibatch Loss: 0.005216\n",
            "Step 3825: Minibatch Loss: 0.005463\n",
            "Step 3825: Minibatch Loss: 0.006219\n",
            "Step 3825: Minibatch Loss: 0.005378\n",
            "Step 3825: Minibatch Loss: 0.005998\n",
            "Step 3825: Minibatch Loss: 0.004897\n",
            "Step 3825: Minibatch Loss: 0.006306\n",
            "Step 3825: Minibatch Loss: 0.006435\n",
            "Step 3825: Minibatch Loss: 0.004880\n",
            "Step 3825: Minibatch Loss: 0.005188\n",
            "Step 3825: Minibatch Loss: 0.006136\n",
            "Step 3825: Minibatch Loss: 0.005415\n",
            "Step 3825: Minibatch Loss: 0.005112\n",
            "Training for SNR= 9.0  sigma= 0.35481338923357547\n",
            "Step 3825: Minibatch Loss: 0.005075\n",
            "Step 3825: Minibatch Loss: 0.004866\n",
            "Step 3825: Minibatch Loss: 0.004220\n",
            "Step 3825: Minibatch Loss: 0.004734\n",
            "Step 3825: Minibatch Loss: 0.005034\n",
            "Step 3825: Minibatch Loss: 0.004839\n",
            "Step 3825: Minibatch Loss: 0.004372\n",
            "Step 3825: Minibatch Loss: 0.004760\n",
            "Step 3825: Minibatch Loss: 0.005892\n",
            "Step 3825: Minibatch Loss: 0.005680\n",
            "Step 3825: Minibatch Loss: 0.006106\n",
            "Step 3825: Minibatch Loss: 0.005310\n",
            "Step 3825: Minibatch Loss: 0.004702\n",
            "Step 3825: Minibatch Loss: 0.005010\n",
            "Step 3825: Minibatch Loss: 0.005339\n",
            "Step 3825: Minibatch Loss: 0.005173\n",
            "Step 3825: Minibatch Loss: 0.004384\n",
            "Step 3825: Minibatch Loss: 0.005105\n",
            "Step 3825: Minibatch Loss: 0.004910\n",
            "Step 3825: Minibatch Loss: 0.005245\n",
            "Step 3825: Minibatch Loss: 0.004335\n",
            "Step 3825: Minibatch Loss: 0.004468\n",
            "Step 3825: Minibatch Loss: 0.004488\n",
            "Step 3825: Minibatch Loss: 0.004271\n",
            "Step 3825: Minibatch Loss: 0.005764\n",
            "Step 3825: Minibatch Loss: 0.005862\n",
            "Step 3825: Minibatch Loss: 0.004515\n",
            "Step 3825: Minibatch Loss: 0.004506\n",
            "Step 3825: Minibatch Loss: 0.005638\n",
            "Step 3825: Minibatch Loss: 0.004732\n",
            "Step 3825: Minibatch Loss: 0.004611\n",
            "Step 3825: Minibatch Loss: 0.004941\n",
            "Step 3825: Minibatch Loss: 0.004609\n",
            "Step 3825: Minibatch Loss: 0.006237\n",
            "Step 3825: Minibatch Loss: 0.004747\n",
            "Step 3825: Minibatch Loss: 0.004835\n",
            "Step 3825: Minibatch Loss: 0.005144\n",
            "Step 3825: Minibatch Loss: 0.004532\n",
            "Step 3825: Minibatch Loss: 0.004420\n",
            "Step 3825: Minibatch Loss: 0.004954\n",
            "Step 3825: Minibatch Loss: 0.005551\n",
            "Step 3825: Minibatch Loss: 0.004690\n",
            "Step 3825: Minibatch Loss: 0.004821\n",
            "Step 3825: Minibatch Loss: 0.004570\n",
            "Step 3825: Minibatch Loss: 0.005067\n",
            "Step 3825: Minibatch Loss: 0.005173\n",
            "Step 3825: Minibatch Loss: 0.004888\n",
            "Step 3825: Minibatch Loss: 0.006586\n",
            "Step 3825: Minibatch Loss: 0.004799\n",
            "Step 3825: Minibatch Loss: 0.006607\n",
            "Training for SNR= 9.25  sigma= 0.3447466065731494\n",
            "Step 3825: Minibatch Loss: 0.004150\n",
            "Step 3825: Minibatch Loss: 0.004662\n",
            "Step 3825: Minibatch Loss: 0.004085\n",
            "Step 3825: Minibatch Loss: 0.004383\n",
            "Step 3825: Minibatch Loss: 0.004725\n",
            "Step 3825: Minibatch Loss: 0.004072\n",
            "Step 3825: Minibatch Loss: 0.006054\n",
            "Step 3825: Minibatch Loss: 0.004510\n",
            "Step 3825: Minibatch Loss: 0.003840\n",
            "Step 3825: Minibatch Loss: 0.005664\n",
            "Step 3825: Minibatch Loss: 0.004291\n",
            "Step 3825: Minibatch Loss: 0.004940\n",
            "Step 3825: Minibatch Loss: 0.004233\n",
            "Step 3825: Minibatch Loss: 0.005217\n",
            "Step 3825: Minibatch Loss: 0.004981\n",
            "Step 3825: Minibatch Loss: 0.004325\n",
            "Step 3825: Minibatch Loss: 0.003914\n",
            "Step 3825: Minibatch Loss: 0.004551\n",
            "Step 3825: Minibatch Loss: 0.005160\n",
            "Step 3825: Minibatch Loss: 0.004298\n",
            "Step 3825: Minibatch Loss: 0.004273\n",
            "Step 3825: Minibatch Loss: 0.004178\n",
            "Step 3825: Minibatch Loss: 0.004148\n",
            "Step 3825: Minibatch Loss: 0.004242\n",
            "Step 3825: Minibatch Loss: 0.005493\n",
            "Step 3825: Minibatch Loss: 0.005036\n",
            "Step 3825: Minibatch Loss: 0.004256\n",
            "Step 3825: Minibatch Loss: 0.004353\n",
            "Step 3825: Minibatch Loss: 0.003867\n",
            "Step 3825: Minibatch Loss: 0.004614\n",
            "Step 3825: Minibatch Loss: 0.004846\n",
            "Step 3825: Minibatch Loss: 0.004703\n",
            "Step 3825: Minibatch Loss: 0.004585\n",
            "Step 3825: Minibatch Loss: 0.004718\n",
            "Step 3825: Minibatch Loss: 0.003896\n",
            "Step 3825: Minibatch Loss: 0.004668\n",
            "Step 3825: Minibatch Loss: 0.004199\n",
            "Step 3825: Minibatch Loss: 0.004109\n",
            "Step 3825: Minibatch Loss: 0.004568\n",
            "Step 3825: Minibatch Loss: 0.003908\n",
            "Step 3825: Minibatch Loss: 0.004418\n",
            "Step 3825: Minibatch Loss: 0.004864\n",
            "Step 3825: Minibatch Loss: 0.005000\n",
            "Step 3825: Minibatch Loss: 0.005339\n",
            "Step 3825: Minibatch Loss: 0.004324\n",
            "Step 3825: Minibatch Loss: 0.004401\n",
            "Step 3825: Minibatch Loss: 0.004653\n",
            "Step 3825: Minibatch Loss: 0.004269\n",
            "Step 3825: Minibatch Loss: 0.004108\n",
            "Step 3825: Minibatch Loss: 0.004339\n",
            "Training for SNR= 9.5  sigma= 0.33496543915782767\n",
            "Step 3825: Minibatch Loss: 0.004907\n",
            "Step 3825: Minibatch Loss: 0.004135\n",
            "Step 3825: Minibatch Loss: 0.003735\n",
            "Step 3825: Minibatch Loss: 0.004352\n",
            "Step 3825: Minibatch Loss: 0.004891\n",
            "Step 3825: Minibatch Loss: 0.004389\n",
            "Step 3825: Minibatch Loss: 0.003734\n",
            "Step 3825: Minibatch Loss: 0.003575\n",
            "Step 3825: Minibatch Loss: 0.004698\n",
            "Step 3825: Minibatch Loss: 0.004080\n",
            "Step 3825: Minibatch Loss: 0.004230\n",
            "Step 3825: Minibatch Loss: 0.004341\n",
            "Step 3825: Minibatch Loss: 0.004116\n",
            "Step 3825: Minibatch Loss: 0.003926\n",
            "Step 3825: Minibatch Loss: 0.004299\n",
            "Step 3825: Minibatch Loss: 0.003938\n",
            "Step 3825: Minibatch Loss: 0.004618\n",
            "Step 3825: Minibatch Loss: 0.003879\n",
            "Step 3825: Minibatch Loss: 0.004410\n",
            "Step 3825: Minibatch Loss: 0.004589\n",
            "Step 3825: Minibatch Loss: 0.003732\n",
            "Step 3825: Minibatch Loss: 0.003924\n",
            "Step 3825: Minibatch Loss: 0.003992\n",
            "Step 3825: Minibatch Loss: 0.003916\n",
            "Step 3825: Minibatch Loss: 0.004651\n",
            "Step 3825: Minibatch Loss: 0.003541\n",
            "Step 3825: Minibatch Loss: 0.003642\n",
            "Step 3825: Minibatch Loss: 0.004476\n",
            "Step 3825: Minibatch Loss: 0.003668\n",
            "Step 3825: Minibatch Loss: 0.004007\n",
            "Step 3825: Minibatch Loss: 0.004256\n",
            "Step 3825: Minibatch Loss: 0.004443\n",
            "Step 3825: Minibatch Loss: 0.003941\n",
            "Step 3825: Minibatch Loss: 0.003456\n",
            "Step 3825: Minibatch Loss: 0.004224\n",
            "Step 3825: Minibatch Loss: 0.003721\n",
            "Step 3825: Minibatch Loss: 0.003715\n",
            "Step 3825: Minibatch Loss: 0.003555\n",
            "Step 3825: Minibatch Loss: 0.004427\n",
            "Step 3825: Minibatch Loss: 0.004191\n",
            "Step 3825: Minibatch Loss: 0.003912\n",
            "Step 3825: Minibatch Loss: 0.003589\n",
            "Step 3825: Minibatch Loss: 0.003922\n",
            "Step 3825: Minibatch Loss: 0.003942\n",
            "Step 3825: Minibatch Loss: 0.003756\n",
            "Step 3825: Minibatch Loss: 0.004467\n",
            "Step 3825: Minibatch Loss: 0.004060\n",
            "Step 3825: Minibatch Loss: 0.004029\n",
            "Step 3825: Minibatch Loss: 0.003869\n",
            "Step 3825: Minibatch Loss: 0.004173\n",
            "Training for SNR= 9.75  sigma= 0.3254617834980459\n",
            "Step 3825: Minibatch Loss: 0.003976\n",
            "Step 3825: Minibatch Loss: 0.003525\n",
            "Step 3825: Minibatch Loss: 0.003864\n",
            "Step 3825: Minibatch Loss: 0.003802\n",
            "Step 3825: Minibatch Loss: 0.004265\n",
            "Step 3825: Minibatch Loss: 0.004084\n",
            "Step 3825: Minibatch Loss: 0.003851\n",
            "Step 3825: Minibatch Loss: 0.004028\n",
            "Step 3825: Minibatch Loss: 0.004111\n",
            "Step 3825: Minibatch Loss: 0.003827\n",
            "Step 3825: Minibatch Loss: 0.003487\n",
            "Step 3825: Minibatch Loss: 0.003584\n",
            "Step 3825: Minibatch Loss: 0.003541\n",
            "Step 3825: Minibatch Loss: 0.003868\n",
            "Step 3825: Minibatch Loss: 0.004127\n",
            "Step 3825: Minibatch Loss: 0.003859\n",
            "Step 3825: Minibatch Loss: 0.003919\n",
            "Step 3825: Minibatch Loss: 0.004046\n",
            "Step 3825: Minibatch Loss: 0.003668\n",
            "Step 3825: Minibatch Loss: 0.003262\n",
            "Step 3825: Minibatch Loss: 0.003954\n",
            "Step 3825: Minibatch Loss: 0.003532\n",
            "Step 3825: Minibatch Loss: 0.003751\n",
            "Step 3825: Minibatch Loss: 0.004517\n",
            "Step 3825: Minibatch Loss: 0.003530\n",
            "Step 3825: Minibatch Loss: 0.003826\n",
            "Step 3825: Minibatch Loss: 0.003560\n",
            "Step 3825: Minibatch Loss: 0.003398\n",
            "Step 3825: Minibatch Loss: 0.004289\n",
            "Step 3825: Minibatch Loss: 0.003556\n",
            "Step 3825: Minibatch Loss: 0.003938\n",
            "Step 3825: Minibatch Loss: 0.003404\n",
            "Step 3825: Minibatch Loss: 0.004176\n",
            "Step 3825: Minibatch Loss: 0.003306\n",
            "Step 3825: Minibatch Loss: 0.004089\n",
            "Step 3825: Minibatch Loss: 0.003888\n",
            "Step 3825: Minibatch Loss: 0.003632\n",
            "Step 3825: Minibatch Loss: 0.003870\n",
            "Step 3825: Minibatch Loss: 0.004021\n",
            "Step 3825: Minibatch Loss: 0.003766\n",
            "Step 3825: Minibatch Loss: 0.003558\n",
            "Step 3825: Minibatch Loss: 0.003899\n",
            "Step 3825: Minibatch Loss: 0.003690\n",
            "Step 3825: Minibatch Loss: 0.003464\n",
            "Step 3825: Minibatch Loss: 0.003843\n",
            "Step 3825: Minibatch Loss: 0.003849\n",
            "Step 3825: Minibatch Loss: 0.003638\n",
            "Step 3825: Minibatch Loss: 0.003552\n",
            "Step 3825: Minibatch Loss: 0.003923\n",
            "Step 3825: Minibatch Loss: 0.003816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHByzQbTUqbv",
        "outputId": "0ea307e7-c9b9-4301-ec99-380154fc89ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using trained model\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_dl_tensor  = numpy.array(())\n",
        "times_per_iter_dl_tensor = numpy.array(())\n",
        "\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    input_message_xx = input_message [i:i+1]\n",
        "    input_message_xx_float = input_message_xx.astype(\"float32\")\n",
        "    encoded_message = train_sess.run ([dl_encoder_output], feed_dict={input_message_x:input_message_xx_float})\n",
        "    #print (encoded_message[0][0])\n",
        "    awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message[0][0]})\n",
        "    #print (awgn_channel_output_message)\n",
        "    decoded_message = train_sess.run ([dl_decoder_only_output], feed_dict={input_channel_x:awgn_channel_output_message})\n",
        "    #print (\"input\", input_message[i])\n",
        "    decoded_message = numpy.around(decoded_message[0][0]).astype(int)\n",
        "    #print (\"output\", decoded_message)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_dl_tensor=numpy.append(ber_per_iter_dl_tensor ,ber)\n",
        "  times_per_iter_dl_tensor=numpy.append(times_per_iter_dl_tensor, total_time)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 0.34s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 0.58s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 0.83s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 1.06s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.72\n",
            " -> Total Time: 2.81s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.68\n",
            " -> Total Time: 2.46s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 0.78s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 1.03s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.59\n",
            " -> Total Time: 2.60s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.53\n",
            " -> Total Time: 2.43s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 0.76s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 1.02s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.48\n",
            " -> Total Time: 2.53s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.42\n",
            " -> Total Time: 2.43s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.33\n",
            " -> Total Time: 2.48s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 0.77s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 1.02s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.29\n",
            " -> Total Time: 2.54s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.23\n",
            " -> Total Time: 2.44s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.19\n",
            " -> Total Time: 2.51s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.14\n",
            " -> Total Time: 2.43s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.09\n",
            " -> Total Time: 2.50s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.08\n",
            " -> Total Time: 2.46s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 0.96s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.05\n",
            " -> Total Time: 2.39s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 2.46s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 2.48s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.47s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.76s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.52s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.43s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.72s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.41s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syUQij3fuxRm",
        "outputId": "687719f3-831b-4748-bcff-b226c3a218e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1) = plt.subplots(1,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"ldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor,'', label=\"ai-dl\") # plot BER vs SNR\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "#ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "#ax2.set_xlabel('$E_b/$N_0$')\n",
        "#ax2.set_ylabel('Decoding Time [s]')\n",
        "#ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "#            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "#            xy=(1, 0.35), xycoords='axes fraction',\n",
        "#            xytext=(-20, 20), textcoords='offset pixels',\n",
        "#            horizontalalignment='right',\n",
        "#            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSMcQPWrMKoR",
        "outputId": "70aef3f9-ea27-4a3e-c67e-99b6bf0265d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "training_input_message = numpy.random.randint(2, size=(NUM_OF_INPUT_MESSAGE,input_message_length))\n",
        "print (training_input_message)\n",
        "print (len(training_input_message))\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 1 ... 0 1 0]\n",
            " [1 0 0 ... 0 1 1]\n",
            " [1 1 0 ... 0 0 1]\n",
            " ...\n",
            " [0 0 0 ... 1 1 1]\n",
            " [1 0 1 ... 1 0 1]\n",
            " [1 0 0 ... 1 1 1]]\n",
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umlgDvtmL_b9",
        "outputId": "844b69e0-aa30-45ab-a908-c9504781e4ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here I am using trained model\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_dl_tensor  = numpy.array(())\n",
        "times_per_iter_dl_tensor = numpy.array(())\n",
        "\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    input_message_xx = input_message [i:i+1]\n",
        "    input_message_xx_float = input_message_xx.astype(\"float32\")\n",
        "    encoded_message = train_sess.run ([dl_encoder_output], feed_dict={input_message_x:input_message_xx_float})\n",
        "    #print (encoded_message[0][0])\n",
        "    awgn_channel_output_message = sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message[0][0]})\n",
        "    #print (awgn_channel_output_message)\n",
        "    decoded_message = train_sess.run ([dl_decoder_only_output], feed_dict={input_channel_x:awgn_channel_output_message})\n",
        "    #print (\"input\", input_message[i])\n",
        "    decoded_message = numpy.around(decoded_message[0][0]).astype(int)\n",
        "    #print (\"output\", decoded_message)\n",
        "    if abs(decoded_message-input_message[i]).sum() != 0 :\n",
        "      total_msg_error = total_msg_error + 1\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_dl_tensor=numpy.append(ber_per_iter_dl_tensor ,ber)\n",
        "  times_per_iter_dl_tensor=numpy.append(times_per_iter_dl_tensor, total_time)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 0.76s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.70\n",
            " -> Total Time: 2.52s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.67\n",
            " -> Total Time: 2.46s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 0.77s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 1.02s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.61\n",
            " -> Total Time: 2.57s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.52\n",
            " -> Total Time: 2.50s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 0.53s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 0.79s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 1.03s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.49\n",
            " -> Total Time: 2.61s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.38\n",
            " -> Total Time: 2.46s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.29s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 0.55s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 0.80s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 1.07s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.34\n",
            " -> Total Time: 2.72s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.32\n",
            " -> Total Time: 2.50s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 0.50s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.24\n",
            " -> Total Time: 2.49s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 0.76s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.20\n",
            " -> Total Time: 2.55s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 0.76s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 1.01s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.14\n",
            " -> Total Time: 2.56s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 0.98s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.10\n",
            " -> Total Time: 2.45s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 0.74s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.08\n",
            " -> Total Time: 2.48s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.27s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 0.77s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 1.01s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.06\n",
            " -> Total Time: 2.57s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 0.75s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.03\n",
            " -> Total Time: 2.48s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 0.97s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 2.44s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 0.52s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 0.78s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 1.02s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.58s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.24s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 0.48s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.45s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.25s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 0.49s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 0.73s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 0.99s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 2.47s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.26s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 0.51s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 0.76s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 1.00s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 2.54s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewmNzOIGMBdJ",
        "outputId": "9e633c83-23b8-4030-d446-7a45ce5bb451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1) = plt.subplots(1,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,ber_per_iter_tensor,'', label=\"ldpc\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor,'', label=\"ai-dl\") # plot BER vs SNR\n",
        "ax1.set_ylabel('BER')\n",
        "ax1.set_title('Regular LDPC ({},{},{})'.format(CHANEL_SIZE,input_message_length,CHANEL_SIZE-input_message_length))\n",
        "#ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "#ax2.set_xlabel('$E_b/$N_0$')\n",
        "#ax2.set_ylabel('Decoding Time [s]')\n",
        "#ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "#            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "#            xy=(1, 0.35), xycoords='axes fraction',\n",
        "#            xytext=(-20, 20), textcoords='offset pixels',\n",
        "#            horizontalalignment='right',\n",
        "#            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(CHANEL_SIZE,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}